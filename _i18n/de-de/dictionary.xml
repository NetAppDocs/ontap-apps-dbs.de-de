<?xml version="1.0" encoding="UTF-8"?>
<blocks>
  <block id="d41d8cd98f00b204e9800998ecf8427e" category="summary"></block>
  <block id="30d965eef5ba25c6b9998ae38270b43e" category="doc">Rechtliche Hinweise</block>
  <block id="e9c44bbfd795a5d63d74c6a77afee70d" category="paragraph">Rechtliche Hinweise ermöglichen den Zugriff auf Copyright-Erklärungen, Marken, Patente und mehr.</block>
  <block id="6016a2b341113bf496b719905398ecd2" category="section-title">Urheberrecht</block>
  <block id="52009bb7ee17227f566cd26a02caee56" category="inline-link-macro"><block ref="52009bb7ee17227f566cd26a02caee56" category="inline-link-rx"></block></block>
  <block id="a1a9afcf552a769c282769271829889a" category="paragraph"><block ref="a1a9afcf552a769c282769271829889a" category="inline-link-macro-rx"></block></block>
  <block id="126a02652da6de02962cf1b654fd6376" category="section-title">Marken</block>
  <block id="c4ce4761e466527d26b3e3d5ed1006fd" category="paragraph">NetApp, das NETAPP Logo und die auf der NetApp Markenseite aufgeführten Marken sind Marken von NetApp Inc. Andere Firmen- und Produktnamen können Marken der jeweiligen Eigentümer sein.</block>
  <block id="f99aa604031e5049799e73b5c3748a98" category="inline-link-macro"><block ref="f99aa604031e5049799e73b5c3748a98" category="inline-link-rx"></block></block>
  <block id="5d545fe5152641e2ebe654e336e520e5" category="paragraph"><block ref="5d545fe5152641e2ebe654e336e520e5" category="inline-link-macro-rx"></block></block>
  <block id="be89498d2f8a22ce47c02ba9795fe2af" category="section-title">Patente</block>
  <block id="d0b19d36be2c5f16e9aef46c8a452d3d" category="paragraph">Eine aktuelle Liste der NetApp Patente finden Sie unter:</block>
  <block id="88e5eabd3917048b6927c42496b98f86" category="inline-link-macro"><block ref="88e5eabd3917048b6927c42496b98f86" category="inline-link-rx"></block></block>
  <block id="dd38f906b37d412de7d1c1dcf4cbf31c" category="paragraph"><block ref="dd38f906b37d412de7d1c1dcf4cbf31c" category="inline-link-macro-rx"></block></block>
  <block id="56c34c6410dd45c5cec44149ad0ce037" category="section-title">Datenschutzrichtlinie</block>
  <block id="8acb58cbd50ef1b468a020ee0bd351d3" category="inline-link-macro"><block ref="8acb58cbd50ef1b468a020ee0bd351d3" category="inline-link-rx"></block></block>
  <block id="2352c4e1f4d0024ade0869e00e6243f4" category="paragraph"><block ref="2352c4e1f4d0024ade0869e00e6243f4" category="inline-link-macro-rx"></block></block>
  <block id="c0227cef6f07a8cd2ac72f2945b031aa" category="section-title">Open Source</block>
  <block id="9b73989307c1975dfa4d5e1581e4afe8" category="paragraph">In den Benachrichtigungsdateien finden Sie Informationen zu Urheberrechten und Lizenzen von Drittanbietern, die in der NetApp Software verwendet werden.</block>
  <block id="253b40ae359ba25b56231803430c4873" category="section-title">ONTAP</block>
  <block id="856c8958afd787f748a4a025f649154a" category="inline-link-macro">Hinweis für ONTAP 9.13.1</block>
  <block id="b22b3b2970d843f99e7e6904bea05207" category="inline-link-macro">Hinweis zu ONTAP 9.12.1</block>
  <block id="0358f41254df2d512849db5a04b7e3d4" category="inline-link-macro">Hinweis zu ONTAP 9.12.0</block>
  <block id="28f1290102a277ef2fd452d558c74219" category="inline-link-macro">Hinweis zu ONTAP 9.11.1</block>
  <block id="57b07f14c8eb4660f94a86d29d53362d" category="inline-link-macro">Hinweis zu ONTAP 9.10.1</block>
  <block id="396712eb9c1644241047ac30619b2b3e" category="inline-link-macro">Hinweis für ONTAP 9.10.0</block>
  <block id="1e6000d4849ef9f3e08d1d9908e9f163" category="inline-link-macro">Hinweis zu ONTAP 9.9.1</block>
  <block id="6f408b9c999245c7aa32bc9cb1a020f6" category="inline-link-macro">Hinweis zu ONTAP 9.8</block>
  <block id="04b0bd5b1b4d414fdde93f809162d36d" category="inline-link-macro">Hinweis für ONTAP 9.7</block>
  <block id="0c1d7dffcba488555d5eb39b8991e822" category="inline-link-macro">Hinweis für ONTAP 9.6</block>
  <block id="a11ac6880e72412c0f54ae19bea3d191" category="inline-link-macro">Hinweis für ONTAP 9.5</block>
  <block id="e49c3c01b15761b5f011d0bd2a0661a3" category="inline-link-macro">Hinweis für ONTAP 9.4</block>
  <block id="bf7dcf6467a8a03b05b2d37d31d6eccd" category="inline-link-macro">Hinweis für ONTAP 9.3</block>
  <block id="0328352fdfda84af6968462d1a67d3a6" category="inline-link-macro">Hinweis für ONTAP 9.2</block>
  <block id="83904100c7d6fe0102e8b2b5bd8ec4bb" category="inline-link-macro">Hinweis für ONTAP 9.1</block>
  <block id="5672f5979be77bb31dd559817c9e1e76" category="paragraph"><block ref="f7a2b1db149ed9886af0ce846dbc3e14" category="inline-link-macro-rx"></block>
<block ref="b349c509ad858348b30db7e73307e3aa" category="inline-link-macro-rx"></block>
<block ref="0dce234dfded58bd461541473b86e42a" category="inline-link-macro-rx"></block>
<block ref="9760ec4df13f0afbdfec88ae16c871ae" category="inline-link-macro-rx"></block>
<block ref="b6dab2933dfb92cf0c1355707a4aec7c" category="inline-link-macro-rx"></block>
<block ref="1b7c80b26162b03bd0addd587f70df55" category="inline-link-macro-rx"></block>
<block ref="14faf912aa42102a6628af4551e02583" category="inline-link-macro-rx"></block>
<block ref="a90f36d649d3be5d386cc3563ab044fb" category="inline-link-macro-rx"></block>
<block ref="2cd65b4a044075cfd9ad4a91f42fdff4" category="inline-link-macro-rx"></block>
<block ref="5e764cc3fdcc19ff667796e414159bdc" category="inline-link-macro-rx"></block>
<block ref="561451f09c7bca700ea0d7833cd9e92e" category="inline-link-macro-rx"></block>
<block ref="d23d341a344216fbf99a91152429d49c" category="inline-link-macro-rx"></block>
<block ref="b35e8fbd7b0369b23734510994911ec4" category="inline-link-macro-rx"></block>
<block ref="41d4305cf7a6acb595086f123121a781" category="inline-link-macro-rx"></block>
<block ref="ea685649fabe77da53e843fec56f72b8" category="inline-link-macro-rx"></block></block>
  <block id="07ee2fe6f236deffba69a7cf80a680fd" category="section-title">ONTAP Mediator für MCC IP</block>
  <block id="f089ab2b9f25f609795bdd46ae636f18" category="inline-link-macro">9.9.1 Hinweis für ONTAP Mediator für MCC IP</block>
  <block id="239794a299abe62705440f2dab3114cb" category="inline-link-macro">9.8 Hinweis für ONTAP Mediator für MCC IP</block>
  <block id="51fcca20d278d2d03192c01120f17442" category="inline-link-macro">9.7 Hinweis für ONTAP Mediator für MCC IP</block>
  <block id="3ba2aeb22339424d5f5b15dafd2c3eca" category="paragraph"><block ref="c8696a7854fcd089ea112145e4968b10" category="inline-link-macro-rx"></block>
<block ref="4d19e06382f392d0f1a50df789d77c13" category="inline-link-macro-rx"></block>
<block ref="fe4ac88b6b3f1cb9f1dc6d5d4b5086ee" category="inline-link-macro-rx"></block></block>
  <block id="9e476387322a5c250893cf9c5c4ce78c" category="doc">Richtlinien</block>
  <block id="b9e18a3460bc17eb9ea65b03f0167453" category="paragraph">Dies ist bei Datenbanken üblich. Datenbanken, für die bekanntermaßen inaktive Blöcke enthalten sind, eignen sich auch für das FabricPool Tiering. Beispielsweise kann eine Supply-Chain-Management-Datenbank historische Informationen enthalten, die bei Bedarf verfügbar sein müssen, aber während des normalen Betriebs nicht aufgerufen werden. Mit FabricPool können die inaktiven Blöcke selektiv verschoben werden.</block>
  <block id="95e955bbbe5dd9520a6bf45b0d8a9d4c" category="paragraph">Beispielsweise Datendateien, die auf einem FabricPool Volume mit einem ausgeführt werden<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Im Zeitraum von 90 Tagen werden sämtliche Blöcke aufbewahrt, auf die in den vorangegangenen 90 Tagen auf der Performance Tier zugegriffen wurde. Alle Daten, auf die 90 Tage lang nicht zugegriffen wird, werden jedoch auf die Kapazitäts-Tier verlagert. In anderen Fällen bleiben bei normalen Applikationsaktivitäten die richtigen Blöcke auf der richtigen Tier erhalten. Wenn beispielsweise eine Datenbank normalerweise dazu verwendet wird, die Daten der letzten 60 Tage regelmäßig zu verarbeiten, ist dies wesentlich geringer<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Zeitraum kann festgelegt werden, da die natürliche Aktivität der Anwendung dafür sorgt, dass Blöcke nicht vorzeitig verschoben werden.</block>
  <block id="c33af7c93d461803ceaf8531e861017d" category="paragraph">Der<block ref="9df22f196a33acd0b372fe502de51211" prefix=" " category="inline-code"></block> Richtlinien sollten mit Vorsicht bei Datenbanken verwendet werden. Viele Datenbanken verfügen über periodische Aktivitäten wie etwa Vorgänge zum Quartalsende oder die Neuindizierung. Wenn der Zeitraum dieser Vorgänge größer ist als der<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Es können Performance-Probleme auftreten. Wenn zum Quartalsende beispielsweise 1 TB an Daten verarbeitet werden müssen, die ansonsten nicht verarbeitet wurden, befinden sich diese Daten möglicherweise nun auf der Kapazitäts-Tier. Lesezugriffe von der Kapazitäts-Tier sind oft extrem schnell und verursachen möglicherweise keine Performance-Probleme. Die genauen Ergebnisse hängen jedoch von der Objektspeicher-Konfiguration ab.</block>
  <block id="03431a55183cb73a12c1bbc3e1927678" category="paragraph">Der<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Die Richtlinie sollte so hoch eingestellt werden, dass Dateien, die auf der Performance-Tier erforderlich sind, aufbewahrt werden. Beispielsweise müsste eine Datenbank, in der die letzten 60 Tage Daten bei einer optimalen Performance benötigt werden, die festlegen<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Zeitraum bis 60 Tage. Ähnliche Ergebnisse lassen sich auch anhand der Zugriffsmuster von Dateien erzielen. Wenn beispielsweise die Daten der letzten 90 Tage benötigt werden und die Applikation auf diese 90-Tage-Datenspanne zugreift, verbleiben die Daten in der Performance-Tier. Einstellen des<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Zeitraum bis 2 Tage würde die Daten sofort nach dem Zeitpunkt verschieben, an dem die Daten weniger aktiv sind.</block>
  <block id="25d9496e96e698b08c4fc713bfc9a5ca" category="paragraph">Der<block ref="9df22f196a33acd0b372fe502de51211" prefix=" " category="inline-code"></block> Eine Richtlinie ist für das Tiering dieser Blöcke erforderlich, da nur die<block ref="9df22f196a33acd0b372fe502de51211" prefix=" " category="inline-code"></block> Die Richtlinie wirkt sich auf Blöcke aus, die sich im aktiven Filesystem befinden.</block>
  <block id="55fad400d892fb6062e67904fa9be485" category="admonition">Jeder Zugriff auf Daten setzt die Heatmap-Daten zurück. Daher verhindert die Überprüfung der vollständigen Tabelle der Datenbank und sogar die Backup-Aktivitäten, die die Quelldateien lesen, Tiering, da die erforderlichen<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Schwellenwert wird nie erreicht.</block>
  <block id="117c4b6bad0236ab23e0fe29aead8aef" category="paragraph">Obwohl die LUN-Größenänderung eine Option ist, um die Kapazität zu erhöhen, ist es im Allgemeinen besser, eine LVM zu verwenden, einschließlich Oracle ASM. Einer der Hauptgründe für die Existenz von LVMs ist, dass keine LUN-Größe benötigt wird. Mit einer LVM werden mehrere LUNs zu einem virtuellen Speicherpool verknüpft. Die aus diesem Pool ausgearbeiteten logischen Volumes werden von der LVM gemanagt und können problemlos in der Größe geändert werden. Ein weiterer Vorteil besteht darin, dass Hotspots auf einem bestimmten Laufwerk vermieden werden, indem ein bestimmtes logisches Volume auf alle verfügbaren LUNs verteilt wird. Transparente Migration kann in der Regel mithilfe des Volume-Managers durchgeführt werden, um die zugrunde liegenden Extents eines logischen Volumes auf neue LUNs zu verschieben.</block>
  <block id="4d4855a15e1f9d6fd8f8bdf1668c537e" category="doc">NVFAIL manuell erzwungen</block>
  <block id="8772c1c8bed97f5c942657feeb8bfb6f" category="admonition">Dieser Abschnitt erweitert die Erläuterung der grundlegenden ONTAP NVFAIL, um MetroCluster-spezifische Themen zu behandeln.</block>
  <block id="14d74316044293d1ebd8c5a01ff90055" category="paragraph">Bei MetroCluster wird ein Schreibvorgang erst bestätigt, wenn er in lokalem NVRAM und NVRAM auf mindestens einem anderen Controller angemeldet wurde. Dieser Ansatz stellt sicher, dass ein Hardware-Ausfall oder ein Stromausfall nicht zum Verlust der aktiven I/O führen Wenn der lokale NVRAM ausfällt oder die Verbindung zu anderen Nodes ausfällt, werden die Daten nicht mehr gespiegelt.</block>
  <block id="d820d86d09ea05a0de19c3aacbfb72b2" category="paragraph">Wenn der lokale NVRAM einen Fehler meldet, wird der Node heruntergefahren. Dieses Herunterfahren führt zu einem Failover auf einen Partner-Controller, wenn HA-Paare verwendet werden. Bei MetroCluster hängt das Verhalten von der gewählten Gesamtkonfiguration ab, kann jedoch zu einem automatischen Failover auf die entfernte Notiz führen. In jedem Fall gehen keine Daten verloren, da der Controller den Schreibvorgang nicht bestätigt hat.</block>
  <block id="886174de078cee4595eb8a8d07f81703" category="paragraph">Komplizierter wird dies, wenn die Verbindung zwischen Standorten ausfällt, die die NVRAM-Replizierung auf Remote-Nodes blockiert. Schreibvorgänge werden nicht mehr auf die Remote-Nodes repliziert. Dadurch besteht die Möglichkeit eines Datenverlusts, falls ein schwerwiegender Fehler auf einem Controller auftritt. Noch wichtiger ist, dass der Versuch, während dieser Bedingungen ein Failover auf einen anderen Node durchzuführen, zu Datenverlust führt.</block>
  <block id="790ec042c9b89dac2390ea81b9c29a51" category="paragraph">Der Steuerungsfaktor ist, ob NVRAM synchronisiert wird. Bei NVRAM-Synchronisierung kann ein Node-to-Node Failover ohne das Risiko eines Datenverlusts fortgesetzt werden. Wenn in einer MetroCluster Konfiguration NVRAM und die zugrunde liegenden Aggregat-Plexe synchron sind, ist es sicher, mit der Umschaltung fortzufahren, ohne das Risiko eines Datenverlusts zu verursachen.</block>
  <block id="3d936bb059b98a2fc7c177d20eb755c9" category="paragraph">ONTAP lässt kein Failover oder Switchover zu, wenn die Daten nicht synchron sind, es sei denn, das Failover oder die Umschaltung ist erzwungen. Durch das Erzwingen einer solchen Änderung der Bedingungen wird bestätigt, dass Daten im ursprünglichen Controller zurückgelassen werden können und dass ein Datenverlust akzeptabel ist.</block>
  <block id="3d37a3a8ed77bebc986238aedccecbc9" category="paragraph">Datenbanken sind besonders anfällig für Beschädigungen, wenn ein Failover oder Switchover erzwungen wird, da Datenbanken größere interne Daten-Caches auf der Festplatte beibehalten. Wenn ein erzwungenes Failover oder eine Umschaltung auftritt, werden zuvor bestätigte Änderungen effektiv verworfen. Der Inhalt des Storage Arrays springt effektiv zurück in die Zeit, und der Zustand des Datenbank-Cache entspricht nicht mehr dem Status der Daten auf der Festplatte.</block>
  <block id="7ad23b7fa394c8666b132d78a58b1987" category="paragraph">Um Applikationen vor dieser Situation zu schützen, können mit ONTAP Volumes für speziellen Schutz vor NVRAM-Ausfällen konfiguriert werden. Wenn dieser Schutzmechanismus ausgelöst wird, gelangt ein Volume in den Status „NVFAIL“. Dieser Status führt zu I/O-Fehlern, die dazu führen, dass Applikationen heruntergefahren werden, sodass keine veralteten Daten verwendet werden. Daten sollten nicht verloren gehen, da alle bestätigten Schreibvorgänge noch auf dem Speichersystem vorhanden sind, und bei Datenbanken sollten alle festgeschriebenen Transaktionsdaten in den Protokollen vorhanden sein.</block>
  <block id="99259586124503e22927506a7ed3738d" category="paragraph">Als Nächstes muss ein Administrator die Hosts vollständig herunterfahren, bevor die LUNs und Volumes manuell wieder online geschaltet werden. Obwohl diese Schritte etwas Arbeit erfordern können, ist dieser Ansatz der sicherste Weg, um die Datenintegrität zu gewährleisten. Nicht alle Daten erfordern diesen Schutz. Daher kann ein NVFAIL-Verhalten auf Volume-Basis konfiguriert werden.</block>
  <block id="13fbc5a220fe8007dcaea69217bba134" category="paragraph">Die sicherste Option, um ein Switchover mit einem Anwendungs-Cluster (einschließlich VMware, Oracle RAC und anderen) zu erzwingen, das über Standorte verteilt ist, ist durch Angabe<block ref="71e644310ff0f1164d905d6e80174023" prefix=" " category="inline-code"></block> An der Kommandozeile. Diese Option ist als Notfallmaßnahme verfügbar, um sicherzustellen, dass alle zwischengespeicherten Daten gelöscht werden. Wenn ein Host Speicherressourcen verwendet, die sich ursprünglich am Standort mit Notfällen befinden, erhält er entweder I/O-Fehler oder eine veraltete Dateihandle <block ref="a3c6541ac12f1f06a81cc9b03e6bd094" prefix="(" category="inline-code"></block>) Fehler. Oracle Datenbanken stürzen ab und Dateisysteme gehen entweder vollständig offline oder wechseln in den schreibgeschützten Modus.</block>
  <block id="dc6683d37e50abd82911fa91c036fbb2" category="paragraph">Nachdem die Umschaltung abgeschlossen ist, wird der angezeigt<block ref="6b0a5ec5ecf08db5d4b32d860214d098" prefix=" " category="inline-code"></block> Flag muss gelöscht werden und die LUNs müssen in den Online-Modus versetzt werden. Nach Abschluss dieser Aktivität kann die Datenbank neu gestartet werden. Diese Aufgaben können automatisiert werden, um die RTO zu reduzieren.</block>
  <block id="a2626552f293b2377efe829e69d14daa" category="section-title">dr-Force-NV-Fehler</block>
  <block id="f0cc4bbe3d196251009dbe9f0ac42ffc" category="paragraph">Stellen Sie als allgemeine Sicherheitsmaßnahme die ein<block ref="a2626552f293b2377efe829e69d14daa" prefix=" " category="inline-code"></block> Markieren Sie alle Volumes, auf die während des normalen Betriebs von einem Remote-Standort aus zugegriffen werden kann, d. h. sie sind Aktivitäten, die vor dem Failover verwendet werden. Das Ergebnis dieser Einstellung ist, dass ausgewählte Remote-Volumes beim Aufrufen nicht mehr verfügbar sind<block ref="6b0a5ec5ecf08db5d4b32d860214d098" prefix=" " category="inline-code"></block> Während einer Umschaltung. Nachdem die Umschaltung abgeschlossen ist, wird der angezeigt<block ref="6b0a5ec5ecf08db5d4b32d860214d098" prefix=" " category="inline-code"></block> Flag muss gelöscht und die LUNs müssen in den Online-Modus versetzt werden. Nach Abschluss dieser Aktivitäten können die Anwendungen neu gestartet werden. Diese Aufgaben können automatisiert werden, um die RTO zu reduzieren.</block>
  <block id="5f6aa76cc0fe886c07f242f921486ef7" category="paragraph">Das Ergebnis ist wie bei der Verwendung von<block ref="71e644310ff0f1164d905d6e80174023" prefix=" " category="inline-code"></block> Markierung für manuelle Umschaltung. Die Anzahl der betroffenen Volumes kann jedoch auf die Volumes beschränkt werden, die vor Anwendungen oder Betriebssystemen mit veralteten Caches geschützt werden müssen.</block>
  <block id="4df566a29b44806612369c0c1f67521b" category="paragraph">Es gibt zwei entscheidende Anforderungen an eine Umgebung, die nicht verwendet wird<block ref="a2626552f293b2377efe829e69d14daa" prefix=" " category="inline-code"></block> Auf Anwendungsvolumes:</block>
  <block id="aaf1cc67bd88c39a71e691d413ded49f" category="list-text">Ein erzwungenes Switchover darf nicht mehr als 30 Sekunden nach dem Ausfall des primären Standorts erfolgen.</block>
  <block id="7f9cc623e2a11875714cf4bc64f148fd" category="list-text">Eine Umschaltung darf nicht während Wartungsaufgaben oder unter anderen Bedingungen erfolgen, unter denen SyncMirror Plexe oder NVRAM-Replikation nicht synchron sind. Die erste Anforderung ist über eine Tiebreaker Software möglich, die im Fall eines Standortausfalls innerhalb von 30 Sekunden umgeschaltet wird. Dies bedeutet jedoch nicht, dass die Umschaltung innerhalb von 30 Sekunden nach Erkennung eines Standortausfalls durchgeführt werden muss. Das bedeutet, dass es nicht mehr sicher ist, eine Umschaltung zu erzwingen, wenn 30 Sekunden vergangen sind, seit die Betriebsbereitschaft eines Standorts bestätigt wurde.</block>
  <block id="0929cc8043c21dcf0163f06424677ede" category="paragraph">Die zweite Anforderung wird teilweise erfüllt, indem alle Funktionen zum automatisierten Switchover deaktiviert werden, wenn bekannt ist, dass die MetroCluster-Konfiguration nicht synchron ist. Eine bessere Option ist die Nutzung einer Tiebreaker Lösung, mit der der Systemzustand der NVRAM-Replizierung und der SyncMirror Plexe überwacht werden kann. Wenn das Cluster nicht vollständig synchronisiert ist, sollte Tiebreaker keine Umschaltung auslösen.</block>
  <block id="a168430c68d0310385371eabe149a118" category="paragraph">Die NetApp-MCTB-Software kann den Synchronisierungsstatus nicht überwachen, daher sollte sie deaktiviert werden, wenn MetroCluster aus irgendeinem Grund nicht synchron ist. ClusterLion verfügt über Funktionen zur NVRAM-Überwachung und Plex-Überwachung und kann so konfiguriert werden, dass das Switchover nur ausgelöst wird, wenn für das MetroCluster-System eine vollständige Synchronisierung bestätigt wurde.</block>
  <block id="baba038803e218b2ac3b2688bc2fd5db" category="doc">LUN-Anzahl</block>
  <block id="b0e4db975ae00108abe9f47db9be5668" category="paragraph">Eine LUN ist ein virtualisiertes Objekt auf ONTAP, das über alle Laufwerke im Hosting-Aggregat hinweg existiert. Die Performance der LUN wird daher von ihrer Größe nicht beeinflusst, da die LUN unabhängig von der gewählten Größe das volle Performance-Potenzial des Aggregats schöpft.</block>
  <block id="7235d4b71fadc2fe03d09d863c7cdd66" category="paragraph">Aus praktischen Gründen möchten Kunden möglicherweise eine LUN einer bestimmten Größe verwenden. Wenn beispielsweise eine Datenbank auf einer LVM oder einer Oracle ASM-Datenträgergruppe erstellt wird, die aus zwei LUNs mit jeweils 1 TB besteht, muss diese Datenträgergruppe in Schritten von 1 TB erweitert werden. Es könnte besser sein, die Datenträgergruppe aus acht LUNs mit jeweils 500 GB zu erstellen, damit die Datenträgergruppe in kleineren Schritten erhöht werden kann.</block>
  <block id="4b1ffe65479a23391a2c7f9caf467a35" category="paragraph">Die Praxis, eine universelle Standard-LUN-Größe zu etablieren, wird davon abgeraten, da dies die Managebarkeit erschweren kann. Beispielsweise funktioniert eine standardmäßige LUN-Größe von 100 GB gut, wenn eine Datenbank oder ein Datastore im Bereich von 1 TB bis 2 TB liegt, jedoch erfordert eine Datenbank oder ein Datenspeicher mit einer Größe von 20 TB 200 LUNs. Das bedeutet, dass der Server-Neustart länger dauert, mehr Objekte in den verschiedenen Benutzeroberflächen zu verwalten sind und Produkte wie SnapCenter eine Erkennung für viele Objekte durchführen müssen. Derartige Probleme werden durch die Verwendung von weniger und größeren LUNs vermieden.</block>
  <block id="4afa3de974c36a09c2055d30989bb405" category="list-text">Die Anzahl der LUNs ist wichtiger als die LUN-Größe.</block>
  <block id="691c70133d96d57796a9299359666f80" category="list-text">Die LUN-Größe wird überwiegend durch die Anforderungen der LUN-Anzahl gesteuert.</block>
  <block id="8af9e525c143179b948a11fc382a331e" category="list-text">Erstellen Sie nicht mehr LUNs als erforderlich.</block>
  <block id="407b2218ad77513a7e3964f169d07e66" category="paragraph">Anders als die LUN-Größe wirkt sich die Anzahl der LUNs auf die Performance aus. Die Applikations-Performance hängt häufig von der Fähigkeit ab, parallelen I/O über die SCSI-Schicht auszuführen. Dadurch bieten zwei LUNs eine bessere Performance als eine einzelne LUN. Die Verwendung einer LVM wie Veritas VxVM, Linux LVM2 oder Oracle ASM ist die einfachste Methode, um die Parallelität zu erhöhen.</block>
  <block id="39143fe6204f383e4ce3bb3b77926455" category="paragraph">NetApp Kunden konnten im Allgemeinen nur einen minimalen Nutzen aus der Erhöhung der Anzahl von LUNs über sechzehn hinaus verzeichnen, obwohl sich bei den Tests mit 100 % SSD-Umgebungen mit sehr hoher zufälliger I/O-Last weitere Verbesserungen auf bis zu 64 LUNs gezeigt haben.</block>
  <block id="9875f7503e59521e91e519abd87f5c9e" category="paragraph">*NetApp empfiehlt* Folgendes:</block>
  <block id="57153ab478a4958250d76ac6bf4e3ac0" category="paragraph">Im Allgemeinen reichen vier bis sechzehn LUNs aus, um die I/O-Anforderungen jedes gegebenen Datenbank-Workloads zu unterstützen. Aufgrund der Einschränkungen bei Host-SCSI-Implementierungen könnten weniger als vier LUNs zu Performance-Einschränkungen führen.</block>
  <block id="de932976b195d4755fcbea64295fc7cb" category="doc">Einstellungen für das Host-Betriebssystem</block>
  <block id="f62cb371f65c322cd9e17a5c2cecd1c8" category="paragraph">Die Dokumentation der meisten Anwendungsanbieter enthält bestimmte TCP- und ethernet-Einstellungen, die sicherstellen sollen, dass die Anwendung optimal funktioniert. Diese Einstellungen reichen in der Regel aus, um auch eine optimale IP-basierte Speicherleistung zu erzielen.</block>
  <block id="ae7a0bb210cbd1edb935128116a21c55" category="section-title">Ethernet-Flusskontrolle</block>
  <block id="a3c15fb208efc85bdfe72c57d56c92b8" category="paragraph">Mit dieser Technologie kann ein Client verlangen, dass ein Sender die Datenübertragung vorübergehend stoppt. Dies geschieht normalerweise, weil der Empfänger eingehende Daten nicht schnell genug verarbeiten kann. Die Anforderung, dass ein Sender die Übertragung abbricht, war zu einem Zeitpunkt weniger störend, als dass ein Empfänger Pakete verwirft, weil die Puffer voll waren. Dies ist bei den heute in Betriebssystemen verwendeten TCP-Stacks nicht mehr der Fall. Tatsächlich verursacht die Flusskontrolle mehr Probleme als sie löst.</block>
  <block id="950d31923e253d170d6c994801ce7cec" category="paragraph">Leistungsprobleme, die durch die Ethernet-Flusssteuerung verursacht werden, haben in den letzten Jahren zugenommen. Der Grund dafür ist, dass die Ethernet-Flusssteuerung auf der physischen Ebene ausgeführt wird. Wenn eine Netzwerkkonfiguration es einem Host-Betriebssystem ermöglicht, eine Ethernet-Datenflusssteuerungsanforderung an ein Storage-System zu senden, führt dies zu einer I/O-Pause für alle verbundenen Clients. Da immer mehr Clients von einem einzelnen Storage Controller bedient werden, steigt die Wahrscheinlichkeit, dass ein oder mehrere dieser Clients Flow Control-Anfragen senden. Das Problem ist bei Kundenstandorten mit umfassender Betriebssystemvirtualisierung häufig aufgetreten.</block>
  <block id="6bfd8563627c68f73c30581843441a74" category="paragraph">Eine NIC auf einem NetApp-System sollte keine Anfragen zur Flusskontrolle empfangen. Die Methode, mit der dieses Ergebnis erzielt wird, hängt vom Hersteller des Netzwerk-Switches ab. In den meisten Fällen kann die Flusssteuerung auf einem Ethernet-Switch eingestellt werden<block ref="a67b5ddb674c9ca32d9c9e1ca81578ee" prefix=" " category="inline-code"></block> Oder<block ref="f4eb0bfa09695eb47ff4f3bd3ca4449d" prefix=" " category="inline-code"></block>, Das bedeutet, dass eine Durchflussregelanforderung nicht an den Speichercontroller weitergeleitet wird. In anderen Fällen lässt die Netzwerkverbindung auf dem Storage Controller möglicherweise die Deaktivierung der Flusssteuerung nicht zu. In diesen Fällen müssen die Clients so konfiguriert werden, dass sie keine Flow-Control-Anforderungen senden, entweder indem sie auf die NIC-Konfiguration auf dem Host-Server selbst oder auf die Switch-Ports wechseln, mit denen der Host-Server verbunden ist.</block>
  <block id="7496f3ff64e30d4732c2c64880faf186" category="admonition">*NetApp empfiehlt* sicherzustellen, dass NetApp-Speicher-Controller keine Ethernet-Flow-Control-Pakete empfangen. Dies kann im Allgemeinen durch Einstellen der Switch Ports geschehen, an die der Controller angeschlossen ist. Bei einigen Switch-Hardware bestehen jedoch Einschränkungen, die stattdessen clientseitige Änderungen erfordern können.</block>
  <block id="ae6d0fc57efaf02ad7dfdacd3575e315" category="section-title">MTU-Größen</block>
  <block id="ff13ea04765cd9c9aed7b839f15eb2b4" category="paragraph">Der Einsatz von Jumbo Frames hat gezeigt, dass sich die Performance in 1-GB-Netzwerken durch Reduzierung des CPU- und Netzwerk-Overheads verbessert. Die Vorteile sind jedoch in der Regel nicht signifikant.</block>
  <block id="ee3f364287a30ef10dfdce2f37d94ed2" category="admonition">*NetApp empfiehlt*, wenn möglich Jumbo Frames zu implementieren, sowohl um potenzielle Leistungsvorteile zu realisieren als auch um die Lösung zukunftssicher zu machen.</block>
  <block id="2c6ba27b7ed602a9089da6ad9f762c9a" category="paragraph">Die Verwendung von Jumbo Frames in einem 10-Gbit-Netzwerk ist fast zwingend erforderlich. Der Grund dafür ist, dass die meisten 10-GB-Implementierungen vor Erreichen der 10-GB-Marke ohne Jumbo-Frames eine Grenze von Paketen pro Sekunde erreichen. Die Verwendung von Jumbo Frames verbessert die Effizienz bei der TCP/IP-Verarbeitung, da Betriebssystem, Server, NICs und Speichersystem weniger, aber größere Pakete verarbeiten können. Die Leistungsverbesserung variiert von NIC zu NIC, ist jedoch signifikant.</block>
  <block id="a44be1bbfde97e4a64b50282325165e7" category="paragraph">Bei Jumbo-Frame-Implementierungen besteht die allgemeine, aber falsche Annahme, dass alle verbundenen Geräte Jumbo-Frames unterstützen müssen und dass die MTU-Größe End-to-End entsprechen muss Stattdessen verhandeln die beiden Netzwerkendpunkte beim Herstellen einer Verbindung die höchste für beide Seiten akzeptable Frame-Größe. In einer typischen Umgebung ist ein Netzwerk-Switch auf eine MTU-Größe von 9216, der NetApp-Controller auf 9000 und die Clients auf 9000 und 1514 eingestellt. Clients, die eine MTU von 9000 unterstützen, können Jumbo-Frames verwenden, und Clients, die nur 1514 unterstützen, können einen niedrigeren Wert aushandeln.</block>
  <block id="a31484167f31e1c3f36f7cea821bc581" category="paragraph">Probleme mit dieser Anordnung sind in einer komplett geschalteten Umgebung selten. Achten Sie jedoch in einer gerouteten Umgebung darauf, dass kein Zwischenrouter gezwungen ist, Jumbo-Frames zu fragmentieren.</block>
  <block id="c805e9846e6fca449dbe3233cc5dbdf5" category="paragraph">*NetApp empfiehlt* die Konfiguration folgender Komponenten:</block>
  <block id="b4446b5ca60d915111af271495bd4c17" category="list-text">Jumbo Frames sind wünschenswert, jedoch nicht erforderlich mit 1Gb Ethernet (GbE).</block>
  <block id="5b1ea13ac12a9c8be0d09303f637dba4" category="list-text">Jumbo Frames sind für maximale Performance mit 10 GbE und schneller erforderlich.</block>
  <block id="08a8dce78637f5dd14ed052163feb9da" category="section-title">TCP-Parameter</block>
  <block id="1a999618685598bee637262fe589de70" category="paragraph">Drei Einstellungen sind oft falsch konfiguriert: TCP-Zeitstempel, selektive Bestätigung (SACK) und TCP-Fenster-Skalierung. Viele veraltete Dokumente im Internet empfehlen, einen oder mehrere dieser Parameter zu deaktivieren, um die Leistung zu verbessern. Vor vielen Jahren war diese Empfehlung verdienlich, als die CPU-Kapazitäten wesentlich geringer waren und der Overhead für die TCP-Verarbeitung, wenn möglich, reduziert werden konnte.</block>
  <block id="6a594ec6f0bb742c1d2b0f631cf65ba1" category="paragraph">Bei modernen Betriebssystemen führt die Deaktivierung dieser TCP-Funktionen jedoch in der Regel nicht zu nachweisbaren Vorteilen und kann gleichzeitig die Leistung beeinträchtigen. In virtualisierten Netzwerkumgebungen sind Performance-Schäden besonders wahrscheinlich, da diese Funktionen für eine effiziente Handhabung von Paketverlusten und Änderungen der Netzwerkqualität erforderlich sind.</block>
  <block id="cdc44d05b1633e8a8c7835010c423dd6" category="admonition">*NetApp empfiehlt*, TCP-Zeitstempel, SACK und TCP-Fenster-Skalierung auf dem Host zu aktivieren, und alle drei dieser Parameter sollten in jedem aktuellen Betriebssystem standardmäßig aktiviert sein.</block>
  <block id="341d84aa26cb4521474864e04ea2a63b" category="inline-image-macro">Fehler: Fehlendes Grafikbild</block>
  <block id="b56265ceb6311e42003e668438136a59" category="section-title">Synchrone Replizierung</block>
  <block id="0cf4a786acebd750bbfdfc4416c89406" category="section-title">Storage-Hardware</block>
  <block id="ea9f3de5dd21190fbc6bca180b2040f1" category="section-title">ONTAP Mediator</block>
  <block id="0f54fb931dbdeea735f18a8e73f9eab5" category="doc">Datensicherung mit SyncMirror</block>
  <block id="d7023f86951b6c3c009891a468c3effd" category="paragraph">Auf der einfachsten Ebene bedeutet synchrone Replikation, dass jede Änderung an beiden Seiten des gespiegelten Speichers vorgenommen werden muss, bevor sie bestätigt wird. Wenn beispielsweise eine Datenbank ein Protokoll schreibt oder ein VMware Gast gepatcht wird, darf ein Schreibvorgang nie verloren gehen. Als Protokollebene darf das Storage-System den Schreibvorgang erst dann bestätigen, wenn es auf nichtflüchtigen Medien an beiden Standorten gespeichert wurde. Nur dann ist es sicher, ohne das Risiko eines Datenverlusts zu gehen.</block>
  <block id="eee4f7a4e0ed3a8646a97bcf3e118fb9" category="paragraph">Die Verwendung einer Technologie zur synchronen Replizierung ist der erste Schritt beim Entwurf und Management einer Lösung zur synchronen Replizierung. Die wichtigste Überlegung ist, zu verstehen, was in verschiedenen geplanten und ungeplanten Ausfallszenarien passieren könnte. Nicht alle Lösungen zur synchronen Replizierung bieten dieselben Funktionen. Wenn Sie eine Lösung benötigen, die einen Recovery Point Objective (RPO) von null bietet, d. h. keinen Datenverlust verursacht, müssen alle Ausfallszenarien in Betracht gezogen werden. Welches ist insbesondere das erwartete Ergebnis, wenn die Replikation aufgrund des Verlusts der Verbindung zwischen Standorten nicht möglich ist?</block>
  <block id="71554672a088c43ab21c759ddd161928" category="section-title">SyncMirror Datenverfügbarkeit</block>
  <block id="8646bb558f8f18a200cce2f70602627a" category="paragraph">SyncMirror kann nicht nur nahtlos aus dem synchronen Modus wechseln, wenn der Remote-Standort nicht erreichbar ist, sondern auch bei der Wiederherstellung der Konnektivität schnell zu einem RPO = 0-Zustand neu synchronisieren. Die veraltete Kopie der Daten am Remote-Standort kann während der Resynchronisierung auch in einem nutzbaren Zustand aufbewahrt werden. Auf diese Weise ist gewährleistet, dass lokale und Remote-Kopien der Daten jederzeit vorhanden sind.</block>
  <block id="1d12d7a206d9b5337ae6fa442f23d377" category="doc">Nur Snapshot</block>
  <block id="64c29f2f2feedeb607ff588b13d9758b" category="paragraph">Der<block ref="8805d416ce540c5f2df34af5b18d742b" prefix=" " category="inline-code"></block><block ref="964193481f440d73ce3474a7a09bc3ac" prefix=" " category="inline-code"></block> Gilt nur für Blöcke, die nicht mit dem aktiven Dateisystem gemeinsam genutzt werden. Im Wesentlichen führt dies zum Tiering von Datenbank-Backups. Blöcke eignen sich als Tiering-Kandidaten, nachdem ein Snapshot erstellt wurde und der Block dann überschrieben wird. Das Ergebnis ist ein Block, der nur innerhalb des Snapshots vorhanden ist. Die Verzögerung vor einem<block ref="8805d416ce540c5f2df34af5b18d742b" prefix=" " category="inline-code"></block> Der Block wird als cool betrachtet und wird vom gesteuert<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Einstellung für die Lautstärke. Der Bereich ab ONTAP 9.8 liegt zwischen 2 und 183 Tagen.</block>
  <block id="ebd9889c847a7a3ddeccd2717ead46f2" category="paragraph">Viele Datensätze verfügen über niedrige Änderungsraten, wodurch diese Richtlinien nur minimal eingespart werden. Eine typische Datenbank mit ONTAP hat beispielsweise eine Änderungsrate von weniger als 5 % pro Woche. Protokolle für Datenbankarchive können umfangreichen Speicherplatz belegen, existieren jedoch normalerweise weiterhin im aktiven File-System und sind daher nicht für Tiering im Rahmen dieser Richtlinie geeignet.</block>
  <block id="06b9281e396db002010bde1de57262eb" category="section-title">Automatisch</block>
  <block id="ecf602bd5abbd6a605debedbb6c3d4e0" category="paragraph">Der<block ref="9df22f196a33acd0b372fe502de51211" prefix=" " category="inline-code"></block> die Tiering-Richtlinie erweitert das Tiering sowohl auf Snapshot-spezifische Blöcke als auch auf Blöcke innerhalb des aktiven File-Systems. Die Verzögerung, bevor ein Block als cool betrachtet wird, wird vom gesteuert<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Einstellung für die Lautstärke. Der Bereich ab ONTAP 9.8 liegt zwischen 2 und 183 Tagen.</block>
  <block id="6256ce48698ff0bde6f818679cdfb382" category="paragraph">Dieser Ansatz ermöglicht Tiering-Optionen, die mit dem nicht verfügbar sind<block ref="8805d416ce540c5f2df34af5b18d742b" prefix=" " category="inline-code"></block> Richtlinie: Eine Datensicherungsrichtlinie kann beispielsweise die Aufbewahrung bestimmter Protokolldateien von 90 Tagen erfordern. Wenn Sie einen Abkühlzeitraum von 3 Tagen festlegen, werden Protokolldateien, die älter als 3 Tage sind, aus der Performance-Schicht verschoben. Dadurch wird ein erheblicher Teil des Speicherplatzes auf dem Performance-Tier freigesetzt, und Sie können die Daten der gesamten 90 Tage anzeigen und managen.</block>
  <block id="6adf97f83acf6453d4a6a4b1070f3754" category="section-title">Keine</block>
  <block id="7026ff5c5d4ab3946ff2d86f0e2cca6f" category="paragraph">Der<block ref="334c4a4c42fdb79d7ebc3e73b517e6f8" prefix=" " category="inline-code"></block> die tiering-Richtlinie verhindert, dass zusätzliche Blöcke von der Storage-Ebene aus verschoben werden, doch alle Daten, die sich noch in der Kapazitäts-Tier befinden, bleiben bis sie gelesen werden. Wenn der Block dann gelesen wird, wird er zurückgezogen und auf die Performance-Tier platziert.</block>
  <block id="d152c7b0939fb6d35fdb0433de0d6369" category="paragraph">Der Hauptgrund für die Verwendung des<block ref="334c4a4c42fdb79d7ebc3e73b517e6f8" prefix=" " category="inline-code"></block> mittels tiering-Richtlinie soll verhindert werden, dass Blöcke in Tiers verschoben werden, es könnte sich jedoch nützlich sein, die Richtlinien im Laufe der Zeit zu ändern. Nehmen wir beispielsweise an, dass ein bestimmter Datensatz häufig auf die Kapazitätsebene gestaffelt ist, doch entsteht ein unerwarteter Bedarf an vollständigen Performance-Funktionen. Die Richtlinie kann geändert werden, um ein zusätzliches Tiering zu vermeiden und sicherzustellen, dass alle Blöcke, die bei einer Zunahme der I/O-Vorgänge zurückgelesen werden, weiterhin in der Performance-Tier verbleiben.</block>
  <block id="b1c94ca2fbc3e78fc30069c8d0f01680" category="section-title">Alle</block>
  <block id="84826d7e23dfac7549819d5115fdcedd" category="paragraph">Der<block ref="a181a603769c1f98ad927e7367c7aa51" prefix=" " category="inline-code"></block> Die tiering-Richtlinie ersetzt die<block ref="402051f4be0cc3aad33bcf3ac3d6532b" prefix=" " category="inline-code"></block> Richtlinie ab ONTAP 9.6. Der<block ref="402051f4be0cc3aad33bcf3ac3d6532b" prefix=" " category="inline-code"></block> Richtlinie gilt nur für Datensicherungs-Volumes, d. h. ein Ziel für SnapMirror oder NetApp SnapVault. Der<block ref="a181a603769c1f98ad927e7367c7aa51" prefix=" " category="inline-code"></block> Richtlinienfunktionen identisch, aber nicht beschränkt auf Datensicherungs-Volumes</block>
  <block id="10d8500e282e8bbacd668af6f773f412" category="paragraph">Mit dieser Richtlinie gelten Blöcke sofort als „cool“ und können sofort auf die Kapazitätsebene verschoben werden.</block>
  <block id="ec583de0216b95bc673c84e4ec9356e6" category="paragraph">Diese Richtlinie eignet sich besonders für langfristige Backups. Es kann auch als eine Form von Hierarchical Storage Management (HSM) verwendet werden. In der Vergangenheit wurde HSM häufig verwendet, um die Datenblöcke einer Datei auf Band zu verschieben, während die Datei selbst im Dateisystem sichtbar gehalten wurde. Ein FabricPool Volume mit dem<block ref="a181a603769c1f98ad927e7367c7aa51" prefix=" " category="inline-code"></block> Richtlinien ermöglichen das Speichern von Dateien in einem sichtbaren und leicht zu verwaltenden System, wobei jedoch so gut wie kein Speicherplatz auf der lokalen Storage Tier belegt wird.</block>
  <block id="79768a069a539d4f130e672a660bf45f" category="doc">Zoning</block>
  <block id="a0bdb98e8b9da585816478d24278e42f" category="paragraph">Eine FC-Zone sollte nie mehr als einen Initiator enthalten. Eine solche Anordnung mag zunächst zu funktionieren scheinen, doch Crosstalk zwischen Initiatoren beeinträchtigt letztendlich die Performance und Stabilität.</block>
  <block id="8ef958d9e25538841b306960ab117626" category="paragraph">Multitarget-Zonen werden allgemein als sicher angesehen, obwohl in seltenen Fällen das Verhalten von FC-Zielports unterschiedlicher Anbieter Probleme verursacht hat. Es ist beispielsweise zu vermeiden, die Ziel-Ports von einem NetApp und einem nicht-NetApp Storage-Array in derselben Zone zu integrieren. Darüber hinaus besteht mit noch größerer Wahrscheinlichkeit die Gefahr, dass ein NetApp Storage-System und ein Bandgerät in dieselbe Zone platziert werden.</block>
  <block id="642223473862db290604321fe2ebe763" category="doc">SVMs</block>
  <block id="47ea917cf14ad91c68c407f90a7a189d" category="paragraph">Eine SVM, in der ONTAP CLI als vServer bezeichnet, ist eine grundlegende Funktionseinheit des Storage. Es ist hilfreich, eine SVM mit einem Gast auf einem VMware ESX Server zu vergleichen.</block>
  <block id="13ce2622b2cc2118c44d3aae7df8167d" category="paragraph">Wie bei anderen Aspekten der Storage-Architektur hängen die besten Optionen für das Design von SVMs und Logical Interface (LIF) stark von den Skalierungsanforderungen und geschäftlichen Anforderungen ab.</block>
  <block id="8fb41a27984f0beba2e12fb920a486aa" category="paragraph">Es gibt keine offizielle Best Practice für die Bereitstellung von SVMs für ONTAP. Der richtige Ansatz hängt von Management- und Sicherheitsanforderungen ab.</block>
  <block id="f4d8541bb6d07f7533a6b5aa6cfb2455" category="paragraph">Die meisten Kunden betreiben für die meisten ihrer täglichen Anforderungen eine primäre SVM, erstellen jedoch für besondere Anforderungen eine geringe Anzahl an SVMs. Sie können beispielsweise Folgendes erstellen:</block>
  <block id="259908e9e431c0fbb9d425249ceb7930" category="list-text">Eine SVM für eine kritische Geschäftsdatenbank, die von einem Expertenteam gemanagt wird</block>
  <block id="0e10f2715884f3538282397d69b2cd00" category="list-text">Eine SVM für eine Entwicklungsgruppe, der eine vollständige administrative Kontrolle gegeben wurde, damit sie ihren eigenen Storage unabhängig managen können</block>
  <block id="7624156e307247e4ea1ec8bf574136a1" category="list-text">Eine SVM für sensible Geschäftsdaten wie Personaldaten oder Daten für Finanzberichte, für die das Administrationsteam begrenzt werden muss</block>
  <block id="e5e09031843479ef9ef8cbbd4c2f404c" category="inline-link-macro">NetApp Hardware Universe</block>
  <block id="fb012f1e7970d8285bd5d6c5a7f7d523" category="doc">SSD-Aggregate, einschließlich AFF Systeme</block>
  <block id="cb8e4651c457f71d5788f38eac01471f" category="paragraph">Der freie Speicherplatz wird definiert als jeder Speicherplatz, der nicht für tatsächliche Daten verwendet wird. Er umfasst nicht zugewiesenen Speicherplatz im Aggregat selbst und ungenutzten Speicherplatz innerhalb der einzelnen Volumes. Thin Provisioning ist ebenfalls zu berücksichtigen. Ein Volume kann beispielsweise eine LUN mit 1 TB enthalten, von der nur 50 % von echten Daten genutzt werden. In einer Thin Provisioning Umgebung wird hierdurch scheinbar 500 GB Speicherplatz belegt. In einer vollständig bereitgestellten Umgebung scheint jedoch die volle Kapazität von 1 TB genutzt zu sein. Der nicht zugewiesene Speicherplatz von 500 GB ist ausgeblendet. Dieser Platz wird von tatsächlichen Daten nicht genutzt und sollte daher bei der Berechnung des gesamten freien Speicherplatzes berücksichtigt werden.</block>
  <block id="d3487661d9a0702bd5915d49070f62dc" category="paragraph">NetApp Empfehlungen für Storage-Systeme, die für Enterprise-Applikationen verwendet werden, sind wie folgt:</block>
  <block id="678e476783bceae80094bcb4c1f32969" category="admonition">*NetApp empfiehlt* mindestens 10% freien Platz. Dazu gehört der gesamte ungenutzte Speicherplatz, einschließlich freiem Speicherplatz innerhalb des Aggregats oder eines Volumes und sämtlicher freier Speicherplatz, der aufgrund der vollständigen Bereitstellung zugewiesen wird, aber nicht von tatsächlichen Daten genutzt wird. Logischer Speicherplatz ist dabei unwichtig. Die Frage lautet, wie viel tatsächlich freier physischer Speicherplatz für Daten zur Verfügung steht.</block>
  <block id="2171d172d184f1202a686849c85dce91" category="paragraph">Die Empfehlung von 10 % freiem Platz ist sehr konservativ. SSD-Aggregate können Workloads mit noch höherer Auslastung ohne Auswirkungen auf die Performance unterstützen. Wenn die Auslastung des Aggregats jedoch nicht sorgfältig überwacht wird, steigt auch das Risiko, dass der Speicherplatz nicht ausgelastet wird. Darüber hinaus kann es bei der Ausführung eines Systems mit einer Kapazität von 99 % nicht zu einer Performance-Beeinträchtigung kommen, doch wäre damit wahrscheinlich ein Management-Aufwand verbunden, um zu verhindern, dass das System während der Bestellung zusätzlicher Hardware vollständig gefüllt wird. Zudem kann es einige Zeit dauern, bis zusätzliche Laufwerke beschaffen und installiert sind.</block>
  <block id="789243e17acb0b134f435d6c4c1350f2" category="section-title">HDD-Aggregate, einschließlich Flash Pool Aggregaten</block>
  <block id="0b0a68e2dedde9c0b9a3ea4276f4b85c" category="paragraph">Der<block ref="9df22f196a33acd0b372fe502de51211" prefix=" " category="inline-code"></block> Die Richtlinie ist die am besten geeignete Richtlinie für Backup-Daten. Dadurch wird ein sofortiges Tiering sichergestellt, wenn der Kühlschwellenwert erreicht wurde, unabhängig davon, ob die Dateien gelöscht wurden oder weiterhin im primären Dateisystem vorhanden sind. Das Speichern aller potenziell erforderlichen Dateien an einem zentralen Speicherort im aktiven Dateisystem vereinfacht ebenfalls das Management. Es gibt keinen Grund, Snapshots zu durchsuchen, um eine Datei zu finden, die wiederhergestellt werden muss.</block>
  <block id="11a75a1257cc68a4a46bc553dabe0c0d" category="paragraph">Der<block ref="8805d416ce540c5f2df34af5b18d742b" prefix=" " category="inline-code"></block> Richtlinien können zwar funktionieren, sie gelten jedoch nur für Blöcke, die sich nicht mehr im aktiven File-System befinden. Daher müssen Dateien auf einer NFS- oder SMB-Freigabe vor dem Daten-Tiering zuerst gelöscht werden.</block>
  <block id="b4af96e8cea99b56592db816cab30fce" category="paragraph">Diese Richtlinie wäre bei einer LUN-Konfiguration sogar noch weniger effizient, da beim Löschen einer Datei aus einer LUN nur Dateiverweise aus den Metadaten des Filesystems entfernt werden. Die tatsächlichen Blöcke auf den LUNs bleiben vorhanden, bis sie überschrieben werden. Dies kann zu einer sehr langen Verzögerung zwischen dem Löschen einer Datei und dem Überschreiben der Blöcke führen und zu Tiering-Kandidaten werden. Der Wechsel des bietet einige Vorteile<block ref="8805d416ce540c5f2df34af5b18d742b" prefix=" " category="inline-code"></block> Blöcke auf die Kapazitäts-Tier, aber insgesamt funktioniert das FabricPool Management von Backup-Daten am besten mit der<block ref="9df22f196a33acd0b372fe502de51211" prefix=" " category="inline-code"></block> Richtlinie:</block>
  <block id="1c237d1ebcdeff13ae0145c741d9401d" category="admonition">Mit diesem Ansatz können Benutzer den für Backups erforderlichen Speicherplatz effizienter managen. FabricPool selbst ist jedoch keine Backup-Technologie. Das Tiering von Backup-Dateien in Objektspeicher vereinfacht das Management, da die Dateien noch auf dem ursprünglichen Storage-System sichtbar sind, die Datenblöcke im Zielspeicherort jedoch vom ursprünglichen Storage-System abhängig sind. Wenn das Quell-Volume verloren geht, sind die Objektspeicher-Daten nicht mehr nutzbar.</block>
  <block id="a19665c8f7c0210f96ad2934ad0ee361" category="paragraph">Auf einem ONTAP-System wird der Storage in 4-KB-Einheiten organisiert. Ein Datenbank- oder Filesystem-8-KB-Block sollte exakt zwei 4-KB-Blöcken zugeordnet werden. Wenn ein Fehler in der LUN-Konfiguration die Ausrichtung um 1 KB in beide Richtungen verschiebt, wäre jeder 8-KB-Block auf drei verschiedenen 4-KB-Storage-Blöcken vorhanden anstatt auf zwei. Diese Anordnung würde zu einer erhöhten Latenz führen und dazu führen, dass zusätzliche I/O-Vorgänge innerhalb des Speichersystems ausgeführt werden.</block>
  <block id="211a9fe299057fe8665b591dddbac56d" category="paragraph">Die Ausrichtung wirkt sich auch auf LVM-Architekturen aus. Wenn ein physisches Volume innerhalb einer logischen Volume-Gruppe auf dem gesamten Laufwerk definiert wird (es werden keine Partitionen erstellt), wird der erste 4-KB-Block auf der LUN auf den ersten 4-KB-Block im Storage-System ausgerichtet. Dies ist eine korrekte Ausrichtung. Probleme ergeben sich bei Partitionen, da sie den Startort verschieben, an dem das Betriebssystem die LUN verwendet. Solange der Offset in ganzen 4-KB-Einheiten verschoben wird, ist die LUN ausgerichtet.</block>
  <block id="7c8a0aad1f59988f8160921a07203bb4" category="paragraph">Erstellen Sie in Linux-Umgebungen logische Volume-Gruppen auf dem gesamten Laufwerkgerät. Wenn eine Partition erforderlich ist, überprüfen Sie die Ausrichtung, indem Sie ausführen<block ref="16ae8c2dc7757a3180ce37bad780251a" prefix=" " category="inline-code"></block> Es wird überprüft, ob die Starts jeder Partition ein Vielfaches von acht sind. Dies bedeutet, dass die Partition bei einem Vielfachen von acht 512-Byte-Sektoren beginnt, was 4 KB ist.</block>
  <block id="e98a692d783d8d778aab49ed4c55e214" category="doc">Schutz vor Standortausfällen: NVRAM und MetroCluster</block>
  <block id="186577daf29184ed8a55cf899ba2979c" category="paragraph">MetroCluster erweitert die NVRAM-Datensicherung auf folgende Weise:</block>
  <block id="679f5a5873d7d3378cf4b3035fd5cb13" category="list-text">In einer Konfiguration mit zwei Nodes werden NVRAM-Daten mithilfe von Inter-Switch Links (ISLs) zum Remote-Partner repliziert.</block>
  <block id="00bec98f52b9151dd2e0c760becd4ca8" category="list-text">In einer HA-Paar-Konfiguration werden NVRAM-Daten sowohl auf den lokalen Partner als auch auf einen Remote-Partner repliziert.</block>
  <block id="94deccfbf8f798c1661000b27a568b18" category="list-text">Ein Schreibvorgang wird erst bestätigt, wenn er für alle Partner repliziert wird. Diese Architektur schützt aktive I/O-Vorgänge vor Standortausfällen, indem NVRAM-Daten zu einem Remote-Partner repliziert werden. Dieser Prozess ist nicht mit der Datenreplizierung auf Laufwerksebene verbunden. Der Controller, der die Aggregate besitzt, ist für die Datenreplizierung verantwortlich, indem er auf beide Plexe im Aggregat schreibt. Bei einem Standortausfall muss jedoch weiterhin ein Schutz vor inaktiven I/O-Datenverlusten gewährleistet sein. Replizierte NVRAM-Daten werden nur verwendet, wenn ein Partner-Controller für einen ausgefallenen Controller übernehmen muss.</block>
  <block id="996aa1ed8a906e55c7f157a1643021b7" category="section-title">Schutz vor Standort- und Shelf-Ausfällen: SyncMirror und Plexe</block>
  <block id="1c114a932963683e04dc7dece97c15c9" category="paragraph">SyncMirror ist eine Spiegelungstechnologie, die RAID DP oder RAID-TEC verbessert, aber nicht ersetzt. Es spiegelt den Inhalt von zwei unabhängigen RAID-Gruppen. Die logische Konfiguration ist wie folgt:</block>
  <block id="b4b4f1d65cb5f1370402220b00584e79" category="list-text">Laufwerke werden je nach Standort in zwei Pools konfiguriert. Ein Pool besteht aus allen Laufwerken an Standort A und der zweite Pool besteht aus allen Laufwerken an Standort B</block>
  <block id="418f0763b76bff8e324f1afdd5456b49" category="list-text">Ein gemeinsamer Storage Pool, auch bekannt als Aggregat, wird dann auf der Basis gespiegelter Gruppen von RAID-Gruppen erstellt. Von jedem Standort wird eine gleiche Anzahl von Laufwerken gezogen. Ein SyncMirror Aggregat für 20 Laufwerke würde beispielsweise aus 10 Laufwerken an Standort A und 10 Laufwerken an Standort B bestehen</block>
  <block id="51bf74357f742acfa0a7cd11ce52ed7f" category="list-text">Jeder Laufwerkssatz an einem bestimmten Standort wird automatisch als eine oder mehrere vollständig redundante RAID DP- oder RAID-TEC-Gruppen konfiguriert, und zwar unabhängig von der Verwendung von Spiegelung. Diese Verwendung von RAID unter der Spiegelung bietet Datensicherheit auch nach dem Verlust eines Standorts.</block>
  <block id="8a35ad1e53533c7a3a8ff427bda0deb2" category="paragraph"><block ref="8a35ad1e53533c7a3a8ff427bda0deb2" category="inline-image-macro-rx" type="image"></block></block>
  <block id="abf64dbc3ac20af1b4a30150a016ca58" category="paragraph">Die Abbildung oben zeigt eine Beispiel-SyncMirror-Konfiguration. Es wurde ein Aggregat mit 24 Laufwerken auf dem Controller mit 12 Laufwerken aus einem an Standort A zugewiesenen Shelf und 12 Laufwerken aus einem an Standort B zugewiesenen Shelf erstellt Die Laufwerke wurden in zwei gespiegelte RAID-Gruppen gruppiert. RAID-Gruppe 0 enthält einen Plex mit 6 Laufwerken an Standort A, der auf einen Plex mit 6 Laufwerken an Standort B gespiegelt wurde Ebenso enthält die RAID-Gruppe 1 einen Plex mit 6 Laufwerken an Standort A, der auf einen Plex mit 6 Laufwerken an Standort B gespiegelt wird</block>
  <block id="d9945e1dbadc87ba8e52a5e69376005c" category="paragraph">Normalerweise wird SyncMirror für die Remote-Spiegelung bei MetroCluster Systemen verwendet, wobei eine Kopie der Daten an jedem Standort vorhanden ist. Gelegentlich wurde es verwendet, um eine zusätzliche Redundanz in einem einzigen System bereitzustellen. Insbesondere bietet sie Redundanz auf Shelf-Ebene. Ein Festplatten-Shelf enthält bereits duale Netzteile und Controller und ist im Großen und Ganzen etwas mehr als Bleche, doch in einigen Fällen ist möglicherweise der zusätzliche Schutz gewährleistet. Ein NetApp Kunde beispielsweise hat SyncMirror für eine mobile Echtzeitanalyse-Plattform für Automobiltests implementiert. Das System wurde in zwei physische Racks mit unabhängigen Stromeinspeisungen und unabhängigen USV-Systemen getrennt.</block>
  <block id="f3495a1e6cf563a617c71164f3b11ca3" category="section-title">Redundanzfehler: NVFAIL</block>
  <block id="c7474a3ed1a224804e4741a256bdd953" category="paragraph">Wie zuvor bereits erläutert, wird ein Schreibvorgang erst bestätigt, wenn er in lokalem NVRAM und NVRAM auf mindestens einem anderen Controller angemeldet wurde. Dieser Ansatz stellt sicher, dass ein Hardware-Ausfall oder ein Stromausfall nicht zum Verlust der aktiven I/O führen Wenn der lokale NVRAM ausfällt oder die Verbindung zu anderen Nodes ausfällt, werden die Daten nicht mehr gespiegelt.</block>
  <block id="4774101e81ea03c43fe20e0d7f2d21d1" category="paragraph">Der Steuerungsfaktor ist, ob NVRAM synchronisiert wird. Bei NVRAM-Synchronisierung kann ein Node-to-Node Failover ohne das Risiko eines Datenverlusts fortgesetzt werden. Wenn in einer MetroCluster Konfiguration NVRAM und die zugrunde liegenden Aggregat-Plexe synchron sind, kann ohne das Risiko eines Datenverlusts eine Umschaltung durchgeführt werden.</block>
  <block id="03e97c023521445cd70307afabdcf0d5" category="paragraph">Datenbanken und andere Applikationen sind besonders anfällig für Beschädigungen, wenn ein Failover oder Switchover erzwungen wird, da sie größere interne Daten-Caches auf Festplatten beibehalten. Wenn ein erzwungenes Failover oder eine Umschaltung auftritt, werden zuvor bestätigte Änderungen effektiv verworfen. Der Inhalt des Storage Arrays springt effektiv zurück in die Zeit, und der Cache-Status gibt nicht mehr den Status der Daten auf der Festplatte wieder.</block>
  <block id="ff7a32ed70b7df57dd29a3ed75e8c787" category="paragraph">Um dies zu verhindern, können Volumes mit ONTAP für speziellen Schutz vor NVRAM-Ausfällen konfiguriert werden. Wenn dieser Schutzmechanismus ausgelöst wird, gelangt ein Volume in den Status „NVFAIL“. Dieser Zustand führt zu I/O-Fehlern, die einen Absturz der Applikation verursachen. Dieser Absturz führt dazu, dass die Applikationen heruntergefahren werden, damit keine veralteten Daten verwendet werden. Daten dürfen nicht verloren gehen, da alle festzugebenden Transaktionsdaten in den Protokollen vorhanden sein sollten. Als Nächstes muss ein Administrator die Hosts vollständig herunterfahren, bevor die LUNs und Volumes manuell wieder online geschaltet werden. Obwohl diese Schritte etwas Arbeit erfordern können, ist dieser Ansatz der sicherste Weg, um die Datenintegrität zu gewährleisten. Nicht alle Daten erfordern diesen Schutz. Daher kann ein NVFAIL-Verhalten auf Volume-Basis konfiguriert werden.</block>
  <block id="637d4b37868fb7825461df6edd674158" category="section-title">HA-Paare und MetroCluster</block>
  <block id="9d8738f531e6c6a809ba66315beaa2f4" category="paragraph">MetroCluster ist in zwei Konfigurationen erhältlich: Zwei Nodes und ein HA-Paar. Die Konfiguration mit zwei Nodes verhält sich in Bezug auf NVRAM wie ein HA-Paar. Im Falle eines plötzlichen Ausfalls kann der Partner-Node NVRAM-Daten wiedergeben, um die Laufwerke konsistent zu machen und sicherzustellen, dass keine bestätigten Schreibvorgänge verloren gegangen sind.</block>
  <block id="936f4311b16ee4dc74a5564892f8976d" category="paragraph">Die HA-Paar-Konfiguration repliziert NVRAM auch auf den lokalen Partner-Node. Ein einfacher Controller-Ausfall führt zu einer NVRAM-Wiedergabe auf dem Partner-Node, wie dies bei einem Standalone HA-Paar ohne MetroCluster der Fall ist. Bei einem plötzlichen vollständigen Standortausfall verfügt der Remote Standort außerdem über den NVRAM, der erforderlich ist, um die Laufwerke konsistent zu gestalten und Daten bereitzustellen.</block>
  <block id="83b6c3c3018b913ecf1aca67e0daa35e" category="paragraph">Ein wichtiger Aspekt von MetroCluster ist, dass die Remote Nodes unter normalen Betriebsbedingungen keinen Zugriff auf Partnerdaten haben. Jeder Standort funktioniert im Wesentlichen als ein unabhängiges System, das die Persönlichkeit des gegenüberliegenden Standorts übernehmen kann. Dieser Prozess wird als Umschaltung bezeichnet und umfasst ein geplantes Switchover, bei dem Standortvorgänge unterbrechungsfrei zum anderen Standort migriert werden. Auch ungeplante Situationen, in denen ein Standort verloren geht und bei der Disaster Recovery ein manuelles oder automatisches Switchover erforderlich ist, werden berücksichtigt.</block>
  <block id="58504b36cfdfd10d5f2802d7b943a379" category="section-title">Umschaltung und Switchback</block>
  <block id="d505751d1708fb5d57b0e23d089507a9" category="paragraph">Die Begriffe Switchover und Switchback beziehen sich auf den Prozess, bei dem Volumes zwischen Remote Controllern in einer MetroCluster Konfiguration migriert werden. Dieser Vorgang gilt nur für die Remote-Knoten. Wenn MetroCluster in einer Konfiguration mit vier Volumes zum Einsatz kommt, entspricht das lokale Node Failover dem zuvor beschriebenen Takeover- und Giveback-Prozess.</block>
  <block id="1127608e73df5ede22b149c8f19fc860" category="section-title">Geplante Umschaltung und Umschaltung</block>
  <block id="a6733335678588619c977608e891e54e" category="paragraph">Ein geplanter Switchover oder Switchback ähnelt einer Übernahme oder einem Giveback zwischen Nodes. Der Prozess umfasst mehrere Schritte und scheint möglicherweise mehrere Minuten zu erfordern. Aber was wirklich geschieht, ist eine mehrstufige Übertragung der Storage- und Netzwerkressourcen. Der Moment, in dem Kontrolltransfers schneller erfolgen, als der vollständige Befehl ausgeführt werden muss.</block>
  <block id="4bb5647dcff332f0b53819ecd631303c" category="paragraph">Der Hauptunterschied zwischen Takeover/Giveback und Switchover/Switchback besteht in den Auswirkungen auf die FC SAN-Konnektivität. Durch lokale Übernahme/Giveback wird der Verlust aller FC-Pfade zum lokalen Node durch den Host erlebbar und verlässt sich auf natives MPIO, um auf verfügbare alternative Pfade umzusteigen. Ports werden nicht verlegt. Mit Switchover und Switchback werden die virtuellen FC-Ziel-Ports der Controller zum anderen Standort übertragen. Sie existieren praktisch einen Moment lang nicht mehr auf dem SAN und werden dann auf einem alternativen Controller wieder angezeigt.</block>
  <block id="0ba902ec3f6ce91c2e801715bb8b96fa" category="section-title">SyncMirror-Timeouts</block>
  <block id="9c8a1814b60b84d1aa68fa7bba69138b" category="paragraph">Bei SyncMirror handelt es sich um eine ONTAP-Spiegelungstechnologie, die Schutz vor Shelf-Ausfällen bietet. Wenn Shelfs über eine Entfernung voneinander getrennt sind, führt dies zu einer Remote-Datensicherung.</block>
  <block id="68cf36f65a7e56ff3b6c0894be3ed9e8" category="paragraph">SyncMirror bietet kein universelles synchrones Spiegeln. Das Ergebnis ist eine höhere Verfügbarkeit. Einige Speichersysteme nutzen eine konstante Spiegelung alles oder nichts, die manchmal auch Domino-Modus genannt wird. Diese Form der Spiegelung ist in der Anwendung beschränkt, da alle Schreibaktivitäten unterbrochen werden müssen, wenn die Verbindung zum Remote-Standort verloren geht. Andernfalls würde ein Schreiben an einer Stelle, aber nicht an der anderen existieren. Solche Umgebungen sind normalerweise so konfiguriert, dass LUNs offline geschaltet werden, wenn die Verbindung zwischen Standorten länger als einen kurzen Zeitraum (wie etwa 30 Sekunden) unterbrochen wird.</block>
  <block id="84ad4c21def9c355e80ef250d910bfa2" category="paragraph">Dieses Verhalten ist für eine kleine Untermenge von Umgebungen wünschenswert. Die meisten Anwendungen benötigen jedoch eine Lösung, die eine garantierte synchrone Replikation unter normalen Betriebsbedingungen bietet, aber die Replikation unterbrechen kann. Ein vollständiger Verlust der Verbindung zwischen Standorten wird häufig als nahezu katastrophennahe Situation betrachtet. In der Regel werden solche Umgebungen online gehalten und stellen Daten bereit, bis die Konnektivität repariert wird oder eine formale Entscheidung getroffen wird, die Umgebung zum Schutz der Daten herunterzufahren. Eine Notwendigkeit für das automatische Herunterfahren der Anwendung allein aufgrund eines Fehlers bei der Remote-Replikation ist ungewöhnlich.</block>
  <block id="e3bd381a7bbaee6c741aee230972acf4" category="paragraph">SyncMirror unterstützt Anforderungen an die synchrone Spiegelung mit der Flexibilität einer Zeitüberschreitung. Wenn die Verbindung zum Remote-Controller und/oder Plex unterbrochen wird, beginnt ein 30-Sekunden-Timer zu zählen. Wenn der Zähler 0 erreicht, wird die Schreib-I/O-Verarbeitung mithilfe der lokalen Daten fortgesetzt. Die Remote-Kopie der Daten ist nutzbar, wird aber rechtzeitig eingefroren, bis die Verbindung wiederhergestellt ist. Die Neusynchronisierung nutzt Snapshots auf Aggregatebene, um das System so schnell wie möglich in den synchronen Modus zurückzuversetzen.</block>
  <block id="0c919cd6511df6d83b811234b0448374" category="paragraph">Bemerkenswert ist, dass in vielen Fällen diese Art universeller Domino-Modus-Replikation auf Anwendungsebene besser implementiert wird. Beispielsweise verfügt Oracle DataGuard über einen maximalen Schutzmodus, der unter allen Umständen eine Replizierung mit einer langen Instanz garantiert. Wenn die Replikationsverbindung für einen Zeitraum fehlschlägt, der ein konfigurierbares Timeout überschreitet, werden die Datenbanken heruntergefahren.</block>
  <block id="ba92cbe464cfee8aa12c41e6aecd1071" category="section-title">Automatische, unbeaufsichtigte Umschaltung mit Fabric Attached MetroCluster</block>
  <block id="515094655d10031595da776c94b3d710" category="paragraph">AUSO (Automatic unbeaufsichtigter Switchover) ist eine Fabric Attached MetroCluster Funktion, die eine Form standortübergreifender Hochverfügbarkeit bietet. Wie zuvor erläutert, gibt es bei MetroCluster zwei Typen: Einen einzigen Controller an jedem Standort oder ein HA-Paar an jedem Standort. Der Hauptvorteil der HA-Option besteht darin, dass bei geplanter oder ungeplanter Controller-Abschaltung alle I/O-Vorgänge weiterhin lokal ausgeführt werden können. Der Vorteil der Single-Node-Option liegt in der Reduzierung der Kosten, der Komplexität und der Infrastruktur.</block>
  <block id="29d4da26af76b66db3a172a39edf8367" category="paragraph">Der wichtigste Vorteil von AUSO ist die Verbesserung der Hochverfügbarkeitsfunktionen von Fabric Attached MetroCluster Systemen. Jeder Standort überwacht den Zustand des anderen Standorts. Falls kein Node mehr vorhanden ist, um Daten bereitzustellen, ermöglicht AUSO ein schnelles Switchover. Dieser Ansatz erweist sich insbesondere für MetroCluster Konfigurationen mit nur einem einzigen Node pro Standort, da er die Konfiguration in Bezug auf die Verfügbarkeit näher an ein HA-Paar bringt.</block>
  <block id="6b3a5f7923b2d803fd3b28c255fef420" category="paragraph">AUSO kann auf Ebene eines HA-Paars kein umfassendes Monitoring bieten. Ein HA-Paar kann für eine extrem hohe Verfügbarkeit sorgen, da es zwei redundante physische Kabel für eine direkte Kommunikation zwischen den Nodes umfasst. Darüber hinaus haben beide Nodes in einem HA-Paar Zugriff auf den gleichen Satz an Festplatten in redundanten Loops, die einen weiteren Weg für einen Node zur Überwachung des Systemzustands eines anderen bereitstellen.</block>
  <block id="da87dad268507a3523bc38275b1d3fbc" category="paragraph">MetroCluster Cluster sind über Standorte verteilt, bei denen sowohl die Node-to-Node-Kommunikation als auch der Festplattenzugriff auf die Site-to-Site-Netzwerkverbindung angewiesen sind. Die Fähigkeit, den Heartbeat des restlichen Clusters zu überwachen, ist begrenzt. AUSO muss zwischen Situationen unterscheiden, in denen der andere Standort aufgrund eines Netzwerkproblems nicht verfügbar ist, sondern tatsächlich ausgefallen ist.</block>
  <block id="e8e4f55c4203e7fb24d4543a7a0f65da" category="paragraph">So kann ein Controller in einem HA-Paar eine Übernahme veranlassen, wenn ein Controller-Ausfall erkannt wird, der aus einem bestimmten Grund, wie z. B. einem Systempanik, aufgetreten ist. Es kann auch zu einem Takeover führen, wenn ein vollständiger Verbindungsverlust besteht, manchmal auch als verlorener Herzschlag bezeichnet.</block>
  <block id="91aa4ce36cb243281f3777d66c8f89e0" category="paragraph">Ein MetroCluster System kann eine automatische Umschaltung nur sicher durchführen, wenn ein bestimmter Fehler am ursprünglichen Standort erkannt wird. Darüber hinaus muss der Controller, der das Storage-System übernimmt, in der Lage sein, die Synchronisierung von Festplatten- und NVRAM-Daten zu gewährleisten. Der Controller kann die Sicherheit einer Umschaltung nicht garantieren, nur weil er den Kontakt zum Quellstandort verloren hat, der noch betriebsbereit sein könnte. Weitere Optionen zur Automatisierung einer Umschaltung finden Sie im nächsten Abschnitt zur MetroCluster Tiebreaker Lösung (MCTB).</block>
  <block id="8e7705fe4aea384d7f6a99ab6dc0f408" category="section-title">MetroCluster Tiebreaker mit Fabric Attached MetroCluster</block>
  <block id="897b7698cb343dbc3452e845705ab0f3" category="inline-link">NetApp MetroCluster Tiebreaker</block>
  <block id="9c093f14319e8253b8c9b37290597239" category="inline-link">NetApp Support Website</block>
  <block id="a1d9fc4fb49119a9fe9b39abec0a769b" category="paragraph">Der<block ref="9332969062716487f0feefe076babf99" category="inline-link-rx"></block> Die Software kann an einem dritten Standort ausgeführt werden, um den Zustand der MetroCluster Umgebung zu überwachen, Benachrichtigungen zu senden und in einer Notfallsituation optional eine Umschaltung zu erzwingen. Eine vollständige Beschreibung des Tiebreaker finden Sie auf dem<block ref="c21658567f794984b03c21186a56713d" category="inline-link-rx"></block>, Aber der primäre Zweck des MetroCluster Tiebreaker ist es, Standortverluste zu erkennen. Außerdem muss zwischen Standortausfällen und Verbindungsverlust unterschieden werden. So sollte beispielsweise keine Umschaltung erfolgen, da der primäre Standort nicht erreichbar war. Aus diesem Grund überwacht Tiebreaker auch die Fähigkeit des Remote-Standorts, mit dem primären Standort in Kontakt zu treten.</block>
  <block id="6f29b1849750ceee5eb7f6f02d100e6b" category="paragraph">Die automatische Umschaltung mit AUSO ist auch mit der MCTB kompatibel. AUSO reagiert sehr schnell, da es darauf ausgelegt ist, bestimmte Fehlerereignisse zu erkennen und dann die Umschaltung nur dann aufzurufen, wenn NVRAM und SyncMirror Plexe synchron sind.</block>
  <block id="6f5c7ae76595eca2ca83c5dd30cd8e9f" category="paragraph">Im Gegensatz dazu befindet sich das Tiebreaker Remote und muss daher warten, bis ein Timer verstrichen ist, bevor ein Standort für tot erklärt wird. Über Tiebreaker wird schließlich festgestellt, wie ein Controller-Ausfall von AUSO abgedeckt ist, doch im Allgemeinen hat AUSO bereits die Umschaltung gestartet und möglicherweise die Umschaltung abgeschlossen, bevor es Tiebreaker wirkt. Der resultierende zweite Switchover-Befehl aus dem Tiebreaker würde abgelehnt.</block>
  <block id="842547e1622bb12d9201167b0c39cf6d" category="paragraph">*Achtung: *Die MCTB-Software überprüft nicht, ob NVRAM und/oder Plexe synchron sind, wenn eine Umschaltung erzwungen wird. Sofern konfiguriert, sollte die automatische Umschaltung während Wartungsaktivitäten deaktiviert werden, die zu einem Verlust der Synchronisierung von NVRAM- oder SyncMirror-Plexen führen.</block>
  <block id="7110edd59c3d0f25a584723f6fea26a0" category="paragraph">Darüber hinaus geht die MCTB möglicherweise nicht bei einem rollierenden Notfall ein, der zu der folgenden Ereignisabfolge führt:</block>
  <block id="93d732b784eaf1a323f191e75c10f233" category="list-text">Die Konnektivität zwischen Standorten wird für mehr als 30 Sekunden unterbrochen.</block>
  <block id="81d4275cc3534a9fa8cfdcdb5172eb94" category="list-text">Die SyncMirror-Replizierung ist zeitgemäß, und der Betrieb wird am primären Standort fortgesetzt, sodass das Remote-Replikat nicht mehr zeitgemäß ist.</block>
  <block id="44441f68631008a941ce92e985131ff9" category="list-text">Der primäre Standort geht verloren.das Ergebnis sind nicht replizierte Änderungen am primären Standort. Eine Umschaltung könnte dann aus verschiedenen Gründen unerwünscht sein, unter anderem aus folgenden Gründen:</block>
  <block id="499527dd40fad77b119b8b33c99cd614" category="list-text">Am primären Standort befinden sich möglicherweise kritische Daten, und diese Daten können nach und nach wiederhergestellt werden. Mit einer Umschaltung, die eine Weiterführung des Betriebs der Applikation ermöglichte, würden die kritischen Daten praktisch verworfen.</block>
  <block id="86de4e3737e3d871dacf8ffef353fc31" category="list-text">Möglicherweise haben Daten im Cache einer Applikation gespeichert, die am verbleibenden Standort zum Zeitpunkt des Standortverlusts die Storage-Ressourcen am primären Standort nutzte. Durch ein Switchover würde eine veraltete Version der Daten eingeführt, die nicht mit dem Cache übereinstimmt.</block>
  <block id="dbb2a01afe28c8310daf781bb80b795f" category="list-text">Möglicherweise haben Daten im Cache eines Betriebssystems, das auf dem verbleibenden Standort zum Zeitpunkt eines Standortausfalls Speicherressourcen am primären Standort genutzt hat, gespeichert. Durch ein Switchover würde eine veraltete Version der Daten eingeführt, die nicht mit dem Cache übereinstimmt. Am sichersten ist es, dass Sie Tiebreaker so konfigurieren, dass eine Warnmeldung ausgegeben wird, wenn ein Standortausfall erkannt wird und anschließend eine Person Entscheidungen darüber treffen muss, ob eine Umschaltung erzwungen werden soll. Applikationen und/oder Betriebssysteme müssen möglicherweise zunächst heruntergefahren werden, um zwischengespeicherte Daten zu löschen. Darüber hinaus können die NVFAIL-Einstellungen verwendet werden, um einen zusätzlichen Schutz zu bieten und den Failover-Prozess zu rationalisieren.</block>
  <block id="ec6b0ea9c511479c118aa0f028f6519d" category="section-title">ONTAP Mediator mit MetroCluster IP</block>
  <block id="8ada6a626a475d631df231bbb3a88ac9" category="paragraph">Der ONTAP Mediator wird mit MetroCluster IP und bestimmten anderen ONTAP-Lösungen verwendet. Es fungiert als herkömmlicher Tiebreaker Service, ähnlich wie die oben beschriebene MetroCluster Tiebreaker Software, verfügt aber auch über eine wichtige Funktion zum automatisierten, unbeaufsichtigten Switchover.</block>
  <block id="991d7d7d7388ccdd87c65ca09aa804f2" category="paragraph">Ein Fabric-Attached MetroCluster hat direkten Zugriff auf die Storage-Geräte am gegenüberliegenden Standort. Dadurch kann ein MetroCluster-Controller den Zustand der anderen Controller überwachen, indem er die Heartbeat-Daten von den Laufwerken liest. So kann ein Controller den Ausfall eines anderen Controllers erkennen und eine Umschaltung durchführen.</block>
  <block id="db2e932a11478bcf14c75d38e8b9a170" category="paragraph">Im Gegensatz dazu leitet die MetroCluster IP Architektur alle I/O ausschließlich über die Controller-Controller-Verbindung weiter; es besteht kein direkter Zugriff auf Speichergeräte am Remote-Standort. Dadurch wird die Fähigkeit eines Controllers eingeschränkt, Ausfälle zu erkennen und eine Umschaltung durchzuführen. Der ONTAP Mediator ist daher als Tiebreaker-Gerät erforderlich, um Standortverluste zu erkennen und automatisch eine Umschaltung durchzuführen.</block>
  <block id="8004e6d63f487a456290225b74768a6c" category="section-title">Virtueller dritter Standort mit ClusterLion</block>
  <block id="f1d5a7305654a402a719675fbec810e7" category="paragraph">ClusterLion ist eine fortschrittliche MetroCluster Monitoring-Appliance, die als virtueller dritter Standort fungiert. Dieser Ansatz ermöglicht die sichere Implementierung von MetroCluster in einer Konfiguration mit zwei Standorten und einer vollständig automatisierten Umschaltfunktion. Des Weiteren kann ClusterLion zusätzliche Überwachung auf Netzwerkebene durchführen und Vorgänge nach der Umschaltung ausführen. Die vollständige Dokumentation ist bei ProLion erhältlich.</block>
  <block id="a60b8f728a13371898fea9947ef1e0dc" category="paragraph"><block ref="a60b8f728a13371898fea9947ef1e0dc" category="inline-image-macro-rx" type="image"></block></block>
  <block id="f8aae92ec17ec82868ef9ffc767dbd03" category="list-text">Die ClusterLion Appliances überwachen den Zustand der Controller mit direkt angeschlossenem Ethernet und seriellen Kabeln.</block>
  <block id="c5f8963a91e3d8e7ac0abda2eab9f17d" category="list-text">Die beiden Geräte sind über redundante 3G-Wireless-Verbindungen miteinander verbunden.</block>
  <block id="24de16a13a1b584832d19117982cbfa6" category="list-text">Die Stromversorgung des ONTAP-Controllers erfolgt über interne Relais. Bei einem Standortausfall trennt ClusterLion, das ein internes USV-System enthält, die Stromanschlüsse, bevor eine Umschaltung initiiert wird. Dieser Prozess stellt sicher, dass kein Split-Brain-Zustand auftritt.</block>
  <block id="b2bcd3c3c555e2e4f27d453de4e0f06d" category="list-text">ClusterLion führt eine Umschaltung innerhalb der SyncMirror-Zeitüberschreitung von 30 Sekunden oder überhaupt nicht aus.</block>
  <block id="390fb5f827a3fbcdc7d05e6e7db03223" category="list-text">ClusterLion führt nur eine Umschaltung durch, wenn die Zustände NVRAM und SyncMirror Plexe synchron sind.</block>
  <block id="f322f850d55e87946f9660e4ccb1bbb6" category="list-text">Da ClusterLion nur umgeschaltet wird, wenn die MetroCluster vollständig synchron ist, ist das NVFAIL nicht erforderlich. Diese Konfiguration ermöglicht es, standortübergreifende Umgebungen wie beispielsweise einen erweiterten Oracle RAC auch während einer ungeplanten Umschaltung online zu bleiben.</block>
  <block id="5c132e05fb0e306f56955603619de359" category="list-text">Die Unterstützung umfasst sowohl Fabric-Attached MetroCluster als auch MetroCluster IP</block>
  <block id="86bcf99dbc3944da56f06b48074d09f6" category="doc">Normaler Betrieb</block>
  <block id="e48472f7daf781b63506f9e296443027" category="paragraph">Im normalen Betrieb kann auf eine LUN entweder über das lokale oder Remote-Replikat zugegriffen werden. Die rote Linie zeigt den optimierten Pfad an, wie von ALUA angekündigt, und das Ergebnis sollte sein, dass IO bevorzugt über diesen Pfad gesendet wird.</block>
  <block id="e139a585510a502bbf1841cf589f5086" category="section-title">Ausfall</block>
  <block id="7ed516552b652790c92df0bf237264b2" category="paragraph">Wenn die aktive gespiegelte Kopie aufgrund eines geplanten oder ungeplanten Failover nicht mehr verfügbar ist, ist sie offensichtlich nicht mehr nutzbar. Das Remote-System verfügt jedoch über ein synchrones Replikat, und es sind bereits SAN-Pfade zum Remote-Standort vorhanden. Das Remote-System kann E/A für diese LUN bedienen.</block>
  <block id="7388404ef116c3ff812bfd290b094d9e" category="section-title">Failover</block>
  <block id="997dfe060d2107e84407cfb41e692c54" category="paragraph">Durch Failover wird die Remote-Kopie zur aktiven Kopie. Die Pfade werden von „aktiv“ in „aktiv/optimiert“ geändert und I/O-Vorgänge werden weiterhin ohne Datenverlust verarbeitet.</block>
  <block id="0493d27e20d11e356e53e0a730b0ad08" category="section-title">Reparieren</block>
  <block id="6d80a3efb80520f47b63dc279e1bea3d" category="section-title">Failback</block>
  <block id="d47f47372b9b94d274cceb3638b28845" category="paragraph">Auf Wunsch kann ein Administrator ein Failback durchführen und die aktive Kopie der LUN(s) zurück auf die ursprünglichen Controller verschieben.</block>
  <block id="0c99cedb6f39d8ac16d5b4c01c162c23" category="paragraph">Die Datenbank-Wiederherstellungs-/Transaktionsprotokollierung erzeugt normalerweise nicht ausgerichtete I/O-Vorgänge, die irreführende Warnungen zu falsch ausgerichteten LUNs auf ONTAP verursachen können.</block>
  <block id="b2f7bb8e75b21ee3e883141c6ac1defe" category="paragraph">Die Protokollierung führt einen sequenziellen Schreibvorgang der Protokolldatei mit unterschiedlich großen Schreibvorgängen durch. Ein Protokollschreibvorgang, der sich nicht an 4-KB-Grenzen ausrichtet, verursacht normalerweise keine Performance-Probleme, da der nächste Protokollschreibvorgang den Block abgeschlossen hat. Das Ergebnis: ONTAP ist in der Lage, fast alle Schreibvorgänge als komplette 4-KB-Blöcke zu verarbeiten, obwohl die Daten in einigen 4-KB-Blöcken in zwei separaten Operationen geschrieben wurden.</block>
  <block id="0e635e1f8ffad809203304cf41ed45c4" category="inline-link-macro">Überprüfung der WAFL-Ausrichtung</block>
  <block id="bf837d24f10c904e697e3c0092da43b9" category="paragraph">Überprüfen Sie die Ausrichtung mithilfe von Dienstprogrammen wie<block ref="40883add63f1fbabc7fe6158f93c1246" prefix=" " category="inline-code"></block> Oder<block ref="1aabac6d068eef6a7bad3fdf50a05cc8" prefix=" " category="inline-code"></block> Sie können I/O mit einer definierten Blockgröße generieren. Die I/O-Ausrichtungsstatistiken auf dem Storage-System können mit dem angezeigt werden<block ref="446501053769c06c565094b26d26e8ef" prefix=" " category="inline-code"></block> Befehl. Siehe <block ref="ebccec7997af507c87f985ec4ccec634" category="inline-link-macro-rx"></block> Finden Sie weitere Informationen.</block>
  <block id="b5ffbfb0c082804ad41d0f61c87525ed" category="paragraph">Viele Applikationsdatensätze sind nach Datum geordnet. Solche Daten sind im Allgemeinen immer seltener zugänglich, wenn sie älter werden. Beispielsweise verfügt eine Bank möglicherweise über ein Repository mit PDF-Dateien, die fünf Jahre Kundenabrechnungen enthalten, aber nur die letzten Monate sind aktiv. FabricPool kann verwendet werden, um ältere Datendateien in die Kapazitäts-Tier zu verschieben. Eine Abkühlzeit von 14 Tagen würde dafür sorgen, dass die letzten 14 Tage der PDF-Dateien auf der Performance-Ebene verbleiben. Darüber hinaus würden Dateien, die mindestens alle 14 Tage gelesen werden, „heiß“ bleiben und daher auf der Performance-Ebene verbleiben.</block>
  <block id="ed7e2cc7ff107d4b2032fe65cf4e756a" category="paragraph">Um einen dateibasierten Tiering-Ansatz zu implementieren, müssen Sie über Dateien verfügen, die geschrieben und nicht nachträglich geändert werden. Der<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Richtlinien sollten so hoch eingestellt werden, dass Dateien, die Sie möglicherweise benötigen, auf der Performance-Tier verbleiben. Ein Datensatz, für den die letzten 60 Tage Daten mit optimaler Performance benötigt werden, erfordert beispielsweise die Einstellung<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Bis 60. Ähnliche Ergebnisse lassen sich auch anhand der Dateizugriffsmuster erzielen. Wenn beispielsweise die Daten der letzten 90 Tage benötigt werden und die Applikation auf diese 90-Tage-Zeitspanne zugreift, verbleiben die Daten in der Performance-Tier. Durch Einstellen der<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Zeitraum bis 2, erhalten Sie prompt Tiering, nachdem die Daten weniger aktiv werden.</block>
  <block id="8a467b35326e4738b1bd39865c3b21a0" category="admonition">Jeder Zugriff auf Daten setzt die Heatmap-Daten zurück. Virus-Scan, Indizierung und sogar Backup-Aktivitäten, die die Quelldateien lesen, verhindern Tiering, da dies erforderlich ist<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Schwellenwert wird nie erreicht.</block>
  <block id="7a1920d61156abc05a60135aefe8bc67" category="doc">Standard</block>
  <block id="5ba46d288640832c345e169b04a7458a" category="paragraph">Alle FabricPool-Volumes sind zunächst auf festgelegt<block ref="c21f969b5f03d33d43e04f8f136e7682" prefix=" " category="inline-code"></block>, D.h. das Verhalten wird durch die `Cloud-Retrieval-Policy gesteuert. `das genaue Verhalten hängt von der verwendeten Tiering Policy ab.</block>
  <block id="5ec354970d7934d92ba67ccaa0de0121" category="list-text"><block ref="9df22f196a33acd0b372fe502de51211" prefix="" category="inline-code"></block>– Nur abrufen zufällig gelesenen Daten</block>
  <block id="74092cfa5efab8e6ddc5c2991ee2cfc9" category="list-text"><block ref="8805d416ce540c5f2df34af5b18d742b" prefix="" category="inline-code"></block>– Abrufen aller sequentiellen oder zufällig gelesenen Daten</block>
  <block id="2de036541477f953c6fd33d14a8db0e8" category="list-text"><block ref="334c4a4c42fdb79d7ebc3e73b517e6f8" prefix="" category="inline-code"></block>– Abrufen aller sequentiellen oder zufällig gelesenen Daten</block>
  <block id="b6ffdb7ccf86e99ecf73899ec23bdd46" category="list-text"><block ref="a181a603769c1f98ad927e7367c7aa51" prefix="" category="inline-code"></block>– Daten nicht aus der Kapazitäts-Tier abrufen</block>
  <block id="d556a0d2cad80695d1d803ebb49de9cd" category="section-title">Gelesen</block>
  <block id="7c39eb141ca855e5a211b468bd67636c" category="paragraph">Einstellung<block ref="d253e682212338dd6cbe577947f0511e" prefix=" " category="inline-code"></block> Das Lesen überschreibt das Standardverhalten, sodass ein Lesen von Tiered-Daten dazu führt, dass diese Daten an die Performance-Tier zurückgegeben werden.</block>
  <block id="e07df43ea07fecb178484ef278644eb2" category="paragraph">Ein Volume könnte beispielsweise lange Zeit unter der wenig verwendet worden sein<block ref="9df22f196a33acd0b372fe502de51211" prefix=" " category="inline-code"></block> die tiering-Richtlinie, und die meisten Blöcke sind nun Tiered Storage.</block>
  <block id="98c8614a9e0467f220df33a02cf7550a" category="paragraph">Wenn bei einer unerwarteten Änderung des Geschäfts ein Teil der Daten wiederholt gescannt werden muss, um einen bestimmten Bericht zu erstellen, kann es wünschenswert sein, den zu ändern<block ref="d253e682212338dd6cbe577947f0511e" prefix=" " category="inline-code"></block> Bis<block ref="0ba0dc933ac714a0bef4467360437336" prefix=" " category="inline-code"></block> Um sicherzustellen, dass alle gelesenen Daten in die Performance-Tier zurückgegeben werden, einschließlich sequenzieller und zufällig gelesener Daten. Dies würde die Performance sequenzieller I/O-Vorgänge für das Volume verbessern.</block>
  <block id="3ea4ba4aadef21fbda74c9b242c99efa" category="section-title">Heraufstufen</block>
  <block id="cb674c0cbcbaa23bdd3c8e4f6182f865" category="paragraph">Das Verhalten der „heraufstufen“-Richtlinie hängt von der Tiering-Richtlinie ab. Wenn die Tiering-Richtlinie lautet<block ref="9df22f196a33acd0b372fe502de51211" prefix=" " category="inline-code"></block>, Dann Einstellung der<block ref="723c2b1883eb9949dd821267c2d1b5c7" prefix=" " category="inline-code"></block> Ruft beim nächsten Tiering-Scan alle Blöcke aus der Kapazitäts-Tier zurück.</block>
  <block id="452d85cc34fd3a4cf55ac61e732239f4" category="paragraph">Wenn die Tiering-Richtlinie lautet<block ref="8805d416ce540c5f2df34af5b18d742b" prefix=" " category="inline-code"></block>, Dann sind die einzigen Blöcke, die zurückgegeben werden, die mit dem aktiven Dateisystem verbunden sind. Normalerweise hätte dies keine Auswirkung, weil die einzigen Blöcke unter das gestaffelt wären<block ref="8805d416ce540c5f2df34af5b18d742b" prefix=" " category="inline-code"></block> Richtlinie wären Blöcke, die ausschließlich mit Snapshots verknüpft wären. Es gäbe keine Tiered Blocks im aktiven File-System.</block>
  <block id="d6269fb1a970b6c261fedbc79464b3dc" category="paragraph">Wenn jedoch die Daten auf einem Volume von einem Volume-SnapRestore oder Datei-Klon-Vorgang aus einem Snapshot wiederhergestellt wurden, können einige der Blöcke, die aufgrund ihrer lediglich mit Snapshots verknüpften Speicherebenen verschoben wurden, jetzt vom aktiven File-System benötigt werden. Es kann wünschenswert sein, die vorübergehend zu ändern<block ref="d253e682212338dd6cbe577947f0511e" prefix=" " category="inline-code"></block> Richtlinie an<block ref="eeefc0add5ccd6e20ac4214923d27fbc" prefix=" " category="inline-code"></block> Alle lokal erforderlichen Blöcke schnell abrufen.</block>
  <block id="6e7b34fa59e1bd229b207892956dc41c" category="section-title">Nie</block>
  <block id="c45e30a0f86af155e2feb17dd7ba6e44" category="paragraph">Nehmen Sie keine Blöcke aus der Kapazitäts-Tier heraus.</block>
  <block id="2d242bb36ec91b32005f9296ff03a912" category="doc">Der Netapp Architektur Sind</block>
  <block id="ac30d4a74ecef302adab70e9669a4bf2" category="paragraph">FabricPool ist eine Tiering-Technologie, mit der Blöcke als „heiß“ oder „kalt“ klassifiziert werden und in dem Storage Tier platziert werden, der am besten geeignet ist. Die Performance-Tier befindet sich am häufigsten auf SSD-Storage und hostet die wichtigen Datenblöcke. Die Kapazitäts-Tier befindet sich in einem Objektspeicher und hostet die kühlen Datenblöcke. Unterstützung für Objekt-Storage: NetApp StorageGRID, ONTAP S3, Microsoft Azure Blob Storage, Alibaba Cloud Object Storage-Service, IBM Cloud Object Storage, Google Cloud Storage und Amazon AWS S3</block>
  <block id="f9f5fe2f7456d37c955f3291744c93b6" category="paragraph">Es stehen mehrere Tiering-Richtlinien zur Verfügung, die steuern, wie Blöcke als „heiß“ oder „kalt“ klassifiziert werden. Die Richtlinien lassen sich für einzelne Volumes festlegen und bei Bedarf ändern. Es werden nur die Datenblöcke zwischen den Performance- und Kapazitäts-Tiers verschoben. Die Metadaten, die die Struktur der LUN und des File-Systems definieren, verbleiben immer auf der Performance-Tier. Dadurch wird das Management auf ONTAP zentralisiert. Dateien und LUNs unterscheiden sich offenbar nicht von Daten, die auf einer anderen ONTAP-Konfiguration gespeichert sind. Der NetApp AFF oder FAS Controller wendet die definierten Richtlinien an, um Daten auf die entsprechende Tier zu verschieben.</block>
  <block id="156a1cf692c4ba9a4f9574fb16428b01" category="paragraph"><block ref="156a1cf692c4ba9a4f9574fb16428b01" category="inline-image-macro-rx" type="image"></block></block>
  <block id="616ce0475334fe37834d13972a168ca5" category="section-title">Objektspeicher-Anbieter</block>
  <block id="71e79501cc067e6b35ac3d3e97c940a4" category="paragraph">Bei Objekt-Storage-Protokollen werden einfache HTTP- oder HTTPS-Anfragen zum Speichern einer großen Anzahl von Datenobjekten verwendet. Der Zugriff auf den Objektspeicher muss zuverlässig sein, da der Datenzugriff von ONTAP von der umgehende Erfüllung von Anfragen abhängt. Zu den Optionen gehören Amazon S3 Standard und infrequent Access sowie Microsoft Azure Hot and Cool Blob Storage, IBM Cloud und Google Cloud. Archivierungsoptionen wie Amazon Glacier und Amazon Archive werden nicht unterstützt, da die zum Abrufen von Daten erforderliche Zeit die Toleranzen der Host-Betriebssysteme und -Applikationen überschreiten kann.</block>
  <block id="e114e830308285cd9085063fad91662c" category="paragraph">Zudem wird NetApp StorageGRID unterstützt und stellt eine optimale Lösung der Enterprise-Klasse dar. Es ist ein hochperformantes, skalierbares und hochsicheres Objekt-Storage-System, das geografische Redundanz für FabricPool Daten und andere Objektspeicher-Applikationen bietet, die zunehmend Teil von Enterprise-Applikationsumgebungen sind.</block>
  <block id="fb318c5cd9ed18c79e1f7a298dd96665" category="paragraph">StorageGRID kann zudem die Kosten senken, indem es die Egress-Gebühren vermeidet, die viele Public-Cloud-Provider beim Lesen der Daten aus ihren Services auferlegen.</block>
  <block id="119d8a1213b5b7cae4bb567153b028cf" category="section-title">Daten und Metadaten</block>
  <block id="b59d6123a3191bd85fe14e57b7a01e11" category="paragraph">Beachten Sie, dass der Begriff „Daten“ hier für die tatsächlichen Datenblöcke gilt, nicht für die Metadaten. Es werden nur Datenblöcke als Tiering übertragen, wobei die Metadaten in der Performance-Tier verbleiben. Darüber hinaus wird der Status eines Blocks als „heiß“ oder „kalt“ nur beeinflusst, wenn der eigentliche Datenblock gelesen wird. Das einfache Lesen des Namens, des Zeitstempels oder der Eigentümermetadaten einer Datei hat keine Auswirkung auf den Speicherort der zugrunde liegenden Datenblöcke.</block>
  <block id="1414cdc093d39dd56d0b0d20049e17fc" category="section-title">Backups</block>
  <block id="e0b766a7a0a7633679ecddd0f78a7390" category="paragraph">Obwohl FabricPool den Storage-Platzbedarf deutlich reduzieren kann, ist es nicht für sich genommen eine Backup-Lösung. NetApp WAFL Metadaten bleiben immer auf der Performance-Tier. Falls ein schwerwiegender Ausfall die Performance-Tier zerstört, kann keine neue Umgebung aus den Daten auf der Kapazitäts-Tier erstellt werden, da sie keine WAFL-Metadaten enthält.</block>
  <block id="7a25ce12890e524732cb71784b5915ab" category="paragraph">FabricPool kann jedoch Teil einer Backup-Strategie werden. FabricPool lässt sich beispielsweise mit der Replizierungstechnologie NetApp SnapMirror konfigurieren. Jede Hälfte der Spiegelung kann über eine eigene Verbindung mit einem Objekt-Storage-Ziel verfügen. Daraus ergeben sich zwei unabhängige Kopien der Daten. Die primäre Kopie besteht aus den Blöcken auf der Performance-Tier und den zugehörigen Blöcken auf der Kapazitäts-Tier, während das Replikat einen zweiten Satz von Performance- und Kapazitätsblöcken darstellt.</block>
  <block id="82af841589057aa8922b1ac3bb4a28a4" category="doc">Komprimierung</block>
  <block id="e242ff0701d9ed7d8317b32fd762a100" category="paragraph">Funktionen für Platzeffizienz wie Komprimierung, Data-Compaction und Deduplizierung sind darauf ausgelegt, die Menge der logischen Daten zu einer bestimmten Menge des physischen Storage zu erhöhen. Das Ergebnis sind niedrigere Kosten und geringerer Management-Overhead.</block>
  <block id="13b992c3d358d786db03341886472c4f" category="paragraph">Auf hohem Niveau ist Komprimierung ein mathematischer Prozess, bei dem Muster in Daten erkannt und so kodiert werden, dass der Platzbedarf reduziert wird. Dagegen erkennt die Deduplizierung tatsächlich wiederholte Datenblöcke und entfernt die fremden Kopien. Durch Data-Compaction können mehrere logische Datenblöcke denselben physischen Block auf den Medien gemeinsam nutzen.</block>
  <block id="786d7cc1b18aa3d9c3e8d3e30865240e" category="paragraph">Vor der Verfügbarkeit von All-Flash-Storage-Systemen war die Array-basierte Komprimierung nur eingeschränkt verfügbar, da die meisten I/O-intensiven Workloads eine sehr große Anzahl von Spindeln erforderten, um eine akzeptable Performance zu erreichen. Als Nebeneffekt der großen Anzahl von Laufwerken enthielten Storage-Systeme grundsätzlich viel mehr Kapazität als erforderlich. Mit dem Trend hin zu Solid-State-Storage hat sich die Situation verändert. Eine enorme Überprovisionierung von Laufwerken entfällt, nur weil eine gute Performance erzielt werden kann. Der Speicherplatz in einem Storage-System kann den tatsächlichen Kapazitätsanforderungen angepasst werden.</block>
  <block id="b207c460d31fea9e1cfab71057e8152c" category="paragraph">Die gesteigerte IOPS-Fähigkeit von Solid-State-Laufwerken (SSDs) bringt im Vergleich zu rotierenden Laufwerken fast immer Kosteneinsparungen mit sich. Allerdings kann die Komprimierung durch eine höhere effektive Kapazität von Solid-State-Medien weitere Einsparungen erzielen.</block>
  <block id="6296c8b7406b90d37edf269f1ea6f6ee" category="section-title">Anpassungsfähige Komprimierung</block>
  <block id="1245fef302b5da2c27766d9a98ad358c" category="paragraph">Die adaptive Komprimierung wurde vollständig mit Enterprise-Workloads getestet, ohne dabei die Performance zu beeinträchtigen – selbst in einer All-Flash-Umgebung, in der die Latenz im Mikrosekunden-Bereich gemessen wird. Einige Kunden haben bei Verwendung der Komprimierung sogar eine Performance-Steigerung festgestellt, da die Daten im Cache komprimiert bleiben. Dadurch konnte die Menge des verfügbaren Cache in einem Controller erhöht werden.</block>
  <block id="93e0a644c8f1b7f7f378bc071d15d1e9" category="paragraph">ONTAP managt physische Blöcke in 4-KB-Einheiten. Die anpassungsfähige Komprimierung verwendet eine Standardkomprimierung von 8 KB. Dies bedeutet, dass Daten in 8-KB-Einheiten komprimiert werden. Dies entspricht der 8-KB-Blockgröße, die von relationalen Datenbanken am häufigsten verwendet wird. Kompressionsalgorithmen werden effizienter, da mehr Daten als eine Einheit komprimiert werden. Eine Komprimierungs-Blockgröße von 32 KB wäre speichereffizienter als eine Komprimierungsblockeinheit mit 8 KB. Das bedeutet, dass die adaptive Komprimierung bei Verwendung der standardmäßigen 8-KB-Blockgröße zu etwas niedrigeren Effizienzraten führt, jedoch bietet die Verwendung kleinerer Blockgrößen zur Komprimierung auch einen signifikanten Vorteil. Datenbank-Workloads umfassen einen großen Anteil an Überschreibungsaktivitäten. Beim Überschreiben eines komprimierten 32-KB-Datenblocks müssen die gesamten 32-KB-Daten zurückgelesen, dekomprimiert, der erforderliche 8-KB-Bereich aktualisiert, neu komprimiert und dann die gesamten 32-KB-Daten wieder auf die Laufwerke geschrieben werden. Dies ist für ein Storage-System ein sehr teurer Vorgang und der Grund dafür, dass bei einigen Storage Arrays anderer Anbieter, die auf größeren Komprimierungsblockgrößen basieren, auch die Performance bei Datenbank-Workloads erheblich beeinträchtigt wird.</block>
  <block id="1f9367b7afff301ee24188d35050ae0a" category="section-title">Temperaturempfindliche Speichereffizienz</block>
  <block id="02e44d620def629a6c145ccd0d9a0fd3" category="section-title">Kompressionsausrichtung</block>
  <block id="3b6e6c232a02ab9c64199eb9efae012c" category="paragraph">Die anpassungsfähige Komprimierung in einer Datenbankumgebung erfordert bestimmte Überlegungen zur Blockausrichtung der Komprimierung. Dies ist nur für Daten relevant, die Random Überschreibungen sehr spezifischer Blöcke unterliegen. Dieser Ansatz ähnelt im Konzept der gesamten Filesystem-Ausrichtung, wobei der Beginn eines Dateisystems an einer Grenze von 4K-Geräten ausgerichtet werden muss und die Blockgröße eines Dateisystems ein Vielfaches von 4K sein muss.</block>
  <block id="1d1a594959ec615f56516f5d0f5e8ddb" category="section-title">NFS</block>
  <block id="62a0282d39568be094470486eaf70c4f" category="section-title">San</block>
  <block id="14c3c56ff2a98d1a9c47b63a917f67a3" category="section-title">Data-Compaction</block>
  <block id="0b01f66d49b9df153ee76048de44f6eb" category="paragraph">Data-Compaction sorgt dafür, dass mehrere logische Blöcke innerhalb physischer Blöcke gespeichert werden können. Beispielsweise kann eine Datenbank mit stark komprimierbaren Daten wie Text oder teilweise vollständigen Blöcken von 8 KB bis 1 KB komprimieren. Ohne Data-Compaction belegen diese 1 KB Daten immer noch einen gesamten 4-KB-Block. Durch die Inline-Data-Compaction können 1 KB komprimierte Daten zusammen mit anderen komprimierten Daten auf nur 1 KB physischen Speicherplatz gespeichert werden. Es handelt sich nicht um eine Komprimierungstechnologie. Es ist einfach eine effizientere Möglichkeit, Speicherplatz auf den Laufwerken zuzuweisen und sollte daher keine erkennbaren Performance-Auswirkungen verursachen.</block>
  <block id="83f45cad5e6f535ff10e564a526baad0" category="section-title">Deduplizierung</block>
  <block id="f591ef30da06841378ee7f595e63a93c" category="paragraph">Deduplizierung ist die Entfernung von Blockduplikaten aus einem Datensatz. Wenn beispielsweise derselbe 4-KB-Block in 10 verschiedenen Dateien vorhanden war, leitet die Deduplizierung diesen 4-KB-Block innerhalb aller 10 Dateien auf denselben physischen 4-KB-Block um. Im Ergebnis würde sich die Effizienz dieser Daten um 10:1 verbessern.</block>
  <block id="699c85c5df1985b2af588f9e6c1ad353" category="paragraph">Daten wie Boot-LUNs von VMware lassen sich in der Regel sehr gut deduplizieren, da sie aus mehreren Kopien derselben Betriebssystemdateien bestehen. Es wurde eine Effizienz von 100:1 und höher festgestellt.</block>
  <block id="9653a3d4907877aae00bb7d39eeec66d" category="paragraph">In einigen Fällen wurde eine Speicherersparnis von bis zu 15 % bei Datenbanken mit 16 KB und großen Blockgrößen beobachtet. Die ersten 4-KB-Blöcke enthalten die global eindeutige Kopfzeile, und der letzte 4-KB-Block enthält den nahezu einzigartigen Trailer. Die internen Blöcke eignen sich für eine Deduplizierung, obwohl dies in der Praxis fast vollständig der Deduplizierung von gelöschten Daten zugeordnet ist.</block>
  <block id="32aeb47267cb2045610b468115f3ae0e" category="section-title">Effizienz und Thin Provisioning</block>
  <block id="3381946611f34fb44ad8a38a0c44e0a8" category="paragraph">Effizienzfunktionen sind Formen von Thin Provisioning. Beispielsweise kann eine 100-GB-LUN, die ein 100-GB-Volume belegt, bis zu 50 GB komprimiert werden. Es wurden noch keine tatsächlichen Einsparungen realisiert, da das Volume noch 100 GB beträgt. Das Volume muss zunächst verkleinert werden, damit der eingesparte Speicherplatz an anderer Stelle im System genutzt werden kann. Wenn spätere Änderungen an der 100GB-LUN dazu führen, dass die Daten weniger komprimierbar werden, dann vergrößert sich die LUN und das Volume könnte sich füllen.</block>
  <block id="38ef8b6f2fc332958c293d93c61d7756" category="paragraph">Einige Kunden bevorzugen Thick Provisioning entweder für bestimmte Workloads oder generell basierend auf bestehenden Betriebs- und Beschaffungsmethoden.</block>
  <block id="f8ea8eaa5ca0e58c6a43ec0117b7bd8d" category="paragraph">*Achtung:* Wenn ein Volume mit Thick Provisioning bereitgestellt wird, ist darauf zu achten, dass alle Effizienzfunktionen für dieses Volume, einschließlich Dekomprimierung und Entfernung der Deduplizierung mit dem, vollständig deaktiviert werden<block ref="3cfeff511a11b8c5f54ce9749a719a58" prefix=" " category="inline-code"></block> Befehl. Das Volume sollte nicht in angezeigt werden<block ref="df899f2d908ec33afe1d01ee26b3dd47" prefix=" " category="inline-code"></block> Ausgabe: Ist dies der Fall, ist das Volume für Effizienzfunktionen noch teilweise konfiguriert. Daher funktionieren Überschreibungsgarantien anders. Dies erhöht die Wahrscheinlichkeit, dass Konfigurationsübersehungen dazu führen, dass das Volume unerwartet aus dem Speicherplatz kommt und zu Datenbank-I/O-Fehlern führt.</block>
  <block id="0e7c78164adb6d4650ea685a79d8e99e" category="section-title">Best Practices für Effizienz</block>
  <block id="ec499ad25fa5a765ba43ec2c87819623" category="section-title">AFF-Standards</block>
  <block id="9f7127e69d50bf7b844ff3723b86fbd1" category="section-title">Allgemeine Empfehlungen</block>
  <block id="2c24e4751310e03f119a4df187a3bc08" category="list-text">Wenn Volumes und/oder LUNs nicht über Thin Provisioning bereitgestellt werden, müssen Sie alle Effizienzeinstellungen deaktivieren, da die Verwendung dieser Funktionen keine Einsparungen bietet. Die Kombination von Thick Provisioning mit aktivierter Speicherplatzeffizienz kann zu unerwartetem Verhalten führen, einschließlich Fehlern aufgrund von fehelterem Speicherplatz.</block>
  <block id="4922fe547c351d6a121465da036d478e" category="list-text">Wenn Daten nicht überschrieben werden, wie etwa bei Backups oder Datenbanktransaktionsprotokollen, können Sie die Effizienz steigern, indem Sie TSSE mit einem niedrigen Kühlzeitraum aktivieren.</block>
  <block id="e75fd383e4106eb78482b18896975daf" category="list-text">Einige Dateien enthalten möglicherweise eine beträchtliche Menge an nicht komprimierbaren Daten. Ein Beispiel: Wenn die Komprimierung bereits auf Applikationsebene aktiviert ist, werden Dateien verschlüsselt. Wenn eines dieser Szenarien zutrifft, sollten Sie die Komprimierung deaktivieren, um einen effizienteren Betrieb auf anderen Volumes mit komprimierbaren Daten zu ermöglichen.</block>
  <block id="64d5def835c194cac8afe4fafded756e" category="doc">Richtlinien: Lokale Snapshots</block>
  <block id="baa596d0db91317fa83739a95649752e" category="paragraph">Die erste Version von FabricPool war auf den Backup-Anwendungsfall ausgerichtet. Als einzige Art von Blöcken, die Tiering ermöglichen konnten, handelte es sich um Blöcke, die nicht mehr mit Daten im aktiven File-System verknüpft waren. Daher können nur die Snapshot Datenblöcke auf diese Kapazitäts-Tier verschoben werden. Dies bleibt eine der sichersten Tiering-Optionen, wenn Sie sicherstellen müssen, dass die Performance nie beeinträchtigt wird.</block>
  <block id="b21eb7f0b67c3e27e915183638bf92cf" category="paragraph">Es gibt zwei Optionen für das Tiering inaktiver Snapshot-Blöcke auf die Kapazitäts-Tier. Zunächst einmal die<block ref="8805d416ce540c5f2df34af5b18d742b" prefix=" " category="inline-code"></block> Die Richtlinie zielt nur auf die Snapshot-Blöcke ab. Obwohl der<block ref="9df22f196a33acd0b372fe502de51211" prefix=" " category="inline-code"></block> Die Richtlinie umfasst die<block ref="8805d416ce540c5f2df34af5b18d742b" prefix=" " category="inline-code"></block> Blöcke, sondern auch Tiering Blöcke aus dem aktiven File-System. Dies ist möglicherweise nicht wünschenswert.</block>
  <block id="eeba990304f236b36a9fec2434c77470" category="paragraph">Der<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Der Wert sollte auf einen Zeitraum festgelegt werden, in dem Daten, die während einer Wiederherstellung erforderlich sein könnten, auf der Performance-Tier zur Verfügung stehen. So enthalten die meisten Wiederherstellungsszenarien einer kritischen Produktionsdatenbank zu einem bestimmten Zeitpunkt in den letzten Tagen einen Wiederherstellungspunkt. Einstellung A<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Mit dem Wert 3 würde sichergestellt, dass bei einer Wiederherstellung der Datei eine Datei entsteht, die sofort die maximale Performance liefert. Alle Blöcke in den aktiven Dateien befinden sich immer noch auf schnellem Storage, ohne dass eine Wiederherstellung aus dem Kapazitäts-Tier erforderlich ist.</block>
  <block id="baf531ed01ad56f5614e1a68e8a2e7aa" category="section-title">Richtlinien – replizierte Snapshots</block>
  <block id="27e5121a4c75181022e7be2b738339d0" category="paragraph">Ein Snapshot, der mit SnapMirror oder SnapVault repliziert wird, der nur für die Wiederherstellung verwendet wird, sollte im Allgemeinen die FabricPool verwenden<block ref="a181a603769c1f98ad927e7367c7aa51" prefix=" " category="inline-code"></block> Richtlinie: Bei dieser Richtlinie werden Metadaten repliziert. Alle Datenblöcke werden jedoch sofort an die Kapazitäts-Tier gesendet, was maximale Performance liefert. Die meisten Recovery-Prozesse arbeiten mit sequenziellem I/O, was von vornherein effizient ist. Die Recovery-Zeit vom Zielort des Objektspeichers ist zu bewerten, in einer gut durchdachten Architektur muss dieser Recovery-Prozess jedoch nicht wesentlich langsamer sein als die Wiederherstellung von lokalen Daten.</block>
  <block id="639e481de0ffe002bd83251b26a2540a" category="paragraph">Wenn die replizierten Daten auch für das Klonen verwendet werden sollen, wird der verwendet<block ref="9df22f196a33acd0b372fe502de51211" prefix=" " category="inline-code"></block> Die Politik ist angemessener, mit einem<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Wert, der Daten umfasst, von denen erwartet wird, dass sie regelmäßig in einer Klonumgebung verwendet werden. Der aktive Arbeitsdatensatz einer Datenbank kann beispielsweise Daten enthalten, die in den letzten drei Tagen gelesen oder geschrieben wurden, aber es können auch weitere Verlaufsdaten von 6 Monaten enthalten sein. Wenn ja, dann die<block ref="9df22f196a33acd0b372fe502de51211" prefix=" " category="inline-code"></block> Durch eine Richtlinie am Ziel von SnapMirror wird das Arbeitsdatensatz auf der Performance-Ebene verfügbar.</block>
  <block id="ace0bd0c1be3a9e17d9736d93e7a1da2" category="doc">IOPS QoS</block>
  <block id="786ee59e0cdaa09335e12fea3a168653" category="paragraph">Insbesondere die zunehmende Verbreitung von All-Flash-Storage ermöglicht die Konsolidierung von Workloads. Storage-Arrays mit rotierenden Medien unterstützten in der Regel nur eine begrenzte Anzahl I/O-intensiver Workloads, da die IOPS-Funktionen einer älteren Rotationslaufwerkstechnologie begrenzt sind. Ein oder zwei hochaktive Datenbanken würden die zugrunde liegenden Laufwerke lange sättigen, bevor die Storage-Controller ihre Grenzen erreichten. Das hat sich geändert. Die Performance-Fähigkeit einer relativ kleinen Anzahl von SSD-Laufwerken kann sogar die leistungsstärksten Storage-Controller auslasten. So können Sie alle Funktionen der Controller nutzen, ohne die Gefahr eines plötzlichen Performance-Einbruchs aufgrund von Latenzspitzen der rotierenden Medien befürchten zu müssen.</block>
  <block id="270d79669200ef485add1ecc1833cc94" category="paragraph">Ein einfaches HA-AFF A800 System mit zwei Nodes kann bis zu eine Million zufällige IOPS verarbeiten, bevor die Latenz auf über eine Millisekunde steigt. Für sehr wenige einzelne Workloads wird erwartet, dass sie ein solches Niveau erreichen. Die vollständige Nutzung dieses AFF A800 System-Arrays beinhaltet das Hosting mehrerer Workloads. Für eine sichere Durchführung sind QoS-Kontrollen erforderlich, um die Vorhersehbarkeit zu gewährleisten.</block>
  <block id="4dca0c3547523a2e5bd1b8d0e2ff8de5" category="paragraph">ONTAP bietet zwei Arten von Quality of Service (QoS): IOPS und Bandbreite. QoS-Steuerungen können auf SVMs, Volumes, LUNs und Dateien angewendet werden.</block>
  <block id="3ea539ed2021ba46a3658039e8af419f" category="paragraph">Eine Steuerung der IOPS-QoS basiert offensichtlich auf dem IOPS-Wert einer bestimmten Ressource. Es gibt jedoch eine Reihe von Aspekten der IOPS-QoS, die möglicherweise nicht intuitiv sind. Einige Kunden waren anfangs verwirrt über den scheinbaren Anstieg der Latenz, wenn ein IOPS-Schwellenwert erreicht wurde. Die steigende Latenz ist das natürliche Ergebnis der IOPS-Begrenzung. Logischerweise funktioniert es ähnlich wie ein Token-System. Wenn beispielsweise ein bestimmtes Volume mit Datendateien über ein Limit von 10.000 IOPS verfügt, muss jede eintreffende I/O zuerst ein Token erhalten, um die Verarbeitung fortzusetzen. Solange in einer bestimmten Sekunde nicht mehr als 10.000 Token verbraucht wurden, sind keine Verzögerungen vorhanden. Wenn I/O-Vorgänge auf den Erhalt ihres Tokens warten müssen, wird diese Wartezeit als zusätzliche Latenz angezeigt. Je schwieriger eine Workload das QoS-Limit erreicht, desto länger muss jede I/O in der Warteschlange warten, bis sie verarbeitet wird. Dies scheint für den Benutzer eine höhere Latenz zu sein.</block>
  <block id="34a96885dd7f4501920b13026e7f2cab" category="admonition">Gehen Sie vorsichtig vor, wenn Sie QoS-Kontrollen auf Datenbanktransaktions-/Wiederherstellungsprotokolldaten anwenden. Die Performance-Anforderungen der Wiederherstellungsprotokollierung sind in der Regel viel, viel niedriger als Datendateien, jedoch erfolgt die Aktivität des Wiederherstellungsprotokolls sprunghafter. Die I/O-Vorgänge erfolgen in kurzen Impulsen, und ein QoS-Limit, das für die durchschnittlichen Wiederherstellungs-I/O-Level angemessen erscheint, kann für die tatsächlichen Anforderungen zu niedrig sein. Das Ergebnis können schwerwiegende Performance-Einschränkungen sein, wenn die QoS sich mit jedem Burst des Wiederherstellungsprotokolls befasst. Die Wiederherstellungs- und Archivprotokollierung sollte im Allgemeinen nicht durch QoS beschränkt sein.</block>
  <block id="c79dc76371299e08f4b4ebd609314ca4" category="section-title">Bandbreiten QoS</block>
  <block id="b9d5c520039715de9b997c5a69f4aeaa" category="paragraph">Nicht alle I/O-Größen sind gleich. Beispielsweise führt eine Datenbank möglicherweise eine große Anzahl kleiner Blocklesevorgänge durch, wodurch der IOPS-Schwellenwert erreicht wird. Datenbanken führen aber möglicherweise auch einen vollständigen Tabellenscan durch, der aus einer sehr kleinen Anzahl an großen Blocklesevorgängen bestehen würde, die eine sehr große Menge an Bandbreite verbrauchen, aber relativ wenig IOPS.</block>
  <block id="4930ae83bb8dd5e539dc9f7e73c6d74c" category="paragraph">Ebenso könnte eine VMware Umgebung eine sehr hohe Anzahl zufälliger IOPS während des Startvorgangs verursachen, würde jedoch während eines externen Backups weniger, aber größere I/O-Vorgänge ausführen.</block>
  <block id="eabb5a523a77d4521c12622ee9388aa6" category="paragraph">Manchmal erfordert ein effektives Performance-Management entweder Einschränkungen für die IOPS-Leistung oder die Bandbreite oder sogar beides.</block>
  <block id="13dac55a5f26cefab26368ea5a67e29f" category="section-title">Minimum/garantierte QoS</block>
  <block id="68559688130772c84bb39b2a9ca2c2af" category="paragraph">Viele Kunden wünschen sich eine Lösung mit garantierter QoS, was schwieriger zu erreichen ist, als sie möglicherweise verschwenderisch erscheint. Wenn Sie beispielsweise 10 Datenbanken mit einer Garantie von 10.000 IOPS platzieren, müssen Sie ein System für ein Szenario dimensionieren, in dem alle 10 Datenbanken gleichzeitig mit 10.000 IOPS, also insgesamt 100.000, laufen.</block>
  <block id="c0938f706c890ea85c86388391ffad22" category="paragraph">Eine minimale QoS-Steuerung soll am besten zum Schutz kritischer Workloads eingesetzt werden. Ein Beispiel wäre ein ONTAP Controller mit maximal 500.000 IOPS und einer Kombination aus Produktions- und Entwicklungs-Workloads. Sie sollten maximale QoS-Richtlinien auf Entwicklungs-Workloads anwenden, um zu verhindern, dass eine gegebene Datenbank den Controller für sich einmonopolisiert. Sie würden dann minimale QoS-Richtlinien auf Produktions-Workloads anwenden, um sicherzustellen, dass immer die erforderlichen IOPS bei Bedarf zur Verfügung stehen.</block>
  <block id="acdda1c20fb3985ce840929f7525c803" category="section-title">Anpassungsfähige QoS</block>
  <block id="dd5bc58ed6ab031b170a409470d9a763" category="paragraph">Adaptive QoS bezeichnet die ONTAP-Funktion, bei der die QoS-Begrenzung auf der Kapazität des Storage-Objekts basiert. Sie wird selten bei Datenbanken eingesetzt, da zwischen der Größe einer Datenbank und ihren Performance-Anforderungen in der Regel keine Verknüpfung besteht. Große Datenbanken können nahezu inert sein, während kleinere Datenbanken die IOPS-intensivsten sein können.</block>
  <block id="eb595833577352045f92d14ac4315ad2" category="paragraph">Adaptive QoS kann bei Virtualisierungs-Datastores sehr hilfreich sein, da die IOPS-Anforderungen solcher Datensätze in der Regel mit der Gesamtgröße der Datenbank korrelieren. Ein neuerer Datenspeicher mit 1 TB VMDK-Dateien benötigt wahrscheinlich etwa die Hälfte der Performance als 2-TB-Datenspeicher. Durch anpassungsfähige QoS können Sie die QoS-Limits automatisch vergrößern, wenn der Datastore mit Daten gefüllt wird.</block>
  <block id="23f3d415bb4fe6f99165df604b24da0e" category="paragraph">Tiering ein Datensatz mit FabricPool ergibt eine Abhängigkeit zwischen dem primären Storage Array und der Objektspeicher-Ebene. Es gibt zahlreiche Objektspeicher-Optionen, die eine unterschiedliche Verfügbarkeit bieten. Es ist wichtig, die Auswirkungen eines möglichen Verbindungsverlusts zwischen dem primären Storage-Array und dem Objekt-Storage-Tier zu verstehen.</block>
  <block id="7e856c11a853d276fd5d6274bf60fafd" category="paragraph">Wenn ein an ONTAP ausgehändigt I/O Daten aus der Kapazitäts-Tier benötigt und ONTAP die Kapazitäts-Tier nicht erreichen kann, um Blöcke abzurufen, wird schließlich ein Ausfall des I/O-Systems erreicht. Die Auswirkung dieses Timeouts hängt vom verwendeten Protokoll ab. In einer NFS-Umgebung antwortet ONTAP je nach Protokoll entweder mit einer EJUKEBOX- oder EDELAY-Antwort. Einige ältere Betriebssysteme interpretieren dies möglicherweise als Fehler, aber aktuelle Betriebssysteme und aktuelle Patch-Level des Oracle Direct NFS-Clients behandeln dies als Retrievable-Fehler und warten weiterhin auf den Abschluss des I/O.</block>
  <block id="207d9fe0f57910936bfa043248f3afca" category="paragraph">Ein kürzeres Timeout gilt für SAN-Umgebungen. Wenn ein Block in der Objektspeicherumgebung erforderlich ist und zwei Minuten lang nicht erreichbar bleibt, wird ein Lesefehler an den Host zurückgegeben. Das ONTAP Volume und die LUNs bleiben online, das Host-Betriebssystem kennzeichnet das Filesystem jedoch möglicherweise als fehlerhaft.</block>
  <block id="c9251835da8df507ca4f6ce02d4216be" category="paragraph">Konnektivitätsprobleme bei Objekt-Storage<block ref="8805d416ce540c5f2df34af5b18d742b" prefix=" " category="inline-code"></block> Die Richtlinie ist weniger bedenklich, da nur Backup-Daten als Tiering übertragen werden. Kommunikationsprobleme würden die Datenwiederherstellung verlangsamen, würden jedoch die aktive Nutzung der Daten nicht beeinträchtigen. Der<block ref="9df22f196a33acd0b372fe502de51211" prefix=" " category="inline-code"></block> Und<block ref="a181a603769c1f98ad927e7367c7aa51" prefix=" " category="inline-code"></block> Mithilfe von Richtlinien wird das Tiering „kalter“ Daten von der aktiven LUN ermöglicht. Ein Fehler beim Abrufen von Objektspeicher-Daten kann sich somit auf die Datenbankverfügbarkeit auswirken. Eine SAN-Implementierung mit diesen Richtlinien sollte nur mit Objektspeicher der Enterprise-Klasse und Netzwerkverbindungen genutzt werden, die auf Hochverfügbarkeit ausgelegt sind. NetApp StorageGRID ist die überlegene Option.</block>
  <block id="0f8062f9220efe4f38e7236fe8885379" category="paragraph">Die meisten relationalen Datenbanken arbeiten im Transaktionsprotokoll-Archivierungsmodus, um Point-in-Time Recovery bereitzustellen. Änderungen an den Datenbanken werden durch die Aufzeichnung der Änderungen in den Transaktionsprotokollen vorgenommen und das Transaktionsprotokoll wird ohne Überschreibung beibehalten. Dies kann zur Anforderung führen, eine enorme Menge an archivierten Transaktions-Logs aufzubewahren. Ähnliche Beispiele gibt es bei vielen anderen Applikations-Workflows, die Daten generieren, die aufbewahrt werden müssen, auf die jedoch mit hoher Wahrscheinlichkeit jemals zugegriffen werden wird.</block>
  <block id="229384bbfb1db19c4897d87ba4b9bc2d" category="paragraph">FabricPool löst diese Probleme mit einer einzigen Lösung mit integriertem Tiering. Dateien werden gespeichert und bleiben an ihrem üblichen Speicherort zugänglich, belegen jedoch praktisch keinen Speicherplatz auf dem primären Array.</block>
  <block id="f60aec7aaed8420e09f17a8cfd1d7d37" category="paragraph">Verwenden Sie A<block ref="e5e88684921dd21f3f86c28343a3082f" prefix=" " category="inline-code"></block> Eine Richtlinie von wenigen Tagen führt zur Aufbewahrung von Blöcken in den kürzlich erstellten Dateien (die Dateien sind, die in naher Zukunft am wahrscheinlichsten erforderlich sind) auf der Performance-Tier. Die Datenblöcke aus älteren Dateien werden dann auf die Kapazitäts-Tier verschoben.</block>
  <block id="485e24df9e0ebeecf416daa6e5441bba" category="paragraph">Der<block ref="9df22f196a33acd0b372fe502de51211" prefix=" " category="inline-code"></block> Erzwingt sofortiges Tiering, wenn der Kühlschwellenwert erreicht wurde, unabhängig davon, ob die Protokolle gelöscht wurden oder weiterhin im primären Dateisystem vorhanden sind. Auch das Speichern aller potenziell erforderlichen Protokolle an einer zentralen Stelle im aktiven Filesystem vereinfacht das Management. Es gibt keinen Grund, Snapshots zu durchsuchen, um eine Datei zu finden, die wiederhergestellt werden muss.</block>
  <block id="bbe0132696bc482e0ba7a1f293d80083" category="paragraph">Einige Applikationen, wie z. B. Microsoft SQL Server, schneiden Transaktions-Log-Dateien während von Backup-Vorgängen ab, sodass sich die Protokolle nicht mehr im aktiven File-System befinden. Die Kapazität kann mithilfe des gespeichert werden<block ref="8805d416ce540c5f2df34af5b18d742b" prefix=" " category="inline-code"></block> tiering-Richtlinie, aber die<block ref="9df22f196a33acd0b372fe502de51211" prefix=" " category="inline-code"></block> Die Richtlinie ist für Protokolldaten nicht nützlich, da Protokolldaten im aktiven Dateisystem selten abgekühlt werden sollten.</block>
  <block id="0694b4972669f201ca85f9427a230208" category="doc">MetroCluster ist in 3 verschiedenen Konfigurationen erhältlich</block>
  <block id="948676ffa8073153cbe404795950ad85" category="list-text">HA-Paare mit IP-Konnektivität</block>
  <block id="ef012b81fd45a95681a584a59f8df77e" category="list-text">HA-Paare mit FC-Konnektivität</block>
  <block id="6ffec869ba36dff4e093b5ed8bac798a" category="list-text">Single Controller mit FC-Konnektivität</block>
  <block id="4767099b22895a8101a1cba45d8cf6e9" category="section-title">MetroCluster IP</block>
  <block id="adc397d87c1a7827f2df9fdd362d5ff7" category="paragraph">Die HA-Paar-MetroCluster IP-Konfiguration nutzt zwei oder vier Nodes pro Standort. Diese Konfigurationsoption erhöht die Komplexität und die Kosten im Vergleich zur Option mit zwei Nodes, bietet aber einen wichtigen Vorteil: intrasite-Redundanz. Bei einem einfachen Controller-Ausfall ist kein Datenzugriff über das WAN erforderlich. Der Datenzugriff bleibt über den alternativen lokalen Controller lokal.</block>
  <block id="c15560f2ae5432223c591c5f5d9db5c1" category="paragraph">Die meisten Kunden entscheiden sich für IP-Konnektivität, da die Infrastrukturanforderungen einfacher sind. In der Vergangenheit war die Bereitstellung von ultraschnellen standortübergreifenden Verbindungen über Dark Fibre und FC Switches im Allgemeinen einfacher, heute sind jedoch ultraschnelle IP-Verbindungen mit niedriger Latenz schneller verfügbar.</block>
  <block id="b28c5a429800ea9669a191a402a72b49" category="paragraph">Auch die Architektur ist einfacher, da die einzigen standortübergreifenden Verbindungen für die Controller gelten. Bei FC SAN Attached MetroCluster schreibt ein Controller direkt auf die Laufwerke am entgegengesetzten Standort und benötigt somit zusätzliche SAN-Verbindungen, Switches und Bridges. Ein Controller in einer IP-Konfiguration hingegen schreibt über den Controller auf die entgegengesetzten Laufwerke.</block>
  <block id="457b0075e8f0333975a8e2aaeaceb149" category="inline-link">Architektur und Design der MetroCluster IP-Lösung</block>
  <block id="82aa44c8aa5ebe78f1496c8fc80cb954" category="paragraph">Weitere Informationen finden Sie in der offiziellen ONTAP-Dokumentation und<block ref="03a3c76178d2d43147a2756857a0985e" category="inline-link-rx"></block>.</block>
  <block id="68cf128ce140240c63e9a47e2a82a333" category="paragraph"><block ref="68cf128ce140240c63e9a47e2a82a333" category="inline-image-macro-rx" type="image"></block></block>
  <block id="0d2d438a72f6c558744351265fa49a62" category="section-title">HA-Paar FC SAN Attached MetroCluster</block>
  <block id="3e9133ff590581b29a911d9631ab1737" category="paragraph">Die HA-Paar-Konfiguration von MetroCluster FC nutzt zwei oder vier Nodes pro Standort. Diese Konfigurationsoption erhöht die Komplexität und die Kosten im Vergleich zur Option mit zwei Nodes, bietet aber einen wichtigen Vorteil: intrasite-Redundanz. Bei einem einfachen Controller-Ausfall ist kein Datenzugriff über das WAN erforderlich. Der Datenzugriff bleibt über den alternativen lokalen Controller lokal.</block>
  <block id="7ea740801794ba8a2ce3f87db010c319" category="paragraph"><block ref="7ea740801794ba8a2ce3f87db010c319" category="inline-image-macro-rx" type="image"></block></block>
  <block id="4bc2926f030255369db3e4435de5c774" category="paragraph">Einige Infrastrukturen mehrerer Standorte sind nicht für den aktiv/aktiv-Betrieb konzipiert, sondern werden eher als primärer Standort und Disaster-Recovery-Standort genutzt. In dieser Situation ist eine MetroCluster-Option für HA-Paare aus den folgenden Gründen im Allgemeinen vorzuziehen:</block>
  <block id="5f49f48dd22b405a813e2ef7cc02e9f8" category="list-text">Obwohl es sich bei einem MetroCluster Cluster mit zwei Nodes um ein HA-System handelt, müssen für einen unerwarteten Ausfall eines Controllers oder einer geplanten Wartung die Datenservices am anderen Standort online geschaltet werden. Wenn die Netzwerkverbindung zwischen Standorten die erforderliche Bandbreite nicht unterstützen kann, ist die Performance beeinträchtigt. Die einzige Option wäre auch ein Failover der verschiedenen Host-Betriebssysteme und der damit verbundenen Services zum alternativen Standort. Das HA-Paar MetroCluster Cluster eliminiert dieses Problem, da der Verlust eines Controllers zu einfachem Failover innerhalb desselben Standorts führt.</block>
  <block id="0ce798a4f2e77d6aa1a6e0d718fa8e98" category="list-text">Einige Netzwerktopologien sind nicht für den standortübergreifenden Zugriff ausgelegt, sondern verwenden stattdessen unterschiedliche Subnetze oder isolierte FC-SANs. In diesen Fällen fungiert der MetroCluster Cluster mit zwei Nodes nicht mehr als HA-System, da der alternative Controller keine Daten für die Server am gegenüberliegenden Standort bereitstellen kann. Um vollständige Redundanz zu gewährleisten, ist die MetroCluster Option für das HA-Paar erforderlich.</block>
  <block id="098fa44ef37572e0f8ea717199ac72e6" category="list-text">Wird eine Infrastruktur mit zwei Standorten als eine einzelne hochverfügbare Infrastruktur angesehen, eignet sich die MetroCluster Konfiguration mit zwei Nodes. Falls das System jedoch nach einem Standortausfall über einen längeren Zeitraum hinweg funktionieren muss, ist ein HA-Paar vorzuziehen, da es weiterhin HA innerhalb eines einzelnen Standorts bereitstellen muss.</block>
  <block id="ea8bb3e5a357ca97851352f6944342c9" category="section-title">FC SAN-Attached MetroCluster mit zwei Nodes</block>
  <block id="b759079fef92c80eca80e7349bf84363" category="paragraph">Die MetroCluster Konfiguration mit zwei Nodes verwendet nur einen Node pro Standort. Dieses Design ist einfacher als die Option für HA-Paare, da weniger Komponenten konfiguriert und gewartet werden müssen. Zudem wurden die Infrastrukturanforderungen hinsichtlich Verkabelung und FC-Switching gesenkt. Und schließlich senkt es die Kosten.</block>
  <block id="5bfcbf762ac959212294cdf71bbec2b5" category="paragraph"><block ref="5bfcbf762ac959212294cdf71bbec2b5" category="inline-image-macro-rx" type="image"></block></block>
  <block id="bcf549481eb36a62594e938bca6a10bb" category="paragraph">Ein solches Design hat ganz offensichtlich zur Folge, dass der Controller-Ausfall an einem einzigen Standort dazu führt, dass die Daten am entgegengesetzten Standort verfügbar sind. Diese Einschränkung ist nicht unbedingt ein Problem. Viele Unternehmen verfügen über standortübergreifende Datacenter-Betriebsabläufe mit verteilten, schnellen Netzwerken mit niedriger Latenz, die im Wesentlichen als eine einzige Infrastruktur fungieren. In diesen Fällen ist die MetroCluster Version mit zwei Nodes die bevorzugte Konfiguration. Systeme mit zwei Nodes werden derzeit im Petabyte-Bereich von mehreren Service-Providern eingesetzt.</block>
  <block id="78558542e724655ba80fc4879399f103" category="section-title">Funktionen zur Ausfallsicherheit von MetroCluster</block>
  <block id="5427d2eb91f25a425f13544e56fb12c4" category="paragraph">Es gibt keine Single Points of Failure in einer MetroCluster Lösung:</block>
  <block id="6a988bf07e73bcbf69f62ce9724dd0f3" category="list-text">Jeder Controller verfügt über zwei unabhängige Pfade zu den Laufwerk-Shelfs am lokalen Standort.</block>
  <block id="2c98a75e02c6e103b43724f02e99e393" category="list-text">Jeder Controller verfügt über zwei unabhängige Pfade zu den Laufwerk-Shelfs am Remote-Standort.</block>
  <block id="e18f703538cd1f5af3b9185eed57b49b" category="list-text">Jeder Controller verfügt über zwei unabhängige Pfade zu den Controllern am gegenüberliegenden Standort.</block>
  <block id="800ec0ea85a62c2fb192ac2f79ee1d02" category="list-text">In der HA-Paar-Konfiguration besitzt jeder Controller zwei Pfade zu seinem lokalen Partner.</block>
  <block id="a3d7a569ad6ea771a0ad9b995619251b" category="paragraph">Zusammenfassend lässt sich sagen, dass jede Komponente der Konfiguration entfernt werden kann, ohne dass die Fähigkeit von MetroCluster zur Datenbereitstellung beeinträchtigt wird. Der einzige Unterschied in Bezug auf die Ausfallsicherheit zwischen den beiden Optionen ist, dass die HA-Paar-Version nach einem Standortausfall weiterhin ein insgesamt HA-Storage-System ist.</block>
  <block id="9b4c3976d453e66a3dd22e2793fc3a8e" category="doc">Speicherplatzmanagement</block>
  <block id="e89d3732a2fbc592923c30e54522dcdb" category="paragraph">Thin Provisioning ist in vielerlei Form verfügbar und integraler Bestandteil zahlreicher Funktionen von ONTAP für Enterprise-Applikationsumgebungen. Aus diesem Grund steht Thin Provisioning auch eng mit Effizienztechnologien im Zusammenhang: Mithilfe von Effizienzfunktionen können mehr logische Daten gespeichert werden, als dies technisch auf dem Storage-System möglich ist.</block>
  <block id="537642ed3bfef7f2bb3e580b3b266bd3" category="paragraph">Fast jede Verwendung von Snapshots beinhaltet Thin Provisioning. Zum Beispiel umfasst eine typische 10-TB-Datenbank auf NetApp Storage etwa 30 Tage Snapshots. Diese Anordnung führt dazu, dass ca. 10 TB Daten im aktiven File-System sichtbar sind und 300 TB für Snapshots dediziert. Die insgesamt 310 TB Storage-Kapazität befindet sich in der Regel auf einem Speicherplatz von 12 TB bis 15 TB. Die aktive Datenbank benötigt 10 TB Storage. Die verbleibenden 300 TB an Daten benötigen nur 2 TB bis 5 TB Speicherplatz, da nur die Änderungen an den Originaldaten gespeichert werden.</block>
  <block id="a19609016ad1cb6b7bae7cac796a26a4" category="paragraph">Das Klonen ist ebenfalls ein Beispiel für Thin Provisioning. Ein großer NetApp Kunde hat 40 Klone einer 80-TB-Datenbank für die Entwicklung erstellt. Wenn alle 40 Entwickler, die diese Klone verwenden, jeden Block in jeder Datendatei übergeschrieben haben, wäre mehr als 3,2 PB Storage erforderlich. In der Praxis sind Umsätze gering und der kollektive Platzbedarf liegt bei näher bei 40 TB, da nur Änderungen auf den Laufwerken gespeichert werden.</block>
  <block id="2433af6347e4871be5af0f2af0f6bd6e" category="paragraph">Bei Thin Provisioning in einer Applikationsumgebung ist Vorsicht geboten, da sich die Datenänderungsraten unerwartet erhöhen können. Beispielsweise kann der Speicherplatzverbrauch aufgrund von Snapshots schnell ansteigen, wenn Datenbanktabellen neu indiziert werden, oder es werden umfangreiche Patches für VMware Gäste angewendet. Ein falsch platziertes Backup kann in sehr kurzer Zeit große Datenmengen schreiben. Schließlich kann es schwierig sein, einige Anwendungen wiederherzustellen, wenn ein Dateisystem unerwartet über den freien Speicherplatz verfügt.</block>
  <block id="5cf4dc4d1ce52ff655ea7f1157a72e2e" category="paragraph">Glücklicherweise können diese Risiken mit einer sorgfältigen Konfiguration von behoben werden<block ref="f9e1f2ea1a2becbaf82dd54e55a12e6b" prefix=" " category="inline-code"></block> Und<block ref="24d554277b3513bb6cef3f0a336725a9" prefix=" " category="inline-code"></block> Richtlinien. Mit diesen Optionen kann ein Benutzer Richtlinien erstellen, die automatisch den von Snapshots belegten Speicherplatz freigeben oder ein Volume erweitern, um zusätzliche Daten aufzunehmen. Es stehen zahlreiche Optionen zur Verfügung, und die Anforderungen variieren je nach Kunde.</block>
  <block id="6a28738961e6f6e4b57e4dd70c446600" category="inline-link-macro">Dokumentation des Managements von logischem Storage</block>
  <block id="01570cb5a3aec2324ab56e318e1de04d" category="paragraph">Siehe <block ref="e69bd75bdb49183ee90588843edb33f4" category="inline-link-macro-rx"></block> Für eine vollständige Diskussion dieser Funktionen.</block>
  <block id="e10d46632bc86306cf4c22633ee930ab" category="paragraph">Eine sorgfältige Planung der LVM-Konfiguration kann die Effizienz steigern und den Bedarf an Storage-Bereitstellung und LUN-Anpassung minimieren. Wenn eine LVM wie Veritas VxVM oder Oracle ASM verwendet wird, werden die zugrunde liegenden LUNs in Extents unterteilt, die nur bei Bedarf verwendet werden. Wenn beispielsweise ein Datensatz bei einer Größe von 2 TB beginnt, jedoch im Laufe der Zeit bis auf 10 TB anwachsen könnte, könnte dieser Datensatz auf 10 TB an LUNs platziert werden, die über Thin Provisioning in einer LVM-Festplattengruppe organisiert sind. Zum Zeitpunkt der Erstellung würden nur 2 TB Speicherplatz belegt und zusätzlichen Speicherplatz beanspruchen, wenn Extents zugewiesen werden, um dem Datenwachstum gerecht zu werden. Dieser Prozess ist sicher, solange der Speicherplatz überwacht wird.</block>
  <block id="66a41fd6fc38c454973e91d92cf0e291" category="section-title">Fraktionale Reservierungen</block>
  <block id="e8a206fe65814e0ea465e32487544751" category="paragraph">Die fraktionale Reserve bezieht sich auf das Verhalten einer LUN in einem Volume in Bezug auf die Platzeffizienz. Wenn die Option<block ref="c1acac5c4332e37ec1f281fcbaf0765a" prefix=" " category="inline-code"></block> Ist auf 100 % festgelegt, können alle Daten im Volume mit jedem Datenmuster 100 % Umsatz verzeichnen, ohne Speicherplatz auf dem Volume zu belegen.</block>
  <block id="a3cd28aef7d0becd57b60187fdc4a24e" category="paragraph">Betrachten Sie beispielsweise eine Datenbank auf einer einzigen 250 GB LUN und einem Volume mit 1 TB. Wenn ein Snapshot erstellt wird, würde sofort eine zusätzliche 250GB an Speicherplatz auf dem Volume reserviert werden, um zu garantieren, dass auf dem Volume aus irgendeinem Grund nicht mehr genügend Speicherplatz verfügbar ist. Die Verwendung von fraktionalen Reserven ist im Allgemeinen aufwändig, da es äußerst unwahrscheinlich ist, dass jedes Byte im Datenbank-Volume überschrieben werden müsste. Es gibt keinen Grund, Platz für ein Ereignis zu reservieren, das nie passiert. Wenn ein Kunde jedoch den Speicherplatzverbrauch in einem Storage-System nicht überwachen kann und sicher sein muss, dass der Platz nie knapp wird, wären für die Nutzung von Snapshots 100 % fraktionale Reservierungen erforderlich.</block>
  <block id="e30fbe8981929fb070a820a213381bab" category="section-title">Komprimierung und Deduplizierung</block>
  <block id="1b6e4e9a2be031edc6f3da1f559ef884" category="paragraph">Komprimierung und Deduplizierung sind beide Formen von Thin Provisioning. Beispielsweise kann ein 50 TB Platzbedarf für Daten auf 30 TB komprimiert werden, was zu Einsparungen von 20 TB führt. Um die Komprimierung nutzen zu können, müssen einige dieser 20 TB für andere Daten verwendet werden. Alternativ muss das Storage-System mit weniger als 50 TB erworben werden. Das Ergebnis sind Speicherung von mehr Daten als technisch auf dem Speichersystem verfügbar ist. Aus Sicht der Daten gibt es 50 TB an Daten, obwohl diese auf den Laufwerken nur 30 TB belegen.</block>
  <block id="dbca4bb791f36eac2c11c32f73f2b4ec" category="paragraph">Es besteht immer die Möglichkeit, dass sich die Komprimierbarkeit eines Datensatzes ändert. Dies würde zu einem erhöhten Verbrauch an echtem Speicherplatz führen. Dieser Anstieg des Verbrauchs bedeutet, dass die Komprimierung wie bei anderen Thin Provisioning-Methoden zur Überwachung und Nutzung gemanagt werden muss<block ref="f9e1f2ea1a2becbaf82dd54e55a12e6b" prefix=" " category="inline-code"></block> Und<block ref="24d554277b3513bb6cef3f0a336725a9" prefix=" " category="inline-code"></block>.</block>
  <block id="ad241666a88b5414e8c2fe6d1cc1edb0" category="paragraph">Die Komprimierung und Deduplizierung werden im Abschnitt Link:efficiency.html ausführlicher behandelt</block>
  <block id="8ecfcadfebb472a089481e8554ebed87" category="section-title">Komprimierung und fraktionale Reservierungen</block>
  <block id="4f8fcc146d2e8e712dcd8c56b1684fcc" category="paragraph">Komprimierung ist eine Form von Thin Provisioning. Fraktionale Reservierungen beeinflussen die Komprimierung. Ein wichtiger Hinweis: Vor der Snapshot-Erstellung wird Speicherplatz reserviert. Normalerweise ist eine fraktionale Reserve nur wichtig, wenn ein Snapshot vorhanden ist. Wenn es keinen Snapshot gibt, ist die fraktionale Reserve nicht wichtig. Dies ist bei der Komprimierung nicht der Fall. Wenn eine LUN auf einem Volume mit Komprimierung erstellt wird, behält ONTAP den Speicherplatz bei, um einen Snapshot aufzunehmen. Dieses Verhalten kann während der Konfiguration verwirrend sein, aber es wird erwartet.</block>
  <block id="e26f8048b4bf60dad827c5a425271361" category="paragraph">Als Beispiel betrachten Sie ein 10GB Volume mit einer 5GB LUN, die bis auf 2,5 GB ohne Snapshots komprimiert wurde. Betrachten wir die beiden folgenden Szenarien:</block>
  <block id="ade18971a2dab7e37335e95d81da0ef1" category="list-text">Die fraktionale Reserve = 100 ergibt eine Auslastung von 7,5 GB</block>
  <block id="1d07b8e7c92488a55301ea201328cf35" category="list-text">Die fraktionale Reserve = 0 ergibt eine Auslastung von 2,5 GB</block>
  <block id="aa410d0e46ec94b6efcf344f0d870c1a" category="paragraph">Das erste Szenario umfasst 2,5 GB Speicherplatzverbrauch für aktuelle Daten und 5 GB Speicherplatz, um 100 % des Umsatzes der Quelle in Erwartung der Snapshot-Nutzung zu berücksichtigen. Das zweite Szenario reserviert keinen zusätzlichen Speicherplatz.</block>
  <block id="e79414ad391b126cc9bd53af1624c8de" category="paragraph">Obwohl diese Situation verwirrend erscheinen mag, ist es unwahrscheinlich, dass sie in der Praxis angetroffen wird. Komprimierung impliziert Thin Provisioning, und Thin Provisioning in einer LUN-Umgebung erfordert nur fraktionale Reservierungen. Es ist immer möglich, dass komprimierte Daten durch eine nicht komprimierbare Funktion überschrieben werden. Aus diesem Grund muss ein Volume für die Komprimierung bereitgestellt werden, um mögliche Einsparungen zu erzielen.</block>
  <block id="499a10b92cb9ca6b3412d4bf5a91a2b0" category="paragraph">*NetApp empfiehlt* die folgenden Reservekonfigurationen:</block>
  <block id="7f26e875cee50c1b3baf71a014043794" category="list-text">Einstellen<block ref="c1acac5c4332e37ec1f281fcbaf0765a" prefix=" " category="inline-code"></block> Auf 0, wenn die grundlegende Kapazitätsüberwachung zusammen mit eingerichtet ist<block ref="f9e1f2ea1a2becbaf82dd54e55a12e6b" prefix=" " category="inline-code"></block> Und<block ref="24d554277b3513bb6cef3f0a336725a9" prefix=" " category="inline-code"></block>.</block>
  <block id="675996bd34ec0d8c134215f5be73fb3a" category="list-text">Einstellen<block ref="c1acac5c4332e37ec1f281fcbaf0765a" prefix=" " category="inline-code"></block> Zu 100, wenn es keine Überwachungsfähigkeit gibt oder wenn es unmöglich ist, unter keinen Umständen Raum abzulassen.</block>
  <block id="8ee38535add58992c14b23a6f7cf8d27" category="doc">LIF-Typen</block>
  <block id="50d034374ef68094a0f6c8e8d06a77af" category="inline-link-macro">Dokumentation zum ONTAP-Netzwerkmanagement</block>
  <block id="fc25025f3f0a2afd42465a55f35c02d0" category="paragraph">Dieser Abschnitt bietet einen Überblick über die wichtigsten LIF-Designprinzipien. Eine ausführlichere Dokumentation finden Sie im <block ref="a3e6361e1a52d384f542a2f2d8bc9bb1" category="inline-link-macro-rx"></block>. Wie andere Aspekte der Datenbankarchitektur hängen die besten Optionen für die Storage Virtual Machine (SVM, in der CLI als vServer bezeichnet) und das Design der logischen Schnittstelle (LIF) stark von den Skalierungsanforderungen und den geschäftlichen Anforderungen ab.</block>
  <block id="be0801f7fb241be642bec73b799d45cb" category="paragraph">Berücksichtigen Sie bei der Entwicklung einer LIF-Strategie die folgenden primären Themen:</block>
  <block id="362c2978dddcf6988ce266d3d60f5beb" category="list-text">*Leistung.* ist die Netzwerkbandbreite ausreichend?</block>
  <block id="cfc88efb72b2c7ea83a4a3d68b760d55" category="list-text">*Ausfallsicherheit.* gibt es Single Points of Failure im Design?</block>
  <block id="af02972fd4dc1e949d800731d9518812" category="list-text">*Verwaltbarkeit.* kann das Netzwerk unterbrechungsfrei skaliert werden?</block>
  <block id="fdcbadc80d2dd31527cc91cf81fb725d" category="paragraph">Diese Themen beziehen sich auf die End-to-End-Lösung, vom Host über die Switches bis zum Speichersystem.</block>
  <block id="f45fe079103703d0b315ff2e339728fd" category="inline-link-macro">ONTAP-Dokumentation zu LIF-Typen</block>
  <block id="cc4583278db25277db9ddcd781cbf992" category="paragraph">Es gibt mehrere LIF-Typen. <block ref="9d27879d9d2bd4f0f081ffed63159522" category="inline-link-macro-rx"></block> Stellen Sie umfassendere Informationen zu diesem Thema bereit, LIFs können jedoch aus funktionaler Sicht in die folgenden Gruppen unterteilt werden:</block>
  <block id="f34a52d788c5a4cf56d7ae8ccb85f8c2" category="list-text">*Cluster- und Node-Management-LIFs.* LIFs, die zum Verwalten des Storage-Clusters verwendet werden.</block>
  <block id="73ac4e00a9ff3caf31e6df13a6df2f78" category="list-text">*SVM-Management-LIFs.* Schnittstellen, die den Zugriff auf eine SVM über die REST-API oder ONTAPI (auch bekannt als ZAPI) für Funktionen wie Snapshot-Erstellung oder Volume-Anpassung erlauben. Produkte wie SnapManager für Oracle (SMO) müssen Zugriff auf eine SVM-Management-LIF haben.</block>
  <block id="b7010e46f4769f638197aa230f448ba0" category="list-text">*Daten-LIFs.* Schnittstellen für FC, iSCSI, NVMe/FC, NVMe/TCP, NFS, oder SMB/CIFS-Daten.</block>
  <block id="1eb4f53fa8ee8f0954514a1b9858bf6b" category="admonition">Eine logische Datenschnittstelle für NFS-Datenverkehr kann durch Änderung der Firewallrichtlinie von auch zum Management verwendet werden<block ref="8d777f385d3dfec8815d20f7496026dc" prefix=" " category="inline-code"></block> Bis<block ref="d724f231a9a7b89757c4394d6622bc47" prefix=" " category="inline-code"></block> Oder eine andere Richtlinie, die HTTP, HTTPS oder SSH erlaubt. Diese Änderung kann die Netzwerkkonfiguration vereinfachen, indem die Konfiguration jedes Hosts für den Zugriff auf die LIF der NFS-Daten und eine separate Management-LIF vermieden wird. Es ist nicht möglich, eine Schnittstelle für iSCSI- und Managementverkehr zu konfigurieren, obwohl beide ein IP-Protokoll verwenden. In iSCSI-Umgebungen ist eine separate Management-LIF erforderlich.</block>
  <block id="85d465743ad38be108b50648ab283d78" category="section-title">Design von SAN LIF</block>
  <block id="8800a54085e1b2987d83537423221cf9" category="paragraph">Das LIF-Design in einer SAN-Umgebung ist aus einem Grund relativ einfach: Multipathing. Alle modernen SAN-Implementierungen ermöglichen es einem Client, über mehrere unabhängige Netzwerkpfade auf Daten zuzugreifen und den optimalen Pfad oder die besten Pfade für den Zugriff auszuwählen. So lässt sich die Performance in Bezug auf LIF-Design einfacher bewältigen, da SAN-Clients automatisch den I/O-Lastausgleich über die besten verfügbaren Pfade durchführen.</block>
  <block id="e33c923a4d6280bf6f2b2a982ff74ee3" category="paragraph">Wenn ein Pfad nicht mehr verfügbar ist, wählt der Client automatisch einen anderen Pfad aus. Das daraus resultierende einfache Design macht SAN LIFs im Allgemeinen einfacher zu managen. Das bedeutet nicht, dass eine SAN-Umgebung immer einfacher zu managen ist, da viele andere Aspekte des SAN-Storage viel komplizierter sind als NFS. Es bedeutet schlichtweg, dass das LIF-Design von SAN einfacher ist.</block>
  <block id="9446a98ad14416153cc4d45ab8b531bf" category="section-title">Leistung</block>
  <block id="141614e246b14131ec01a449d61ed657" category="paragraph">Der wichtigste Aspekt bei der LIF-Performance in einer SAN-Umgebung ist die Bandbreite. In einem ONTAP AFF-Cluster mit zwei Nodes mit zwei 16-GB-FC-Ports pro Node können beispielsweise bis zu 32 GB Bandbreite von jedem Node bereitgestellt werden.</block>
  <block id="02bc0523497de72834c4c7990e32d155" category="section-title">Ausfallsicherheit</block>
  <block id="01b1d8276ca14813ff4804089a9ac082" category="paragraph">SAN LIFs führen keinen Failover auf einem AFF Storage-System durch. Wenn eine SAN-LIF aufgrund eines Controller-Failovers ausfällt, erkennt die Multipathing-Software des Clients den Verlust eines Pfads und leitet den I/O an eine andere LIF um. Bei ASA Storage-Systemen wird für LIFs nach kurzer Verzögerung ein Failover durchgeführt. Die I/O wird jedoch nicht unterbrochen, da auf dem anderen Controller bereits aktive Pfade vorhanden sind. Der Failover-Prozess erfolgt, um den Hostzugriff auf allen definierten Ports wiederherzustellen.</block>
  <block id="b7c94ef25c0629a65f8a1f6ce7014d95" category="section-title">Managebarkeit</block>
  <block id="1325935a6fa0e88cfec45038005f18e3" category="paragraph">Die LIF-Migration ist in einer NFS-Umgebung viel üblicher, da die LIF-Migration häufig mit dem Verschieben von Volumes innerhalb des Clusters verknüpft ist. Wenn Volumes innerhalb des HA-Paars verschoben werden, ist keine Migration einer LIF in eine SAN-Umgebung erforderlich. Der Grund dafür ist, dass ONTAP nach Abschluss der Volume-Verschiebung eine Benachrichtigung über eine Pfadänderung an das SAN sendet, die die SAN-Clients automatisch neu optimieren. Die LIF-Migration mit SAN steht in erster Linie in Verbindung mit größeren Änderungen an physischer Hardware. Wenn beispielsweise ein unterbrechungsfreies Upgrade der Controller erforderlich ist, wird eine SAN LIF auf die neue Hardware migriert. Wenn ein FC-Port defekt ist, kann eine LIF zu einem nicht verwendeten Port migriert werden.</block>
  <block id="dbb887b1373e51dab774976c5a556964" category="section-title">Designempfehlungen</block>
  <block id="076872b2444637e6142c3c6f0f157423" category="paragraph">NetApp gibt die folgenden Empfehlungen:</block>
  <block id="d8d6bca7c981c13479781da85883cfef" category="list-text">Erstellen Sie nicht mehr Pfade, als erforderlich sind. Eine übermäßige Anzahl von Pfaden erschwert das gesamte Management und kann zu Problemen mit dem Pfad-Failover auf einigen Hosts führen. Darüber hinaus weisen einige Hosts unerwartete Pfadeinschränkungen für Konfigurationen wie das Booten von SAN auf.</block>
  <block id="bd662d86fcca370cddf188ff5378fb1b" category="list-text">Nur sehr wenige Konfigurationen sollten mehr als vier Pfade zu einem LUN erfordern. Der Wert von mehr als zwei Nodes, um LUNs bekannt zu machen, ist beschränkt, da das Aggregat, das eine LUN hostet, nicht zugänglich ist, wenn der Node, der Eigentümer der LUN und dessen HA-Partner ausfällt. In solch einem Szenario ist es nicht hilfreich, Pfade auf anderen Nodes als dem primären HA-Paar zu erstellen.</block>
  <block id="8873f044ab7c082f16372e4b8bdc6e03" category="list-text">Obwohl die Anzahl der sichtbaren LUN-Pfade durch Auswählen der in FC-Zonen enthaltenen Ports gemanagt werden kann, ist es im Allgemeinen einfacher, alle potenziellen Zielpunkte in die FC-Zone aufzunehmen und die LUN-Sichtbarkeit auf ONTAP-Ebene zu kontrollieren.</block>
  <block id="0ebc6267761306c4daefb9871a470e4b" category="list-text">In ONTAP 8.3 und höher ist die Funktion für die selektive LUN-Zuordnung (SLM) die Standardeinstellung. Bei SLM wird jede neue LUN automatisch von dem Node bereitgestellt, dem das zugrunde liegende Aggregat und der HA-Partner des Node gehören. Durch diese Anordnung müssen keine Portsätze erstellt oder Zoning konfiguriert werden, um den Zugriff auf den Port zu beschränken. Jede LUN ist mit der Mindestanzahl der Nodes verfügbar, die für eine optimale Performance und Stabilität erforderlich sind.
*Falls eine LUN außerhalb der beiden Controller migriert werden muss, können die zusätzlichen Knoten mit dem hinzugefügt werden<block ref="02d0bbdc14988c85bbd7994723f4dd7f" prefix=" " category="inline-code"></block> Befehl, sodass die LUNs auf den neuen Nodes angekündigt werden. Dadurch werden zusätzliche SAN-Pfade zu den LUNs für die LUN-Migration erstellt. Der Host muss jedoch einen Erkennungsvorgang durchführen, um die neuen Pfade verwenden zu können.</block>
  <block id="cc4b24f288afae5889c6061466dda472" category="list-text">Seien Sie nicht übermäßig besorgt über indirekten Verkehr. Es empfiehlt sich, in einer sehr I/O-intensiven Umgebung, in der jede Mikrosekunde von großer Latenz ist, indirekten Verkehr zu vermeiden, aber der sichtbare Performance-Effekt ist bei typischen Workloads zu vernachlässigen.</block>
  <block id="f54fccb82abaa995eb9f86264f431505" category="section-title">LIF-Design von NFS</block>
  <block id="e68dee93f11ae71aebd7a1e4c2abfc5a" category="paragraph">Im Gegensatz zu SAN-Protokollen kann bei NFS nur bedingt mehrere Pfade zu Daten definiert werden. Die parallelen NFS-Erweiterungen (pNFS) zu NFSv4 beheben diese Einschränkung. Da die ethernet-Geschwindigkeit jedoch 100 GB erreicht hat, ist das Hinzufügen weiterer Pfade selten ein Nutzen.</block>
  <block id="391dc675ec3d853200129b799ed5923e" category="section-title">Performance und Ausfallsicherheit</block>
  <block id="a0bd34f908e3947aa4aba15382354836" category="paragraph">Obwohl die Messung der LIF-Performance in erster Linie dazu dient, die gesamte Bandbreite von allen primären Pfaden zu berechnen, muss die Bestimmung der Performance von NFS LIF genau die Netzwerkkonfiguration durchgeführt werden. Beispielsweise können zwei 10-Gbit-Ports als physische Rohports konfiguriert oder als LACP-Interface-Gruppe (Link Aggregation Control Protocol) konfiguriert werden. Wenn sie als Schnittstellengruppe konfiguriert sind, stehen mehrere Load-Balancing-Richtlinien zur Verfügung, die je nachdem, ob der Datenverkehr geswitcht oder geroutet wird, unterschiedlich funktionieren. Oracle Direct NFS (dNFS) bietet Load-Balancing-Konfigurationen, die derzeit in keinen NFS-Clients des Betriebssystems vorhanden sind.</block>
  <block id="e500ca27c6b92454c989d80e0445c359" category="paragraph">Im Gegensatz zu SAN-Protokollen erfordern NFS-Filesysteme Ausfallsicherheit auf Protokollebene. Beispielsweise wird eine LUN immer mit aktiviertem Multipathing konfiguriert, was bedeutet, dass dem Storage-System mehrere redundante Kanäle zur Verfügung stehen, von denen jeder das FC-Protokoll verwendet. Ein NFS-Dateisystem hingegen hängt von der Verfügbarkeit eines einzelnen TCP/IP-Kanals ab, der nur auf der physischen Ebene geschützt werden kann. Diese Anordnung ist, warum Optionen wie Port-Failover und LACP Port-Aggregation existieren.</block>
  <block id="35926e2ada969ad80fd3b16c0cd7d35a" category="paragraph">In einer NFS-Umgebung werden sowohl Performance als auch Ausfallsicherheit auf der Netzwerkprotokollebene bereitgestellt. Dadurch sind beide Themen miteinander verflochten und müssen gemeinsam diskutiert werden.</block>
  <block id="84f4a5dfaa9613a469cd783c353a186c" category="section-title">Binden Sie LIFs an Portgruppen</block>
  <block id="44bfb87c11d83c1b91c3589f2081e7ca" category="paragraph">Um ein LIF an eine Portgruppe zu binden, ordnen Sie die LIF-IP-Adresse einer Gruppe physischer Ports zu. Die primäre Methode zur Aggregation physischer Ports ist LACP. Die Fehlertoleranz-Funktion von LACP ist ziemlich einfach. Jeder Port in einer LACP-Gruppe wird überwacht und im Falle einer Störung aus der Portgruppe entfernt. Es gibt jedoch viele Missverständnisse darüber, wie LACP in Bezug auf Performance funktioniert:</block>
  <block id="19a43026949fb6ef1017a54cf7c94f9b" category="list-text">Für LACP ist keine Konfiguration auf dem Switch erforderlich, um mit dem Endpunkt übereinstimmen zu können. Beispielsweise kann ONTAP mit IP-basiertem Lastausgleich konfiguriert werden, während ein Switch MAC-basierten Lastausgleich verwenden kann.</block>
  <block id="86501252b34395de5303a65f5e9943e3" category="list-text">Jeder Endpunkt, der eine LACP-Verbindung verwendet, kann den Port für die Paketübertragung unabhängig auswählen, jedoch nicht den für den Empfang verwendeten Port auswählen. Das bedeutet, dass Datenverkehr von ONTAP zu einem bestimmten Ziel an einen bestimmten Port gebunden ist, und der Rückverkehr könnte auf einer anderen Schnittstelle eintreffen. Dies verursacht jedoch keine Probleme.</block>
  <block id="298ca40b99dd3540dcd82ba5537f3f8d" category="list-text">LACP verteilt den Datenverkehr nicht ständig gleichmäßig. In einer großen Umgebung mit vielen NFS-Clients wird normalerweise sogar alle Ports in einer LACP-Aggregation genutzt. Jedoch ist jedes ein NFS-Dateisystem in der Umgebung auf die Bandbreite von nur einem Port beschränkt, nicht die gesamte Aggregation.</block>
  <block id="d071b5ccd1569aace97a0b4d6c0a71ac" category="list-text">Obwohl LACP-Richtlinien für die Robin-Lösung auf ONTAP verfügbar sind, adressieren diese Richtlinien nicht die Verbindung von einem Switch zu einem Host. Beispielsweise ist eine Konfiguration mit einem LACP Trunk mit vier Ports auf einem Host und einem LACP Trunk mit vier Ports auf einem ONTAP immer noch nur in der Lage, ein Filesystem über einen einzelnen Port zu lesen. Obwohl ONTAP Daten über alle vier Ports übertragen kann, sind derzeit keine Switch-Technologien verfügbar, die über alle vier Ports vom Switch an den Host gesendet werden. Es wird nur eine verwendet.</block>
  <block id="6e9e98dfaea1ca7860606f1fc05e414a" category="paragraph">In größeren Umgebungen, die aus vielen Datenbank-Hosts bestehen, ist der geläufigste Ansatz, mithilfe eines IP-Lastausgleichs ein LACP Aggregat mit einer entsprechenden Anzahl von 10 GB (oder schneller) Schnittstellen zu erstellen. Mit diesem Ansatz kann ONTAP sogar die Nutzung aller Ports ermöglichen, sofern genügend Clients vorhanden sind. Der Lastausgleich wird unterbrochen, wenn weniger Clients in der Konfiguration vorhanden sind, da LACP Trunking die Last nicht dynamisch neu verteilt.</block>
  <block id="7815da600ab459d2b05c5792f2271c1a" category="paragraph">Wenn eine Verbindung hergestellt wird, wird der Datenverkehr in eine bestimmte Richtung nur an einem Port platziert. Beispielsweise liest eine Datenbank, die einen vollständigen Tabellenscan gegen ein NFS-Dateisystem durchführt, das über einen LACP-Trunk mit vier Ports verbunden ist, Daten über nur eine Netzwerkkarte (NIC). Wenn sich nur drei Datenbankserver in einer solchen Umgebung befinden, ist es möglich, dass alle drei vom gleichen Port lesen, während die anderen drei Ports inaktiv sind.</block>
  <block id="153f704ea16440133d51b1877af2a657" category="section-title">Binden Sie LIFs an physische Ports</block>
  <block id="70e655aa3c03f840747c5272142b3527" category="paragraph">Das Binden einer LIF an einen physischen Port führt zu einer granulareren Kontrolle der Netzwerkkonfiguration, da eine gegebene IP-Adresse auf einem ONTAP-System jeweils nur mit einem Netzwerk-Port verknüpft ist. Stabilität wird dann durch die Konfiguration von Failover-Gruppen und Failover-Richtlinien erreicht.</block>
  <block id="218f00463f5281b8ead536582c69ac3a" category="section-title">Failover-Richtlinien und Failover-Gruppen</block>
  <block id="0a53c26a1bc4036c84fc3b4b63542744" category="inline-link-macro">ONTAP Netzwerkmanagement-Dokumentation für Failover-Gruppen und Richtlinien</block>
  <block id="f29ae31443947c9782482ee9683dbc0e" category="paragraph">Das Verhalten von LIFs wird während der Netzwerkunterbrechung durch Failover-Richtlinien und Failover-Gruppen gesteuert. Die Konfigurationsoptionen wurden mit den verschiedenen Versionen von ONTAP geändert. Konsultieren Sie die <block ref="96c8cd47b667604fb9991ace7d2eff29" category="inline-link-macro-rx"></block> Finden Sie spezifische Details zur implementierten Version von ONTAP.</block>
  <block id="0f2f345dd1247ae428cee703c1bf1c84" category="paragraph">ONTAP 8.3 und höher ermöglichen das Management von LIF-Failovers basierend auf Broadcast-Domänen. Daher kann ein Administrator alle Ports definieren, die Zugriff auf ein bestimmtes Subnetz haben, und ONTAP erlauben, eine entsprechende Failover-LIF auszuwählen. Einige Kunden verwenden diesen Ansatz durchaus, weist jedoch aufgrund der mangelnden Planbarkeit in einer Storage-Netzwerkumgebung mit hoher Geschwindigkeit Einschränkungen auf. Beispielsweise kann eine Umgebung sowohl 1-Gbit-Ports für routinemäßigen Filesystem-Zugriff als auch 10-Gbit-Ports für Datendatei-I/O. Wenn beide Ports in derselben Broadcast-Domäne vorhanden sind, kann ein LIF-Failover dazu führen, Datendatei-I/O von einem 10-GB-Port auf einen 1-GB-Port zu verschieben.</block>
  <block id="ced0729d57c08312b2b12382f2147d26" category="paragraph">Zusammenfassend lassen sich die folgenden Vorgehensweisen berücksichtigen:</block>
  <block id="57cb6a7ba8acb7faa31cc5219bae83b3" category="list-text">Konfigurieren Sie eine Failover-Gruppe als benutzerdefiniert.</block>
  <block id="b444855eccae552d7fb79bf3d31a4f7b" category="list-text">Füllen Sie die Failover-Gruppe mit Ports am Partner-Controller für Storage Failover (SFO), damit die LIFs beim Storage Failover den Aggregaten folgen. Dadurch wird die Erstellung indirekter Verkehrsströme vermieden.</block>
  <block id="86c48b21d166ed8e440a07e1d4a0b15d" category="list-text">Verwenden Sie Failover-Ports, deren Performance-Merkmale mit der ursprünglichen logischen Schnittstelle übereinstimmen. Beispielsweise sollte eine LIF auf einem einzelnen physischen 10-Gbit-Port eine Failover-Gruppe mit einem einzelnen 10-Gbit-Port enthalten. Ein LACP LIF mit vier Ports sollte ein Failover auf eine andere LACP LIF mit vier Ports durchführen. Diese Ports wären eine Teilmenge der Ports, die in der Broadcast-Domäne definiert sind.</block>
  <block id="3e839a73f7dc1262508f23a89628ca2d" category="list-text">Setzen Sie die Failover-Richtlinie auf nur SFO-Partner. Dadurch wird sichergestellt, dass die LIF während des Failovers dem Aggregat folgt.</block>
  <block id="85e82ce4c9842f978fa774c4fe96b14c" category="section-title">Autom. Rücksetzung</block>
  <block id="3dc8610f78478640e47688101cecbfad" category="paragraph">Stellen Sie die ein<block ref="9f1b7141f09f19c4e33409646aef43cf" prefix=" " category="inline-code"></block> Parameter wie gewünscht. Die meisten Kunden bevorzugen es, diesen Parameter auf zu setzen<block ref="b326b5062b2f0e69046810717534cb09" prefix=" " category="inline-code"></block> Um das LIF auf seinen Home Port zurückzusetzen. In einigen Fällen haben Kunden dies jedoch auf `false `so gesetzt, dass ein unerwartetes Failover untersucht werden kann, bevor eine LIF an ihren Home Port zurückgegeben wird.</block>
  <block id="edf6d4c299bc2890ba63c7eb3ef8b4d2" category="section-title">LIF-Volume-Verhältnis</block>
  <block id="9c75becef8c432a0152a40c7c037a04c" category="paragraph">Ein weit verbreitetes Missverständnis ist, dass es eine 1:1 Beziehung zwischen Volumes und NFS LIFs geben muss. Diese Konfiguration ist zwar erforderlich, um ein Volume ohne zusätzlichen Interconnect-Verkehr an eine beliebige Stelle in einem Cluster zu verschieben, ist jedoch kategorisch keine Anforderung. Der Intercluster-Datenverkehr muss berücksichtigt werden, aber die bloße Anwesenheit von Intercluster-Datenverkehr verursacht keine Probleme. Viele der für ONTAP veröffentlichten Benchmarks sind überwiegend indirekte I/O-Vorgänge</block>
  <block id="2b4ebcd522b07b80d3fd76b301c506e8" category="paragraph">Ein Datenbankprojekt mit einer relativ kleinen Anzahl Performance-kritischer Datenbanken, für die nur insgesamt 40 Volumes benötigt wurden, könnte beispielsweise eine LIF-Strategie für das 1:1 Volume rechtfertigen. Dieses Arrangement würde 40 IP-Adressen erfordern. Jedes Volume könnte dann zusammen mit der zugehörigen LIF an jeden beliebigen Ort im Cluster verschoben werden. Der Datenverkehr würde dann immer direkt erfolgen, wodurch jede Latenzquelle sogar auf Mikrosekunden-Ebene minimiert wird.</block>
  <block id="38bad4911bcdf1117d37eb1195566f8c" category="paragraph">Zählerbeispiel: Eine große, gehostete Umgebung kann durch eine 1:1:1-Beziehung zwischen Kunden und LIFs einfacher gemanagt werden. Im Laufe der Zeit muss ein Volume möglicherweise auf einen anderen Node migriert werden, was zu einem indirekten Traffic führen würde. Der Performance-Effekt sollte jedoch nicht nachweisbar sein, es sei denn, die Netzwerk-Ports auf dem Interconnect-Switch sind voll ausgelastet. Falls Bedenken bestehen, kann eine neue LIF auf zusätzlichen Nodes erstellt werden, und der Host kann im nächsten Wartungsfenster aktualisiert werden, um indirekten Traffic aus der Konfiguration zu entfernen.</block>
  <block id="d75b0d8f723c3e97eaa4ca39630c7420" category="doc">MetroCluster und mehrere Aggregate</block>
  <block id="df5855269b493a27a5565dfd9f2584f0" category="list-text">Unter normalen Bedingungen werden eingehende Schreibvorgänge an einen bestimmten Controller synchron mit seinem Partner gespiegelt. In einer NetApp MetroCluster-Umgebung werden Schreibvorgänge auch auf einem Remote Controller gespiegelt. Bis ein Schreibvorgang auf nicht-flüchtigen Medien an allen Standorten gespeichert wird, wird er für die Host-Applikation nicht bestätigt.</block>
  <block id="05b60db315aa45e1524b7a6ffa242fa8" category="list-text">Das Medium, auf dem die Schreibdaten gespeichert sind, wird als nichtflüchtiger Speicher oder NVMEM bezeichnet. Gelegentlich wird dieser Speicher auch als NVRAM (Nonvolatile Random Access Memory) bezeichnet. Er kann als Schreib-Cache verwendet werden, obwohl er als Journal fungiert. Im normalen Betrieb werden die Daten von NVMEM nicht gelesen, sondern nur zum Schutz der Daten bei einem Software- oder Hardwareausfall verwendet. Wenn die Daten auf die Laufwerke geschrieben werden, werden die Daten vom RAM im System und nicht von NVMEM übertragen.</block>
  <block id="e6dd8b559da6e046f2043043a55faafc" category="list-text">Während eines Übernahmevorgangs übernimmt ein Node in einem Hochverfügbarkeitspaar (HA) den Betrieb seines Partners. Eine Umschaltung ist im Wesentlichen dieselbe, gilt aber für MetroCluster Konfigurationen, bei denen ein Remote Node die Funktionen eines lokalen Node übernimmt.</block>
  <block id="33c0c466972aae7427262e3075a2ed07" category="paragraph">Bei routinemäßigen Wartungsvorgängen sollte ein Storage-Takeover- oder Switchover-Vorgang transparent sein. Anders als bei Änderungen der Netzwerkpfade besteht hier eine potenzielle kurze Betriebsunterbrechung. Networking kann jedoch kompliziert sein und es sind leicht Fehler zu machen. NetApp empfiehlt daher dringend, Takeover- und Switchover-Vorgänge sorgfältig zu testen, bevor das Storage-System in Betrieb geht. Nur so können Sie sicherstellen, dass alle Netzwerkpfade korrekt konfiguriert sind. Prüfen Sie in einer SAN-Umgebung die Ausgabe des Befehls sorgfältig<block ref="41dc2c94e82e1c56ba876085acf5a974" prefix=" " category="inline-code"></block> Um sicherzustellen, dass alle erwarteten primären und sekundären Pfade verfügbar sind.</block>
  <block id="d33ea1ad1493de23aafd480e95fea159" category="paragraph">Bei erzwungener Übernahme oder Umschaltung ist Vorsicht zu beachten. Eine Änderung der Storage-Konfiguration mit diesen Optionen erzwingen, bedeutet, dass der Status des Controllers, dem die Laufwerke gehören, nicht berücksichtigt wird und der alternative Node gewaltsam die Kontrolle über die Laufwerke übernimmt. Ein falscher erzwingen eines Takeover kann zu Datenverlust oder Datenkorruption führen. Das liegt daran, dass durch eine erzwungene Übernahme oder Umschaltung die Inhalte von NVMEM verworfen werden. Nach Abschluss der Übernahme oder Umschaltung bedeutet der Verlust dieser Daten, dass die auf den Laufwerken gespeicherten Daten aus Sicht der Datenbank möglicherweise wieder in einen etwas älteren Zustand zurückgesetzt werden.</block>
  <block id="c226371d96b605ccff84ad23db0069b8" category="paragraph">Eine erzwungene Übernahme mit einem normalen HA-Paar sollte selten erforderlich sein. In fast allen Ausfallszenarien schaltet ein Node ab und informiert den Partner, sodass eine automatische Ausfallsicherung stattfindet. Es gibt einige Edge-Fälle, beispielsweise einen Rolling Failure, bei dem die Verbindung zwischen den Nodes unterbrochen wird und dann ein Controller verloren geht. Dadurch ist eine erzwungene Übernahme erforderlich. In einer solchen Situation geht die Spiegelung zwischen Nodes vor dem Controller-Ausfall verloren. Das bedeutet, dass der verbleibende Controller nicht mehr über eine Kopie der laufenden Schreibvorgänge verfügt. Das Takeover muss anschließend forciert werden, d. h., dass Daten potenziell verloren gehen.</block>
  <block id="418f81f529a577cb711b44adece1376f" category="paragraph">Dieselbe Logik gilt auch für eine MetroCluster-Umschaltung. Unter normalen Bedingungen ist eine Umschaltung nahezu transparent. Bei einem Ausfall kann es jedoch zu einem Verlust der Verbindung zwischen dem noch intakten Standort und dem Notfallstandort kommen. Aus Sicht des verbleibenden Standorts könnte das Problem lediglich eine Unterbrechung der Verbindung zwischen den Standorten sein, wobei der ursprüngliche Standort möglicherweise noch die Daten verarbeitet. Wenn ein Node den Status des primären Controllers nicht überprüfen kann, ist nur eine erzwungene Umschaltung möglich.</block>
  <block id="ae0c16d397348d8f8fd4e56b08b06c7c" category="paragraph">*NetApp empfiehlt* die folgenden Vorsichtsmaßnahmen zu ergreifen:</block>
  <block id="79fd18cb18befbe5c2113757478be193" category="list-text">Seien Sie vorsichtig, damit Sie nicht versehentlich eine Übernahme oder Umschaltung erzwingen. Normalerweise sollte das Erzwingen nicht erforderlich sein, und das Erzwingen der Änderung kann zu Datenverlust führen.</block>
  <block id="26cece11254903c5ba8bfdd8d54ae779" category="list-text">Wenn eine erzwungene Übernahme oder Umschaltung erforderlich ist, stellen Sie sicher, dass die Applikationen heruntergefahren, alle Filesysteme getrennt und die Volume-Gruppen des Logical Volume Manager (LVM) unterschiedlich sind. ASM-Diskgroups müssen abgehängt werden.</block>
  <block id="0b6986eb1285d28a9987ea13adbca358" category="list-text">Sollte eine erzwungene MetroCluster-Umschaltung stattfinden, sollte der ausgefallene Node von allen verbleibenden Storage-Ressourcen abgetrennt werden. Weitere Informationen zur entsprechenden Version von ONTAP finden Sie im MetroCluster-Management- und Disaster-Recovery-Leitfaden.</block>
  <block id="71303adf87737046acd4381c72151278" category="paragraph">MetroCluster ist eine Technologie für die synchrone Replizierung, die bei einer Unterbrechung der Verbindung zum asynchronen Modus wechselt. Dies ist die häufigste Anforderung von Kunden, da durch die garantierte synchrone Replizierung eine Unterbrechung der Standortkonnektivität zu einem vollständigen Stillstand der Datenbank-I/O führt und die Datenbank außer Betrieb genommen wird.</block>
  <block id="cb791a44ee361703eb7594a7932f57de" category="paragraph">Mit MetroCluster synchronisieren sie Aggregate nach der Wiederherstellung der Konnektivität schnell neu. Im Gegensatz zu anderen Storage-Technologien sollte bei MetroCluster nach einem Standortausfall nie eine vollständige Respiegelung erforderlich sein. Es müssen nur Delta-Änderungen versendet werden.</block>
  <block id="c1536091d7c1ca33c9dc4dfa8c0852a2" category="paragraph">Bei Datensätzen, die sich über Aggregate verteilen, besteht das geringe Risiko, dass bei einem rollierenden Disaster-Szenario zusätzliche Schritte zur Daten-Recovery erforderlich wären. Insbesondere, wenn (a) die Verbindung zwischen den Standorten unterbrochen wird, (b) die Konnektivität wiederhergestellt wird, (c) die Aggregate einen Zustand erreichen, in dem einige synchronisiert werden und andere nicht, und dann (d) der primäre Standort verloren geht, ist das Ergebnis ein noch existender Standort, an dem die Aggregate nicht miteinander synchronisiert werden. In diesem Fall werden Teile des Datensatzes miteinander synchronisiert. Ohne Recovery können Applikationen, Datenbanken oder Datastores nicht mehr angezeigt werden. Wenn ein Datensatz über mehrere Aggregate verteilt ist, empfiehlt NetApp dringend, Snapshot-basierte Backups mit einem der vielen verfügbaren Tools einzusetzen, um in diesem ungewöhnlichen Szenario eine schnelle Wiederherstellbarkeit zu überprüfen.</block>
  <block id="a42c87064c284490a38efac4bf0b7f7f" category="paragraph">RAID 4, RAID 5, RAID 6, RAID DP und RAID-TEC nutzen Parität, um sicherzustellen, dass es bei einem Laufwerksausfall zu keinem Datenverlust kommt. Diese RAID-Optionen bieten im Vergleich zur Spiegelung eine viel bessere Speichernutzung, aber die meisten RAID-Implementierungen haben einen Nachteil, der Schreibvorgänge beeinträchtigt. Für den Abschluss eines Schreibvorgangs in anderen RAID-Implementierungen sind möglicherweise mehrere Laufwerkszugriffe erforderlich, um die Paritätsdaten neu zu generieren. Dies ist ein Prozess, der allgemein als RAID-Abzug bezeichnet wird.</block>
  <block id="3cfcf9a3299db75df9aa0e024ebef965" category="paragraph">Bei ONTAP fallen jedoch keine RAID-Einbußen an. Dies liegt an der Integration von NetApp WAFL (Write Anywhere File Layout) mit der RAID-Schicht. Schreibvorgänge werden im RAM zusammengeführt und als vollständiger RAID-Stripe einschließlich der Paritätsgenerierung vorbereitet. ONTAP muss für einen Schreibvorgang keinen Lesevorgang durchführen. Das bedeutet, dass ONTAP und WAFL die RAID-Einbußen vermeiden. Die Performance für latenzkritische Vorgänge, wie die Protokollierung von Wiederherstellungen, wird ohne Behinderung durchgeführt und zufällige Schreibvorgänge von Datendateien verursachen keine RAID-Beeinträchtigungen, da die Parität neu generiert werden muss.</block>
  <block id="5ff9a871332f0f82f38d880b68a9874b" category="paragraph">In Bezug auf statistische Zuverlässigkeit bietet selbst RAID DP besseren Schutz als RAID Mirroring. Das Hauptproblem besteht in der Nachfrage nach Laufwerken während einer RAID-Wiederherstellung. Bei einem gespiegelten RAID-Satz ist das Risiko eines Datenverlusts aufgrund eines Laufwerksausfalls bei der Wiederherstellung an seinen Partner im RAID-Satz deutlich größer als das Risiko eines dreifachen Laufwerksausfalls in einem RAID DP-Satz.</block>
  <block id="12c3fe87fd5a746ce8a7cbe25dc1c25b" category="paragraph">Vor der Ära der Flash-Laufwerke wurde Striping verwendet, um die Performance-Einschränkungen rotierender Laufwerke zu überwinden. Beispiel: Wenn ein Betriebssystem einen Lesevorgang von 1 MB ausführen muss, würde das Lesen dieser 1 MB Daten von einem einzigen Laufwerk viel Festplattenkopf erfordern, der sucht und liest, da die 1 MB langsam übertragen wird. Wenn diese 1 MB Daten über 8 LUNs verteilt wurden, kann das Betriebssystem acht 128K-Lesevorgänge parallel ausführen und die für die 1-MB-Übertragung erforderliche Zeit verringern.</block>
  <block id="e337352c3d57672fd08af8ba709b544f" category="paragraph">Das Striping mit rotierenden Laufwerken war schwieriger, da das I/O-Muster bereits im Vorfeld bekannt sein musste. Wenn das Striping nicht richtig auf die wahren I/O-Muster abgestimmt wurde, können Striping-Konfigurationen die Performance beeinträchtigen. Bei Oracle Datenbanken und insbesondere bei All-Flash-Konfigurationen ist Striping einfacher zu konfigurieren und hat sich nachweislich für eine drastische Verbesserung der Performance bewährt.</block>
  <block id="7025652291dc6872022b0f79a2b2be21" category="paragraph">Logische Volume-Manager wie Oracle ASM Stripe sind standardmäßig aktiviert, aber native OS LVM nicht. Einige von ihnen verbinden mehrere LUNs als verkettete Geräte. Dies führt zu Datendateien, die auf einem und nur einem LUN-Gerät vorhanden sind. Dies verursacht Hotspots. Andere LVM-Implementierungen sind standardmäßig auf verteilte Extents eingestellt. Das ist ähnlich wie Striping, aber es ist gröber. Die LUNs in der Volume-Gruppe werden in große Teile geteilt, die als Extents bezeichnet werden und in der Regel in vielen Megabyte gemessen werden. Die logischen Volumes werden dann über diese Extents verteilt. Das Ergebnis ist ein zufälliger I/O-Vorgang für eine Datei, der auf LUNs verteilt werden sollte. Sequenzielle I/O-Vorgänge sind jedoch nicht so effizient wie möglich.</block>
  <block id="0c2f3b2ec9d90baa0a0a0b6fb673a113" category="paragraph">Die Performance-intensiven Applikations-I/O-Vorgänge erfolgen fast immer entweder (a) in Einheiten der grundlegenden Blockgröße oder (b) in Megabyte.</block>
  <block id="804d5b1f06570b4d242be0c2f0c3392c" category="paragraph">Das primäre Ziel einer Striped-Konfiguration ist es, sicherzustellen, dass Single-File I/O als eine Einheit ausgeführt werden kann. Multiblock-I/O, die eine Größe von 1 MB haben sollte, kann gleichmäßig über alle LUNs im Striped Volume hinweg parallelisiert werden. Das bedeutet, dass die Stripe-Größe nicht kleiner als die Blockgröße der Datenbank sein darf und die Stripe-Größe multipliziert mit der Anzahl der LUNs 1 MB betragen sollte.</block>
  <block id="0598f393baefd05d48b48076cc7df606" category="paragraph">Die folgende Abbildung zeigt drei mögliche Optionen für die Stripe-Größe und Breitenabstimmung. Die Anzahl der LUNs wird ausgewählt, um die oben beschriebenen Performance-Anforderungen zu erfüllen. In allen Fällen beträgt die Gesamtzahl der Daten innerhalb eines einzigen Stripes jedoch 1 MB.</block>
  <block id="9ca503fae9ccd4d6d8e67806b23adfa0" category="paragraph"><block ref="9ca503fae9ccd4d6d8e67806b23adfa0" category="inline-image-macro-rx" type="image"></block></block>
  <block id="0353522b39b87d54d40d3f09ec668851" category="doc">ONTAP-Konfiguration</block>
  <block id="8e52de0d4b03d5fe87dc88da09616c7f" category="inline-link-macro">Hier.</block>
  <block id="337c62c6b6eb7319bae58701105e889c" category="doc">Tiering</block>
  <block id="3493794b3ccb587f71a663f51f4484ef" category="summary">PostgreSQL-Datenbanken auf ONTAP</block>
  <block id="2d3298c651b356a82db5d664ce68e65e" category="doc">Native Datensicherung</block>
  <block id="31c6611c8e0bd237a3ee51db1728a440" category="paragraph">Einer der wichtigsten Aspekte des Storage-Designs ist die Sicherung von PostgreSQL Volumes. Kunden können ihre PostgreSQL-Datenbanken entweder mithilfe des Dump-Ansatzes oder mit Dateisystem-Backups sichern. In diesem Abschnitt werden die verschiedenen Ansätze zur Sicherung einzelner Datenbanken oder des gesamten Clusters erläutert.</block>
  <block id="9f3265914462c64fd02bc50ab4c9ebe9" category="paragraph">Es gibt drei Ansätze für die Sicherung von PostgreSQL-Daten:</block>
  <block id="6b592c7950245a5a9f54d9424e851e97" category="list-text">SQL Server Dump</block>
  <block id="c8b1fc69d12d1da6589e384671c8e82c" category="list-text">Backup auf Filesystem-Ebene</block>
  <block id="3c25062a0babfa1495dd604698707610" category="list-text">Kontinuierliche Archivierung</block>
  <block id="58c132e2be12705408a7456c6183b617" category="paragraph">Die Idee hinter der SQL Server Dump-Methode besteht darin, eine Datei mit SQL Server-Befehlen zu generieren, die, wenn sie an den Server zurückgegeben wird, die Datenbank so neu erstellen kann, wie sie zum Zeitpunkt des Dump war. PostgreSQL stellt die Dienstprogramme zur Verfügung<block ref="5a5149b2817f30cbb3805307f9cadae4" prefix=" " category="inline-code"></block> Und<block ref="0ead269b3d000482b5a44a1ef4e8cd54" prefix=" " category="inline-code"></block> Zur Erstellung von individuellen Backups und Backups auf Cluster-Ebene. Diese Dumps sind logisch und enthalten nicht genügend Informationen, die von WAL Replay verwendet werden können.</block>
  <block id="1d05b80fc6f199681263e063190e8325" category="paragraph">Eine alternative Backup-Strategie ist die Verwendung von Backup auf Dateisystem-Ebene, bei der Administratoren direkt kopieren die Dateien, die PostgreSQL verwendet, um die Daten in der Datenbank zu speichern. Diese Methode erfolgt im Offline-Modus: Die Datenbank oder das Cluster muss heruntergefahren werden. Eine weitere Alternative ist die Verwendung<block ref="35679fc424d9eec0c30f38459fd9b322" prefix=" " category="inline-code"></block> Zum Ausführen von Hot-Streaming-Backups der PostgreSQL-Datenbank.</block>
  <block id="10907e7ac5ab6e5235a23898a2331b31" category="doc">PostgreSQL-Datenbanken auf ONTAP</block>
  <block id="841a21e58e7c22a71c07b89a4362d2b6" category="admonition">Diese Dokumentation zu ONTAP und der PostgreSQL-Datenbank ersetzt die zuvor veröffentlichte _TR-4770: PostgreSQL-Datenbank unter ONTAP Best Practices._</block>
  <block id="9147e5e61a7b9260dec09f3a6eb3e5be" category="doc">Snapshots</block>
  <block id="0b9d99167191c5d9ad23ca07bc6a9b38" category="paragraph">Snapshot-basierte Backups mit PostgreSQL erfordern die Konfiguration von Snapshots für Datendateien, WAL-Dateien und archivierte WAL-Dateien, um eine vollständige oder zeitpunktgenaue Recovery zu ermöglichen.</block>
  <block id="8c6a53edc4449a4cc3983d47ecffa851" category="paragraph">Bei PostgreSQL-Datenbanken liegt die durchschnittliche Backup-Zeit mit Snapshots im Bereich von wenigen Sekunden bis zu wenigen Minuten. Diese Backup-Geschwindigkeit ist 60 bis 100 Mal schneller als<block ref="35679fc424d9eec0c30f38459fd9b322" prefix=" " category="inline-code"></block> Und anderen Filesystem-basierten Backup-Ansätzen.</block>
  <block id="c798632224ae09152f66f60e0edc757e" category="paragraph">Snapshots auf NetApp Storage können sowohl ausfallkonsistent als auch applikationskonsistent sein. Ein Crash-konsistenter Snapshot wird auf dem Storage erstellt, ohne die Datenbank stillzustehen. Während sich die Datenbank im Backup-Modus befindet, wird ein applikationskonsistenter Snapshot erstellt. NetApp sorgt außerdem dafür, dass nachfolgende Snapshots dauerhaft inkrementelle Backups sind, um die Storage-Einsparungen und die Netzwerkeffizienz zu erhöhen.</block>
  <block id="d0e369216eeac8de66e915909364970a" category="paragraph">Da Snapshots schnell sind und die System-Performance nicht beeinträchtigen, können Sie mehrere Snapshots täglich planen, anstatt wie bei anderen Streaming-Backup-Technologien täglich ein einziges Backup zu erstellen. Wenn ein Wiederherstellungs- und Wiederherstellungsvorgang erforderlich ist, verringert sich die Systemausfallzeit um zwei wichtige Funktionen:</block>
  <block id="316eb8adba4d16a34c92eaf9db381b9e" category="list-text">Dank der NetApp SnapRestore Datenwiederherstellungs-Technologie erfolgt die Wiederherstellung in Sekundenschnelle.</block>
  <block id="c128c475c12703e37685edd6db8144bc" category="list-text">Durch aggressive Recovery Point Objectives (RPOs) müssen weniger Datenbankprotokolle angewendet werden und auch die Recovery wird beschleunigt.</block>
  <block id="9b9f26085ff7149d4dbb916b1db0f121" category="paragraph">Für das Backup von PostgreSQL müssen Sie sicherstellen, dass die Datenvolumes gleichzeitig mit (Consistency Group) WAL und den archivierten Protokollen geschützt sind. Stellen Sie beim Kopieren von WAL-Dateien mit der Snapshot-Technologie sicher, dass Sie ausgeführt werden<block ref="a3b8c8ee15ba60bfb2b999195d2b4e7d" prefix=" " category="inline-code"></block> Um alle WAL-Einträge zu löschen, die archiviert werden müssen. Wenn Sie die WAL-Einträge während der Wiederherstellung löschen, müssen Sie nur die Datenbank anhalten, das vorhandene Datenverzeichnis aufheben oder löschen und einen SnapRestore-Vorgang auf dem Speicher ausführen. Nachdem die Wiederherstellung abgeschlossen ist, können Sie das System mounten und in den aktuellen Status zurückversetzen. Für Point-in-Time Recovery können Sie WAL wiederherstellen und Protokolle archivieren. PostgreSQL entscheidet dann über den konsistentesten Punkt und stellt ihn automatisch wieder her.</block>
  <block id="8e8d6187572eace409fee5bf2dbb7cf9" category="paragraph">Konsistenzgruppen sind in ONTAP eine Funktion, die ebenfalls empfohlen werden, wenn mehrere Volumes in eine einzelne Instanz oder in eine Datenbank mit mehreren Tablespaces gemountet sind. Ein Snapshot einer Konsistenzgruppe stellt sicher, dass alle Volumes gruppiert und geschützt sind. Eine Konsistenzgruppe kann über den ONTAP System Manager effizient gemanagt werden und Sie können sie sogar klonen, um eine Instanzkopie einer Datenbank zu Test- oder Entwicklungszwecken zu erstellen.</block>
  <block id="7e7397a7b79323762c61941fc0e6b5f9" category="doc">Datensicherung</block>
  <block id="16a49d0bc0249ef3dd42949589a0ed4e" category="doc">Tablespaces</block>
  <block id="d6d544de56c329d9f68bb176c6245e7c" category="paragraph">Bei der Initialisierung des Datenbank-Clusters werden automatisch zwei Tablespaces erstellt.</block>
  <block id="6df924905e21856e4e8b8f936767cdd6" category="paragraph">Der<block ref="bd676cbef0d168a62a062c8709af1824" prefix=" " category="inline-code"></block> Tablespace wird für freigegebene Systemkataloge verwendet. Der<block ref="3224684b8e7208962a73daeb0bb24110" prefix=" " category="inline-code"></block> Tablespace ist der Standard-Tablespace der Datenbanken temple1 und template0. Wenn die Partition oder das Volume, auf der das Cluster initialisiert wurde, nicht mehr genügend Speicherplatz hat und nicht erweitert werden kann, kann ein Tablespace auf einer anderen Partition erstellt und verwendet werden, bis das System neu konfiguriert werden kann.</block>
  <block id="364c9d7b9593277eee9477befaf9f66b" category="paragraph">Ein stark genutzter Index kann auf einer schnellen, hochverfügbaren Festplatte wie einem Solid-State-Gerät platziert werden. Darüber hinaus kann eine Tabelle mit archivierten Daten, die selten verwendet oder nicht Performance-kritisch sind, auf einem kostengünstigeren, langsameren Festplattensystem wie SAS- oder SATA-Laufwerken gespeichert werden.</block>
  <block id="9bfe8d8e1117bd1d4b8597220e3f166e" category="paragraph">Tablespaces sind Bestandteil des Datenbank-Clusters und können nicht als eigenständige Erfassung von Datendateien behandelt werden. Sie sind von Metadaten im Hauptdatenverzeichnis abhängig und können daher nicht an einen anderen Datenbankcluster angeschlossen oder einzeln gesichert werden. Wenn Sie einen Tablespace verlieren (durch Dateilöschung, Festplattenfehler usw.), kann der Datenbankcluster möglicherweise unlesbar werden oder nicht mehr starten. Wenn ein Tablespace auf einem temporären Dateisystem wie einer RAM-Festplatte platziert wird, besteht die Gefahr, dass der gesamte Cluster zuverlässig ist.</block>
  <block id="7d9573b1ed3543ec6ea42c81830dc27c" category="paragraph">Nach der Erstellung kann ein Tablespace aus jeder beliebigen Datenbank verwendet werden, wenn der anfordernde Benutzer über ausreichende Berechtigungen verfügt. PostgreSQL verwendet symbolische Links, um die Implementierung von Tablespaces zu vereinfachen. PostgreSQL fügt dem eine Zeile hinzu<block ref="4a926dc6e7549c49a13755513af41e67" prefix=" " category="inline-code"></block> Tabelle (eine clusterwide-Tabelle) und weist dieser Zeile eine neue Objektkennung (OID) zu. Schließlich verwendet der Server die OID, um einen symbolischen Link zwischen Ihrem Cluster und dem angegebenen Verzeichnis zu erstellen. Das Verzeichnis<block ref="8e3da386e945c59fb1e1cd1d7a47697c" prefix=" " category="inline-code"></block> Enthält symbolische Links, die auf jeden nicht integrierten Tablespace verweisen, der im Cluster definiert ist.</block>
  <block id="249fd7d68098688fab42436eed51be7d" category="doc">Datenbankkonfiguration</block>
  <block id="8daf26b2c72dd18a89fa89a6b1aa988d" category="paragraph">Es gibt mehrere PostgreSQL-Tuning-Konfigurationen, die die Performance verbessern können.</block>
  <block id="abb3dae412afe5c801e9ab90de71675b" category="paragraph">Die am häufigsten verwendeten Parameter sind:</block>
  <block id="bfdf741f4acb3dfee6e87075b535610e" category="list-text"><block ref="39d79378db03fed7c5ba62efba38e327" prefix="" category="inline-code"></block>: Die maximale Anzahl von Datenbankverbindungen, die gleichzeitig verfügbar sind. Verwenden Sie diesen Parameter, um den Austausch auf Festplatte zu beschränken und die Leistung zu unterbinden. Je nach Anwendungsanforderung können Sie diesen Parameter auch für die Einstellungen des Verbindungspools anpassen.</block>
  <block id="9625e1a6f5d1f695fd1c72401afaa07e" category="list-text"><block ref="904d399f43708a8a5b6d10c1ee3f8356" prefix="" category="inline-code"></block>: Die einfachste Methode zur Verbesserung der Leistung Ihres Datenbankservers. Die Standardeinstellung ist niedrig für die meisten modernen Hardware. Sie wird während der Bereitstellung auf ca. 25 % des verfügbaren RAM auf dem System eingestellt. Diese Parametereinstellung hängt davon ab, wie sie mit bestimmten Datenbankinstanzen funktioniert; Sie müssen die Werte möglicherweise durch Versuch und Fehler erhöhen oder verringern. Bei einer hohen Einstellung wird die Performance jedoch wahrscheinlich beeinträchtigt.</block>
  <block id="d1960c56572383fc401fec3f0b767cf9" category="list-text"><block ref="5edeed59821790d8bec6d32da26bca4c" prefix="" category="inline-code"></block>: Dieser Wert teilt PostgreSQL's Optimizer mit, wie viel Speicher PostgreSQL für das Caching von Daten zur Verfügung hat und hilft bei der Bestimmung, ob ein Index verwendet werden soll. Ein größerer Wert erhöht die Wahrscheinlichkeit, einen Index zu verwenden. Dieser Parameter sollte auf die Größe des zugewiesenen Speichers eingestellt werden<block ref="e15ed0f69e5c2475450b97691d6a2cee" prefix=" " category="inline-code"></block> Und die Menge an verfügbarem BS-Cache. Dieser Wert liegt häufig bei mehr als 50 % des gesamten Systemspeichers.</block>
  <block id="508df54f84fc7abb00f3a92937ca3506" category="list-text"><block ref="1edef4d43b06d4deb28010505df5724d" prefix="" category="inline-code"></block>: Dieser Parameter steuert die Speichermenge, die in Sort-Operationen und Hash-Tabellen verwendet werden soll. Wenn Sie eine starke Sortierung in Ihrer Anwendung ausführen, müssen Sie möglicherweise den Speicherplatz erhöhen, aber seien Sie vorsichtig. Es handelt sich nicht um einen systemweiten Parameter, sondern um einen pro-Operation-Parameter. Wenn eine komplexe Abfrage mehrere Sortieroperationen enthält, verwendet sie mehrere Work_mem-Speichereinheiten, und mehrere Back-Ends könnten dies gleichzeitig tun. Diese Abfrage kann oft dazu führen, dass Ihr Datenbankserver ausgetauscht wird, wenn der Wert zu groß ist. Diese Option wurde zuvor in älteren PostgreSQL-Versionen als sort_mem bezeichnet.</block>
  <block id="59d9770060aecba84c7e8ed849b5653d" category="list-text"><block ref="65768448b4f275b95af20a317c7355a9" prefix="" category="inline-code"></block>: Dieser Parameter legt fest, ob alle WAL-Seiten mit fsync() synchronisiert werden sollen, bevor eine Transaktion durchgeführt wird. Wenn Sie sie deaktivieren, kann die Schreibleistung manchmal verbessert werden, und wenn Sie sie einschalten, erhöht sich der Schutz vor dem Risiko von Beschädigungen, wenn das System abstürzt.</block>
  <block id="625a65dd6cef83bac66319bde3894899" category="list-text"><block ref="ff5a06a39dd6824f30a778a51cf7ea69" prefix="" category="inline-code"></block>: Der Checkpoint-Prozess überträgt die Daten auf die Festplatte. Dies beinhaltet viele Lese-/Schreibvorgänge auf der Festplatte. Der Wert wird in Sekunden festgelegt, und niedrigere Werte verringern die Absturzwiederherstellungszeit, und höhere Werte können die Belastung der Systemressourcen verringern, indem die Checkpoint-Anrufe reduziert werden. Legen Sie je nach Wichtigkeit der Anwendung, Auslastung und Verfügbarkeit der Datenbank den Wert von Checkpoint_Timeout fest.</block>
  <block id="309b1036c45e7ad490448925dc4f9597" category="list-text"><block ref="6a5c0586429891e84e05d6ab15088405" prefix="" category="inline-code"></block> Und<block ref="9080796d32078fea7d191100eab64f92" prefix=" " category="inline-code"></block>: Diese Optionen werden zusammen verwendet, um die Leistung zu verbessern, indem mehrere Transaktionen, die auf einmal begehen, ausgeschrieben werden. Wenn mehrere commit_Geschwister-Objekte aktiv sind, sobald Ihre Transaktion abgeschlossen ist, wartet der Server auf commit_delay Mikrosekunden, um zu versuchen, mehrere Transaktionen gleichzeitig zu begehen.</block>
  <block id="8d5137ce002c6328c2b5f4f328662e1d" category="list-text"><block ref="fa4ca2cd95c20a44171f964fb8f5fdb9" prefix="" category="inline-code"></block>: Konfigurieren Sie die optimale Anzahl von Arbeitern für Prozesse. Max_Parallel_Workers entspricht der Anzahl der verfügbaren CPUs. Je nach Anwendungsdesign erfordern Abfragen möglicherweise weniger Mitarbeiter für parallele Vorgänge. Es ist besser, den Wert für beide Parameter gleich zu halten, aber den Wert nach dem Testen anzupassen.</block>
  <block id="c8736e7ca19e9096796c624f3030dae5" category="list-text"><block ref="8dda85a5feb6de66d8f5f9a4cbdb0754" prefix="" category="inline-code"></block>: Dieser Wert steuert die Art und Weise, wie PostgreSQL nicht-sequentielle Datenträger liest. Ein höherer Wert bedeutet, dass PostgreSQL eher einen sequenziellen Scan anstelle eines Indexscans verwendet, was darauf hinweist, dass Ihr Server über schnelle Festplatten verfügt.Ändern Sie diese Einstellung, nachdem Sie andere Optionen wie planbasierte Optimierung, Staubsaugen, Indexieren auf Abfragen oder Schema überprüft haben.</block>
  <block id="17edf3863f12b7d9480b3336a9fca773" category="list-text"><block ref="709989c844c22473cb8406550423d39a" prefix="" category="inline-code"></block>: Dieser Parameter legt die Anzahl der gleichzeitigen Festplatten-I/O-Operationen fest, die PostgreSQL gleichzeitig auszuführen versucht. Wenn Sie diesen Wert erhöhen, erhöht sich die Anzahl der I/O-Vorgänge, die jede einzelne PostgreSQL-Sitzung parallel initiieren möchte. Der zulässige Bereich ist 1 bis 1,000 oder Null, um die Ausgabe asynchroner I/O-Anfragen zu deaktivieren. Derzeit wirkt sich diese Einstellung nur auf Bitmap-Heap-Scans aus. Solid State Drives (SSDs) und anderer speicherbasierter Storage (NVMe) können oft zahlreiche gleichzeitige Anforderungen verarbeiten, sodass der beste Nutzen aus Hunderten von Laufwerken zu ziehen ist.</block>
  <block id="82b38edb10ad73f9645c5d65e73a659b" category="paragraph">Eine vollständige Liste der PostgreSQL-Konfigurationsparameter finden Sie in der PostgreSQL-Dokumentation.</block>
  <block id="402fbc243dbef50cd5c8e57ccbdd92f5" category="section-title">TOAST</block>
  <block id="0d2b62abb26bd0b4109f5bc967250933" category="paragraph">TOAST steht für die Oversized-Attribute Storage-Technik. PostgreSQL verwendet eine feste Seitengröße (üblicherweise 8 KB) und erlaubt nicht, dass Tupel mehrere Seiten umfassen. Daher ist es nicht möglich, große Feldwerte direkt zu speichern. Wenn Sie versuchen, eine Zeile zu speichern, die diese Größe überschreitet, bricht TOAST die Daten großer Spalten in kleinere „Stücke“ und speichert sie in einem TOAST Tisch.</block>
  <block id="3f11f21bdc22c3c955c3f0fca39e9bc0" category="paragraph">Die großen Werte der getoasteten Attribute werden nur dann herausgezogen (wenn sie überhaupt ausgewählt sind), wenn der Ergebnissatz an den Client gesendet wird. Die Tabelle selbst ist viel kleiner und kann mehr Zeilen in den gemeinsam genutzten Puffer-Cache passen als ohne Out-of-Line Storage (TOAST).</block>
  <block id="9a311ef6dad816cb7a15c0ab41920ad8" category="section-title">VAKUUM</block>
  <block id="385e23b8ebe64dfd0005f6846cc8e85e" category="paragraph">Im normalen PostgreSQL-Betrieb werden Tupel, die durch eine Aktualisierung gelöscht oder veraltet gemacht werden, nicht physisch aus ihrer Tabelle entfernt; sie bleiben vorhanden, bis VAKUUM ausgeführt wird. Daher müssen Sie regelmäßig VAKUUM betreiben, insbesondere auf häufig aktualisierten Tabellen. Der belegte Speicherplatz muss dann zur Wiederverwendung durch neue Zeilen zurückgewonnen werden, um einen Ausfall von Festplattenspeicher zu vermeiden. Er gibt jedoch nicht den Speicherplatz an das Betriebssystem zurück.</block>
  <block id="db81319c35b48ed94867775a9a1ce1d7" category="paragraph">Der freie Platz innerhalb einer Seite ist nicht fragmentiert. VACUUM schreibt den gesamten Block neu, verpackt die restlichen Zeilen und hinterlässt einen einzigen zusammenhängenden Block freien Speicherplatz auf einer Seite.</block>
  <block id="8a5c17164c9920590fc057ee01f2fcb3" category="paragraph">Dagegen verdichtet VACUUM FULL Tabellen aktiv, indem eine völlig neue Version der Tabellendatei ohne Totraum geschrieben wird. Diese Aktion minimiert die Größe des Tisches, kann aber lange dauern. Außerdem wird zusätzlicher Speicherplatz für die neue Kopie der Tabelle benötigt, bis der Vorgang abgeschlossen ist. Ziel des routinemäßigen VAKUUMS ist es, die VOLLE VAKUUMAKTIVITÄT zu vermeiden. Bei diesem Prozess werden nicht nur Tabellen auf der Mindestgröße gespeichert, sondern auch der Festplattenspeicherplatz weiterhin gleichmäßig genutzt.</block>
  <block id="61bcd96a2c1f8026527cbf2019d6e9a4" category="doc">Initialisierung</block>
  <block id="f4f9dac58f7c47e819293af5f2dd1177" category="paragraph">Sie erstellen mithilfe von ein neues Datenbankcluster<block ref="abc91e782cd3950bfa31202aff74ca69" prefix=" " category="inline-code"></block> Programm. An<block ref="abc91e782cd3950bfa31202aff74ca69" prefix=" " category="inline-code"></block> Skript erstellt die Datendateien, Systemtabellen und Vorlagendatenbanken (template0 und template 1), die den Cluster definieren.</block>
  <block id="b668460552732302350bbb840d57931c" category="paragraph">Die Vorlagendatenbank stellt eine Bestandsdatenbank dar. Es enthält Definitionen für Systemtabellen, Standardansichten, Funktionen und Datentypen.<block ref="3865cf98fe802a965570b4a10a1d894b" prefix=" " category="inline-code"></block> Fungiert als Argument für den<block ref="abc91e782cd3950bfa31202aff74ca69" prefix=" " category="inline-code"></block> Skript, das den Speicherort des Datenbank-Clusters angibt.</block>
  <block id="8fcd14822a5b94d212898fe3e006526e" category="paragraph">Alle Datenbankobjekte in PostgreSQL werden intern von den jeweiligen OIDs verwaltet. Tabellen und Indizes werden auch von einzelnen OIDs verwaltet. Die Beziehungen zwischen Datenbankobjekten und ihren jeweiligen OIDs werden je nach Objekttyp in entsprechenden Systemkatalogtabellen gespeichert. OIDs von Datenbanken und Heap-Tabellen werden beispielsweise in gespeichert<block ref="d0fcebb608269edbd5d67486884ebed7" prefix=" " category="inline-code"></block> Und `pg_class. Sie können die OIDs durch Abfragen auf dem PostgreSQL-Client ermitteln.</block>
  <block id="d0e97e34fe92edd2facd0f7377b6340b" category="paragraph">Jede Datenbank verfügt über eigene einzelne Tabellen und Indexdateien, die auf 1 GB beschränkt sind. Jede Tabelle hat zwei zugehörige Dateien, die jeweils mit dem Suffix versehen sind<block ref="7b9b00c45b01f7b7c6d3a3c8ba2e9275" prefix=" " category="inline-code"></block> Und<block ref="a1d869a79e2f43693a6a416fcbdc2724" prefix=" " category="inline-code"></block>. Sie werden als freie Raumkarte und Sichtbarkeitskarte bezeichnet. Diese Dateien speichern die Informationen über die freie Speicherkapazität und haben Sichtbarkeit auf jeder Seite in der Tabellendatei. Indizes verfügen nur über individuelle freie Speicherplatzkarten und haben keine Sichtbarkeits-Karten.</block>
  <block id="048a7ed79caf84bb5c725efc5f474911" category="paragraph">Der<block ref="c28f4f32219f5ff99fb4b116980b8549" prefix=" " category="inline-code"></block> Das Verzeichnis enthält die Write-Ahead-Protokolle. Mit Write-Ahead-Protokollen werden die Zuverlässigkeit und Performance der Datenbank verbessert. Immer wenn Sie eine Zeile in einer Tabelle aktualisieren, schreibt PostgreSQL die Änderung zuerst in das Write-Ahead-Protokoll und schreibt später die Änderungen auf die eigentlichen Datenseiten auf eine Festplatte. Der<block ref="f7223cfaa9416056a91e259e326ac5cf" prefix=" " category="inline-code"></block> Das Verzeichnis enthält normalerweise mehrere Dateien, aber initdb erstellt nur die erste. Zusätzliche Dateien werden bei Bedarf hinzugefügt. Jede xlog-Datei ist 16 MB lang.</block>
  <block id="789a6c7dd865d16eb1df122bcc5c9a3a" category="admonition">*NetApp empfiehlt* NFSv4.1 zu verwenden, wenn NFSv4-Funktionen erforderlich sind. Es gibt einige funktionale Verbesserungen am NFSv4-Protokoll in NFSv4.1, die die Ausfallsicherheit in bestimmten Edge-Fällen verbessern.</block>
  <block id="8e86e0aee135afb67c6a1d3d6e0f3306" category="inline-link-macro">NFS-Übertragungsgrößen</block>
  <block id="d84b41126744795c4ee82f7b256e7234" category="inline-link-macro">FC SAN</block>
  <block id="2c8c2171fc87c96e58fb40d8714f85cf" category="doc">PostgreSQL Architektur</block>
  <block id="ee62b4420d426c21a46d892ac084872a" category="paragraph">PostgreSQL ist ein RDBMS, das auf Client- und Serverarchitektur basiert. Eine PostgreSQL-Instanz wird als Datenbank-Cluster bezeichnet, bei dem es sich um eine Sammlung von Datenbanken und nicht um eine Sammlung von Servern handelt.</block>
  <block id="89c30c326e106773af32af40255b92a8" category="inline-image-macro">Fehler: Grafik nicht gefunden</block>
  <block id="54c2dd41fd1ad3efa25e80bee0bae549" category="paragraph">In einer PostgreSQL-Datenbank gibt es drei Hauptelemente: Den Postmaster, das Frontend (Client) und das Backend Der Client sendet Anfragen an den Postmaster mit Informationen wie IP-Protokoll und zu welcher Datenbank eine Verbindung hergestellt werden soll. Der Postmaster authentifiziert die Verbindung und leitet sie zur weiteren Kommunikation an den Back-End-Prozess weiter. Der Back-End-Prozess führt die Abfrage aus und sendet Ergebnisse direkt an das Frontend (Client).</block>
  <block id="9f96e07bbaf7c0237047d6d0f95301a8" category="paragraph">Eine PostgreSQL-Instanz basiert auf einem Multiprocess-Modell statt auf einem Multithread-Modell. Es gibt mehrere Prozesse für verschiedene Jobs, und jeder Prozess hat seine eigene Funktionalität. Zu den wichtigsten Prozessen gehören der Clientprozess, der WAL Writer-Prozess, der Background Writer-Prozess und der Checkpointer-Prozess:</block>
  <block id="6aebf901bd4be26a765c56e4133b3798" category="list-text">Wenn ein Client-Prozess (Vordergrund) Lese- oder Schreibanforderungen an die PostgreSQL-Instanz sendet, werden keine Daten direkt auf die Festplatte geschrieben oder gelesen. Zuerst werden die Daten in gemeinsam genutzten Puffern und WAL-Puffern (Write-Ahead Logging) gepuffert.</block>
  <block id="458239ee6c634dc5fef96f9dfab275f0" category="list-text">Ein WAL-Schreibprozess manipuliert den Inhalt der gemeinsam genutzten Puffer und WAL-Puffer, um in die WAL-Protokolle zu schreiben. WAL-Protokolle sind in der Regel Transaktionsprotokolle von PostgreSQL und werden sequenziell geschrieben. Um die Reaktionszeit aus der Datenbank zu verbessern, schreibt PostgreSQL zunächst in die Transaktionsprotokolle und bestätigt den Client.</block>
  <block id="9ec2999256fe568cd596b42e7da2cc39" category="list-text">Um die Datenbank in einen konsistenten Zustand zu versetzen, überprüft der Background Writer-Prozess den gemeinsam genutzten Puffer regelmäßig auf fehlerhafte Seiten. Anschließend überträgt es die Daten auf die Datendateien, die auf NetApp Volumes oder LUNs gespeichert sind.</block>
  <block id="5c852dd8ba7215c01dfde8e98f6c46c6" category="list-text">Der Checkpointer-Prozess läuft auch periodisch (seltener als der Hintergrundprozess) und verhindert jegliche Änderung der Puffer. Er signalisiert dem WAL Writer-Prozess, den Checkpoint-Datensatz zu schreiben und an das Ende der WAL-Protokolle zu löschen, die auf der NetApp-Festplatte gespeichert sind. Er signalisiert auch, dass der Background Writer-Prozess alle fehlerhaften Seiten auf die Festplatte schreibt und auf diese schreibt.</block>
  <block id="0a7b1215fd0b9621d4d7300f88b2906c" category="doc">Datensicherungssoftware</block>
  <block id="e265011e47d11db43ee112fd2fdc9d5b" category="paragraph">Das NetApp SnapCenter Plug-in für die PostgreSQL Datenbank bietet in Kombination mit Snapshot und NetApp FlexClone Technologien folgende Vorteile:</block>
  <block id="4dcdaba06a92b8fecfdc823643a71723" category="list-text">Schnelles Backup und Restore:</block>
  <block id="aee746a577a1baebdc33197fb5f3a279" category="list-text">Platzsparende Klone:</block>
  <block id="b96bb0cd2ebc0389b6670602ac80d983" category="list-text">Aufbau eines schnellen und effektiven Disaster Recovery-Systems</block>
  <block id="9e060ad00fced94f2616dd71fbc49f76" category="paragraph">Unter den folgenden Umständen bevorzugen Sie die Premium-Backup-Partner von NetApp, z. B. Veeam Software und CommVault:</block>
  <block id="0ee17b3b55ea2b0af3d90985bc347775" category="list-text">Management von Workloads in heterogener Umgebung</block>
  <block id="88fbde7bbe0274fbe9e7ce01c563d03c" category="list-text">Speichern von Backups in der Cloud oder auf Tape zur langfristigen Aufbewahrung</block>
  <block id="445e3e0cce7f959696dde65b3be27843" category="list-text">Unterstützung für eine Vielzahl von Betriebssystemversionen und -Typen</block>
  <block id="21c5d67cc22beaf7d7c9c84c8deaabbe" category="paragraph">SnapCenter Plugin für PostgreSQL ist Community-unterstütztes Plugin und das Setup und die Dokumentation ist auf NetApp Automation Store verfügbar. Mit SnapCenter können Anwender Datenbanken sichern sowie Daten Remote klonen und wiederherstellen.</block>
  <block id="7d334ca369e4a752ec91b1503183f0ed" category="summary">Lösungen für SAP HANA und AnyDB</block>
  <block id="1ac73696f4529e3901b0d4e5430365cb" category="doc">SAP HANA und SAP mit AnyDB</block>
  <block id="2c1b27a986b5e37a810d61f72b23ee4b" category="paragraph">Best Practices für die Konfiguration, Verwaltung und Automatisierung von SAP-Lösungen finden Sie auf der Seite NetApp SAP-Lösungen.</block>
  <block id="6c92285fa6d3e827b198d120ea3ac674" category="inline-link-macro">Hier</block>
  <block id="ee1e67308b27672a8f54eace4c615a98" category="paragraph">Klicken Sie auf <block ref="c95e4a3e1de9e00fefc4bf8a7ce42893" category="inline-link-macro-rx"></block> Für mehr.</block>
  <block id="beb3ca55ed0a53a38a2b97540b05d6e4" category="summary">Microsoft SQL Server auf ONTAP</block>
  <block id="7636ae91f443b605f05bed59d7d57a02" category="doc">Disaster Recovery für Microsoft SQL Server</block>
  <block id="18c6a681ea46e1ed394e092461bb53ef" category="list-text">Verteilen Sie Volumes, die SQL Server-Daten enthalten, auf verschiedene Nodes im Cluster, damit alle Clusterknoten SnapMirror-Replikationsaktivitäten gemeinsam nutzen können. Diese Verteilung optimiert die Nutzung von Knotenressourcen.</block>
  <block id="0a01e6837f5c691536f0b6cbdd0232ec" category="inline-link-macro">TR-4015: SnapMirror Konfigurations- und Best Practices-Leitfaden für ONTAP 9</block>
  <block id="1cca1a3fad8c6dd7e2dadbd2aa08bf07" category="paragraph">Weitere Informationen zu SnapMirror finden Sie unter <block ref="64b916f792bad525a069a243c092b62e" category="inline-link-macro-rx"></block>.</block>
  <block id="77641f4a3e4406855e878235cbd89b87" category="doc">CPU-Konfiguration</block>
  <block id="a51adad5719779145ac252822e328611" category="section-title">Hyperthreading</block>
  <block id="673c216e20d1cf8827f347c01dace664" category="paragraph">Der Nachteil hierbei ist, dass jede SQL Server-Version ihre eigenen Einschränkungen hinsichtlich der Rechenleistung hat, die sie verwenden kann. Weitere Informationen finden Sie unter Kapazitätsgrenzen nach Edition von SQL Server berechnen.</block>
  <block id="cf0e2551e639758cc3c4d82df655eccd" category="inline-link-macro">SQL Server 2022: Ihre moderne Datenplattform</block>
  <block id="809c9e1f6ce0bc82bf42fae361c6f43e" category="paragraph">Um alle CPUs zu verwenden, sollten Sie daher die Prozessorkern-Lizenz verwenden. Weitere Informationen zur SQL Server-Lizenzierung finden Sie unter <block ref="457abfaf1083b1cbfc49f2783069c6cd" category="inline-link-macro-rx"></block>.</block>
  <block id="cb8d56a3511d402b7355db8502a54c5a" category="section-title">CPU-Affinität</block>
  <block id="74f896768ee487acfd4c42370c202bed" category="paragraph">SQL Server unterstützt die Prozessoraffinität durch zwei Optionen:</block>
  <block id="cb315fb2b62c836ba51357d46f2e8c3f" category="list-text">CPU-Affinitätsmaske</block>
  <block id="cb00a2408ea648cf85edfd7dbb23fbe0" category="list-text">Affinity-E/A-Maske</block>
  <block id="49e103832576f4486179df8868372d17" category="section-title">Max. Parallelitätsgrad (MAXDOP)</block>
  <block id="a99f7ebb3b0f5b0ecea93ed586b2a17d" category="section-title">Max. Worker-Threads</block>
  <block id="aff27140f50a549b1331add8d5025f7e" category="paragraph">Die Option Max. Worker-Threads hilft, die Leistung zu optimieren, wenn eine große Anzahl von Clients mit SQL Server verbunden ist.</block>
  <block id="decd79137325e1efbc7b994f946ea24f" category="paragraph">Der Standardwert ist 0, wodurch SQL Server die Anzahl der Worker-Threads beim Start automatisch konfigurieren kann. Dies funktioniert für die meisten Systeme. Max Worker-Threads sind eine erweiterte Option und sollten nicht ohne Unterstützung durch einen erfahrenen Datenbankadministrator (DBA) geändert werden.</block>
  <block id="030a270970f70c1bd91d3689b6f95f3f" category="inline-link-macro">Konfigurieren Sie die Option Max Worker Threads Server Configuration</block>
  <block id="2a0c648df721b9bc2643d59c4a3a310a" category="paragraph">Wann sollten Sie SQL Server so konfigurieren, dass mehr Worker-Threads verwendet werden? Wenn die durchschnittliche Länge der Arbeitswarteschlange für jeden Scheduler über 1 liegt, können Sie vom Hinzufügen weiterer Threads zum System profitieren, jedoch nur, wenn die Last nicht CPU-gebunden ist oder andere schwere Wartezeiten auftritt. Wenn einer dieser Vorgänge stattfindet, sind weitere Threads nicht hilfreich, da sie schließlich auf andere Systemengpässe warten müssen. Weitere Informationen zu max. Worker-Threads finden Sie unter <block ref="77c391df5f9f06cdf367c2a7314ce351" category="inline-link-macro-rx"></block>.</block>
  <block id="29d04e616d2293c4c7538dd6226feac5" category="section-title">Konfigurieren von max Worker-Threads mit SQL Server Management Studio</block>
  <block id="44dbc82801d84781083ed9e23b63be00" category="list-text">Für &lt; oder = bis 8 Kerne: Tempdb-Datendateien = Anzahl der Kerne</block>
  <block id="6203fcdbdad10bb603fd00edd2f0fcf5" category="list-text">Für &gt; 8 Kerne: 8 tempdb-Datendateien</block>
  <block id="b9f341e4fb0e6b384aa7ec0b5c33f964" category="paragraph">Mit dem folgenden Beispielskript wird tempdb geändert, indem acht tempdb-Dateien erstellt und tempdb auf den Mount-Punkt verschoben wird<block ref="ae6fbe62f3ddc46fa9a9885244d23675" prefix=" " category="inline-code"></block> Für SQL Server 2012 und höher.</block>
  <block id="0424163d6b24f29436021dfd7072863b" category="paragraph">Ab SQL Server 2016 wird die Anzahl der für das Betriebssystem sichtbaren CPU-Kerne während der Installation automatisch erkannt. Auf Basis dieser Anzahl berechnet und konfiguriert SQL Server die Anzahl der für eine optimale Performance erforderlichen tempdb-Dateien.</block>
  <block id="3b5b2ca12603a36517ce602ffe47361c" category="list-text">NetApp SnapCenter als Backup-Software, einschließlich:</block>
  <block id="36a92a010308100e1efc10d9eb98c369" category="list-text">SnapCenter Plug-in für Microsoft Windows</block>
  <block id="ec51578d7d8f930329548bf3d00d7038" category="list-text">SnapCenter Plug-in für SQL Server</block>
  <block id="a6d698a68f555339f6e91957f636ad69" category="list-text">Architektur und Administration von Microsoft SQL Server</block>
  <block id="697ea025ecb303d4f40d1bf3a8b4bde0" category="inline-link-macro">NetApp Interoperabilitäts-Matrix-Tool (IMT)</block>
  <block id="fbd4c4f9516af91c6c3cb5fa35fb5720" category="doc">Überlegungen zum Microsoft SQL Server-Storage</block>
  <block id="3da07c62fca4dd242f7ffcfcf72998be" category="section-title">Datenspeicher Design</block>
  <block id="9e45010900186b3cbbe04f954a6f3562" category="paragraph">Für SQL Server-Datenbanken, die keine Backups mit SnapCenter durchführen, empfiehlt Microsoft, die Daten und Log-Dateien auf separaten Laufwerken zu platzieren. Bei Anwendungen, die gleichzeitig Daten aktualisieren und anfordern, ist die Protokolldatei schreibintensiv und die Datendatei (je nach Anwendung) ist Lese-/schreibintensiv. Für den Datenabruf wird die Protokolldatei nicht benötigt. Daher können Datenanfragen aus der Datendatei auf dem eigenen Laufwerk bearbeitet werden.</block>
  <block id="df018ea4abdb71f91802ea1d4fb0fb37" category="inline-link-macro">Platzieren Sie Daten- und Protokolldateien auf separaten Laufwerken</block>
  <block id="f2f25c68d67c561be1926b4b06e676b0" category="paragraph">Wenn Sie eine neue Datenbank erstellen, empfiehlt Microsoft, getrennte Laufwerke für die Daten und Protokolle anzugeben. Um Dateien nach der Datenbankerstellung zu verschieben, muss die Datenbank offline geschaltet werden. Weitere Empfehlungen von Microsoft finden Sie unter <block ref="04fab8828edf0d2e95eff3c4f124b372" category="inline-link-macro-rx"></block>.</block>
  <block id="d5350d8759b1ec84607e47908809058e" category="section-title">Aggregate</block>
  <block id="77f1d6b9cbcaf3663f6bdd19a821e773" category="list-text">Ein großes Aggregat ermöglicht die effizienteste Nutzung von Festplattenspeicher.</block>
  <block id="63d9962fabe500361986d05317bd65a5" category="paragraph">Platzieren Sie für Hochverfügbarkeit (HA) das sekundäre synchrone Replikat der SQL Server Always On Availability Group auf einer separaten Storage Virtual Machine (SVM) im Aggregat. Platzieren Sie zum Zweck der Disaster Recovery das asynchrone Replikat in einem Aggregat, das Teil eines separaten Storage-Clusters am DR-Standort ist, und Inhalte werden mithilfe der NetApp SnapMirror Technologie repliziert. NetApp empfiehlt eine Verfügbarkeit von mindestens 10 % freien Speicherplatz in einem Aggregat zugunsten der optimalen Storage-Performance.</block>
  <block id="c6f01c78bfe0a0a495cb5d3ed77824a9" category="section-title">Volumes</block>
  <block id="17fce39141e6fa457c789ce2c3239b0c" category="section-title">Überlegungen zum Volume-Design</block>
  <block id="d1c9fdbcdbb8521971decf35c6d1c0c8" category="paragraph">Bevor Sie ein Datenbank-Volume-Design erstellen, ist es wichtig zu wissen, wie das I/O-Muster und die Merkmale von SQL Server je nach Workload und Backup- und Recovery-Anforderungen variieren. Beachten Sie die folgenden NetApp Empfehlungen für flexible Volumes:</block>
  <block id="674343739e297abb52c536a453ebaad0" category="list-text">Verwenden Sie NTFS-Bereitstellungspunkte anstelle von Laufwerksbuchstaben, um die Beschränkung auf 26 Laufwerksbuchstaben in Windows zu überschreiten. Bei der Verwendung von Volume-Mount-Punkten wird generell empfohlen, dem Volume-Label den gleichen Namen wie dem Mount-Punkt zu geben.</block>
  <block id="d5fe4f7f1c47df1ec78c926d21498f73" category="list-text">Konfigurieren Sie bei Bedarf eine Richtlinie für die automatische Größenanpassung von Volumes, um Speicherplatzbelegung zu verhindern. 17 Best Practice Guide für Microsoft SQL Server mit ONTAP © 2022 NetApp, Inc Alle Rechte vorbehalten.</block>
  <block id="4176288dbc134524f79476ba4e57e3aa" category="list-text">Tempdb ist eine Systemdatenbank, die von SQL Server als temporärer Arbeitsbereich verwendet wird, insbesondere für I/O-intensive DBCC-CHECKDB-Vorgänge. Platzieren Sie diese Datenbank daher auf einem dedizierten Volume mit einem separaten Satz von Spindeln. In großen Umgebungen, in denen die Volume-Anzahl eine Herausforderung ist, können Sie tempdb in weniger Volumes konsolidieren und im gleichen Volume wie andere Systemdatenbanken nach einer sorgfältigen Planung speichern. Datenschutz für tempdb hat keine hohe Priorität, da diese Datenbank bei jedem Neustart von SQL Server neu erstellt wird.</block>
  <block id="b703a2ceaabee81dd9dfcfb3a4b738ee" category="list-text">Stellen Sie sicher, dass sich die Benutzerdatenbankdateien und das Protokollverzeichnis für das Protokoll-Backup auf separaten Volumes befinden, damit die Aufbewahrungsrichtlinie Snapshots bei Verwendung der SnapVault-Technologie nicht überschreibt.</block>
  <block id="ae72f885471636cbfd0dbe902ddf24e1" category="list-text">Wenn Sie LUNs mit DiskManager oder anderen Werkzeugen erstellen, stellen Sie sicher, dass die Größe der Zuordnungseinheit beim Formatieren der LUNs auf 64K für Partitionen festgelegt ist.</block>
  <block id="ac4782a9243acaaff52164f9e47417d5" category="inline-link-macro">Microsoft Windows und natives MPIO unter den Best Practices von ONTAP für modernes SAN</block>
  <block id="934e206705ddafc720e4a8f25a4acded" category="list-text">Siehe <block ref="95506eaa4fca841adc20747ae4650d10" category="inline-link-macro-rx"></block> So wenden Sie Multipathing-Unterstützung unter Windows auf iSCSI-Geräte in den MPIO-Eigenschaften an.</block>
  <block id="31a8d85b6a44cffc2c7dfc6387696f31" category="section-title">Protokollverzeichnis</block>
  <block id="29766ed5967263787fface0a4ad3396f" category="paragraph">Die Größe des Host-Log-Verzeichnisses wird wie folgt berechnet:
Größe des Host-Log-Verzeichnisses = ( (maximale DB-LDF-Größe x tägliche Log-Änderungsrate %) x (Snapshot-Aufbewahrung) ÷ (1 - LUN Overhead-Speicherplatz %)
Die Formel zur Größenbestimmung des Host-Protokollverzeichnisses nimmt einen LUN Overhead von 10 % an</block>
  <block id="31333b887781902e13c263570523ddc1" category="paragraph">Platzieren Sie das Protokollverzeichnis auf einem dedizierten Volume oder LUN. Die Datenmenge im Host-Log-Verzeichnis hängt von der Größe der Backups und der Anzahl der Tage ab, die Backups aufbewahrt werden. SnapCenter erlaubt nur ein Host-Protokollverzeichnis pro SQL Server-Host. Sie können die Host-Protokollverzeichnisse unter SnapCenter --&gt; Host --&gt; Configure Plug-in konfigurieren.</block>
  <block id="e815059019027c04aa969a50787d5b34" category="paragraph">*NetApp empfiehlt* für ein Host-Log-Verzeichnis:</block>
  <block id="07ae4edb1ccfbfe1f5da6006c95ee4a9" category="list-text">Stellen Sie sicher, dass das Host-Protokollverzeichnis nicht von anderen Datentypen gemeinsam genutzt wird, die möglicherweise die Backup-Snapshot-Daten beschädigen können.</block>
  <block id="991bd1f492a593453f65c2a23c1f1612" category="list-text">Platzieren Sie keine Benutzerdatenbanken oder Systemdatenbanken auf einer LUN, die Bereitstellungspunkte hostet.</block>
  <block id="7b01578e73f5081fecdcc6dedf28aa3c" category="list-text">Migrieren Sie Datenbanken mithilfe von SnapCenter-Assistenten in NetApp Storage, damit die Datenbanken an gültigen Speicherorten gespeichert werden und so erfolgreiche SnapCenter-Backup- und -Restore-Vorgänge ermöglichen. Beachten Sie, dass der Migrationsprozess für den Fall von Unterbrechungen verantwortlich ist und dazu führen kann, dass die Datenbanken offline gehen, während die Migration durchgeführt wird.</block>
  <block id="b2d0e2f60ee139155b728ef6933efe7c" category="list-text">Die folgenden Bedingungen müssen für Failover-Cluster-Instanzen (FCIs) von SQL Server gelten:</block>
  <block id="863fda0972208e1e15d0c4a16d6915af" category="list-text">Wenn Sie eine Failover-Cluster-Instanz verwenden, muss das Host-Log-Verzeichnis LUN eine Cluster-Festplattenressource in derselben Cluster-Gruppe sein wie die SQL Server-Instanz, die SnapCenter gesichert wird.</block>
  <block id="ddb1104e1e71caabad3e7639c7675af8" category="list-text">Wenn Sie eine Failover-Cluster-Instanz verwenden, müssen Benutzerdatenbanken auf gemeinsam genutzte LUNs platziert werden, bei denen es sich um physische Festplatten-Cluster-Ressourcen handelt, die der Cluster-Gruppe zugewiesen sind, die der SQL Server-Instanz zugeordnet ist.</block>
  <block id="bca5f9a18696e080113088282fe481de" category="doc">Microsoft SQL Server-Speicherkonfiguration</block>
  <block id="02578ec0e8ce2fbef37ca792fd20c289" category="section-title">Max. Serverspeicher</block>
  <block id="6e345f11e9b2856a7ac40d6cf283606b" category="paragraph">Auf einem SQL Server-Cluster mit mehreren SQL Server-Instanzen könnte jede Instanz mit Ressourcen konkurrieren. Durch die Festlegung einer Speichergrenze für jede SQL Server-Instanz kann eine optimale Performance für jede Instanz gewährleistet werden.</block>
  <block id="318b34a3511a1be6d179a73303da2077" category="admonition">*NetApp empfiehlt*, mindestens 4 GB bis 6 GB RAM für das Betriebssystem zu belassen, um Leistungsprobleme zu vermeiden.</block>
  <block id="cd151dbdf946ad6bd27e1d55ea1cd029" category="section-title">Anpassen des minimalen und maximalen Serverspeichers mit SQL Server Management Studio</block>
  <block id="998c5e632fa0f987c20c182fc9322f67" category="section-title">Uneinheitlicher Speicherzugriff</block>
  <block id="b3b5cd113ed637ebe6cb28eab322307a" category="section-title">Index Speicher erstellen</block>
  <block id="631c6718cc62ad33b6e3b3b48d754569" category="paragraph">Er steuert die maximale RAM-Größe, die ursprünglich für die Erstellung von Indizes zugewiesen wurde. Der Standardwert für diese Option ist 0, was bedeutet, dass sie von SQL Server automatisch verwaltet wird. Wenn Sie jedoch Schwierigkeiten beim Erstellen von Indizes haben, sollten Sie den Wert dieser Option erhöhen.</block>
  <block id="8c91aa3f5264c57bd2d2e66bd70f2ad1" category="section-title">Min. Arbeitsspeicher pro Abfrage</block>
  <block id="181341700086c5e2f2259774b2aaa59c" category="inline-link-macro">Implementierung Der Seitenkomprimierung</block>
  <block id="46c6aaf21d639d23d4f297a1f6922d9a" category="paragraph">Durch die Zeilenkomprimierung wird das Datenspeicherformat geändert. So werden beispielsweise ganze Zahlen und Dezimalzahlen anstelle des nativen Formats mit fester Länge in das Format mit variabler Länge geändert. Außerdem werden Zeichenketten mit fester Länge durch das Entfernen von Leerzeichen in das Format mit variabler Länge geändert. Die Seitenkomprimierung implementiert die Zeilenkomprimierung und zwei weitere Komprimierungsstrategien (Prefix-Komprimierung und Wörterbuchkomprimierung). Weitere Details zur Seitenkomprimierung finden Sie unter <block ref="88eab602b149fe360da37498ad5cdeae" category="inline-link-macro-rx"></block>.</block>
  <block id="0d629ff07f9214182fa91977089ff029" category="paragraph">Die Datenkomprimierung wird derzeit in den Enterprise-, Developer- und Evaluation-Editionen von SQL Server 2008 und höher unterstützt. Obwohl die Komprimierung von der Datenbank selbst durchgeführt werden kann, ist dies in einer SQL Server Umgebung nur selten der Fall.</block>
  <block id="c6757fbc56df4ba20f0ef165aeb214d0" category="paragraph">Hier sind die Empfehlungen für die Verwaltung von Speicherplatz für SQL Server-Datendateien</block>
  <block id="46344e63db1ea2128cc47543b10a0f96" category="list-text">Verwenden Sie Thin Provisioning in SQL Server-Umgebungen, um die Speicherplatzauslastung zu verbessern und bei Einsatz der Speicherplatzgarantiefunktion den gesamten Storage-Bedarf zu senken.</block>
  <block id="924e1b131d6b3a2846efd1a81de8eac1" category="list-text">Verwenden Sie Autogrow für die meisten gängigen Implementierungskonfigurationen, da der Storage-Administrator nur die Speicherplatznutzung im Aggregat überwachen muss.</block>
  <block id="752d71c042ef775879f6db705ccf6b31" category="section-title">Speicherplatzrückgewinnung</block>
  <block id="89f8656e4e33094d6011ef9aaa8fa8de" category="paragraph">Die Rückgewinnung von ungenutztem Speicherplatz in einer LUN kann regelmäßig gestartet werden. Bei SnapCenter können Sie den folgenden PowerShell Befehl verwenden, um die Rückgewinnung von ungenutztem Speicherplatz zu starten.</block>
  <block id="aa864feb1e600aa376e91ac119da387b" category="paragraph">Wenn Sie die Speicherplatzrückgewinnung durchführen müssen, sollte dieser Prozess in Zeiten geringer Aktivität ausgeführt werden, da er anfangs Hostzyklen beansprucht.</block>
  <block id="5c014f8939a89da0166a347a0237cf4f" category="section-title">SnapCenter</block>
  <block id="1567bc0803275089d7cbc1ffc468158c" category="inline-link-macro">TR-4714: Best Practice Guide für SQL Server mit NetApp SnapCenter</block>
  <block id="17f58c1b5cb5feb5dcaf49ca87298c00" category="paragraph">Weitere Informationen zum SQL Server-Plug-in für SnapCenter finden Sie unter <block ref="c94de7812b9bc9ed76b3c4005974958d" category="inline-link-macro-rx"></block>.</block>
  <block id="91a7baba41dc4b5f958101b261cf179b" category="section-title">Datenbanken mit T-SQL-Snapshots werden gesichert</block>
  <block id="02e19b2989ebddca9ad3778807bdc426" category="paragraph">Im Folgenden finden Sie ein Beispiel für einen Backup-Workflow:</block>
  <block id="bd03301405e4dd8f2460ff1788765ac4" category="list-text">Führen Sie Snapshots mehrerer Datenbanken auf den Speichervolumes gleichzeitig mit den neuen Befehlen BACKUP-GRUPPE und BACKUP-SERVER durch.</block>
  <block id="a3b19be7df3ddff7295558d9934cd9b6" category="list-text">Führen Sie VOLLSTÄNDIGE Backups oder COPY_ONLY VOLLSTÄNDIGE Backups durch. Diese Backups werden auch in msdb aufgezeichnet.</block>
  <block id="e2d06152222f63515f4b83229cb5444c" category="list-text">Durchführung einer zeitpunktgenauen Recovery mithilfe von Protokoll-Backups, die mit dem normalen Streaming-Ansatz nach dem VOLLSTÄNDIGEN Snapshot-Backup erstellt wurden. Streaming Differential Backups werden auf Wunsch auch unterstützt.</block>
  <block id="a7a2496066febda8d325e0964e434481" category="inline-link-macro">Microsoft-Dokumentation zu den T-SQL-Snapshots</block>
  <block id="bf06f3f95c5b653499bb4ac53ba44c95" category="paragraph">Weitere Informationen finden Sie unter <block ref="2ba8392a398067f6d2e5f6e4167e9d00" category="inline-link-macro-rx"></block>.</block>
  <block id="450eb6d84ec721a20f17b7778b24b457" category="doc">Microsoft SQL Server-Workloads</block>
  <block id="1b1b4c822e9c1b28b845dcc038fec08e" category="doc">Microsoft SQL Server Datenbankdateien und Dateigruppen</block>
  <block id="5306de2e4bff067a10c1c9b5aafe5920" category="paragraph">Theoretisch unterstützt SQL Server (64-Bit) 32,767 Datenbanken pro Instanz und 524.272 TB Datenbankgröße, obwohl die typische Installation normalerweise über mehrere Datenbanken verfügt. Die Anzahl der Datenbanken, die SQL Server verarbeiten kann, hängt jedoch von der Last und der Hardware ab. Es ist nicht ungewöhnlich, dass SQL Server Instanzen Dutzende, Hunderte oder sogar Tausende kleine Datenbanken hosten.</block>
  <block id="9a3f42cec243006996ab4580d3ce5d56" category="paragraph">Jede Datenbank besteht aus einer oder mehreren Datendateien und einer oder mehreren Transaktions-Log-Dateien. Das Transaktionsprotokoll speichert die Informationen über Datenbanktransaktionen und alle von jeder Sitzung vorgenommenen Datenänderungen. Jedes Mal, wenn die Daten geändert werden, speichert SQL Server genügend Informationen im Transaktionsprotokoll, um die Aktion rückgängig zu machen (Rollback) oder zu wiederholen (Replay). Ein SQL Server-Transaktionsprotokoll ist ein integraler Bestandteil des Rufs von SQL Server für Datenintegrität und Robustheit. Das Transaktionsprotokoll ist für die Atomizität, Konsistenz, Isolation und Strapazierfähigkeit (ACID) von SQL Server von entscheidender Bedeutung. SQL Server schreibt in das Transaktionsprotokoll, sobald eine Änderung an der Datenseite erfolgt. Jede DML-Anweisung (Data Manipulation Language) (z. B. SELECT, Insert, Update oder delete) ist eine vollständige Transaktion, und das Transaktionsprotokoll stellt sicher, dass der gesamte Set-basierte Vorgang durchgeführt wird, um die Atomizität der Transaktion sicherzustellen.</block>
  <block id="cbe6958e46fbab53cdbdb9deb2d75938" category="paragraph">Jede Datenbank verfügt über eine primäre Datendatei, die standardmäßig über die Erweiterung .mdf verfügt. Darüber hinaus kann jede Datenbank sekundäre Datenbankdateien enthalten. Diese Dateien haben standardmäßig .ndf-Erweiterungen.</block>
  <block id="811b578b6dfbb4a4ca16ba0c0911e3df" category="paragraph">Alle Datenbankdateien werden in Dateigruppen gruppiert. Eine Dateigruppe ist die logische Einheit, die die Datenbankverwaltung vereinfacht. Sie ermöglichen die Trennung zwischen einer logischen Objektplatzierung und physischen Datenbankdateien. Wenn Sie die Tabellen für Datenbankobjekte erstellen, geben Sie an, in welcher Dateigruppe sie platziert werden sollen, ohne sich um die zugrunde liegende Datendateikonfiguration zu sorgen.</block>
  <block id="3ac46c2f04dec9d215efbe44d307a020" category="admonition">*NetApp empfiehlt* die Verwendung der primären Dateigruppe für alles andere als Systemobjekte zu vermeiden. Das Erstellen einer separaten Dateigruppe oder einer Gruppe von Dateigruppen für die Benutzerobjekte vereinfacht die Datenbankverwaltung und Disaster Recovery, insbesondere bei großen Datenbanken.</block>
  <block id="e3fc56c012a5d0c9f92dff9b49bcc80d" category="paragraph">Sie können die ursprüngliche Dateigröße und die automatischen Wachstumsparameter angeben, wenn Sie die Datenbank erstellen oder neue Dateien zu einer vorhandenen Datenbank hinzufügen. SQL Server verwendet einen proportionalen Füllalgorithmus bei der Auswahl der Datendatei, in die Daten geschrieben werden sollen. Es schreibt eine Datenmenge proportional zum verfügbaren freien Speicherplatz in den Dateien. Je mehr Speicherplatz in der Datei verfügbar ist, desto mehr Schreibvorgänge werden verarbeitet.</block>
  <block id="91d249de86b62102d6324849de2360fe" category="admonition">*NetApp empfiehlt*, dass alle Dateien in der einzelnen Dateigruppe die gleiche Anfangsgröße und die gleichen Autogrowth-Parameter haben, wobei die Grow-Größe in Megabyte und nicht in Prozentsätzen definiert ist. Dies hilft dem proportionalen Füllalgorithmus, Schreibaktivitäten gleichmäßig über Datendateien hinweg auszugleichen.</block>
  <block id="54db492ee9e4d356cc8722d1f4126f77" category="paragraph">SQL Server löscht das Transaktionsprotokoll immer auf Null, und dieses Verhalten kann nicht geändert werden. Sie können jedoch festlegen, ob Datendateien auf Null gesetzt werden, indem Sie die sofortige Dateiinitialisierung aktivieren oder deaktivieren. Durch die sofortige Dateiinitialisierung wird das Wachstum von Datendateien beschleunigt und der Zeitaufwand für die Erstellung oder Wiederherstellung der Datenbank verringert.</block>
  <block id="92208bd4d9c6fec3f7c1dedeca62e135" category="paragraph">Mit der sofortigen Dateiinitialisierung ist ein kleines Sicherheitsrisiko verbunden. Wenn diese Option aktiviert ist, können nicht zugewiesene Teile der Datendatei Informationen aus zuvor gelöschten Betriebssystemdateien enthalten. Datenbankadministratoren können solche Daten prüfen.</block>
  <block id="2b222ad4000e6e966783fd7bc480180e" category="paragraph">Sie können die sofortige Dateiinitialisierung aktivieren, indem Sie dem SQL Server-Startkonto die Berechtigung SA_MANAGE_VOLUME_NAME, auch bekannt als „Perform Volume Maintenance Task“, hinzufügen. Sie können dies unter der Anwendung zur Verwaltung lokaler Sicherheitsrichtlinien (secpol.msc) tun, wie in der folgenden Abbildung dargestellt. Öffnen Sie die Eigenschaften für die Berechtigung zum Ausführen von Volume-Wartungsaufgaben und fügen Sie das SQL Server-Startkonto zur Liste der Benutzer dort hinzu.</block>
  <block id="c622e0142ed854a10f98fcaa8b3487f8" category="paragraph">Um zu überprüfen, ob die Berechtigung aktiviert ist, können Sie den Code aus dem folgenden Beispiel verwenden. Dieser Code setzt zwei Trace-Flags, die SQL Server zwingen, zusätzliche Informationen in das Fehlerprotokoll zu schreiben, eine kleine Datenbank zu erstellen und den Inhalt des Protokolls zu lesen.</block>
  <block id="eba8c32bde087b29aaa1fe0d3bb32ef3" category="paragraph">Wenn die sofortige Dateiinitialisierung nicht aktiviert ist, zeigt das SQL Server-Fehlerprotokoll an, dass SQL Server die mdf-Datendatei zusätzlich zum Nullsetzen der ldf-Protokolldatei auf Null setzt, wie im folgenden Beispiel gezeigt. Wenn die sofortige Dateiinitialisierung aktiviert ist, wird nur das Nullsetzen der Protokolldatei angezeigt.</block>
  <block id="3b6c913908f5844ccb31a4d3775cb34e" category="doc">Gemeinsame Instanz von Microsoft SQL Server und dedizierte Instanzen</block>
  <block id="37fbf1be337695452699ab9df948aa25" category="paragraph">Die Fehlerbehebung bei Performance-Problemen kann kompliziert sein, da Sie herausfinden müssen, welche Instanz die eigentliche Ursache ist. Diese Frage wird gegen die Kosten von Betriebssystemlizenzen und SQL Server-Lizenzen abgewogen. Wenn die Applikations-Performance oberste Priorität hat, ist eine dedizierte Instanz sehr empfehlenswert.</block>
  <block id="c840abede1d1bcf78e7b3e30048c08b1" category="paragraph">Microsoft lizenziert SQL Server pro Kern auf Serverebene und nicht pro Instanz. Aus diesem Grund sind Datenbankadministratoren versucht, so viele SQL Server-Instanzen zu installieren, wie der Server verarbeiten kann, um Lizenzierungskosten zu sparen, was später zu größeren Performanceproblemen führen kann.</block>
  <block id="8a77155b45f0f2313c8ebf3d755463e5" category="admonition">*NetApp empfiehlt*, wenn möglich dedizierte SQL Server-Instanzen zu wählen, um optimale Leistung zu erzielen.</block>
  <block id="df7423789cebcf51de121abdae4181c3" category="summary">ONTAP- und Enterprise-Applikationen</block>
  <block id="eaf36c98b91893b7f79bd5184a23d377" category="doc">Solaris</block>
  <block id="fb04d97697a77a89d0c3f5c09cd9ba47" category="paragraph">Konfigurationsthemen für das Solaris-Betriebssystem.</block>
  <block id="eefc332f1bd0a0aa81d0ce222989294f" category="section-title">Solaris NFS-Mount-Optionen</block>
  <block id="3e484486d582145f6d93ca5f8c71e228" category="paragraph">In der folgenden Tabelle sind die Solaris NFS-Mount-Optionen für eine einzelne Instanz aufgeführt.</block>
  <block id="6622c14ce91b6b9683505626a5eebdd2" category="cell">Dateityp</block>
  <block id="f18f8a29d3e2281d374f43c78cf8f0f1" category="cell">Mount-Optionen</block>
  <block id="a45543a64530673135f37ff8f9e95e69" category="cell">AdR-Startseite</block>
  <block id="93a02670f814a2df6c830194a7244ea9" category="cell"><block ref="623bbf58c0b2f552d891a1c276bcc3bc" prefix="" category="inline-code"></block></block>
  <block id="80d52d74fd5c383b43f91575d569b42c" category="cell">Steuerdateien
Datendateien
Wiederherstellungsprotokolle</block>
  <block id="73cf9d3814b8678439c37316096219b1" category="cell"><block ref="4c899c62e1ffeb12078476828bb54351" prefix="" category="inline-code"></block></block>
  <block id="72d33cb5c8b7ff5f7fd6a894094b0697" category="cell"><block ref="e1f651debac4f720c13ae930ff3ca14a" prefix="" category="inline-code"></block></block>
  <block id="3f490427e0bf55dc5c837e7b0d4c651c" category="cell"><block ref="b9ce73264210e6d0faa4115ce7525610" prefix="" category="inline-code"></block></block>
  <block id="b00b0ab6e8494db1a230bf3da7f42fd6" category="paragraph">Die Verwendung von<block ref="545e1db0caafaa47ef37ce893c8fb92c" prefix=" " category="inline-code"></block> Hat sich in Kundenumgebungen nachweislich für eine drastische Performance-Steigerung bewährt, da die mit dem Erwerb und Freigeben von Sperren des Storage-Systems verbundene Latenz beseitigt wurde. Verwenden Sie diese Option sorgfältig in Umgebungen, in denen zahlreiche Server für die Bereitstellung derselben Dateisysteme konfiguriert sind und Oracle für das Mounten dieser Datenbanken konfiguriert ist. Dies ist zwar eine äußerst ungewöhnliche Konfiguration, wird jedoch von wenigen Kunden verwendet. Wenn eine Instanz versehentlich ein zweites Mal gestartet wird, kann es zu Datenbeschädigungen kommen, weil Oracle die Sperrdateien auf dem fremden Server nicht erkennen kann. NFS-Sperren bieten sonst keinen Schutz, wie in NFS-Version 3 sind sie nur beratend.</block>
  <block id="f480015b57b289b7dad1473ee5c55216" category="paragraph">Weil die<block ref="545e1db0caafaa47ef37ce893c8fb92c" prefix=" " category="inline-code"></block> Und<block ref="6698a391ee8befa0ca4646a86a1bbd66" prefix=" " category="inline-code"></block> Parameter schließen sich gegenseitig aus, es ist wichtig, dass<block ref="b6a2b868e8bebfc19497b406a12a1071" prefix=" " category="inline-code"></block> Befindet sich im<block ref="b6bd7c570605affc5b50bbd3e436f0c0" prefix=" " category="inline-code"></block> So speichern<block ref="6887bae4cda2d09a283a165c5be3f2b8" prefix=" " category="inline-code"></block> Verwendet wird. Ohne diesen Parameter wird Puffer-Caching des Host-Betriebssystems verwendet und die Performance kann beeinträchtigt werden.</block>
  <block id="5171d5dfdc21b0941b921796638c10a9" category="paragraph">In der folgenden Tabelle sind die Mount-Optionen für Solaris NFS RAC aufgeführt.</block>
  <block id="38e4e10abf410c92cd13e9cb4d54ae35" category="cell"><block ref="41a82b9bd64cb001f8913a34614b00d1" prefix="" category="inline-code"></block></block>
  <block id="e72cc50c204d77a1e47f0dcc8ad28a56" category="cell">Kontrolldateien
Datendateien
Wiederherstellungsprotokolle</block>
  <block id="98ee9e91a49f27abbd9dc831afa9fae5" category="cell"><block ref="f3a0fd4a197a2eae60d2ef904e5366f1" prefix="" category="inline-code"></block></block>
  <block id="f93eecf9c582d76d4e14292dc34c79f8" category="cell">CRS/Abstimmung</block>
  <block id="42c82995de255264e2a8690c3149c16a" category="cell">Dediziert<block ref="e1f651debac4f720c13ae930ff3ca14a" prefix=" " category="inline-code"></block></block>
  <block id="b71e04a676ec52e65edb52c138f2ad98" category="cell"><block ref="ac15a7c050732bc5169b6c17ce30f859" prefix="" category="inline-code"></block></block>
  <block id="273f714e976e2a709a6abe18d34e4b61" category="cell">Freigegeben<block ref="e1f651debac4f720c13ae930ff3ca14a" prefix=" " category="inline-code"></block></block>
  <block id="2052aa052eee25e077178cd737b68f47" category="cell"><block ref="18f533bce293643d1004086db883dd03" prefix="" category="inline-code"></block></block>
  <block id="e9d3beba8b6a40da6f8074236ca3ddbc" category="paragraph">Der Hauptunterschied zwischen Single-Instance- und RAC-Mount-Optionen ist das Hinzufügen von<block ref="79fea224ffd9e8111e4e87cab732464b" prefix=" " category="inline-code"></block> Und<block ref="6698a391ee8befa0ca4646a86a1bbd66" prefix=" " category="inline-code"></block> Zu den Mount-Optionen. Durch diese Ergänzung wird das Caching des Host-Betriebssystems deaktiviert, wodurch alle Instanzen im RAC Cluster eine konsistente Ansicht des Status der Daten haben. Obwohl Sie den verwenden<block ref="b6bd7c570605affc5b50bbd3e436f0c0" prefix=" " category="inline-code"></block> Parameter<block ref="b6a2b868e8bebfc19497b406a12a1071" prefix=" " category="inline-code"></block> Hat die gleiche Wirkung wie die Deaktivierung des Host-Caching, ist es weiterhin erforderlich, zu verwenden<block ref="79fea224ffd9e8111e4e87cab732464b" prefix=" " category="inline-code"></block> Und<block ref="6698a391ee8befa0ca4646a86a1bbd66" prefix=" " category="inline-code"></block>.</block>
  <block id="983c4ff0cb78851d95be06277152653f" category="paragraph">Der Grund<block ref="3504e2b0e7c968444314433be465518d" prefix=" " category="inline-code"></block> Ist für die gemeinsame Nutzung erforderlich<block ref="e1f651debac4f720c13ae930ff3ca14a" prefix=" " category="inline-code"></block> Die Bereitstellung soll die Konsistenz von Dateien wie Oracle-Passwortdateien und SPfiles erleichtern. Wenn jede Instanz in einem RAC-Cluster über einen dedizierten verfügt<block ref="e1f651debac4f720c13ae930ff3ca14a" prefix=" " category="inline-code"></block>, Dieser Parameter ist nicht erforderlich.</block>
  <block id="5a11994cffaa4d0a9c8594d2223e3e96" category="section-title">Solaris UFS-Mount-Optionen</block>
  <block id="130c5ffb786e38c2e6648fd53593681a" category="paragraph">NetApp empfiehlt nachdrücklich die Verwendung der Mount-Option für die Protokollierung, damit die Datenintegrität im Fall eines Solaris Host-Absturzes oder der Unterbrechung der FC-Konnektivität erhalten bleibt. Die Mount-Option für die Protokollierung behält außerdem die Benutzerfreundlichkeit von Snapshot Backups bei.</block>
  <block id="516f7006de2e9b1f7f871d961db4a1ca" category="section-title">Solaris ZFS</block>
  <block id="a92c2c52c7360246c6da2b1169e0fa50" category="paragraph">Solaris ZFS muss sorgfältig installiert und konfiguriert werden, um eine optimale Leistung zu erzielen.</block>
  <block id="c9208a3ad95d5ce3d9e2ba1839426251" category="section-title">Mvektor</block>
  <block id="bc7e9b3c34345bf9ebe88467d6714f1b" category="paragraph">Wenn unerwartete Probleme durch diese Änderung auftreten, kann sie einfach rückgängig gemacht werden, indem der folgende Befehl als Root ausgeführt wird:</block>
  <block id="6ff9f4444ac481652f4412b5e1623846" category="section-title">Kernel</block>
  <block id="78d06f587305b9e5481c8cf660693a61" category="paragraph">Eine zuverlässige ZFS-Performance erfordert einen Solaris-Kernel, der gegen Probleme bei der LUN-Ausrichtung gepatcht ist. Der Fix wurde mit Patch 147440-19 in Solaris 10 und SRU 10.5 für Solaris 11 eingeführt. Verwenden Sie nur Solaris 10 und höher mit ZFS.</block>
  <block id="db8880d80ab20f2f01a010e5c454f7fb" category="section-title">LUN-Konfiguration</block>
  <block id="3b296d65c0c37979abe39ac7f6d8e1d8" category="paragraph">Führen Sie zum Konfigurieren einer LUN die folgenden Schritte aus:</block>
  <block id="3bdc2a5622b76404f8b0ce2fc774e69a" category="list-text">Erstellen Sie eine LUN des Typs<block ref="7bee0477f82303aa43de5d78f7b9cb05" prefix=" " category="inline-code"></block>.</block>
  <block id="c06f6c6685c06bb821295b3a6d5e580c" category="list-text">Installieren Sie das entsprechende Host Utility Kit (HUK), das vom angegeben wird <block ref="b5b16f3cdb0eb25a282348ba54f8c677" category="inline-link-macro-rx"></block>.</block>
  <block id="33db73c9b3936cb8eaae825d3101f60c" category="inline-link-macro">Aktuellste Dokumentation</block>
  <block id="c8646086dca29c1d978ab285faa93709" category="list-text">Befolgen Sie die Anweisungen im HUK genau wie beschrieben. Die grundlegenden Schritte sind unten beschrieben, beziehen Sie sich jedoch auf <block ref="33574dfffc2cb6d01be0edb6738c51bb" category="inline-link-macro-rx"></block> Für das richtige Verfahren.</block>
  <block id="6c06455cad171d61eac3593147bd71d6" category="list-text">Führen Sie die aus<block ref="3062dc928daa4d85f6e34b6dea110a00" prefix=" " category="inline-code"></block> Dienstprogramm zum Aktualisieren des<block ref="d5416d340f5bec1fcb3addb6ae7d059e" prefix=" " category="inline-code"></block> Datei: Dadurch können die SCSI-Laufwerke ONTAP-LUNs korrekt erkennen.</block>
  <block id="7ec6668202fa9a677238143b0445102f" category="list-text">Befolgen Sie die Anweisungen des<block ref="3062dc928daa4d85f6e34b6dea110a00" prefix=" " category="inline-code"></block> Dienstprogramm zur Aktivierung von Multipath Input/Output (MPIO).</block>
  <block id="b53abe6c0013125abea171427b351ccc" category="list-text">Neustart. Dieser Schritt ist erforderlich, damit alle Änderungen im gesamten System erkannt werden.</block>
  <block id="b88caedbe9708683eed783eb534ae5f6" category="list-text">Partitionieren Sie die LUNs und stellen Sie sicher, dass sie ordnungsgemäß ausgerichtet sind. Anweisungen zum direkten Testen und Bestätigen der Ausrichtung finden Sie in Anhang B „Überprüfung der WAFL-Ausrichtung“.</block>
  <block id="a0c1c26e783204cf713b2e1f0823aab3" category="section-title">Zpools</block>
  <block id="c81b8fe4a368c32102406e65fc41a227" category="inline-link-macro">LUN-Konfiguration</block>
  <block id="869dc3a8daf591c7f33e5bb32a4a63c9" category="paragraph">Der Wert von<block ref="81df978a8e3da4bee74b135548f390f3" prefix=" " category="inline-code"></block> Der Standardwert ist 9. Dies bedeutet 2^9 oder 512 Byte. Für eine optimale Leistung, die<block ref="81df978a8e3da4bee74b135548f390f3" prefix=" " category="inline-code"></block> Wert muss 12 (2^12=4K) sein. Dieser Wert wird zum Zeitpunkt der Erstellung des zpool gesetzt und kann nicht geändert werden, was bedeutet, dass Daten in zpools mit<block ref="81df978a8e3da4bee74b135548f390f3" prefix=" " category="inline-code"></block> Andere als 12 sollten durch Kopieren der Daten in einen neu erstellten zpool migriert werden.</block>
  <block id="6a00a2262e030c2656588743cb031710" category="paragraph">Überprüfen Sie nach dem Erstellen eines zpool den Wert von<block ref="81df978a8e3da4bee74b135548f390f3" prefix=" " category="inline-code"></block> Bevor Sie fortfahren. Wenn der Wert nicht 12 lautet, wurden die LUNs nicht richtig erkannt. Zerstören Sie den zpool, überprüfen Sie, ob alle Schritte in der entsprechenden Host Utilities Dokumentation korrekt ausgeführt wurden, und erstellen Sie den zpool neu.</block>
  <block id="15ec722ce3f6694d12f89aeb5b621941" category="section-title">Zpools und Solaris LDOMs</block>
  <block id="8826ef54a7d108df25ed6921b5129cb6" category="paragraph">Solaris LDOMs stellen eine zusätzliche Anforderung dar, um sicherzustellen, dass die I/O-Ausrichtung korrekt ist. Obwohl eine LUN möglicherweise ordnungsgemäß als 4K-Gerät erkannt wird, erbt ein virtuelles vdsk-Gerät auf einem LDOM die Konfiguration nicht von der I/O-Domäne. Die vdsk auf Basis dieser LUN wird standardmäßig auf einen 512-Byte-Block zurückgesetzt.</block>
  <block id="9a984f24f825e4f0bca1f36e11acbca9" category="paragraph">Eine zusätzliche Konfigurationsdatei ist erforderlich. Zunächst müssen die einzelnen LDOMs für Oracle Bug 15824910 gepatcht werden, um die zusätzlichen Konfigurationsoptionen zu aktivieren. Dieser Patch wurde in alle derzeit verwendeten Versionen von Solaris portiert. Sobald das LDOM gepatcht ist, kann es wie folgt konfiguriert werden:</block>
  <block id="e0f306ab540315b015db849b1ae9099e" category="list-text">Identifizieren Sie die LUN oder LUNs, die in dem neuen zpool verwendet werden sollen. In diesem Beispiel handelt es sich um das c2d1-Gerät.</block>
  <block id="f469046d419183a7be08ed5a13d2c65e" category="list-text">Rufen Sie die vdc-Instanz der Geräte ab, die für einen ZFS-Pool verwendet werden sollen:</block>
  <block id="bcaa63dcd5db76bffd8b867e8720a272" category="list-text">Bearbeiten<block ref="ebceeaa7b6ce5abf6c8de7177255fef5" prefix=" " category="inline-code"></block>:</block>
  <block id="ec960a08b4666fe258b6e005064378ab" category="paragraph">Dies bedeutet, dass Geräteinstanz 1 eine Blockgröße von 4096 zugewiesen wird.</block>
  <block id="3df4cdc476a0c3a5a930597e9d8da84c" category="paragraph">Nehmen wir als weiteres Beispiel an, dass die vdsk-Instanzen 1 bis 6 für eine 4-KB-Blockgröße und konfiguriert sein müssen<block ref="74a491ed941b00d0c9909d488eee67bf" prefix=" " category="inline-code"></block> Lautet wie folgt:</block>
  <block id="dfed6d8974f03c5b31f20c9c5c2bbe66" category="list-text">Das Finale<block ref="3328fba80b08b36663ffae0684f6c578" prefix=" " category="inline-code"></block> Die Datei sollte Folgendes enthalten:</block>
  <block id="764a2caff05993fc0d3eff5de7b225e9" category="cell">Achtung</block>
  <block id="5c442834a7707cc96719e468c87b2ff6" category="cell">Das LDOM muss neu gestartet werden, nachdem vdc.conf konfiguriert und vdsk erstellt wurde. Dieser Schritt kann nicht vermieden werden. Die Änderung der Blockgröße wird nur nach einem Neustart wirksam. Fahren Sie mit der Konfiguration von zpool fort und stellen Sie sicher, dass der Ashift wie zuvor beschrieben richtig auf 12 eingestellt ist.</block>
  <block id="2138e0b461ac7e557539bb0b33bde81a" category="section-title">ZFS-Absichtsprotokoll (ZIL)</block>
  <block id="5477cebfd480a13008afdd16bea2b3f9" category="paragraph">Im Allgemeinen gibt es keinen Grund, das ZFS Intent Log (ZIL) auf einem anderen Gerät zu finden. Das Protokoll kann Speicherplatz mit dem Hauptpool teilen. Die primäre Verwendung eines separaten ZIL ist, wenn physische Laufwerke verwendet werden, denen die Schreib-Cache-Funktionen in modernen Speicher-Arrays fehlen.</block>
  <block id="26a004aeb043de19f767bd7daa39cf2f" category="section-title">Logbias</block>
  <block id="07fff183bc4c01bf9bbb21a48b389845" category="paragraph">Stellen Sie die ein<block ref="26a004aeb043de19f767bd7daa39cf2f" prefix=" " category="inline-code"></block> Parameter auf ZFS-Dateisystemen, auf denen Oracle-Daten gehostet werden.</block>
  <block id="ac01fbc5e98d6450f3b565d0617f4a55" category="paragraph">Die Verwendung dieses Parameters verringert die Gesamtschreibebenen. Unter den Standardeinstellungen werden geschriebene Daten zuerst an das ZIL und dann an den Hauptspeicherpool übertragen. Dieser Ansatz eignet sich für eine Konfiguration mit einer einfachen Laufwerkskonfiguration, die ein SSD-basiertes ZIL-Gerät und rotierende Medien für den Hauptspeicherpool umfasst. Dies liegt daran, dass eine Übertragung in einer einzelnen I/O-Transaktion auf den Medien mit der niedrigsten verfügbaren Latenz ausgeführt werden kann.</block>
  <block id="c6cc0f147c42f124079d7fc1a0761886" category="paragraph">Bei Verwendung eines modernen Storage Array mit eigener Caching-Funktion ist dieser Ansatz in der Regel nicht erforderlich. In seltenen Fällen ist es wünschenswert, einen Schreibvorgang mit einer einzigen Transaktion in das Protokoll übertragen zu können, z. B. bei einem Workload, der aus hochkonzentrierten, latenzempfindlichen zufälligen Schreibvorgängen besteht. Die Form der Write Amplification hat Folgen, da die protokollierten Daten schließlich in den Haupt-Storage Pool geschrieben werden, wodurch die Schreibaktivität verdoppelt wird.</block>
  <block id="92b9531657a4ce6a31c623d889a2c55d" category="section-title">Direkter I/O</block>
  <block id="387f4a4c250ed7a5003f3f6a19abfd1f" category="paragraph">Viele Applikationen, darunter auch Oracle Produkte, können den Host-Puffer-Cache umgehen, indem sie direkten I/O aktivieren Diese Strategie funktioniert bei ZFS-Dateisystemen nicht wie erwartet. Obwohl der Host-Puffer-Cache umgangen wird, speichert ZFS selbst weiterhin Daten im Cache. Dies kann zu irreführenden Ergebnissen führen, wenn Tools wie fio oder sio für Performance-Tests verwendet werden, da schwer vorherzusagen ist, ob I/O das Storage-System erreicht oder ob es lokal im BS zwischengespeichert wird. Diese Aktion macht es auch sehr schwierig, solche synthetischen Tests zu verwenden, um ZFS-Leistung mit anderen Dateisystemen zu vergleichen. In der Praxis gibt es bei echten Benutzer-Workloads kaum bis keine Unterschiede in der Filesystem-Performance.</block>
  <block id="ec79b30d56e7bae54aa7efedccc47e93" category="section-title">Mehrere zpools</block>
  <block id="f5aa5b31f0536ddcdb748ae360d91358" category="paragraph">Snapshot-basierte Backups, Wiederherstellungen, Klone und Archivierung von ZFS-basierten Daten müssen auf der Ebene von zpool durchgeführt werden und erfordern in der Regel mehrere zpools. Ein zpool ist analog zu einer LVM-Plattengruppe und sollte mit denselben Regeln konfiguriert werden. Beispielsweise ist eine Datenbank wahrscheinlich am besten mit den Datendateien in ausgelegt<block ref="f1fb162e58045170396a0ac29d648fa5" prefix=" " category="inline-code"></block> Und die Archivprotokolle, Kontrolldateien und Wiederherstellungsprotokolle befinden sich auf<block ref="92073f03d18c8eaec3eb5e0908210e1a" prefix=" " category="inline-code"></block>. Dieser Ansatz ermöglicht ein Standard-Hot Backup, bei dem sich die Datenbank im Hot Backup-Modus befindet, gefolgt von einem Snapshot von<block ref="f1fb162e58045170396a0ac29d648fa5" prefix=" " category="inline-code"></block>. Die Datenbank wird dann aus dem Hot Backup-Modus entfernt, das Protokollarchiv wird erzwungen und ein Snapshot von<block ref="92073f03d18c8eaec3eb5e0908210e1a" prefix=" " category="inline-code"></block> Wird erstellt. Ein Wiederherstellungsvorgang erfordert das Abhängen der zfs-Dateisysteme und den vollständigen Offlining des zpool nach einer SnapRestore-Wiederherstellung. Der zpool kann dann wieder online gebracht werden und die Datenbank wiederhergestellt werden.</block>
  <block id="f1110589be196c2310808482067fce1b" category="section-title">Filesystemio_options</block>
  <block id="c271ff9f255f2d215054d508448781a3" category="paragraph">Der Oracle-Parameter<block ref="f1110589be196c2310808482067fce1b" prefix=" " category="inline-code"></block> Funktioniert anders mit ZFS. Wenn<block ref="aef8f91b1e026cc1bb55e8b08c491d35" prefix=" " category="inline-code"></block> Oder<block ref="6887bae4cda2d09a283a165c5be3f2b8" prefix=" " category="inline-code"></block> Wird verwendet, Schreibvorgänge sind synchron und umgehen den BS-Puffer-Cache, aber Lesevorgänge werden von ZFS gepuffert. Diese Aktion führt zu Schwierigkeiten bei der Performance-Analyse, da I/O manchmal vom ZFS-Cache abgefangen und gewartet wird. Dadurch werden die Speicherlatenz und der gesamte I/O geringer als möglicherweise angezeigt.</block>
  <block id="b1fd823d262032e04291313f72be9452" category="doc">HP-UX ERHÄLTLICH</block>
  <block id="8ec258cd0b6ab050fc478a415a20f2e9" category="section-title">HP-UX NFS Mount-Optionen</block>
  <block id="7c5876d5737071c64af78562fe375386" category="paragraph">In der folgenden Tabelle sind die HP-UX-NFS-Mount-Optionen für eine einzelne Instanz aufgeführt.</block>
  <block id="9c9fb3cc18a83e4e46165b5c3d4ce8ef" category="cell">Kontrolldateien
Datendateien
Wiederherstellungsprotokolle</block>
  <block id="55b835c747261995c0e1022d234e286d" category="cell"><block ref="fa8766c679857b7d589983df0ab5bcf4" prefix="" category="inline-code"></block></block>
  <block id="189dbd39cabd57c5d2e51714d9a5b85b" category="paragraph">In der folgenden Tabelle sind die HP-UX-NFS-Mount-Optionen für RAC aufgeführt.</block>
  <block id="fa9da3a6a7ef8986a94c7395577c322f" category="cell"><block ref="8b080dc74532d24847a6163dcc34d93f" prefix="" category="inline-code"></block></block>
  <block id="add42ff64fa5b57e5abeab49d154a9d9" category="cell"><block ref="acdef4af05a9f0c43f72b7c600f04d62" prefix="" category="inline-code"></block></block>
  <block id="c88a7b7f83f761f7fc2979815d210140" category="cell"><block ref="d29d7b7ed8c19a99fe17fd7f3d07ea7b" prefix="" category="inline-code"></block></block>
  <block id="af8831a1b8129b099114c3237a61db88" category="paragraph">Der Hauptunterschied zwischen Single-Instance- und RAC-Mount-Optionen ist das Hinzufügen von<block ref="79fea224ffd9e8111e4e87cab732464b" prefix=" " category="inline-code"></block> Und<block ref="6698a391ee8befa0ca4646a86a1bbd66" prefix=" " category="inline-code"></block> Zu den Mount-Optionen. Durch diese Ergänzung wird das Caching des Host-Betriebssystems deaktiviert, wodurch alle Instanzen im RAC Cluster eine konsistente Ansicht des Status der Daten haben. Obwohl Sie den verwenden<block ref="b6bd7c570605affc5b50bbd3e436f0c0" prefix=" " category="inline-code"></block> Parameter<block ref="b6a2b868e8bebfc19497b406a12a1071" prefix=" " category="inline-code"></block> Hat die gleiche Wirkung wie die Deaktivierung des Host-Caching, ist es weiterhin erforderlich, zu verwenden<block ref="79fea224ffd9e8111e4e87cab732464b" prefix=" " category="inline-code"></block> Und<block ref="6698a391ee8befa0ca4646a86a1bbd66" prefix=" " category="inline-code"></block>.</block>
  <block id="b865c6aba24c3f61595471aab993d27c" category="paragraph">Der Grund<block ref="79fea224ffd9e8111e4e87cab732464b" prefix=" " category="inline-code"></block> Ist für die gemeinsame Nutzung erforderlich<block ref="e1f651debac4f720c13ae930ff3ca14a" prefix=" " category="inline-code"></block> Die Bereitstellung soll die Konsistenz von Dateien wie Oracle-Passwortdateien und SPfiles erleichtern. Wenn jede Instanz in einem RAC-Cluster über einen dedizierten verfügt<block ref="e1f651debac4f720c13ae930ff3ca14a" prefix=" " category="inline-code"></block>, Dieser Parameter ist nicht erforderlich.</block>
  <block id="898311d838a2fa2fa41b4b52a63a7f1d" category="section-title">HP-UX VxFS-Mount-Optionen</block>
  <block id="85b4a3d44370909d395ab8af28ff4927" category="paragraph">Verwenden Sie die folgenden Mount-Optionen für Dateisysteme, auf denen Oracle-Binärdateien gehostet werden:</block>
  <block id="6630bbc9a901be0d038b2c39e2faf65d" category="paragraph">Verwenden Sie die folgenden Mount-Optionen für Dateisysteme mit Datendateien, Wiederherstellungsprotokollen, Archivprotokollen und Steuerdateien, bei denen die Version von HP-UX keine gleichzeitigen I/O unterstützt:</block>
  <block id="ed37730e0f28a631768a396e7898e197" category="paragraph">Wenn gleichzeitige I/O-Vorgänge unterstützt werden (VxFS 5.0.1 und höher oder mit der ServiceGuard Storage Management Suite), verwenden Sie diese Mount-Optionen für Dateisysteme, die Datendateien, Wiederherstellungsprotokolle, Archivprotokolle und Steuerdateien enthalten:</block>
  <block id="f51ddb98d92e83609db075add5fba4d8" category="admonition">Der Parameter<block ref="d3cc759510a3aba837fc8efeb2e24b91" prefix=" " category="inline-code"></block> Insbesondere in VxFS-Umgebungen kritisch ist. Oracle empfiehlt, dass dieser Parameter in Oracle 10g R1 und höher nicht festgelegt wird, sofern nicht ausdrücklich anders angegeben. Der Standardwert bei einer Oracle 8 KB Blockgröße ist 128. Wenn der Wert dieses Parameters auf 16 oder weniger erzwungen wird, entfernen Sie den<block ref="a6b02283cba560dcfdea44af66ec792c" prefix=" " category="inline-code"></block> Mount-Option, da dadurch die sequenzielle I/O-Performance beeinträchtigt werden kann. Dieser Schritt schädigt andere Aspekte der Leistung und sollte nur erfolgen, wenn der Wert von<block ref="d3cc759510a3aba837fc8efeb2e24b91" prefix=" " category="inline-code"></block> Muss vom Standardwert geändert werden.</block>
  <block id="5802d9527a43305e9e1c903bc6b5a721" category="paragraph">Spezifische Konfigurationsthemen für das Linux-Betriebssystem unter Verwendung von AFD und ASMlib</block>
  <block id="2681013be5b0a9063a7ef0a8fe4d31da" category="section-title">ASMlib-Blockgrößen</block>
  <block id="1943673cc25055e93f0129f2baa8f499" category="paragraph">ASMlib ist eine optionale ASM-Managementbibliothek und zugehörige Dienstprogramme. Sein primärer Wert ist die Fähigkeit, eine LUN oder eine NFS-basierte Datei als ASM-Ressource mit einem für den Benutzer lesbaren Label zu stempeln.</block>
  <block id="51713fdb873939c9c333273947776b8e" category="paragraph">Aktuelle Versionen von ASMlib erkennen einen LUN-Parameter namens Logical Blocks per Physical Block Exponent (LBPPBE). Dieser Wert wurde erst vor kurzem vom ONTAP SCSI-Ziel gemeldet. Es gibt jetzt einen Wert zurück, der angibt, dass eine 4-KB-Blockgröße bevorzugt wird. Dies ist keine Definition der Blockgröße, aber es ist ein Hinweis für jede Anwendung, die LBPPBE verwendet, dass I/OS einer bestimmten Größe effizienter verarbeitet werden könnten. ASMlib interpretiert LBPPBE jedoch als Blockgröße und stempelt den ASM-Header dauerhaft, wenn das ASM-Gerät erstellt wird.</block>
  <block id="65242d5b49071240043eeef598415104" category="paragraph">Dieser Prozess kann auf verschiedene Weise Probleme mit Upgrades und Migrationen verursachen, die auf die Unfähigkeit basieren, ASMlib-Geräte mit unterschiedlichen Blockgrößen in derselben ASM-Diskgruppe zu mischen.</block>
  <block id="ea884eec0628302adc731211dc4e2cc7" category="paragraph">Beispielsweise haben ältere Arrays im Allgemeinen einen LBPPBE-Wert von 0 gemeldet oder diesen Wert überhaupt nicht gemeldet. ASMlib interpretiert dies als 512-Byte-Blockgröße. Neuere Arrays weisen daher eine 4-KB-Blockgröße auf. Es ist nicht möglich, sowohl 512-Byte- als auch 4-KB-Geräte in derselben ASM-Diskgruppe zu mischen. Dies würde verhindern, dass ein Benutzer die Größe der ASM-Diskgruppe mit LUNs aus zwei Arrays vergrößert oder ASM als Migrationstool nutzt. In anderen Fällen erlaubt RMAN möglicherweise nicht das Kopieren von Dateien zwischen einer ASM-Diskgruppe mit einer Blockgröße von 512 Byte und einer ASM-Diskgruppe mit einer Blockgröße von 4 KB.</block>
  <block id="f17ff15b23b87f35906ece98842a080f" category="paragraph">Die bevorzugte Lösung ist das Patchen von ASMlib. Die Oracle-Fehler-ID lautet 13999609, und der Patch ist in oracleasm-Support-2.1.8-1 und höher vorhanden. Mit diesem Patch kann der Benutzer den Parameter festlegen<block ref="442e2026c8afecc4611b52331b5d0d56" prefix=" " category="inline-code"></block> Bis<block ref="b326b5062b2f0e69046810717534cb09" prefix=" " category="inline-code"></block> Im<block ref="92a59ba9a0643a344be2d72cfd504181" prefix=" " category="inline-code"></block> Konfigurationsdatei Dadurch wird die Verwendung des LBPPBE-Parameters durch ASMlib blockiert, was bedeutet, dass LUNs auf dem neuen Array nun als 512-Byte-Blockgeräte erkannt werden.</block>
  <block id="f140ff6b2b71e70e551e39dacb9561ef" category="admonition">Die Option ändert nicht die Blockgröße von LUNs, die zuvor von ASMlib gestempelt wurden. Wenn beispielsweise eine ASM-Datenträgergruppe mit 512-Byte-Blöcken zu einem neuen Speichersystem migriert werden muss, das einen 4-KB-Block meldet, dann ist die Option<block ref="442e2026c8afecc4611b52331b5d0d56" prefix=" " category="inline-code"></block> Muss festgelegt werden, bevor die neuen LUNs mit ASMlib gestempelt werden.  Wenn Geräte bereits durch Oracleasm gestempelt wurden, müssen sie neu formatiert werden, bevor sie mit einer neuen Blockgröße neu aufgestempelt werden. Zuerst, dekonstruieren Sie das Gerät mit<block ref="41bd8c47b8e640da7bdb5c7dc995d435" prefix=" " category="inline-code"></block>, Und löschen Sie dann die ersten 1GB des Geräts mit<block ref="85c2e94baaf4d9c084e19d290907596e" prefix=" " category="inline-code"></block>. Wenn das Gerät zuvor partitioniert worden war, verwenden Sie schließlich die<block ref="9a287d63dabd6fbcbbe1b12344bfe922" prefix=" " category="inline-code"></block> Befehl, um veraltete Partitionen zu entfernen oder einfach das Betriebssystem neu zu starten.</block>
  <block id="75ad83418241561186f96dfed5229aaa" category="paragraph">Wenn ASMlib nicht gepatcht werden kann, kann ASMlib aus der Konfiguration entfernt werden. Diese Änderung führt zu Unterbrechungen und erfordert das Entstempeln von ASM-Festplatten und die Sicherstellung, dass die<block ref="0802edf7303a95bdecde832bbbe3a89c" prefix=" " category="inline-code"></block> Parameter ist korrekt eingestellt. Diese Änderung erfordert jedoch nicht die Migration der Daten.</block>
  <block id="ad9ac4c6d7ed2e11bdcf8c693658a482" category="section-title">Blockgrößen des ASM-Filterlaufwerks (AFD)</block>
  <block id="de59cd97a61c9f9964a35d2cd8b05c88" category="paragraph">AFD ist eine optionale ASM-Managementbibliothek, die zum Ersatz für ASMlib wird. Aus Sicht des Speichers ist es ASMlib sehr ähnlich, aber es enthält zusätzliche Funktionen wie die Möglichkeit, nicht-Oracle-I/O zu blockieren, um die Wahrscheinlichkeit von Benutzer- oder Anwendungsfehlern zu verringern, die Daten beschädigen könnten.</block>
  <block id="36dc27685773851e52f035f82e3deba2" category="section-title">Blockgrößen des Geräts</block>
  <block id="1ef92cf7e561e179d6079cc937b2778a" category="paragraph">Wie ASMlib liest auch AFD den LUN-Parameter Logical Blocks per Physical Block Exponent (LBPPBE) und verwendet standardmäßig die physische Blockgröße, nicht die logische Blockgröße.</block>
  <block id="b95ce6c503801cb15f2f9c0773c58f00" category="paragraph">Dies kann zu einem Problem führen, wenn AFD zu einer bestehenden Konfiguration hinzugefügt wird, bei der die ASM-Geräte bereits als 512-Byte-Blockgeräte formatiert sind. Der AFD-Treiber erkennt die LUN als 4K-Gerät und die Diskrepanz zwischen dem ASM-Label und dem physischen Gerät würde den Zugriff verhindern. Ebenso wären Migrationen betroffen, da es nicht möglich ist, sowohl 512-Byte- als auch 4-KB-Geräte in derselben ASM-Diskgruppe zu mischen. Dies würde verhindern, dass ein Benutzer die Größe der ASM-Diskgruppe mit LUNs aus zwei Arrays vergrößert oder ASM als Migrationstool nutzt. In anderen Fällen erlaubt RMAN möglicherweise nicht das Kopieren von Dateien zwischen einer ASM-Diskgruppe mit einer Blockgröße von 512 Byte und einer ASM-Diskgruppe mit einer Blockgröße von 4 KB.</block>
  <block id="7ef45d3a7c5fff234bf42bdf549b8339" category="paragraph">Die Lösung ist einfach: AFD enthält einen Parameter, mit dem gesteuert werden kann, ob logische oder physische Blockgrößen verwendet werden. Dies ist ein globaler Parameter, der alle Geräte im System betrifft. Um die Verwendung der logischen Blockgröße durch AFD zu erzwingen, legen Sie fest<block ref="722fc896569f3d455bb5ef8f5b8c3703" prefix=" " category="inline-code"></block> Im<block ref="dcd5fd81543c5e08e927c54e89acdd40" prefix=" " category="inline-code"></block> Datei:</block>
  <block id="47b0d7fc4c27720c4fc326722edad27c" category="section-title">Multipath-Übertragungsgrößen</block>
  <block id="7c329811bcc7bb2f50005da5a3aff250" category="paragraph">Durch die jüngsten linux-Kernel-Änderungen werden E/A-Größenbeschränkungen an Multipath-Geräte durchgesetzt, und AFD hält diese Einschränkungen nicht ein. Die I/OS werden dann abgelehnt, was dazu führt, dass der LUN-Pfad offline geschaltet wird. Dies führt dazu, dass Oracle Grid nicht installiert, ASM konfiguriert oder eine Datenbank nicht erstellt werden kann.</block>
  <block id="926e4b7a2822a39a07f61474b92e9d8c" category="paragraph">Die Lösung besteht darin, die maximale Übertragungslänge in der Datei multipath.conf für ONTAP-LUNs manuell anzugeben:</block>
  <block id="3d0c741146f1e8e31bc7d07128ca6e0a" category="admonition">Auch wenn derzeit keine Probleme vorliegen, sollte dieser Parameter eingestellt werden, wenn AFD verwendet wird, um sicherzustellen, dass ein künftiges linux-Upgrade nicht unerwartet Probleme verursacht.</block>
  <block id="d99ae60d37a12f766bc2a1f1c228fb4f" category="section-title">Gleichzeitige I/O-Vorgänge</block>
  <block id="0f3eb508f611e203aeffa91d5753648a" category="paragraph">Um eine optimale Leistung auf IBM AIX zu erzielen, muss gleichzeitig I/O verwendet werden Ohne gleichzeitige I/O-Vorgänge sind Performance-Einschränkungen wahrscheinlich, weil AIX serialisierte, atomare I/O-Vorgänge durchführt, was einen beträchtlichen Overhead nach sich zieht.</block>
  <block id="ecdc41fc6a2adf7e41e7277e12eeb805" category="paragraph">Ursprünglich hat NetApp die Verwendung von empfohlen<block ref="4482571a249db31b3abed938e4567a15" prefix=" " category="inline-code"></block> Mount-Option, um die Verwendung von gleichzeitigen I/O-Vorgängen auf dem Dateisystem zu erzwingen. Dieser Prozess hatte jedoch Nachteile und ist nicht mehr erforderlich. Seit der Einführung von AIX 5.2 und Oracle 10gR1 kann Oracle auf AIX einzelne Dateien für gleichzeitige I/O-Vorgänge öffnen, anstatt gleichzeitige I/O-Vorgänge auf dem gesamten Dateisystem zu erzwingen.</block>
  <block id="445b433b62b675f0ca0bda7acb56d41b" category="paragraph">Die beste Methode für die Aktivierung gleichzeitiger I/O ist, die festzulegen<block ref="b6bd7c570605affc5b50bbd3e436f0c0" prefix=" " category="inline-code"></block> Parameter<block ref="f1110589be196c2310808482067fce1b" prefix=" " category="inline-code"></block> Bis<block ref="aef8f91b1e026cc1bb55e8b08c491d35" prefix=" " category="inline-code"></block>. Auf diese Weise kann Oracle spezifische Dateien zur Verwendung mit gleichzeitigen I/O-Vorgängen öffnen</block>
  <block id="6206c23a0282b18352573158f9a8f9b3" category="paragraph">Wird Verwendet<block ref="4482571a249db31b3abed938e4567a15" prefix=" " category="inline-code"></block> Als Mount-Option erzwingt die Verwendung von gleichzeitigen I/O-Vorgängen, was negative Auswirkungen haben kann. Das Erzwingen von gleichzeitigen I/O-Vorgängen deaktiviert beispielsweise das Vorauslesen auf Dateisystemen, was die Performance für I/O-Vorgänge beeinträchtigen kann, die außerhalb der Oracle-Datenbanksoftware auftreten, z. B. das Kopieren von Dateien und das Durchführen von Bandsicherungen. Darüber hinaus sind Produkte wie Oracle GoldenGate und SAP BR*Tools nicht mit der Verwendung des kompatibel<block ref="4482571a249db31b3abed938e4567a15" prefix=" " category="inline-code"></block> Mount-Option mit bestimmten Versionen von Oracle.</block>
  <block id="770cb9a1e9321ca3012ecbb9ec65c422" category="list-text">Verwenden Sie das nicht<block ref="4482571a249db31b3abed938e4567a15" prefix=" " category="inline-code"></block> Mount-Option auf Filesystem-Ebene. Aktivieren Sie stattdessen Concurrent I/O über<block ref="b6a2b868e8bebfc19497b406a12a1071" prefix=" " category="inline-code"></block>.</block>
  <block id="872080b7d5ec5585776f1426fb157842" category="list-text">Verwenden Sie nur das<block ref="4482571a249db31b3abed938e4567a15" prefix=" " category="inline-code"></block> Die Mount-Option sollte aktiviert werden, wenn keine Einstellung möglich ist<block ref="b6a2b868e8bebfc19497b406a12a1071" prefix=" " category="inline-code"></block>.</block>
  <block id="0c5d86482201f32acb30cd2f7de3048f" category="section-title">Mount-Optionen für AIX NFS</block>
  <block id="1ce07cafb2507c98c8b96ad5f35fb66a" category="paragraph">In der folgenden Tabelle sind die AIX NFS-Mount-Optionen für Oracle Single-Instance-Datenbanken aufgeführt.</block>
  <block id="9a8f040dae3f0f15b337fe69a85239c0" category="cell"><block ref="86d38d6e2b36f6557e583e005e23d258" prefix="" category="inline-code"></block></block>
  <block id="d45a1a2afa30c30042b28f028fe75953" category="cell"><block ref="915be14765d6133923d803e1221f1513" prefix="" category="inline-code"></block></block>
  <block id="b74f5956b4bcda2de3c82ee97e192ab6" category="paragraph">In der folgenden Tabelle sind die AIX-NFS-Mount-Optionen für RAC aufgeführt.</block>
  <block id="b547b55ba7f2a20602494777c5426eba" category="cell"><block ref="7e879bda961bd5b7d64d2ef4f5fb7869" prefix="" category="inline-code"></block></block>
  <block id="875abb256654bd3f6da7abcb1bd92f27" category="cell"><block ref="f93eecf9c582d76d4e14292dc34c79f8" prefix="" category="inline-code"></block></block>
  <block id="3ed8fb68917b28af138fb74ccde62e80" category="cell"><block ref="415e8eb8b9fc05cb48e633d31fd04944" prefix="" category="inline-code"></block></block>
  <block id="8102775fe4080f632332ab8d3312311a" category="paragraph">Der Hauptunterschied zwischen Single-Instance- und RAC-Mount-Optionen ist das Hinzufügen von<block ref="79fea224ffd9e8111e4e87cab732464b" prefix=" " category="inline-code"></block> Zu den Mount-Optionen. Durch diese Ergänzung wird das Caching des Host-Betriebssystems deaktiviert, wodurch alle Instanzen im RAC Cluster eine konsistente Ansicht des Status der Daten haben.</block>
  <block id="1e3f10dfa324520cb7261c0da5bf6ec7" category="paragraph">Obwohl Sie den verwenden<block ref="4482571a249db31b3abed938e4567a15" prefix=" " category="inline-code"></block> Mount-Option und der<block ref="b6bd7c570605affc5b50bbd3e436f0c0" prefix=" " category="inline-code"></block> Parameter<block ref="b6a2b868e8bebfc19497b406a12a1071" prefix=" " category="inline-code"></block> Hat die gleiche Wirkung wie die Deaktivierung des Host-Caching, ist es weiterhin erforderlich, zu verwenden<block ref="79fea224ffd9e8111e4e87cab732464b" prefix=" " category="inline-code"></block>.<block ref="79fea224ffd9e8111e4e87cab732464b" prefix=" " category="inline-code"></block> Ist für die gemeinsame Nutzung erforderlich<block ref="e1f651debac4f720c13ae930ff3ca14a" prefix=" " category="inline-code"></block> Bereitstellung, um die Konsistenz von Dateien wie Oracle-Passwortdateien und zu erleichtern<block ref="1737629d60b511cf45957529a8f046e2" prefix=" " category="inline-code"></block> Parameterdateien. Wenn jede Instanz in einem RAC-Cluster über einen dedizierten verfügt<block ref="e1f651debac4f720c13ae930ff3ca14a" prefix=" " category="inline-code"></block>, Dann ist dieser Parameter nicht erforderlich.</block>
  <block id="5365dd1a92461c9e6cf65017c4a11647" category="section-title">Mount-Optionen für AIX jfs/jfs2</block>
  <block id="d7a578263480a2003f05c459a8961298" category="paragraph">In der folgenden Tabelle sind die AIX jfs/jfs2-Mount-Optionen aufgeführt.</block>
  <block id="0aabf727ab0a83c5f5d078e8b18445e0" category="cell">Standardwerte</block>
  <block id="e1f651debac4f720c13ae930ff3ca14a" category="cell">ORACLE_HOME</block>
  <block id="27a7ffa293f5a4d2a5bb54354486dece" category="paragraph">Vor der Verwendung von AIX<block ref="b7cc91287fc99a8937a55e79376c6f54" prefix=" " category="inline-code"></block> Geräte in jeder Umgebung, einschließlich Datenbanken, überprüfen Sie den Parameter<block ref="9d51746070ef554ad4e93a16de4aed0f" prefix=" " category="inline-code"></block>. Dieser Parameter entspricht nicht der HBA-Warteschlangentiefe, sondern bezieht sich auf die SCSI-Warteschlangentiefe der einzelnen<block ref="66ad66c6ace8456eae4ca6807552e54b" prefix=" " category="inline-code"></block> Ist möglicherweise zu niedrig für eine gute Leistung. Die Prüfung hat ergeben, dass der optimale Wert 64 ist.</block>
  <block id="bb79f96acbff544ded541446c9c7c894" category="doc">Microsoft Windows</block>
  <block id="14ce2582c2080a7f08fe9249f6565f40" category="paragraph">Oracle unterstützt den Einsatz von Microsoft Windows mit dem direkten NFS-Client. Diese Funktion eröffnet neue Möglichkeiten für das Management von NFS. Hierzu zählen beispielsweise die Möglichkeit, Dateien über verschiedene Umgebungen hinweg anzuzeigen, Volumes dynamisch zu skalieren und das kostengünstigere IP-Protokoll zu nutzen. Informationen zur Installation und Konfiguration einer Datenbank unter Microsoft Windows unter Verwendung von DNFS finden Sie in der offiziellen Oracle-Dokumentation. Es gibt keine speziellen Best Practices.</block>
  <block id="f854b764258138b512f30be71e1c7908" category="paragraph">Stellen Sie für eine optimale Komprimierungseffizienz sicher, dass das NTFS-Dateisystem eine Zuweisungseinheit mit 8 KB oder mehr verwendet. Die Verwendung einer 4-KB-Zuweisungseinheit, die im Allgemeinen die Standardeinstellung ist, wirkt sich negativ auf die Komprimierungseffizienz aus.</block>
  <block id="edc9f0a5a5d57797bf68e37364743831" category="doc">Linux</block>
  <block id="dfc95ed5e895617bbd0d4b1a8bfba8ca" category="paragraph">Konfigurationsthemen für das Linux-Betriebssystem.</block>
  <block id="5ecd4a6f4aedfdd45cce80a18b1a7942" category="section-title">Slot-Tabellen</block>
  <block id="2518e65e64213437c921c39a097db167" category="section-title">Mount-Optionen für Linux NFS</block>
  <block id="29cd2e6d4fc96e749c4a0e622cad7f50" category="paragraph">In der folgenden Tabelle sind die Linux-NFS-Mount-Optionen für eine einzelne Instanz aufgeführt.</block>
  <block id="9fafd031075cede0e8a5a796ec45fbff" category="paragraph">In der folgenden Tabelle sind die Linux-NFS-Mount-Optionen für RAC aufgeführt.</block>
  <block id="f16135915fe2ca5c3b0d660acc0325d3" category="cell"><block ref="4c927b3193f1cd00c7cfd3ac2e7ba8ea" prefix="" category="inline-code"></block></block>
  <block id="fc1ea13514277ff14dc705f62c31c0fc" category="cell"><block ref="4d0511346ac74044d79c9bc297d59b09" prefix="" category="inline-code"></block></block>
  <block id="13c64659680c0bc3603209997cc22225" category="cell">CRS/Abstimmung</block>
  <block id="672c5eef2603018159295408f761324b" category="cell"><block ref="26b20bdf0b0e7d3ddef3749a70c17932" prefix="" category="inline-code"></block></block>
  <block id="f7085c7dc406f5b44813ac43c18b80c0" category="paragraph">Der Hauptunterschied zwischen Single-Instance- und RAC-Mount-Optionen ist das Hinzufügen von<block ref="3504e2b0e7c968444314433be465518d" prefix=" " category="inline-code"></block> Zu den Mount-Optionen. Durch diese Ergänzung wird das Caching des Host-Betriebssystems deaktiviert, wodurch alle Instanzen im RAC Cluster eine konsistente Ansicht des Status der Daten haben. Obwohl Sie den verwenden<block ref="b6bd7c570605affc5b50bbd3e436f0c0" prefix=" " category="inline-code"></block> Parameter<block ref="b6a2b868e8bebfc19497b406a12a1071" prefix=" " category="inline-code"></block> Hat die gleiche Wirkung wie die Deaktivierung des Host-Caching, ist es weiterhin erforderlich, zu verwenden<block ref="3504e2b0e7c968444314433be465518d" prefix=" " category="inline-code"></block>.</block>
  <block id="d2537738f34705bc9d100925252c0102" category="paragraph">Der Grund<block ref="3504e2b0e7c968444314433be465518d" prefix=" " category="inline-code"></block> Ist für die gemeinsame Nutzung erforderlich<block ref="e1f651debac4f720c13ae930ff3ca14a" prefix=" " category="inline-code"></block> Die Bereitstellung soll die Konsistenz von Dateien wie den Oracle-Passwortdateien und den SPfiles erleichtern. Wenn jede Instanz in einem RAC-Cluster über einen dedizierten verfügt<block ref="e1f651debac4f720c13ae930ff3ca14a" prefix=" " category="inline-code"></block>, Dann ist dieser Parameter nicht erforderlich.</block>
  <block id="5084019c1051a9937b5ff77568ce579b" category="paragraph">Im Allgemeinen sollten nicht-Datenbankdateien mit denselben Optionen gemountet werden, die für Datendateien mit einer einzigen Instanz verwendet werden, obwohl bestimmte Applikationen unterschiedliche Anforderungen haben können. Vermeiden Sie die Montageoptionen<block ref="79fea224ffd9e8111e4e87cab732464b" prefix=" " category="inline-code"></block> Und<block ref="3504e2b0e7c968444314433be465518d" prefix=" " category="inline-code"></block> Wenn möglich, da diese Optionen das Vorauslesen und Puffern auf Dateisystemebene deaktivieren. Dies kann zu schwerwiegenden Leistungsproblemen bei Prozessen wie Extraktion, Übersetzung und Laden führen.</block>
  <block id="243de27c01fea05cc58186d2e5ad1ce5" category="section-title">ACCESS und GETATTR</block>
  <block id="59dcf78e97c197a0f240067da57a248f" category="paragraph">Einige Kunden bemerken, dass ein extrem hohes Maß an anderen IOPS wie ACCESS und GETATTR ihre Workloads dominieren kann. In Extremfällen können Vorgänge wie Lese- und Schreibvorgänge bis zu 10 % des Gesamtbetrags ausmachen. Dies ist ein normales Verhalten bei jeder Datenbank, die die Verwendung einschließt<block ref="3504e2b0e7c968444314433be465518d" prefix=" " category="inline-code"></block> Und/oder<block ref="79fea224ffd9e8111e4e87cab732464b" prefix=" " category="inline-code"></block> Unter Linux, weil diese Optionen dazu führen, dass das Linux-Betriebssystem ständig Datei-Metadaten aus dem Speichersystem neu lädt. Vorgänge wie ACCESS und GETATTR sind Vorgänge mit geringen Auswirkungen, die aus dem ONTAP Cache in einer Datenbankumgebung bedient werden. Sie sollten nicht als echte IOPS-Werte wie Lese- und Schreibvorgänge betrachtet werden, die einen echten Bedarf an Storage-Systemen verursachen. Diese anderen IOPS erzeugen jedoch eine gewisse Last, insbesondere in RAC-Umgebungen. Um diesem Problem zu begegnen, aktivieren Sie DNFS, wodurch der Puffercache des Betriebssystems umgangen wird und unnötige Metadatenvorgänge vermieden werden.</block>
  <block id="041f371e212c70407e145ae9f752a8b4" category="section-title">Linux Direct NFS</block>
  <block id="dd2f3f344f14ae5daf12cb0a817a72e0" category="paragraph">Eine zusätzliche Mount-Option, genannt<block ref="4740c034d97ca90a96b8050082816605" prefix=" " category="inline-code"></block>, Ist erforderlich, wenn (a) DNFS aktiviert ist und (b) ein Quell-Volume mehr als einmal auf einem einzelnen Server (c) mit einem verschachtelten NFS-Mount gemountet wird. Diese Konfiguration ist hauptsächlich in Umgebungen zu finden, die SAP-Anwendungen unterstützen. Beispielsweise könnte ein einzelnes Volume auf einem NetApp System über ein Verzeichnis verfügen, das sich in befindet<block ref="9f9f4ccb1fc3afcbfe0cc3f5eac4332c" prefix=" " category="inline-code"></block> Und eine Sekunde bei<block ref="ac2f9bbcd397d8f6b39e6713658051ea" prefix=" " category="inline-code"></block>. Wenn<block ref="9f9f4ccb1fc3afcbfe0cc3f5eac4332c" prefix=" " category="inline-code"></block> Ist bei montiert<block ref="9ad16912cc0ba54a259b1a15d233c264" prefix=" " category="inline-code"></block> Und<block ref="ac2f9bbcd397d8f6b39e6713658051ea" prefix=" " category="inline-code"></block> Ist bei montiert<block ref="1da707b60c3b55bd01cb7df7c171a368" prefix=" " category="inline-code"></block>Das Ergebnis sind verschachtelte NFS-Mounts, die von der gleichen Quelle stammen.</block>
  <block id="14dd7af30f62ec838445a1e167d2aff6" category="section-title">Linux Direct NFS und Oracle RAC</block>
  <block id="c6a209e4ad702c6424efb7d63d76feca" category="paragraph">Die Verwendung von DNFS bietet besondere Leistungsvorteile für Oracle RAC auf dem Linux-Betriebssystem, da Linux keine Methode zur Erzwang direkter I/O-Vorgänge bietet, die für die Kohärenz über die Knoten mit RAC erforderlich ist. Als Workaround benötigt Linux die Verwendung von<block ref="3504e2b0e7c968444314433be465518d" prefix=" " category="inline-code"></block> Mount-Option, die dazu führt, dass Dateidaten sofort aus dem OS-Cache ablaufen. Diese Option zwingt den Linux NFS Client wiederum, Attributdaten ständig neu zu lesen, was die Latenz schädigt und die Belastung des Storage Controllers erhöht.</block>
  <block id="3070cb366c38db327b0cf057d63792c1" category="paragraph">Durch die Aktivierung von DNFS wird der Host-NFS-Client umgangen und dieser Schaden wird vermieden. Mehrere Kunden haben bei der Aktivierung von DNFS deutliche Performance-Steigerungen bei RAC Clustern und deutliche geringere ONTAP-Lasten (insbesondere im Hinblick auf andere IOPS) gemeldet.</block>
  <block id="e97ab7f94d503bbec4b013b5b77118c6" category="section-title">Linux Direct NFS- und oranfstab-Datei</block>
  <block id="f63255b70940d25581e0cf5f81dd6cd6" category="paragraph">Bei der Verwendung von DNFS unter Linux mit der Multipathing-Option müssen mehrere Subnetze verwendet werden. Auf anderen Betriebssystemen können mehrere DNFS-Kanäle mithilfe des eingerichtet werden<block ref="54b4c4075463b2e02cd69f5cd139b5b2" prefix=" " category="inline-code"></block> Und<block ref="bbf52a1a57a4eaa83512cab68fd52b70" prefix=" " category="inline-code"></block> Optionen zum Konfigurieren mehrerer DNFS-Kanäle in einem einzigen Subnetz. Dies funktioniert jedoch unter Linux nicht richtig, und es können unerwartete Leistungsprobleme auftreten. Bei Linux muss sich jeder für den DNFS-Verkehr verwendete NIC in einem anderen Subnetz befinden.</block>
  <block id="6c5cf31ca1af553b1f4a9b99e3cb054c" category="section-title">I/O-Planer</block>
  <block id="49bb1f5f4810e5f3d32dac84bd96005d" category="paragraph">Der Linux-Kernel ermöglicht eine Steuerung auf niedriger Ebene über die Art und Weise, wie I/O-Vorgänge zum Blockieren von Geräten geplant werden. Die Standardeinstellungen auf verschiedenen Linux-Distribution variieren erheblich. Tests zeigen, dass Deadline in der Regel die besten Ergebnisse bietet, aber gelegentlich NOOP war etwas besser. Der Unterschied in der Performance ist minimal, aber testen Sie beide Optionen, wenn es erforderlich ist, um die maximal mögliche Performance aus einer Datenbankkonfiguration zu extrahieren. CFQ ist in vielen Konfigurationen der Standard und hat bei Datenbank-Workloads erhebliche Performance-Probleme gezeigt.</block>
  <block id="38f25b8840d56fc601af1024115764b2" category="paragraph">Anweisungen zur Konfiguration des I/O-Planers finden Sie in der entsprechenden Dokumentation des Linux-Anbieters.</block>
  <block id="4150162ad42bffea7bb903b4c7a4ffd9" category="section-title">Multipathing</block>
  <block id="305482bf1c318f0a9a04f94987db06b2" category="paragraph">Einige Kunden sind während der Netzwerkunterbrechung auf Abstürze gestoßen, weil der Multipath-Daemon auf ihrem System nicht ausgeführt wurde. Bei aktuellen Versionen von Linux können der Installationsprozess des Betriebssystems und des Multipathing-Daemons diese Betriebssysteme für dieses Problem anfällig machen. Die Pakete sind ordnungsgemäß installiert, aber nach einem Neustart nicht für den automatischen Start konfiguriert.</block>
  <block id="6fb0bf499bef576f56e0fd6360eba0a6" category="paragraph">Die Standardeinstellung für den Multipath-Daemon unter RHEL5.5 kann beispielsweise wie folgt angezeigt werden:</block>
  <block id="31026ba0b77cbe4c9d44ed6afc859ed6" category="paragraph">Dies kann mit den folgenden Befehlen korrigiert werden:</block>
  <block id="c759a9b7e4db09da8a25fb73d256f6b7" category="section-title">ASM Spiegelung</block>
  <block id="f2f41836b42c08199948378b4a4a6e26" category="paragraph">ASM-Spiegelung erfordert möglicherweise Änderungen an den Linux Multipath-Einstellungen, damit ASM ein Problem erkennen und zu einer alternativen Ausfallgruppe wechseln kann. Die meisten ASM-Konfigurationen auf ONTAP verwenden externe Redundanz. Das bedeutet, dass Datensicherung durch das externe Array bereitgestellt wird und ASM keine Daten spiegelt. Einige Standorte verwenden ASM mit normaler Redundanz, um normalerweise zwei-Wege-Spiegelung über verschiedene Standorte hinweg bereitzustellen.</block>
  <block id="25f47751ccd5df99e3c25a8eb2a21fc5" category="inline-link-macro">NetApp Host Utilities-Dokumentation</block>
  <block id="066a79b72a430b074cdc0476b67bbde3" category="paragraph">Die Linux-Einstellungen, die im angezeigt werden <block ref="210658f6fe0d0061c73c19a9a6b263d1" category="inline-link-macro-rx"></block> Schließen Sie Multipath-Parameter ein, die zu unbestimmter I/O-Warteschlange führen Dies bedeutet, dass ein I/O auf einem LUN-Gerät ohne aktive Pfade so lange wartet, wie es für den I/O-Abschluss erforderlich ist. Dies ist in der Regel wünschenswert, da Linux-Hosts so lange warten, bis die Änderungen des SAN-Pfads abgeschlossen sind, FC-Switches neu gestartet werden oder ein Storage-System einen Failover abschließt.</block>
  <block id="184ea23c04f24d03fc0123bc6eb30de2" category="paragraph">Dieses unbegrenzte Warteschlangenverhalten verursacht ein Problem mit der ASM-Spiegelung, da ASM einen I/O-Fehler empfangen muss, damit er I/O auf einer alternativen LUN erneut versuchen kann.</block>
  <block id="5616945ee545765b51d3fe1871cff4ec" category="paragraph">Legen Sie die folgenden Parameter in Linux fest<block ref="9ac80d53ede05256c28cf9e043eb423c" prefix=" " category="inline-code"></block> Datei für ASM-LUNs, die mit ASM-Spiegelung verwendet werden:</block>
  <block id="49af3bbe0c557480960d217b217502ff" category="paragraph">Mit diesen Einstellungen wird ein Timeout von 120 Sekunden für ASM-Geräte erstellt. Das Timeout wird als berechnet<block ref="e2f0125c5971e1ce714f86f145386ee3" prefix=" " category="inline-code"></block> *<block ref="9434de34302b4f40887a7277cf35a8fc" prefix=" " category="inline-code"></block> Sekunden lang. Der genaue Wert muss unter Umständen angepasst werden, aber ein Timeout von 120 Sekunden sollte für die meisten Anwendungen ausreichen. Insbesondere sollten in 120 Sekunden eine Controller-Übernahme oder -Rückgabe möglich sein, ohne dass ein I/O-Fehler auftritt, der dazu führen würde, dass die Fehlergruppe offline geschaltet wird.</block>
  <block id="dbff1dc161f3f1d61258259b3a0b549c" category="paragraph">A niedriger<block ref="9434de34302b4f40887a7277cf35a8fc" prefix=" " category="inline-code"></block> Value kann die für ASM erforderliche Zeit zum Wechsel zu einer alternativen Ausfallgruppe verkürzen. Dies erhöht jedoch auch das Risiko eines unerwünschten Failovers während Wartungsaktivitäten wie beispielsweise einem Controller-Takeover. Das Risiko kann durch eine sorgfältige Überwachung des ASM-Spiegelungsstatus verringert werden. Wenn ein unerwünschtes Failover auftritt, können die Spiegelungen schnell neu synchronisiert werden, wenn die Resynchronisierung relativ schnell durchgeführt wird. Weitere Informationen finden Sie in der Oracle-Dokumentation zu ASM Fast Mirror Resync für die verwendete Version der Oracle-Software.</block>
  <block id="ec0b68df6b5a0fdd6d73ac710b9684da" category="section-title">Mount-Optionen für Linux xfs, ext3 und ext4</block>
  <block id="278230e4238d02fcb49850454e7d79e0" category="admonition">*NetApp empfiehlt* die Verwendung der Standard-Mount-Optionen.</block>
  <block id="02a5a4d50c5112bb31ff9c94f5b47ce9" category="paragraph">Viele Kunden von Oracle auf ONTAP verwenden ethernet, das Netzwerkprotokoll von NFS, iSCSI, NVMe/TCP sowie insbesondere die Cloud.</block>
  <block id="8a1870adbe8a237106c32d72660a9c42" category="summary">Design der logischen Schnittstelle für Oracle-Datenbanken</block>
  <block id="f46a94d438563c03413adfec27c6bfda" category="paragraph">Oracle Datenbanken benötigen Zugriff auf den Storage. Logische Schnittstellen (LIFs) sind die Netzwerk-Rohrleitungen, die eine Storage Virtual Machine (SVM) mit dem Netzwerk und damit der Datenbank verbinden. Ein angemessenes LIF-Design ist erforderlich, um sicherzustellen, dass für jeden Datenbank-Workload ausreichend Bandbreite vorhanden ist. Das Failover führt nicht zu einem Verlust von Storage-Services.</block>
  <block id="f445528d334d486a14570735705e2223" category="summary">FC-Netzwerkkonfiguration für Oracle-Datenbanken</block>
  <block id="92cdde4ecc529b9f9379ed2bdb6aae34" category="paragraph">Bei der Konfiguration von FC SAN für Oracle-Datenbanken geht es in erster Linie um die Umsetzung der täglichen SAN Best Practices.</block>
  <block id="9eaca569c6f5ca3388f5613da3aa008c" category="doc">Oracle-Datenbanken auf ONTAP</block>
  <block id="528b35962f1679204260ddef6800eece" category="paragraph">ONTAP wurde für Oracle Datenbanken entwickelt. Seit Jahrzehnten ist ONTAP für die speziellen Anforderungen relationaler Datenbank-I/O optimiert. Es wurden mehrere ONTAP-Funktionen speziell dafür entwickelt, die Anforderungen von Oracle Datenbanken zu bedienen – und sogar auf Wunsch von Oracle Inc. Selbst.</block>
  <block id="c9f8ef2db64ce19b15d2cc98fbd3d24c" category="admonition">Diese Dokumentation ersetzt die zuvor veröffentlichten technischen Berichte _TR-3633: Oracle Databases on ONTAP; TR-4591: Oracle Data Protection: Backup, Recovery, Replizierung; TR-4592: Oracle on MetroCluster; und TR-4534: Migration von Oracle Databases to NetApp Storage Systems_</block>
  <block id="b42ad0517ef7394f97178fb9e1de8d78" category="paragraph">Neben den vielfältigen Möglichkeiten, die ONTAP für eine Datenbankumgebung bietet, gibt es auch zahlreiche Benutzeranforderungen, darunter Datenbankgröße, Performance-Anforderungen und Datensicherung. Bekannte Implementierungen von NetApp Storage umfassen alles von einer virtualisierten Umgebung mit ca. 6,000 Datenbanken unter VMware ESX bis hin zu einem Data Warehouse mit einer einzigen Instanz, das derzeit eine Größe von 996 TB aufweist und weiter wächst. Aus diesem Grund gibt es nur wenige klare Best Practices für die Konfiguration einer Oracle Datenbank auf NetApp Storage.</block>
  <block id="7bf0493f27e81fe4c3740e773e137a48" category="paragraph">Die Anforderungen für den Betrieb einer Oracle Database auf NetApp Storage werden auf zweierlei Weise erfüllt. Erstens, wenn eine klare Best Practice besteht, wird sie ausdrücklich genannt. Im Allgemeinen werden viele Designüberlegungen erläutert, die von den Architekten von Oracle-Speicherlösungen auf der Grundlage ihrer spezifischen Geschäftsanforderungen berücksichtigt werden müssen.</block>
  <block id="633d94718c25247da5772ed6f2b244c9" category="summary">Einführung in die Oracle Storage-Migration</block>
  <block id="08beeb11d62fd51ef2742eae3ee3719f" category="admonition">Diese Dokumentation ersetzt den bereits veröffentlichten technischen Bericht _TR-4534: Migration von Oracle-Datenbanken zu NetApp-Speichersystemen_</block>
  <block id="5f2a4190824fd64be3c8cabeb7dc62ff" category="paragraph">Im Falle eines neuen Datenbankprojekts ist dies kein Problem, da die Datenbank- und Anwendungsumgebungen eingerichtet sind. Die Migration stellt Unternehmen jedoch vor besondere Herausforderungen, was die Unterbrechung des Geschäftsbetriebs, den für den Abschluss der Migration erforderlichen Zeitaufwand, die erforderlichen Fachkompetenzen und die Risikominimierung angeht.</block>
  <block id="a83f1ca5ad00b39773d9e6a26b0e70b2" category="section-title">Skripte</block>
  <block id="08293bec81c296b61f4a751175d9caa7" category="paragraph">In dieser Dokumentation sind Beispielskripte enthalten. Diese Skripte bieten Beispielmethoden zur Automatisierung verschiedener Aspekte der Migration, um das Risiko von Benutzerfehlern zu verringern. Die Skripte können die Gesamtanforderungen an die für eine Migration verantwortlichen IT-Mitarbeiter verringern und den Gesamtprozess beschleunigen. Diese Skripte stammen alle aus Migrationsprojekten, die von NetApp Professional Services und NetApp-Partnern durchgeführt werden. Beispiele für deren Verwendung sind in dieser Dokumentation aufgeführt.</block>
  <block id="c28b52826242c7cdffe5f3ae1e917f61" category="paragraph">Aufgrund der Notwendigkeit, die FC-Netzwerkkonfiguration zu ändern, sind Unterbrechungen beim Import fremder LUNs unvermeidbar. Die Unterbrechung muss jedoch nicht viel länger dauern als die Zeit, die für den Neustart der Datenbankumgebung und die Aktualisierung des FC-Zoning für die Umstellung der Host-FC-Konnektivität von der fremden LUN auf ONTAP erforderlich ist.</block>
  <block id="a2248459756ffc1877beb9b39c77668b" category="paragraph">Dieser Prozess lässt sich wie folgt zusammenfassen:</block>
  <block id="c9f77905e39abbada6c73a7a3d5f51f8" category="list-text">Legen Sie alle LUN-Aktivitäten auf den fremden LUNs still.</block>
  <block id="cff6e7c6c4f8dd7e57a0286f28712c58" category="list-text">Umleiten von Host-FC-Verbindungen zum neuen ONTAP-System</block>
  <block id="9fea842823b73e1eb32307ab325f170e" category="list-text">Starten Sie den Importvorgang.</block>
  <block id="df814ea2bf92d4f18e72a648e93103a2" category="list-text">Ermitteln Sie die LUNs neu.</block>
  <block id="030a5009bbe6fb141ca5863ab6c59464" category="list-text">Starten Sie die Datenbank neu.</block>
  <block id="f3bb22d18791c042e889671ad1963239" category="paragraph">Sie müssen nicht warten, bis der Migrationsprozess abgeschlossen ist. Sobald die Migration einer bestimmten LUN beginnt, ist sie auf ONTAP verfügbar und kann Daten bereitstellen, während der Datenkopievorgang fortgesetzt wird. Alle Lesevorgänge werden an die fremde LUN weitergeleitet, und alle Schreibvorgänge werden synchron auf beide Arrays geschrieben. Der Kopiervorgang läuft sehr schnell ab und der Overhead bei der Umleitung des FC-Datenverkehrs ist minimal. Die Auswirkungen auf die Performance sollten daher kurzlebig und minimal sein. Wenn Bedenken bestehen, können Sie den Neustart der Umgebung verzögern, bis der Migrationsprozess abgeschlossen ist und die Importbeziehungen gelöscht wurden.</block>
  <block id="b19e17ab765f91aa0bc0e7152f13779f" category="section-title">Datenbank herunterfahren</block>
  <block id="2869cd69e0fec20ba28a42e74afef158" category="paragraph">Der erste Schritt bei der Stilllegung der Umgebung in diesem Beispiel ist das Herunterfahren der Datenbank.</block>
  <block id="e20dab0880a91f5da2ea05170177576d" category="section-title">Netzdienste herunterfahren</block>
  <block id="89c458e5176d6caf0690f1e61ecb0620" category="paragraph">Zu den migrierten SAN-basierten Dateisystemen gehören auch die Oracle ASM-Services. Um die zugrunde liegenden LUNs stilllegen zu können, müssen die Dateisysteme getrennt werden. Dies bedeutet wiederum, dass alle Prozesse mit offenen Dateien auf diesem Dateisystem angehalten werden.</block>
  <block id="49f05bf7da4eacfa4c80bb987443eb5a" category="section-title">Entfernen Sie Dateisysteme</block>
  <block id="c5a09ac3ff0bb25c97951c7b7f815cd0" category="paragraph">Wenn alle Prozesse heruntergefahren werden, ist der umount-Vorgang erfolgreich. Wenn die Berechtigung verweigert wird, muss es einen Prozess mit einer Sperre auf dem Dateisystem geben. Der<block ref="ace112f9725b6fa0b702e6b3970b2102" prefix=" " category="inline-code"></block> Der Befehl kann bei der Identifizierung dieser Prozesse helfen.</block>
  <block id="765e13ffc3361a9fd6c6088b2603ffbf" category="section-title">Deaktivieren Sie Volume-Gruppen</block>
  <block id="907e5f35f70dc2f5e4b67da56381bdf4" category="paragraph">Nachdem alle Dateisysteme in einer bestimmten Volume-Gruppe getrennt wurden, kann die Volume-Gruppe deaktiviert werden.</block>
  <block id="ad80f935ccd133a802189e3c2f4421d7" category="section-title">Änderungen am FC-Netzwerk</block>
  <block id="a497da6790d9ab4bb5cce3d04b0efd2e" category="paragraph">Die FC-Zonen können jetzt aktualisiert werden, um den gesamten Zugriff vom Host auf das fremde Array zu entfernen und den Zugriff auf ONTAP zu ermöglichen.</block>
  <block id="3ef31115a10790468ad841ec2cdf566f" category="section-title">Importvorgang starten</block>
  <block id="5bc309ef1913ddc70534a0a7135ba938" category="paragraph">Um die LUN-Importprozesse zu starten, führen Sie den aus<block ref="76ffc50817aa4cd6214aa0061339fdf6" prefix=" " category="inline-code"></block> Befehl.</block>
  <block id="0a10f1ad4a0b45d384c5771ec0906c4c" category="section-title">Überwachen Sie den Importfortschritt</block>
  <block id="9a3a7545f4a7407ec022c4b2271e0c34" category="paragraph">Der Importvorgang kann mit dem überwacht werden<block ref="1ab15ef2c245a4434fcea3cc3149f4f4" prefix=" " category="inline-code"></block> Befehl. Wie unten dargestellt, läuft der Import aller 20 LUNs, was bedeutet, dass die Daten jetzt über ONTAP zugänglich sind, obwohl der Kopiervorgang noch fortschreitet.</block>
  <block id="d36f98c8a305e33483232e7b5cd90c49" category="inline-link-macro">Import fremder LUNs – Abschluss</block>
  <block id="a9c97f84ee4958d39ed989b3da7ed0ba" category="paragraph">Wenn Sie eine Online-Migration benötigen, fahren Sie mit der Neuerkennung der LUNs in ihrem neuen Zuhause fort, und führen Sie die Dienste aus.</block>
  <block id="d0369936c6fe8c292874e3bc647893b8" category="section-title">Nach SCSI-Geräteänderungen suchen</block>
  <block id="6d338faddede649cab4c5b498e8e4379" category="paragraph">In den meisten Fällen besteht die einfachste Möglichkeit, neue LUNs neu zu ermitteln, darin, den Host neu zu starten. Dadurch werden alte veraltete Geräte automatisch entfernt, alle neuen LUNs ordnungsgemäß erkannt und verbundene Geräte wie Multipathing-Geräte erstellt. Das Beispiel zeigt einen vollständig online-Prozess zu Demonstrationszwecken.</block>
  <block id="3b493a859f19779c1f7db7d1951785ee" category="paragraph">Achtung: Bevor Sie einen Host neu starten, stellen Sie sicher, dass alle Einträge in sind<block ref="816a3ef77b62ade41c3f936c958aa555" prefix=" " category="inline-code"></block> Diese Referenz migrierte SAN-Ressourcen werden kommentiert. Wenn dies nicht durchgeführt wird und Probleme mit dem LUN-Zugriff auftreten, wird das OS möglicherweise nicht gebootet. Diese Situation beschädigt Daten nicht. Es kann jedoch sehr unbequem sein, in den Rettungsmodus oder einen ähnlichen Modus zu starten und die zu korrigieren<block ref="816a3ef77b62ade41c3f936c958aa555" prefix=" " category="inline-code"></block> Damit das OS gebootet werden kann, um die Fehlerbehebung zu ermöglichen.</block>
  <block id="9d7b16568eb7edb80492b49cb02d7663" category="paragraph">Die LUNs auf der in diesem Beispiel verwendeten Linux-Version können erneut mit dem gescannt werden<block ref="b6f75508bf2af141e9de6f862662661e" prefix=" " category="inline-code"></block> Befehl. Wenn der Befehl erfolgreich war, sollte jeder LUN-Pfad in der Ausgabe angezeigt werden. Die Ausgabe kann schwer zu interpretieren sein, wenn die Zoning- und igroup-Konfiguration korrekt war, sollten viele LUNs scheinen, die eine enthalten<block ref="971c047750d8e3b03c71d24020b3816f" prefix=" " category="inline-code"></block> Anbieterzeichenfolge.</block>
  <block id="3384aa7c090758ad18028832967b143f" category="section-title">Überprüfen Sie auf Multipath-Geräte</block>
  <block id="07a39d42e6d8cc61e0a880e43b83e2a3" category="paragraph">Der LUN-Erkennungsprozess löst auch die Wiederherstellung von Multipath-Geräten aus, der Linux-Multipathing-Treiber hat jedoch bekanntermaßen gelegentlich Probleme. Die Ausgabe von<block ref="eeb30d005cab1d8c69785a3cd5818f55" prefix=" " category="inline-code"></block> Sollte überprüft werden, um sicherzustellen, dass die Ausgabe wie erwartet aussieht. Die folgende Ausgabe zeigt beispielsweise Multipath-Geräte, die mit einem verknüpft sind<block ref="971c047750d8e3b03c71d24020b3816f" prefix=" " category="inline-code"></block> Anbieterzeichenfolge. Jedes Gerät verfügt über vier Pfade, wobei zwei mit einer Priorität von 50 und zwei mit einer Priorität von 10. Obwohl die genaue Ausgabe mit verschiedenen Versionen von Linux variieren kann, sieht diese Ausgabe wie erwartet aus.</block>
  <block id="efd921479c3d57448954050760c568ca" category="admonition">Überprüfen Sie anhand der Dokumentation der Host-Dienstprogramme die Version von Linux, die Sie verwenden<block ref="dd5ff67ba72768d33f75e645aa2b8e56" prefix=" " category="inline-code"></block> Die Einstellungen sind korrekt.</block>
  <block id="28751dc1e914a778d11ca0e69613690b" category="section-title">Reaktivieren Sie die LVM-Volume-Gruppe</block>
  <block id="a66a38e4056f28a310ef6d1fa8bbcc77" category="paragraph">Wenn die LVM-LUNs ordnungsgemäß erkannt wurden, wird das angezeigt<block ref="5239f542d8edbc4b528b1f362153c0c7" prefix=" " category="inline-code"></block> Befehl sollte erfolgreich sein. Dies ist ein gutes Beispiel für den Nutzen eines logischen Volume-Managers. Eine Änderung des WWN einer LUN oder auch einer Seriennummer ist unwichtig, da die Metadaten der Volume-Gruppe auf die LUN selbst geschrieben werden.</block>
  <block id="1d374f8de5ba1b82ab4407b8c6efbd0d" category="paragraph">Das Betriebssystem hat die LUNs gescannt und eine kleine Menge an auf die LUN geschriebenen Daten ermittelt, die sie als physisches Volume des identifizieren<block ref="83b7788c8e4de444d6cd4c69978eaf32" prefix=" " category="inline-code"></block>. Anschließend wurden alle erforderlichen Geräte erstellt. Sie müssen nur die Volume-Gruppe erneut aktivieren.</block>
  <block id="1f0285a57b2357507c3b77ce5130b548" category="section-title">Dateisysteme neu einbinden</block>
  <block id="a9047d68dc7e99f553b71590cff301d6" category="paragraph">Nachdem die Volume-Gruppe wieder aktiviert wurde, können die Dateisysteme mit allen ursprünglichen Daten gemountet werden. Wie bereits erwähnt, sind die Dateisysteme voll funktionsfähig, selbst wenn die Datenreplikation in der Back-Gruppe weiterhin aktiv ist.</block>
  <block id="a787d65e2525ac6bc3a6328efe383b5c" category="section-title">Neuscannen für ASM-Geräte</block>
  <block id="a25bb20f07ed95a633f623678855e8dc" category="paragraph">Die ASMlib-Geräte sollten beim erneuten Scannen der SCSI-Geräte neu erkannt worden sein. Die Wiedererkennung kann online überprüft werden, indem ASMlib neu gestartet und anschließend die Datenträger gescannt werden.</block>
  <block id="37fb12a52b4a5371c12209e89e09b04f" category="admonition">Dieser Schritt ist nur für ASM-Konfigurationen relevant, in denen ASMlib verwendet wird.</block>
  <block id="6de48f39063d81d53fd0ff3e9938af13" category="paragraph">Achtung: Wenn ASMlib nicht verwendet wird, ist die<block ref="9c9f766701f811ef6f170c7a3453c53f" prefix=" " category="inline-code"></block> Geräte sollten automatisch neu erstellt worden sein. Die Berechtigungen sind jedoch möglicherweise nicht korrekt. Sie müssen spezielle Berechtigungen für die zugrunde liegenden Geräte für ASM festlegen, wenn ASMlib nicht vorhanden ist. Dies wird in der Regel durch spezielle Einträge in entweder der erreicht<block ref="dd5ff67ba72768d33f75e645aa2b8e56" prefix=" " category="inline-code"></block> Oder<block ref="ab775c875d5f89d991a670444dd32ad2" prefix=" " category="inline-code"></block> Regeln oder möglicherweise in beiden Regelsätzen. Diese Dateien müssen möglicherweise aktualisiert werden, um Änderungen in der Umgebung in Bezug auf WWNs oder Seriennummern widerzuspiegeln, um sicherzustellen, dass die ASM-Geräte weiterhin über die richtigen Berechtigungen verfügen.</block>
  <block id="08b33d6af4ec78f99dbbc3533589aafc" category="paragraph">In diesem Beispiel werden beim Neustart von ASMlib und beim Scannen nach Festplatten die gleichen 10 ASM-LUNs wie in der ursprünglichen Umgebung angezeigt.</block>
  <block id="17c4ae76000d9a79a56195071665af32" category="section-title">Starten Sie die Grid-Services neu</block>
  <block id="e8c3dc00e15826e3b77a2d41d7fde92a" category="paragraph">Da die LVM- und ASM-Geräte jetzt online und verfügbar sind, können die Grid-Dienste neu gestartet werden.</block>
  <block id="934317e45581988a01f54761705ca58a" category="section-title">Datenbank neu starten</block>
  <block id="41475a737e947d73c674b90e78e246a0" category="paragraph">Nach dem Neustart der Netzdienste kann die Datenbank gestartet werden. Möglicherweise müssen Sie einige Minuten warten, bis die ASM-Dienste vollständig verfügbar sind, bevor Sie versuchen, die Datenbank zu starten.</block>
  <block id="b171714b39ad4102aced4daceef8c678" category="paragraph">Aus Host-Sicht ist die Migration abgeschlossen, aber I/O wird weiterhin vom fremden Array bedient, bis die Importbeziehungen gelöscht werden.</block>
  <block id="f708d001227724fa0a8a3d57aacedb1a" category="paragraph">Bevor Sie die Beziehungen löschen, müssen Sie bestätigen, dass der Migrationsprozess für alle LUNs abgeschlossen ist.</block>
  <block id="85cf20229bc9008553794abdaed507f5" category="section-title">Importbeziehungen löschen</block>
  <block id="b23ec325eb9c6278cb36e4c45c2c3499" category="paragraph">Löschen Sie nach Abschluss des Migrationsprozesses die Migrationsbeziehung. Anschließend wird die I/O ausschließlich von den Laufwerken auf ONTAP bedient.</block>
  <block id="40b98b0023a7eea7c432c29e3c44ba7d" category="section-title">Registrierung ausländischer LUNs aufheben</block>
  <block id="5ccc14ae93ba94bd4183439a9f80f6c8" category="paragraph">Ändern Sie schließlich die Festplatte, um die zu entfernen<block ref="96ea0e5f46787e93e2970a086b47fdb6" prefix=" " category="inline-code"></block> Bezeichnung.</block>
  <block id="875bb561ba851d301052704e0ffb70f4" category="summary">Oracle-Migration über Protokollversand</block>
  <block id="e0f9c3b73fe5700df95f30eb98ca4034" category="doc">Oracle-Protokollversand</block>
  <block id="8e42b5809657e127c7e2d24f30bc493e" category="paragraph">Ziel bei einer Migration mit Protokollversand ist, eine Kopie der ursprünglichen Datendateien an einem neuen Standort zu erstellen und anschließend eine Methode für den Versand von Änderungen in die neue Umgebung zu definieren.</block>
  <block id="599df5f564506c95624dd78b0b75eeac" category="paragraph">Nach der Einrichtung können Protokollversand und -Wiedergabe automatisiert werden, um die Replikatdatenbank weitgehend mit der Quelle synchron zu halten. So kann beispielsweise ein Cron-Job so geplant werden, dass (a) die letzten Protokolle an den neuen Speicherort kopiert und (b) alle 15 Minuten erneut wiedergegeben werden. Dadurch sind zum Zeitpunkt der Umstellung nur minimale Unterbrechungen möglich, da maximal 15 Minuten Archiv-Logs wieder eingespielt werden müssen.</block>
  <block id="41bec1b4ddc17b3367c68a025c0ddfb9" category="paragraph">Das unten abgebildete Verfahren ist außerdem im Wesentlichen eine Datenbankklonoperation. Die gezeigte Logik ähnelt der Engine in NetApp SnapManager für Oracle (SMO) und dem NetApp SnapCenter Oracle Plug-in. Einige Kunden haben das in Skripten oder WFA Workflows angezeigte Verfahren für individuelle Klonvorgänge verwendet. Dieses Verfahren ist zwar mehr manuell als SMO oder SnapCenter, es wird jedoch immer noch ohne Skripte erstellt und die Datenmanagement-APIs in ONTAP vereinfachen den Prozess weiter.</block>
  <block id="f0616883a5ca70aa97676de436a7fb9e" category="section-title">Protokollversand – Dateisystem an Dateisystem</block>
  <block id="8f922b64f0bc64f6bba1d555a61ee3bb" category="paragraph">Dieses Beispiel zeigt die Migration einer Datenbank namens WAFFLE von einem gewöhnlichen Dateisystem zu einem anderen gewöhnlichen Dateisystem auf einem anderen Server. Es veranschaulicht auch die Verwendung von SnapMirror zum Erstellen einer schnellen Kopie der Datendateien, aber dies ist kein integraler Bestandteil des gesamten Verfahrens.</block>
  <block id="60100e1b826a4b972fc113c391f932e7" category="section-title">Erstellen Sie eine Datenbanksicherung</block>
  <block id="80cf34e10dcfbc5e76c6b2f845f2708d" category="paragraph">Der erste Schritt besteht darin, ein Datenbank-Backup zu erstellen. Insbesondere erfordert dieses Verfahren eine Reihe von Datendateien, die für die Wiedergabe des Archivprotokolls verwendet werden können.</block>
  <block id="0ba29c6a1afacf586b03a26162c72274" category="section-title">Umgebung</block>
  <block id="744a73803cdac4a5067725ca5814a2cf" category="section-title">Wiederherstellung in neuer Umgebung</block>
  <block id="46228bb2ce31b0d397faad2b805e8a37" category="paragraph">Das Backup muss nun in der neuen Umgebung wiederhergestellt werden. Dies kann auf verschiedene Arten erfolgen, z. B. Oracle RMAN, Wiederherstellung über eine Backup-Applikation wie NetBackup oder ein einfacher Kopiervorgang von Datendateien, die im Hot-Backup-Modus platziert wurden.</block>
  <block id="1f255556546f1fdfb53692e41f35eb6c" category="list-text">Erstellen Sie ein neues Volume für den Empfang der Snapshot-Daten. Initialisieren Sie die Spiegelung von<block ref="937e9fa808ccfe9a4f6b004a2a7caada" prefix=" " category="inline-code"></block> Bis<block ref="96f9dacb3cb1b1f6f9b4ad02992a1a0e" prefix=" " category="inline-code"></block>.</block>
  <block id="5b1944ec8c0ed933e5b2ea2e3e5fa76e" category="list-text">Nachdem der Status von SnapMirror festgelegt wurde und Sie angeben, dass die Synchronisierung abgeschlossen ist, aktualisieren Sie die Spiegelung speziell auf der Grundlage des gewünschten Snapshots.</block>
  <block id="f4d90c6c29b1f76ab9c87a9996c9751e" category="list-text">Die erfolgreiche Synchronisierung kann durch Anzeigen des überprüft werden<block ref="31952c255e722053d4cc3f82921e95db" prefix=" " category="inline-code"></block> Feld auf der Spiegelungslautstärke.</block>
  <block id="0934c3fcad3065c2a479125e4a80b259" category="list-text">Der Spiegel kann dann gebrochen werden.</block>
  <block id="fcf1f505b94151a36eba8e1563e7176f" category="list-text">Mounten Sie das neue Dateisystem.bei blockbasierten Dateisystemen variieren die genauen Verfahren je nach der verwendeten LVM. FC-Zoning oder iSCSI-Verbindungen müssen konfiguriert werden. Nachdem die Verbindung zu den LUNs hergestellt wurde, Befehle wie Linux<block ref="302c49bbd5bcda5463eb5130804effc1" prefix=" " category="inline-code"></block> Möglicherweise muss ermittelt werden, welche Volume-Gruppen oder LUNs ordnungsgemäß konfiguriert werden müssen, damit sie von ASM erkannt werden können.</block>
  <block id="20c4d1d4163c2463cf15ff7df6424459" category="paragraph">In diesem Beispiel wird ein einfaches NFS-Dateisystem verwendet. Dieses Dateisystem kann direkt gemountet werden.</block>
  <block id="b2dd88a1f03a5fb4c2e3c0d46f425161" category="section-title">Erstellen Sie eine Vorlage für die Erstellung von Steuerdateien</block>
  <block id="1c30e11140f1d0a1093bedee34eef1e4" category="paragraph">Als nächstes müssen Sie eine controlfile-Vorlage erstellen. Der<block ref="3177841b2335a6640ea3054199830f1c" prefix=" " category="inline-code"></block> Befehl erstellt Textbefehle, um eine Steuerdatei neu zu erstellen. Diese Funktion kann unter bestimmten Umständen hilfreich sein, um eine Datenbank aus einem Backup wiederherzustellen. Sie wird häufig bei Skripten verwendet, die Aufgaben wie das Klonen von Datenbanken ausführen.</block>
  <block id="3f51d58ec72b9d36cb5e0636525130b5" category="list-text">Die Ausgabe des folgenden Befehls wird verwendet, um die Steuerdateien für die migrierte Datenbank neu zu erstellen.</block>
  <block id="cb395c3bf18fd427287caecc9b28058a" category="list-text">Nachdem die Steuerdateien erstellt wurden, kopieren Sie die Datei auf den neuen Server.</block>
  <block id="cf5c9046a3617a19c0ccc24e4cf54a33" category="section-title">Parameterdatei sichern</block>
  <block id="0d1d1a7db58f3f93ec977733f4909f5d" category="paragraph">In der neuen Umgebung ist auch eine Parameterdatei erforderlich. Die einfachste Methode ist, aus dem aktuellen spfile oder pfile ein pfile zu erstellen. In diesem Beispiel verwendet die Quelldatenbank eine spfile.</block>
  <block id="051a634cb1e9a53dbf139214e1d24001" category="section-title">Oratab-Eintrag erstellen</block>
  <block id="2dabc26c8b55d44e25a184ba2624eed0" category="paragraph">Die Erstellung eines Oratab-Eintrags ist für das ordnungsgemäße Funktionieren von Dienstprogrammen wie oraenv erforderlich. Führen Sie den folgenden Schritt aus, um einen Oratab-Eintrag zu erstellen.</block>
  <block id="87ef75141d237e3b44e8310aad8fcf19" category="section-title">Verzeichnisstruktur vorbereiten</block>
  <block id="12f833c44c11e6bbd970cc126bce2bcb" category="paragraph">Wenn die benötigten Verzeichnisse noch nicht vorhanden waren, müssen Sie sie erstellen, oder der Datenbankstartvorgang schlägt fehl. Um die Verzeichnisstruktur vorzubereiten, müssen Sie die folgenden Mindestanforderungen erfüllen.</block>
  <block id="60225c364a012796a6d886e8cf4ec486" category="section-title">Aktualisierung der Parameterdatei</block>
  <block id="e36ee2d9a95a6b672917862d714af383" category="list-text">Um die Parameterdatei auf den neuen Server zu kopieren, führen Sie die folgenden Befehle aus. Der Standardspeicherort ist der<block ref="45c7302d9c2e8745b3a2bff0f992b6df" prefix=" " category="inline-code"></block> Verzeichnis. In diesem Fall kann die pfile überall platziert werden. Sie wird nur als Zwischenschritt im Migrationsprozess genutzt.</block>
  <block id="d437a40a59905247cfb39cf9dc9da256" category="list-text">Bearbeiten Sie die Datei nach Bedarf. Wenn sich beispielsweise der Speicherort des Archivprotokolls geändert hat, muss das pfile entsprechend dem neuen Speicherort geändert werden. In diesem Beispiel werden nur die Steuerdateien verschoben, zum Teil, um sie zwischen Protokoll- und Datendateisystemen zu verteilen.</block>
  <block id="55cc35656c3d28c1b3267c246b397521" category="list-text">Nachdem die Bearbeitungen abgeschlossen sind, erstellen Sie auf Basis dieses pfile ein spfile.</block>
  <block id="a833a03e6af0b969524dcccb8f296f1c" category="section-title">Erstellen Sie Steuerdateien neu</block>
  <block id="74a636ee3a009f08cd5623dc8066dd09" category="paragraph">In einem vorherigen Schritt wird die Ausgabe von angezeigt<block ref="3177841b2335a6640ea3054199830f1c" prefix=" " category="inline-code"></block> Wurde auf den neuen Server kopiert. Der spezifische Teil des erforderlichen Ausgangs ist der<block ref="0ca1fba455d799b9a1cc2440b9ba7043" prefix=" " category="inline-code"></block> Befehl. Diese Informationen finden Sie in der Datei unter dem markierten Abschnitt<block ref="9fb76d62be749cec8bdd06f46c77bf5b" prefix=" " category="inline-code"></block>. Es beginnt mit der Linie<block ref="13ac20c4a96b2ecfd703e831ad722907" prefix=" " category="inline-code"></block> Und sollte das Wort enthalten<block ref="c3c62948f78a7ccaabb9ef2eea80a895" prefix=" " category="inline-code"></block>. Er endet mit dem Semikolon (; ).</block>
  <block id="bf4b2a006646a8e77166a2a596425074" category="list-text">In diesem Beispiel liest die Datei wie folgt.</block>
  <block id="a9a27c002ec883341220e267712250a7" category="list-text">Bearbeiten Sie dieses Skript wie gewünscht, um den neuen Speicherort der verschiedenen Dateien anzuzeigen. Beispielsweise können bestimmte Datendateien, von denen bekannt ist, dass sie eine hohe I/O-Last unterstützen, auf ein Filesystem auf einer hochperformanten Storage-Ebene umgeleitet werden. In anderen Fällen könnten die Änderungen lediglich aus Administratorgründen vorgenommen werden, wie z. B. die Isolierung der Datendateien einer bestimmten PDB in dedizierten Volumes.</block>
  <block id="2540fb1670d4f9ee8a704a7eb8d748fe" category="list-text">In diesem Beispiel ist der<block ref="b86df74c6982de8dec7509d039294d7a" prefix=" " category="inline-code"></block> Stanza bleibt unverändert, aber die Redo-Logs werden an einen neuen Speicherort in verschoben<block ref="96bb85ed0fe822778c4e5a821e991ad8" prefix=" " category="inline-code"></block> Statt Speicherplatz für Archivprotokolle freizugeben<block ref="0e62afc91abe157ef67895cca0a7f912" prefix=" " category="inline-code"></block>.</block>
  <block id="8d47ad079859f2976fdc7ebf742f768d" category="paragraph">Wenn Dateien falsch platziert oder Parameter falsch konfiguriert sind, werden Fehler generiert, die angeben, was repariert werden muss. Die Datenbank ist gemountet, aber noch nicht geöffnet und kann nicht geöffnet werden, da die verwendeten Datendateien noch als Hot Backup-Modus markiert sind. Um die Datenbankkonsistenz zu gewährleisten, müssen zunächst Archivprotokolle angewendet werden.</block>
  <block id="6b7c2d588ac708b58f7ac889090608f6" category="section-title">Erste Protokollreplizierung</block>
  <block id="2db2d3d6001a06d1a027904fa66bc78c" category="paragraph">Es ist mindestens ein Protokollantwort erforderlich, um die Datendateien konsistent zu gestalten. Es stehen zahlreiche Optionen zur Wiedergabe von Protokollen zur Verfügung. In einigen Fällen kann der ursprüngliche Speicherort des Archivprotokolls auf dem ursprünglichen Server über NFS freigegeben werden, und die Protokollantwort kann direkt erfolgen. In anderen Fällen müssen die Archivprotokolle kopiert werden.</block>
  <block id="5c394e6951584d114c6706d0768e30d7" category="paragraph">Zum Beispiel, eine einfache<block ref="8a444a43bdf534c0c6338bb7809659b3" prefix=" " category="inline-code"></block> Der Vorgang kann alle aktuellen Protokolle vom Quellserver auf den Migrationsserver kopieren:</block>
  <block id="5fdb3158ed28dd3aa90b94556476b781" category="section-title">Erste Protokollwiedergabe</block>
  <block id="c2f93ef52583f81f0eba05a39483e2d8" category="paragraph">Nachdem sich die Dateien im Archiv-Log-Speicherort befinden, können sie mit dem Befehl wiedergegeben werden<block ref="5fea768eb2d353716ca623ae58ce382c" prefix=" " category="inline-code"></block> Gefolgt von der Antwort<block ref="e1f2d5134ed2543d38a0de9751cf75d9" prefix=" " category="inline-code"></block> Um alle verfügbaren Protokolle automatisch wiederzugeben.</block>
  <block id="f30a45a22256018185c5d235292e4d5e" category="paragraph">Die endgültige Antwort des Archivprotokolls meldet einen Fehler. Dies ist jedoch normal. Das Protokoll zeigt das an<block ref="a84f4f3da79386c29ab6dfa560af786e" prefix=" " category="inline-code"></block> Ich habe eine bestimmte Protokolldatei gesucht und sie nicht gefunden. Der Grund dafür ist höchstwahrscheinlich, dass die Protokolldatei noch nicht existiert.</block>
  <block id="80bea8a60412cace54760b223b2d799e" category="paragraph">Wenn die Quelldatenbank vor dem Kopieren von Archivprotokollen heruntergefahren werden kann, muss dieser Schritt nur einmal durchgeführt werden. Die Archivprotokolle werden kopiert und eingespielt. Anschließend kann der Prozess direkt zum Umstellungsprozess fortgesetzt werden, der die kritischen Wiederherstellungsprotokolle repliziert.</block>
  <block id="1e334141582a9dcb581c9b34ef2df071" category="section-title">Inkrementelle Protokollreplikation und -Wiedergabe</block>
  <block id="5fd2b0bf662c2b1ebdc677ea888536ce" category="paragraph">In den meisten Fällen erfolgt die Migration nicht sofort. Es kann Tage oder sogar Wochen bis zum Abschluss des Migrationsprozesses dauern. Das bedeutet, dass die Protokolle kontinuierlich an die Replikatdatenbank gesendet und erneut eingespielt werden müssen. Bei Ankunft der Umstellung müssen daher nur minimale Daten übertragen und erneut eingespielt werden.</block>
  <block id="0c56d82ec6857bcef5475899b9f7b5cc" category="paragraph">Dies kann auf viele Arten per Skript gesteuert werden, aber eine der beliebtesten Methoden ist die Verwendung von rsync, einem gemeinsamen Dateireplikationsdienstprogramm. Die sicherste Methode, dieses Dienstprogramm zu verwenden, ist es als Daemon zu konfigurieren. Beispiel: Der<block ref="b31375af035ed8a698a803a491084f61" prefix=" " category="inline-code"></block> Die folgende Datei zeigt, wie eine Ressource mit dem Namen erstellt wird<block ref="260e23541ff372b9bc53b4bd3c0c38ba" prefix=" " category="inline-code"></block> Der Zugriff erfolgt mit Oracle-Benutzeranmeldeinformationen und ist zugeordnet<block ref="488a4921a81e36fca411e6f13cefbbf2" prefix=" " category="inline-code"></block>. Am wichtigsten ist jedoch, dass die Ressource schreibgeschützt ist, wodurch die Produktionsdaten gelesen, aber nicht verändert werden können.</block>
  <block id="eabf1b424989b2c7113ba3a538079afa" category="paragraph">Mit dem folgenden Befehl wird das Archivprotokollziel des neuen Servers mit der rsync-Ressource synchronisiert<block ref="260e23541ff372b9bc53b4bd3c0c38ba" prefix=" " category="inline-code"></block> Auf dem ursprünglichen Server. Der<block ref="e358efa489f58062f10dd7316b65649e" prefix=" " category="inline-code"></block> Argument in<block ref="293e33723231f0504effabb52b054a31" prefix=" " category="inline-code"></block> Führt dazu, dass die Dateiliste anhand des Zeitstempels verglichen wird und nur neue Dateien kopiert werden. Dieser Prozess bietet eine inkrementelle Aktualisierung des neuen Servers. Dieser Befehl kann auch in cron so geplant werden, dass er regelmäßig ausgeführt wird.</block>
  <block id="c775c44069da13c8f7ee5c5116e76e36" category="inline-link-macro">Protokolle in der Datenbank wiedergeben</block>
  <block id="c3bf9b2ef3e3ae2ae6a02f7f77cfcbc6" category="paragraph">Nachdem die Protokolle empfangen wurden, müssen sie erneut abgespielt werden. Frühere Beispiele zeigen die Verwendung von sqlplus zum manuellen Ausführen<block ref="5fea768eb2d353716ca623ae58ce382c" prefix=" " category="inline-code"></block>Ein Prozess, der leicht automatisiert werden kann. Das hier abgebildete Beispiel verwendet das in beschriebene Skript <block ref="3e57a2099043fa40c58f1e0d4eed995d" category="inline-link-macro-rx"></block>. Die Skripte akzeptieren ein Argument, das die Datenbank angibt, die einen Wiedergabevorgang erfordert. Damit kann dasselbe Skript bei einer Migration mit mehreren Datenbanken verwendet werden.</block>
  <block id="4ea2a6c39b10b3e3076f976f58a861d1" category="section-title">Umstellung</block>
  <block id="5de3f1ba0bbef459baa93f04f63b9ea1" category="paragraph">Wenn Sie bereit sind, in die neue Umgebung zu schneiden, müssen Sie eine abschließende Synchronisierung durchführen, die sowohl Archivprotokolle als auch Redo-Protokolle enthält. Wenn der ursprüngliche Speicherort des Wiederherstellungsprotokolls nicht bereits bekannt ist, kann er wie folgt identifiziert werden:</block>
  <block id="677f005cc0f8be0f891df5af72fb5688" category="list-text">Fahren Sie die Quelldatenbank herunter.</block>
  <block id="41b182875e12de118d17b3432cca93fa" category="list-text">Führen Sie eine abschließende Synchronisierung der Archivprotokolle auf dem neuen Server mit der gewünschten Methode durch.</block>
  <block id="37d6d9420cef8230249e7691315f0e13" category="list-text">Die Wiederherstellungsprotokolle der Quelle müssen auf den neuen Server kopiert werden. In diesem Beispiel wurden die Wiederherstellungsprotokolle in ein neues Verzeichnis unter verschoben<block ref="96bb85ed0fe822778c4e5a821e991ad8" prefix=" " category="inline-code"></block>.</block>
  <block id="d0a5f1380bd67b0d0fd3bc72f22d88f2" category="list-text">In dieser Phase enthält die neue Datenbankumgebung alle Dateien, die als Quelle erforderlich sind. Die Archivprotokolle müssen ein letztes Mal wiedergegeben werden.</block>
  <block id="c2721157205b2d2e96c656c45ca72992" category="list-text">Nach Abschluss müssen die Wiederherstellungsprotokolle erneut wiedergegeben werden. Wenn die Meldung angezeigt wird<block ref="975df7a5099101ffbe47981cd1d37c2c" prefix=" " category="inline-code"></block> Wird zurückgegeben, der Prozess ist erfolgreich und die Datenbanken sind synchronisiert und können geöffnet werden.</block>
  <block id="6acab27c5f334c66f06f3f74bc62d6d8" category="section-title">Protokollversand: ASM an Dateisystem</block>
  <block id="a67ffcc8b155bb909a6f8c5d5ba2f334" category="paragraph">In diesem Beispiel wird die Verwendung von Oracle RMAN zur Migration einer Datenbank demonstriert. Es ähnelt dem vorherigen Beispiel des Dateisystems zum Protokollversand des Dateisystems, aber die Dateien auf ASM sind für den Host nicht sichtbar. Die einzigen Optionen für die Migration von Daten auf ASM-Geräten sind entweder die Verlagerung der ASM-LUN oder die Durchführung der Kopiervorgänge mithilfe von Oracle RMAN.</block>
  <block id="1c51600a778919f7f3abb5b25175220a" category="paragraph">Auch wenn RMAN für das Kopieren von Dateien aus Oracle ASM erforderlich ist, ist die Verwendung von RMAN nicht auf ASM beschränkt. Mit RMAN können beliebige Storage-Typen zu beliebigen anderen Storage-Typen migriert werden.</block>
  <block id="2cb6a5bfc4a76f651d496cddc685e078" category="paragraph">Dieses Beispiel zeigt die Verlagerung einer Datenbank namens PANCAKE aus dem ASM-Speicher in ein normales Dateisystem, das sich auf einem anderen Server in Pfaden befindet<block ref="f006bf17b06c884c47190c4225d3215a" prefix=" " category="inline-code"></block> Und<block ref="0e62afc91abe157ef67895cca0a7f912" prefix=" " category="inline-code"></block>.</block>
  <block id="d1d789c37f1c96d0a5de07798c14fcc5" category="paragraph">Im ersten Schritt wird ein Backup der Datenbank erstellt, die auf einen alternativen Server migriert werden soll. Da die Quelle Oracle ASM verwendet, muss RMAN verwendet werden. Ein einfaches RMAN-Backup kann wie folgt durchgeführt werden. Diese Methode erstellt ein getaggtes Backup, das später im Verfahren von RMAN leicht identifiziert werden kann.</block>
  <block id="ed927706d8abc2bbf3a5a8bc55e1e562" category="paragraph">Der erste Befehl definiert den Zieltyp für das Backup und den zu verwendenden Speicherort. Die zweite initiiert nur die Sicherung der Datendateien.</block>
  <block id="db85d05166dd65559ae5f426b51198e6" category="section-title">Sicherungscontrolfile</block>
  <block id="a0b95d36955723a82dde7bbe40b6ccb9" category="paragraph">Im weiteren Verlauf des Verfahrens wird eine Sicherungscontrolfile benötigt<block ref="08de2bfcc2d52d5bc4f3f2dcb97558ad" prefix=" " category="inline-code"></block> Betrieb.</block>
  <block id="76ff543de72e62217293a1d0ceade0aa" category="paragraph">In der neuen Umgebung ist auch eine Parameterdatei erforderlich. Die einfachste Methode ist, aus dem aktuellen spfile oder pfile ein pfile zu erstellen. In diesem Beispiel verwendet die Quelldatenbank eine spfile.</block>
  <block id="d28f3a5b77909b45c50e6c711bef8b60" category="section-title">Skript zum Umbenennen der ASM-Datei</block>
  <block id="c242730c760d895368557f78141ff8b1" category="paragraph">Mehrere aktuell in den Steuerdateien definierte Dateispeicherorte ändern sich, wenn die Datenbank verschoben wird. Mit dem folgenden Skript wird ein RMAN-Skript erstellt, um den Prozess zu vereinfachen. Dieses Beispiel zeigt eine Datenbank mit einer sehr kleinen Anzahl von Datendateien, aber in der Regel enthalten Datenbanken Hunderte oder gar Tausende von Datendateien.</block>
  <block id="359b84bcf7447a77b212e844aeaa9d89" category="inline-link-macro">Namenskonvertierung von ASM in Dateisystem</block>
  <block id="374a1a0facd7c604ac09ed0dca6efb3e" category="paragraph">Dieses Skript finden Sie in <block ref="288325982fd7bee6cf9eb66ad33f6f7a" category="inline-link-macro-rx"></block> Und es tut zwei Dinge.</block>
  <block id="42787fdca1cf792241ce03151ec7dbd3" category="paragraph">Zuerst erstellt es einen Parameter, um die Speicherort des Wiederherstellungsprotokolls neu zu definieren<block ref="0f2d7ef9b967c2d9fb1ad5aa5e9eb688" prefix=" " category="inline-code"></block>. Es handelt sich im Wesentlichen um eine Liste von abwechselnden Feldern. Das erste Feld ist der Speicherort eines aktuellen Wiederherstellungsprotokolls und das zweite Feld ist der Speicherort auf dem neuen Server. Das Muster wird dann wiederholt.</block>
  <block id="cf444fd96c87609516f0ad8212f41b01" category="paragraph">Die zweite Funktion ist die Bereitstellung einer Vorlage für die Umbenennung von Datendateien. Das Skript führt eine Schleife durch die Datendateien durch, ruft den Namen und die Dateinummer ab und formatiert sie als RMAN-Skript. Dann macht es das gleiche mit den temporären Dateien. Das Ergebnis ist ein einfaches rman-Skript, das nach Bedarf bearbeitet werden kann, um sicherzustellen, dass die Dateien an dem gewünschten Speicherort wiederhergestellt werden.</block>
  <block id="50614366822786a4f2ee3a5065543634" category="paragraph">Erfassen Sie die Ausgabe dieses Bildschirms. Der<block ref="0f2d7ef9b967c2d9fb1ad5aa5e9eb688" prefix=" " category="inline-code"></block> Der Parameter wird wie unten beschrieben in pfile platziert. Die RMAN-Datendatei umbenennen und das doppelte Skript müssen entsprechend bearbeitet werden, um die Datendateien an den gewünschten Speicherorten zu platzieren. In diesem Beispiel werden sie alle in platziert<block ref="36c3572e381980ee1f1343988ae4a955" prefix=" " category="inline-code"></block>.</block>
  <block id="e60cd818b9f7ae02a73d8b3a6ea0a809" category="paragraph">Die Skripte sind fast fertig zur Ausführung, aber zuerst muss die Verzeichnisstruktur vorhanden sein. Wenn die benötigten Verzeichnisse nicht bereits vorhanden sind, müssen sie erstellt werden, oder der Datenbankstartvorgang schlägt fehl. Das folgende Beispiel gibt die Mindestanforderungen wieder.</block>
  <block id="2f2eed2ed14ba876a017aa9ea9515724" category="paragraph">Der folgende Befehl ist für Dienstprogramme wie oraenv erforderlich, um ordnungsgemäß zu funktionieren.</block>
  <block id="f34a9f7c077457a54f8838b55b9fc025" category="section-title">Parameteraktualisierungen</block>
  <block id="fd5c3a935e7374f8b1577e73e85ef375" category="paragraph">Die gespeicherte pfile muss aktualisiert werden, um alle Pfadänderungen auf dem neuen Server widerzuspiegeln. Die Änderungen des Datendateipfads werden durch das RMAN-Duplizierungsskript geändert, und fast alle Datenbanken erfordern Änderungen am<block ref="d98137bd230dac4372cf20a2e7839463" prefix=" " category="inline-code"></block> Und<block ref="2d205df7ae2efff1576140524b40b93c" prefix=" " category="inline-code"></block> Parameter. Es können auch Prüfdateipositionen vorhanden sein, die geändert werden müssen, und Parameter wie<block ref="40f8833ae28ec69bcc89eb0eba090fe4" prefix=" " category="inline-code"></block> Ist außerhalb von ASM möglicherweise nicht relevant. Ein erfahrener DBA sollte die vorgeschlagenen Änderungen sorgfältig prüfen, bevor er fortfahren kann.</block>
  <block id="b581dd429e9b4c24127b8940ca8e4346" category="paragraph">In diesem Beispiel sind die wichtigsten Änderungen die Speicherorte der Steuerdatei, das Protokollarchivziel und das Hinzufügen des<block ref="0f2d7ef9b967c2d9fb1ad5aa5e9eb688" prefix=" " category="inline-code"></block> Parameter.</block>
  <block id="bbba1f65801697ba3abd94d90c83ba71" category="paragraph">Nachdem die neuen Parameter bestätigt wurden, müssen die Parameter wirksam werden. Es gibt mehrere Optionen, aber die meisten Kunden erstellen ein Spfile basierend auf dem Text pfile.</block>
  <block id="3a17a12678d74dabb1032aaf373166d8" category="section-title">Startbezeichnung</block>
  <block id="a9dc570a0b3161e69edc31f3545cbf22" category="paragraph">Der letzte Schritt vor dem Replizieren der Datenbank ist, die Datenbankprozesse zu laden, aber nicht die Dateien zu mounten. In diesem Schritt können Probleme mit dem spfile offensichtlich werden. Wenn der<block ref="357eedab214bb515c5622e275bc19cdb" prefix=" " category="inline-code"></block> Befehl schlägt aufgrund eines Parameterfehlers fehl, es ist einfach herunterzufahren, die pfile-Vorlage zu korrigieren, sie als spfile neu zu laden und es erneut zu versuchen.</block>
  <block id="0257eff4bf9bf0cb483f2544fd42f109" category="section-title">Duplizieren Sie die Datenbank</block>
  <block id="4632b675c558eda129fe6da4fbbae2b2" category="paragraph">Die Wiederherstellung des vorherigen RMAN-Backups am neuen Speicherort nimmt mehr Zeit in Anspruch als andere Schritte in diesem Prozess. Die Datenbank muss ohne Änderung der Datenbank-ID (DBID) oder Zurücksetzen der Protokolle dupliziert werden. Dadurch wird verhindert, dass Protokolle angewendet werden, was ein erforderlicher Schritt zur vollständigen Synchronisierung der Kopien ist.</block>
  <block id="06206ea0cdea741420d9f7619503db89" category="paragraph">Stellen Sie mit RMAN als AUX eine Verbindung zur Datenbank her, und geben Sie den Befehl Duplicate Database aus, indem Sie das in einem vorherigen Schritt erstellte Skript verwenden.</block>
  <block id="5ae45dfed3c4aff7fe4fd6a7e3606cb0" category="paragraph">Sie müssen die Änderungen nun von der Quelldatenbank an einen neuen Speicherort senden. Dies kann eine Kombination von Schritten erfordern. Die einfachste Methode wäre, RMAN auf der Quelldatenbank zu haben, um Archivprotokolle auf eine freigegebene Netzwerkverbindung zu schreiben. Wenn ein freigegebener Speicherort nicht verfügbar ist, verwenden Sie RMAN zum Schreiben auf ein lokales Dateisystem und anschließend rcp oder rsync zum Kopieren der Dateien.</block>
  <block id="e355807335e5f05930a748afe1c3b23d" category="paragraph">In diesem Beispiel ist der<block ref="45987b06d5eae35fc3e3487749a9ba27" prefix=" " category="inline-code"></block> Verzeichnis ist eine NFS-Freigabe, die sowohl für die ursprüngliche als auch für die migrierte Datenbank verfügbar ist.</block>
  <block id="49fffa356bcd041918ce24ef714f3f72" category="paragraph">Ein wichtiges Thema ist hier die<block ref="6761e3b36547cd115b3966fcbc7192b2" prefix=" " category="inline-code"></block> Klausel. Das Festplattenformat des Backups ist<block ref="9ae0f22d9fa2eac270499e74092af364" prefix=" " category="inline-code"></block>, Das bedeutet, dass Sie das Format der Thread-Nummer, Sequenznummer und Aktivierungs-ID für die Datenbank verwenden müssen. Obwohl die Buchstaben unterschiedlich sind, entspricht dies der<block ref="03e92f2c3a8cae5315f0e42bf6acf436" prefix=" " category="inline-code"></block> Parameter in pfile. Mit diesem Parameter werden auch Archivprotokolle im Format Thread-Nummer, Sequenznummer und Aktivierungs-ID angegeben. Das Endergebnis ist, dass die Protokolldatei-Backups auf der Quelle eine Benennungskonvention verwenden, die von der Datenbank erwartet wird. Dadurch werden z. B. Operationen wie die<block ref="81168dbef15af03d02c51a7447378e76" prefix=" " category="inline-code"></block> Viel einfacher, weil sqlplus richtig vorwegnimmt die Namen der Archiv-Protokolle wiedergegeben werden.</block>
  <block id="76d77393c7ffc6b0a0edd47ad09afe4d" category="paragraph">Nachdem sich die Dateien im Archiv-Log-Speicherort befinden, können sie mit dem Befehl wiedergegeben werden<block ref="5fea768eb2d353716ca623ae58ce382c" prefix=" " category="inline-code"></block> Gefolgt von der Antwort<block ref="e1f2d5134ed2543d38a0de9751cf75d9" prefix=" " category="inline-code"></block> Um alle verfügbaren Protokolle automatisch wiederzugeben. Die Parameterdatei leitet derzeit Archivprotokolle an<block ref="e421a42fc6f604fcc78707336ed222b9" prefix=" " category="inline-code"></block>, Aber dies stimmt nicht mit dem Speicherort überein, an dem RMAN zum Speichern von Protokollen verwendet wurde. Der Speicherort kann vor der Wiederherstellung der Datenbank wie folgt vorübergehend umgeleitet werden.</block>
  <block id="0a96a4559ad3ab8c616a12a207e8daca" category="paragraph">Die endgültige Antwort des Archivprotokolls meldet einen Fehler. Dies ist jedoch normal. Der Fehler zeigt an, dass sqlplus eine bestimmte Protokolldatei gesucht und nicht gefunden hat. Der Grund dafür ist sehr wahrscheinlich, dass die Protokolldatei noch nicht existiert.</block>
  <block id="e2a234d3299b9e66f0756db00c9ee8c7" category="paragraph">In den meisten Fällen erfolgt die Migration nicht sofort. Es kann Tage oder sogar Wochen bis zum Abschluss des Migrationsprozesses dauern. Das bedeutet, dass die Protokolle kontinuierlich an die Replikatdatenbank gesendet und wieder eingespielt werden müssen. So ist sichergestellt, dass bei der Umstellung nur minimale Daten übertragen und eingespielt werden müssen.</block>
  <block id="247e1695519da78866726a4dcb2c5252" category="paragraph">Dieser Prozess kann einfach per Skript ausgeführt werden. Beispielsweise kann der folgende Befehl für die ursprüngliche Datenbank geplant werden, um sicherzustellen, dass der für den Protokollversand verwendete Speicherort fortlaufend aktualisiert wird.</block>
  <block id="3ce19ecb7c874f75b9a46e29abe2012e" category="inline-link-macro">Wiedergabe von Protokollen in der Standby-Datenbank</block>
  <block id="469d3082a0f3ad08223ba8fe69bf9311" category="paragraph">Nachdem die Protokolle empfangen wurden, müssen sie erneut abgespielt werden. Frühere Beispiele zeigten die Verwendung von sqlplus zum manuellen Ausführen<block ref="5fea768eb2d353716ca623ae58ce382c" prefix=" " category="inline-code"></block>, Die leicht automatisiert werden kann. Das hier abgebildete Beispiel verwendet das in beschriebene Skript <block ref="7c7730292e0135c86626e1771f7f02ec" category="inline-link-macro-rx"></block>. Das Skript akzeptiert ein Argument, das die Datenbank angibt, für die eine Wiedergabeoperation erforderlich ist. Bei diesem Prozess kann dasselbe Skript für eine Migration mit mehreren Datenbanken verwendet werden.</block>
  <block id="15d3442f7626bb62f4b255c7ed5c5098" category="paragraph">Wenn Sie bereit sind, in die neue Umgebung zu schneiden, müssen Sie eine abschließende Synchronisierung durchführen. Bei der Arbeit mit normalen Dateisystemen ist es leicht sicherzustellen, dass die migrierte Datenbank zu 100 % mit dem Original synchronisiert wird, da die ursprünglichen Wiederherstellungsprotokolle kopiert und wiedergegeben werden. Es gibt keinen guten Weg, dies mit ASM zu tun. Nur die Archivprotokolle können einfach wiederaufgenommen werden. Um sicherzustellen, dass keine Daten verloren gehen, muss das endgültige Herunterfahren der ursprünglichen Datenbank sorgfältig durchgeführt werden.</block>
  <block id="f9952f2469fbec5fc9dca180322ff9ee" category="list-text">Zunächst muss die Datenbank stillgelegt werden, um sicherzustellen, dass keine Änderungen vorgenommen werden. Diese Stilllegung kann die Deaktivierung geplanter Vorgänge, das Herunterfahren von Listenern und/oder das Herunterfahren von Anwendungen umfassen.</block>
  <block id="58fd1c52efcde7151ac264a8f67b14ff" category="list-text">Nach diesem Schritt erstellen die meisten DBAs eine Dummy-Tabelle, die als Marker für das Herunterfahren dient.</block>
  <block id="442da9ab2cced82a6cbdd112be285bcf" category="list-text">Erzwingen Sie eine Protokollarchivierung, um sicherzustellen, dass die Erstellung der Dummy-Tabelle in den Archivprotokollen aufgezeichnet wird. Führen Sie dazu die folgenden Befehle aus:</block>
  <block id="83fce8a41250d6af49205f9659468351" category="list-text">Führen Sie die folgenden Befehle aus, um die letzten Archivprotokolle zu kopieren. Die Datenbank muss verfügbar, aber nicht geöffnet sein.</block>
  <block id="1b0e887c1394d4617c2a5243da19e44e" category="list-text">Um die Archivprotokolle zu kopieren, führen Sie die folgenden Befehle aus:</block>
  <block id="86a7e05acb17098c514dd570b9220302" category="list-text">Geben Sie abschließend die restlichen Archivprotokolle auf dem neuen Server wieder.</block>
  <block id="a430882deb5403e7b4b8061dec17cc80" category="list-text">In dieser Phase sollten Sie alle Daten replizieren. Die Datenbank kann von einer Standby-Datenbank in eine aktive Betriebsdatenbank konvertiert und dann geöffnet werden.</block>
  <block id="077c7bcbf9a94bb62aecadc85045886e" category="list-text">Bestätigen Sie das Vorhandensein der Dummy-Tabelle und legen Sie sie dann ab.</block>
  <block id="6130a27b8ec8ccf18e06bb3389b0fbbd" category="section-title">Unterbrechungsfreie Migration von Wiederherstellungsprotokollen</block>
  <block id="26c294f993888218237d5f9306bfcfc4" category="paragraph">Es gibt Zeiten, in denen eine Datenbank insgesamt korrekt organisiert ist, mit Ausnahme der Wiederherstellungsprotokolle. Dies kann aus vielen Gründen geschehen, von denen die häufigste im Zusammenhang mit Snapshots steht. Produkte wie SnapManager für Oracle, SnapCenter und das Storage Management Framework NetApp Snap Creator ermöglichen eine nahezu sofortige Wiederherstellung einer Datenbank, jedoch nur, wenn Sie den Zustand der Daten-File-Volumes zurücksetzen. Wenn Redo-Logs Speicherplatz mit den Datendateien teilen, kann Reversion nicht sicher ausgeführt werden, da es zur Zerstörung der Redo-Protokolle führen würde, was wahrscheinlich Datenverlust bedeutet. Daher müssen die Redo-Logs verschoben werden.</block>
  <block id="2c56f5081b94b926c93c40f4c935a44a" category="paragraph">Dieses Verfahren ist einfach und unterbrechungsfrei.</block>
  <block id="2986f9fe7ed4aab5aaa64c2ee9fe4841" category="section-title">Aktuelle Konfiguration des Wiederherstellungsprotokolls</block>
  <block id="dd786d22c88a5f1fe0c10cfec58c1b44" category="list-text">Ermitteln Sie die Anzahl der Redo-Log-Gruppen und deren jeweilige Gruppennummern.</block>
  <block id="3cd0f7a32a4f5ee026702f8125f3dda7" category="list-text">Geben Sie die Größe der Wiederherstellungsprotokolle ein.</block>
  <block id="7c2f6ada37d59f2be0bc1a3ead80f77c" category="section-title">Erstellen Sie neue Protokolle</block>
  <block id="106b215e5d87fb1256e47932f2ebf9cf" category="list-text">Erstellen Sie für jedes Redo-Protokoll eine neue Gruppe mit einer passenden Größe und Anzahl von Mitgliedern.</block>
  <block id="0851b65f20b98420968e488fc3c0085e" category="list-text">Überprüfen Sie die neue Konfiguration.</block>
  <block id="cbccec9e0905e0b7f5cd630a3fa12f64" category="section-title">Alte Protokolle ablegen</block>
  <block id="00f63618719701ddae7fc2ff1d406ec2" category="list-text">Löschen Sie die alten Protokolle (Gruppen 1, 2 und 3).</block>
  <block id="f00c8361a8acc818247282bb6420a8c1" category="list-text">Wenn ein Fehler auftritt, der verhindert, dass Sie ein aktives Protokoll ablegen, erzwingen Sie einen Wechsel zum nächsten Protokoll, um die Sperre freizugeben und einen globalen Kontrollpunkt zu erzwingen. Siehe folgendes Beispiel für diesen Prozess. Der Versuch, die Logfile-Gruppe 2, die sich am alten Speicherort befand, zu löschen, wurde abgelehnt, da noch aktive Daten in dieser Logdatei vorhanden waren.</block>
  <block id="5d49a93437a42d2926c8be7eb3c7a56f" category="list-text">Eine Protokollarchivierung, gefolgt von einem Kontrollpunkt, ermöglicht es Ihnen, die Protokolldatei zu löschen.</block>
  <block id="e3898c187d7edf64fc8e4d4e409fd244" category="list-text">Löschen Sie anschließend die Protokolle aus dem Dateisystem. Sie sollten diesen Vorgang mit äußerster Sorgfalt durchführen.</block>
  <block id="dacc2a633e9981c9f91e43ac6b59e889" category="paragraph">Die Oracle-Datenmigration kann auf einer der drei Ebenen erfolgen: Der Datenbank, dem Host oder dem Storage Array.</block>
  <block id="5153193d50d4c628312db2d5bc0132ed" category="paragraph">Die Unterschiede liegen darin, welche Komponente der Gesamtlösung für das Verschieben von Daten verantwortlich ist: Die Datenbank, das Host-Betriebssystem oder das Speichersystem.</block>
  <block id="cc3e73ae592a355b9beb405232d08fe2" category="paragraph">Die Abbildung unten zeigt ein Beispiel für die Migrationsebenen und den Datenfluss. Bei einer Migration auf Datenbankebene werden die Daten vom ursprünglichen Storage-System über die Host- und Datenbankschichten in die neue Umgebung verschoben. Die Migration auf Host-Ebene ist ähnlich, aber die Daten durchlaufen nicht die Applikationsebene und werden stattdessen mithilfe von Host-Prozessen an den neuen Speicherort geschrieben. Und schließlich ist bei der Migration auf Storage-Ebene ein Array wie ein NetApp FAS System für die Datenverschiebung verantwortlich.</block>
  <block id="204415652114bfb773b51cd76c8c692e" category="paragraph"><block ref="204415652114bfb773b51cd76c8c692e" category="inline-image-macro-rx" type="image"></block></block>
  <block id="b059e0e23ffaa8fbd97f681942ce0018" category="paragraph">Eine Migration auf Datenbankebene bezieht sich im Allgemeinen auf die Verwendung von Oracle-Protokollversand über eine Standby-Datenbank, um eine Migration auf der Oracle-Ebene durchzuführen. Migrationen auf Host-Ebene werden mithilfe der nativen Funktionen der Konfiguration des Host-Betriebssystems durchgeführt. Diese Konfiguration umfasst Dateikopiervorgänge mit Befehlen wie CP, tar und Oracle Recovery Manager (RMAN) oder mit einem Logical Volume Manager (LVM) zur Verlagerung der zugrunde liegenden Bytes eines Dateisystems. Oracle Automatic Storage Management (ASM) wird als Funktion auf Hostebene kategorisiert, da sie unter der Ebene der Datenbankanwendung ausgeführt wird. ASM tritt an die Stelle des üblichen Logical Volume Managers auf einem Host. Und schließlich können Daten auf Storage-Array-Ebene migriert werden, d. h. unter der Ebene des Betriebssystems.</block>
  <block id="228d9e0418aa12bae73676a5743558db" category="section-title">Überlegungen zur Planung</block>
  <block id="ce7eea640cd6ee20e55cbe868f4026e9" category="paragraph">Die beste Option für die Migration hängt von einer Kombination verschiedener Faktoren ab: Vom Umfang der zu migrierenden Umgebung, der Notwendigkeit, Ausfallzeiten zu vermeiden, und dem für die Migration erforderlichen Gesamtaufwand. Große Datenbanken erfordern offensichtlich mehr Zeit und Aufwand für die Migration, aber die Komplexität einer solchen Migration ist minimal. Kleine Datenbanken können schnell migriert werden. Wenn jedoch Tausende migriert werden müssen, kann das Ausmaß des Aufwands zu Komplikationen führen. Und je größer die Datenbank ist, desto wahrscheinlicher ist sie, dass sie geschäftskritisch ist. Dies führt dazu, dass Ausfallzeiten minimiert werden müssen, während gleichzeitig ein Back-Out-Pfad beibehalten wird.</block>
  <block id="d4b9b19593598d98059aae44d6a7bbe1" category="paragraph">Im Folgenden werden einige Überlegungen zur Planung einer Migrationsstrategie erörtert.</block>
  <block id="931c6f2c99380fbd899e3eaa31e97d98" category="section-title">Datengröße</block>
  <block id="64e4d570abeebacf6bdcb19dfb73fcae" category="paragraph">Die Größe der zu migrierenden Datenbanken wirkt sich natürlich auf die Migrationsplanung aus. Die Größe wirkt sich jedoch nicht unbedingt auf die Umstellungszeit aus. Wenn große Datenmengen migriert werden müssen, kommt es in erster Linie auf die Bandbreite an. Kopiervorgänge werden in der Regel mit effizienten sequenziellen I/O-Vorgängen ausgeführt Gehen Sie bei Kopiervorgängen von einer Auslastung der verfügbaren Netzwerkbandbreite von 50 % aus. Ein 8-GB-FC-Port kann theoretisch etwa 800 Mbit/s übertragen. Bei einer Auslastung von 50 % kann eine Datenbank mit einer Geschwindigkeit von etwa 400 Mbps kopiert werden. Somit kann eine 10-TB-Datenbank innerhalb von etwa sieben Stunden kopiert werden.</block>
  <block id="84a80a048e7abc027dd6bdce55fda9d2" category="inline-link-macro">Online-Datendatei verschieben</block>
  <block id="1c169b76761527b28040655a783e50a2" category="section-title">Anzahl der Datenbanken</block>
  <block id="da3770c5d6d1215e9f2524b9d7b7c3c5" category="paragraph">In vielen Fällen liegt das Problem beim Verschieben großer Datenmengen nicht in der Datengröße, sondern in der Komplexität der Konfiguration, die die Datenbank unterstützt. Wenn man einfach nur weiß, dass 50 TB Datenbanken migriert werden müssen, reicht das nicht aus. Dabei kann es sich um eine einzelne 50 TB große geschäftskritische Datenbank, eine Sammlung von 4, 000 älteren Datenbanken oder eine Kombination aus Produktions- und nicht produktiven Daten handeln. In manchen Fällen besteht ein Großteil der Daten aus Klonen einer Quelldatenbank. Diese Klone müssen überhaupt nicht migriert werden, da sie einfach wiederhergestellt werden können. Dies gilt insbesondere dann, wenn die neue Architektur die Nutzung von NetApp FlexClone Volumes ermöglicht.</block>
  <block id="a1739e745a7544d2432b2b990a715817" category="paragraph">Für die Migrationsplanung müssen Sie verstehen, wie viele Datenbanken im Umfang enthalten sind und wie sie priorisiert werden müssen. Mit zunehmender Anzahl an Datenbanken ist die bevorzugte Migrationsoption in der Regel im Stack immer geringer. So kann zum Beispiel das Kopieren einer einzelnen Datenbank mit RMAN problemlos und bei einem kurzen Ausfall durchgeführt werden. Dies ist die Replizierung auf Host-Ebene.</block>
  <block id="284723d6a05b50ace5e5b96c761d03ac" category="paragraph">Wenn es 50 Datenbanken gibt, kann es einfacher sein, die Einrichtung einer neuen Dateisystemstruktur zu vermeiden, um eine RMAN-Kopie zu erhalten und stattdessen die Daten an die Stelle zu verschieben. Dies kann durch hostbasierte LVM-Migration erfolgen, um Daten von alten LUNs auf neue LUNs zu verschieben. Dadurch wird die Verantwortung vom Datenbankadministratorteam (DBA) an das Betriebssystemteam übertragen und die Daten werden in Bezug auf die Datenbank transparent migriert. Die Dateisystemkonfiguration ist unverändert.</block>
  <block id="8e734da04d394519001f47f723341e9d" category="paragraph">Wenn schließlich 500 Datenbanken von 200 Servern migriert werden müssen, können speicherbasierte Optionen wie die ONTAP Funktion zum Importieren fremder LUNs (Foreign LUN Import, FLI) verwendet werden, um eine direkte Migration der LUNs durchzuführen.</block>
  <block id="39f43935349622951e0f392c83b6e736" category="section-title">Anforderungen neu architekturgerecht</block>
  <block id="50f1558a3f9d47464b5d57f9d9d0f9d1" category="paragraph">In der Regel muss das Layout einer Datenbankdatei verändert werden, um die Funktionen des neuen Storage Array nutzen zu können. Dies ist jedoch nicht immer der Fall. Die Funktionen der EF-Series All-Flash-Arrays richten sich beispielsweise primär an SAN-Performance und SAN-Zuverlässigkeit. In den meisten Fällen können Datenbanken auf ein EF-Series Array migriert werden, ohne dass dabei besondere Überlegungen zum Datenlayout angestellt werden müssen. Die einzigen Anforderungen sind hohe IOPS, niedrige Latenz und robuste Zuverlässigkeit. Wenngleich es Best Practices für Faktoren wie RAID-Konfiguration oder Dynamic Disk Pools gibt, sind für EF-Series Projekte kaum nennenswerte Änderungen an der gesamten Storage-Architektur erforderlich, um diese Funktionen nutzen zu können.</block>
  <block id="0e2c78f7e9948e19ccb1bf2e1b7f0faf" category="paragraph">Im Gegensatz dazu erfordert die Migration zu ONTAP in der Regel eine stärkere Berücksichtigung des Datenbanklayouts, um sicherzustellen, dass die endgültige Konfiguration den größtmöglichen Nutzen erzielt. ONTAP bietet für eine Datenbankumgebung bereits viele Funktionen ohne besondere Architekturanstrengungen. Am wichtigsten ist jedoch die Möglichkeit einer unterbrechungsfreien Migration auf neue Hardware, wenn die aktuelle Hardware das Ende ihres Lebenszyklus erreicht. Generell gilt: Eine Migration zu ONTAP ist die letzte Migration, die Sie durchführen müssen. Nachfolgende Hardware Upgrades werden durchgeführt und die Daten werden unterbrechungsfrei auf neue Medien migriert.</block>
  <block id="ad93bd21d59f2286385dc519fd7e39fb" category="paragraph">Bei einigen Planungen sind noch mehr Vorteile verfügbar. Die wichtigsten Überlegungen beziehen sich auf die Verwendung von Snapshots. Snapshots bilden die Grundlage für nahezu sofortige Backups, Restores und Klonvorgänge. Ein Beispiel für die Leistung von Snapshots ist der größte bekannte Einsatz bei einer einzigen Datenbank mit 996 TB, die auf ca. 250 LUNs auf 6 Controllern ausgeführt wird. Diese Datenbank kann innerhalb von 2 Minuten gesichert, innerhalb von 2 Minuten wiederhergestellt und innerhalb von 15 Minuten geklont werden. Zu den weiteren Vorteilen zählen die Möglichkeit, Daten im Cluster als Reaktion auf Workload-Änderungen zu verschieben, sowie die Anwendung von Quality-of-Service-Kontrollen (QoS), um in einer Umgebung mit mehreren Datenbanken eine gute und konsistente Performance zu erreichen.</block>
  <block id="6b48ce64da84423ce167ffb72f5f9a8b" category="inline-link-macro">Oracle-Migrationsverfahren – Überblick</block>
  <block id="1311d3b592e4da5950e91965dac1d593" category="paragraph">Technologien wie QoS-Steuerung, Datenverlagerung, Snapshots und Klonen arbeiten in nahezu jeder Konfiguration. Allerdings ist einige Überlegungen im Allgemeinen erforderlich, um die Vorteile zu maximieren. In einigen Fällen können Änderungen am Design von Datenbank-Storage-Layouts erforderlich sein, um die Investitionen in das neue Speicher-Array zu maximieren. Solche Designänderungen können sich auf die Migrationsstrategie auswirken, da Host- oder Storage-basierte Migrationen das ursprüngliche Datenlayout replizieren. Weitere Schritte sind möglicherweise erforderlich, um die Migration abzuschließen und ein für ONTAP optimiertes Daten-Layout zu liefern. Die in dargestellten Verfahren <block ref="d4d5600aaaced44e203002ec1910822d" category="inline-link-macro-rx"></block> Und später zeigen einige der Methoden, um nicht nur eine Datenbank zu migrieren, sondern sie mit minimalem Aufwand in das optimale finale Layout zu migrieren.</block>
  <block id="1fd9b0a8321228b5b8ad4be85e71cf64" category="section-title">Umstellungszeit</block>
  <block id="95b9194d9f06f6f70febc87c071f3050" category="paragraph">Der maximal zulässige Service-Ausfall während der Umstellung sollte ermittelt werden. Es ist ein häufiger Fehler anzunehmen, dass der gesamte Migrationsprozess zu Störungen führt. Viele Aufgaben können vor Beginn von Serviceunterbrechungen durchgeführt werden. Viele Optionen ermöglichen den Abschluss der Migration ohne Unterbrechungen oder Ausfälle. Auch wenn Unterbrechungen unvermeidbar sind, müssen Sie dennoch den maximal zulässigen Serviceausfall definieren, da die Dauer der Umstellungszeit von Prozedur zu Prozedur variiert.</block>
  <block id="9cfeaf56d2fc41c2695a69adf260c3f8" category="section-title">Rückweg</block>
  <block id="f493e16f7e2caba820eba88a1c5f82d8" category="paragraph">Keine Migration ist völlig risikolos. Auch wenn die Technik einwandfrei funktioniert, besteht immer die Möglichkeit eines Anwenderfehlers. Das mit einem ausgewählten Migrationspfad verbundene Risiko muss neben den Folgen einer fehlgeschlagenen Migration berücksichtigt werden. Die transparente Online-Storage-Migrationsfunktion von Oracle ASM ist beispielsweise eines der wichtigsten Merkmale, und diese Methode ist eine der zuverlässigsten. Mit dieser Methode werden die Daten jedoch irreversibel kopiert. In dem sehr unwahrscheinlichen Fall, dass ein Problem mit ASM auftritt, gibt es keinen einfachen Rückweg. Die einzige Option besteht darin, entweder die ursprüngliche Umgebung wiederherzustellen oder die Migration mit ASM zurück zu den ursprünglichen LUNs rückgängig zu machen. Das Risiko kann durch ein Backup vom Typ Snapshot auf dem ursprünglichen Storage-System minimiert, aber nicht sogar ganz beseitigt werden, vorausgesetzt, das System ist in der Lage, einen solchen Vorgang auszuführen.</block>
  <block id="1886f1ab01daa4511ce537d1af675427" category="section-title">Probe</block>
  <block id="dd5dac65febd7071311b1ae3be1eca90" category="paragraph">Einige Migrationsverfahren müssen vor der Ausführung vollständig überprüft werden. Eine Migration und eine Generalprobe des Umstellungsprozesses ist eine häufige Anfrage bei geschäftskritischen Datenbanken, bei denen die Migration erfolgreich sein und die Downtime minimiert werden muss. Zudem gehören auch die Anwenderakzeptanztests häufig zu den Aufgaben nach der Migration, und das gesamte System kann erst nach Abschluss der Tests in die Produktionsumgebung zurückgeführt werden.</block>
  <block id="7d06b32cdd06d27f7c62fb110d831e60" category="paragraph">Wenn es Bedarf an Proben gibt, können verschiedene ONTAP Funktionen den Prozess wesentlich vereinfachen. Snapshots können insbesondere eine Testumgebung zurücksetzen und schnell mehrere platzsparende Kopien einer Datenbankumgebung erstellen.</block>
  <block id="440a667261bd94f099ef4dde5caf024f" category="summary">Migration einzelner Oracle Datendateien</block>
  <block id="727bb4db430517995ca651c4f57b0c9f" category="doc">Datendatei verschieben</block>
  <block id="fade70b277397e039e100bf0faa98d8e" category="paragraph">Einzelne Oracle Datendateien können mit einem einzigen Befehl verschoben werden.</block>
  <block id="50c3d56e93d52f122bab9de1f2b35780" category="paragraph">Mit dem folgenden Befehl wird beispielsweise die Datendatei IOPST.dbf aus dem Dateisystem verschoben<block ref="87941c54be03c9785752ea54ac9a2dd4" prefix=" " category="inline-code"></block> Zu Dateisystem<block ref="eff8b2f7c4da1cf002bdea257a43fd10" prefix=" " category="inline-code"></block>.</block>
  <block id="53784088a62519d581c3dac4640c41b5" category="paragraph">Das Verschieben einer Datendatei mit dieser Methode kann langsam sein, sollte jedoch normalerweise nicht genügend I/O produzieren, um die täglichen Datenbank-Workloads zu beeinträchtigen. Im Gegensatz dazu kann die Migration über die ASM-Ausbalancierung viel schneller ablaufen, doch dies geschieht auf Kosten der Verlangsamung der gesamten Datenbank, während die Daten verschoben werden.</block>
  <block id="f92065612dfd409585ca08d95cc7a4dc" category="paragraph">Die zum Verschieben von Datendateien erforderliche Zeit kann einfach gemessen werden, indem eine Test-Datendatei erstellt und dann verschoben wird. Die verstrichene Zeit für den Vorgang wird in den Sitzungsdaten mit den folgenden Kosten aufgezeichnet:</block>
  <block id="78fb69ccdf01155db62828d0bc182b84" category="paragraph">In diesem Beispiel handelte es sich bei der verschobenen Datei um die Datendatei 8, deren Größe 21 GB betrug und die Migration ca. 6 Minuten in Anspruch nahm. Der erforderliche Zeitaufwand hängt natürlich von den Funktionen des Storage-Systems, des Storage-Netzwerks und der gesamten Datenbankaktivität zum Zeitpunkt der Migration ab.</block>
  <block id="593b763e01de37a327966ee1abb3f8a2" category="summary">Oracle-Migration mit dem Host-seitigen Storage Stack</block>
  <block id="3dc2d5f8d7209bb75e8d67aed68bc5ed" category="paragraph">Wie bei der Migration auf Datenbankebene bietet auch die Migration auf Hostebene einen vom Storage-Anbieter unabhängigen Ansatz.</block>
  <block id="1686ceaaa0d8fb4f9a8adb5e717b029c" category="paragraph">Mit anderen Worten, manchmal "einfach die Dateien kopieren" ist die beste Option.</block>
  <block id="d5732a4ab4c633ceb871cebdbf9ae34f" category="paragraph">Obwohl dieser Low-Tech-Ansatz zu einfach erscheint, bietet er doch erhebliche Vorteile, da keine spezielle Software erforderlich ist und die Originaldaten während des Prozesses sicher unberührt bleiben. Die primäre Einschränkung besteht darin, dass eine Datenmigration auf Dateikopien einen unterbrechungsfreien Prozess darstellt, da die Datenbank vor Beginn des Kopiervorgangs heruntergefahren werden muss. Es gibt keine gute Möglichkeit, Änderungen innerhalb einer Datei zu synchronisieren, so dass die Dateien vollständig stillgelegt werden müssen, bevor das Kopieren beginnt.</block>
  <block id="706e7514bb30a289d7ce786a1a5c2ca0" category="paragraph">Wenn das für einen Kopiervorgang erforderliche Herunterfahren nicht wünschenswert ist, ist die nächstbeste Host-basierte Option die Nutzung eines Logical Volume Managers (LVM). Es gibt viele LVM-Optionen, einschließlich Oracle ASM, alle mit ähnlichen Funktionen, aber auch mit einigen Einschränkungen, die berücksichtigt werden müssen. In den meisten Fällen lässt sich die Migration ohne Ausfallzeit und Unterbrechung durchführen.</block>
  <block id="f58e97ac8406ca3e8f0bb7a019a81985" category="section-title">Dateisystem wird kopiert</block>
  <block id="a08ab728e75c63a41ce7571cb781cda2" category="paragraph">Der Nutzen einer einfachen Kopieroperation sollte nicht unterschätzt werden. Dieser Vorgang erfordert während des Kopierprozesses Ausfallzeiten, ist jedoch äußerst zuverlässig und erfordert keine besondere Expertise in Bezug auf Betriebssysteme, Datenbanken oder Speichersysteme. Darüber hinaus ist es sehr sicher, weil es die ursprünglichen Daten nicht beeinträchtigt. In der Regel ändert ein Systemadministrator die Quelldateisysteme, die schreibgeschützt gemountet werden, und startet dann einen Server neu, um zu gewährleisten, dass die aktuellen Daten nicht beschädigt werden können. Der Kopiervorgang kann mithilfe eines Skripts durchgeführt werden, um sicherzustellen, dass er so schnell wie möglich ohne das Risiko eines Benutzerfehlers ausgeführt wird. Da der I/O-Typ eine einfache, sequenzielle Datenübertragung ist, ist er eine äußerst effiziente Bandbreitennutzung.</block>
  <block id="ba0bd93801872993ce832352dd03b56a" category="paragraph">Das folgende Beispiel zeigt eine Möglichkeit für eine sichere und schnelle Migration.</block>
  <block id="d8cb8bcd239bbe7a2a488c1d68dbe420" category="paragraph">Die zu migrierende Umgebung ist wie folgt:</block>
  <block id="3ffbdefc66ad1d1e707c1d6820afbdc0" category="list-text">Aktuelle Dateisysteme</block>
  <block id="bdc9e98c6b17bb14a7bce916054413ba" category="list-text">Neue Filesysteme</block>
  <block id="3b878279a04dc47d60932cb294d96259" category="section-title">Überblick</block>
  <block id="e76912cd513cc8cbcabc2735b524e814" category="paragraph">Die Datenbank kann von einem Datenbankadministrator migriert werden, indem er die Datenbank herunterfährt und die Dateien kopiert. Der Prozess kann jedoch problemlos Skripte erstellen, wenn viele Datenbanken migriert werden müssen oder die Ausfallzeit minimiert werden muss. Die Verwendung von Skripten verringert zudem das Risiko von Benutzerfehlern.</block>
  <block id="d21aae9692f3b1d4e53fd73131414b93" category="paragraph">Die Beispielskripte automatisieren die folgenden Vorgänge:</block>
  <block id="66909ae9d06bad444f5a059b8ee2cd41" category="list-text">Die Datenbank wird heruntergefahren</block>
  <block id="ae737f3f8c04ccbefa99b2bbf87dccc2" category="list-text">Konvertieren der vorhandenen Dateisysteme in einen schreibgeschützten Zustand</block>
  <block id="1dd90b7b28cbd03687df6538c66ff494" category="list-text">Kopieren aller Daten von der Quelle auf Zieldateisysteme, wobei alle Dateiberechtigungen erhalten bleiben</block>
  <block id="741341be7809d7f5765b2352e6ab0aab" category="list-text">Heben Sie das Mounten der alten und neuen Dateisysteme auf</block>
  <block id="7465bc030ca3fe236bcf3ecdf8aaebe4" category="list-text">Erneutes Mounten der neuen Dateisysteme in denselben Pfaden wie die vorherigen Dateisysteme</block>
  <block id="8c4271e6faf2cea4cfc8e5b7108d8ee9" category="section-title">Verfahren</block>
  <block id="b273f8d4284bd1f58cfbafe8065af9fa" category="list-text">Fahren Sie die Datenbank herunter.</block>
  <block id="67eadd4b70772835f318e6d3027e4308" category="inline-link-macro">Dateisystem in schreibgeschützt konvertieren</block>
  <block id="474aae6515223cc3ba71074c9d5abcae" category="list-text">Konvertieren Sie die Dateisysteme in schreibgeschützt. Dies ist mit einem Skript schneller möglich, wie in dargestellt <block ref="2d6e37849c641071fe1cf71ac348d28c" category="inline-link-macro-rx"></block>.</block>
  <block id="4879ec77723fce4974f8ae2a7f87121a" category="list-text">Vergewissern Sie sich, dass die Dateisysteme jetzt schreibgeschützt sind.</block>
  <block id="c91e7f5dba3c873d0978f4892bf8b3f2" category="list-text">Synchronisieren Sie Dateisysteminhalte mit dem<block ref="89a4551b0f29c1a652648b1cb8747021" prefix=" " category="inline-code"></block> Befehl.</block>
  <block id="91e702884985853b8a7091ed5f2beceb" category="inline-link-macro">Ersetzen Sie Das Dateisystem</block>
  <block id="ea27cab9fe7b891870e0ce59aa34e853" category="list-text">Heben Sie die Bereitstellung der alten Dateisysteme auf, und verschieben Sie die kopierten Daten. Dies ist mit einem Skript schneller möglich, wie in dargestellt <block ref="fa3b775593af4f3de8972aced258710d" category="inline-link-macro-rx"></block>.</block>
  <block id="5fe2431acb45422c46eadf66a93851af" category="list-text">Vergewissern Sie sich, dass die neuen Dateisysteme in der Position sind.</block>
  <block id="2d53403ab873059f92ad884eb271e8dc" category="list-text">Starten Sie die Datenbank.</block>
  <block id="40351c33ab81b8b374a20fd292057481" category="section-title">Vollständig automatisierte Umstellung</block>
  <block id="3f70361470a11fb351c83257cd7aa63e" category="paragraph">Dieses Beispielskript akzeptiert Argumente der Datenbank-SID gefolgt von gemeinsam getrennten Paaren von Dateisystemen. Für das oben abgebildete Beispiel wird der Befehl wie folgt ausgegeben:</block>
  <block id="5cddd7b314b7ae819e2cfcac85021426" category="paragraph">Wenn das Beispielskript ausgeführt wird, wird die folgende Sequenz ausgeführt. Er wird beendet, wenn in einem beliebigen Schritt ein Fehler auftritt:</block>
  <block id="43d4fce7952ee09c0c8df60a17f5ea56" category="list-text">Konvertieren Sie die aktuellen Dateisysteme in den schreibgeschützten Status.</block>
  <block id="9d53d62d1e6dc69460f47c83e6c1919d" category="list-text">Verwenden Sie jedes durch Kommas getrennte Paar von Dateisystemargumenten, und synchronisieren Sie das erste Dateisystem mit dem zweiten.</block>
  <block id="39ff0e6dfa9915344ec96f9e78a27267" category="list-text">Entfernen Sie die früheren Dateisysteme.</block>
  <block id="e220fa2a2806386a64e9860f27372e43" category="list-text">Aktualisieren Sie die<block ref="816a3ef77b62ade41c3f936c958aa555" prefix=" " category="inline-code"></block> Datei wie folgt:</block>
  <block id="232c2c4ddd45fc028ae13b58251c5f7e" category="list-text">Erstellen Sie ein Backup bei<block ref="9eb56e72a6bb8350a1453c2762d14178" prefix=" " category="inline-code"></block>.</block>
  <block id="d5082ae73767550813688202df21bc4d" category="list-text">Kommentieren Sie die vorherigen Einträge für die vorherigen und neuen Dateisysteme.</block>
  <block id="b8793b424d1738f1e00e545379867b0d" category="list-text">Erstellen Sie einen neuen Eintrag für das neue Dateisystem, das den alten Bereitstellungspunkt verwendet.</block>
  <block id="29478980f354caa483bc0cfb827c7251" category="list-text">Mounten Sie die Dateisysteme.</block>
  <block id="a0ec0d8bede7d9d58172084de7d5ce53" category="paragraph">Der folgende Text enthält ein Ausführungsbeispiel für dieses Skript:</block>
  <block id="350265a378d2cb1183cd51d56f0e5843" category="section-title">Oracle ASM SPFile- und Passthwd-Migration</block>
  <block id="f37bc722651f2fd86ba4da87f947c4db" category="paragraph">Eine Schwierigkeit beim Abschluss der ASM-Migration sind die ASM-spezifische SPFile- und die Passwort-Datei. Standardmäßig werden diese kritischen Metadatendateien auf der ersten definierten ASM-Laufwerksgruppe erstellt. Wenn eine bestimmte ASM-Datenträgergruppe evakuiert und entfernt werden muss, müssen die SPFile- und Passwortdatei, die diese ASM-Instanz regelt, verschoben werden.</block>
  <block id="88e2a97e52f9854b59ce63e74c4b0cac" category="paragraph">Ein weiterer Anwendungsfall, in dem diese Dateien eventuell verschoben werden müssen, ist die Implementierung von Datenbankmanagement-Software wie beispielsweise SnapManager für Oracle oder dem SnapCenter Oracle Plug-in. Eine der Funktionen dieser Produkte besteht darin, eine Datenbank schnell wiederherzustellen, indem der Zustand der ASM-LUNs, die die Datendateien hosten, zurückgesetzt wird. Um dies zu tun, muss die ASM-Laufwerksgruppe offline geschaltet werden, bevor eine Wiederherstellung durchgeführt werden kann. Dies ist kein Problem, solange die Datendateien einer Datenbank in einer dedizierten ASM-Datenträgergruppe isoliert sind.</block>
  <block id="24de04443b6bdae76336fcab04ae946c" category="paragraph">Wenn diese Datenträgergruppe auch die ASM-Datei spfile/passwd enthält, kann die Datenträgergruppe nur offline geschaltet werden, wenn die gesamte ASM-Instanz heruntergefahren wird. Dies ist ein disruptiver Prozess, was bedeutet, dass die Datei spfile/passwd verschoben werden muss.</block>
  <block id="a6221bf4332cc966c69066295a843480" category="list-text">Datenbank-SID = TOAST</block>
  <block id="0af0da2831407b8130e7f2fabdfab87d" category="list-text">Aktuelle Datendateien auf<block ref="c097579d8cb34b312d396304e0b8f487" prefix=" " category="inline-code"></block></block>
  <block id="24aa2824a0b52b46b301afdb3af9336f" category="list-text">Aktuelle Logfiles und Controlfiles auf<block ref="ad6062251d172504e3b3bed3a8256a5a" prefix=" " category="inline-code"></block></block>
  <block id="bb6de9ea84381f7d01fdd038f104e4ae" category="list-text">Neue ASM-Laufwerksgruppen als eingerichtet<block ref="cb2eb5386826561ac7895f1c878e4252" prefix=" " category="inline-code"></block> Und<block ref="5e20ccf28224fc0cc564f9825c208dbd" prefix=" " category="inline-code"></block></block>
  <block id="8b14bd9d310bf8b211c19645576201e3" category="section-title">Speicherorte für ASM-SPfile/passwd-Dateien</block>
  <block id="7c7d8dd5d09a73f06678562b66dcdc68" category="paragraph">Die Verlagerung dieser Dateien kann ohne Unterbrechungen erfolgen. Aus Sicherheitsgründen empfiehlt NetApp jedoch, die Datenbankumgebung herunterzufahren, damit Sie sicher sein können, dass die Dateien verschoben wurden und die Konfiguration ordnungsgemäß aktualisiert wird. Dieses Verfahren muss wiederholt werden, wenn mehrere ASM-Instanzen auf einem Server vorhanden sind.</block>
  <block id="3666b74ddcf77306328ac5d6db94fecd" category="section-title">Ermitteln Sie ASM-Instanzen</block>
  <block id="9d19a85576af8e1b3dbd78e343a2b022" category="paragraph">Ermitteln Sie die ASM-Instanzen anhand der in aufgezeichneten Daten<block ref="9da4474b569071bcb2ece1d0bdfe7560" prefix=" " category="inline-code"></block> Datei: Die ASM-Instanzen werden durch ein +-Symbol gekennzeichnet.</block>
  <block id="3fe2dd8cf6f82ac15544fd96f9b59b28" category="paragraph">Auf diesem Server befindet sich eine ASM-Instanz namens +ASM.</block>
  <block id="35a557b0490d5a93931025eaaf712cac" category="section-title">Stellen Sie sicher, dass alle Datenbanken heruntergefahren werden</block>
  <block id="e6b0819ae1eca7d408a097efc2fbf2cf" category="paragraph">Der einzige sichtbare smon-Prozess sollte der sman für die verwendete ASM-Instanz sein. Ein weiterer smon-Prozess zeigt an, dass eine Datenbank noch läuft.</block>
  <block id="97639c779a60f010af01997e2363f4eb" category="paragraph">Der einzige smon-Prozess ist die ASM-Instanz selbst. Das bedeutet, dass keine anderen Datenbanken ausgeführt werden und ohne das Risiko einer Störung der Datenbankvorgänge sicher fortgesetzt werden kann.</block>
  <block id="a8d157790e55d19378ddbbdfeb675ef8" category="section-title">Suchen Sie Dateien</block>
  <block id="12d8c982eb4072e31ac4bfe375193160" category="paragraph">Ermitteln Sie den aktuellen Speicherort der ASM-Datei und der Passwortdatei mithilfe des<block ref="e258dbf5114b4df07f0d9e72deb386f2" prefix=" " category="inline-code"></block> Und<block ref="bb41cdf6c9bb75ee17856f938070f23d" prefix=" " category="inline-code"></block> Befehle.</block>
  <block id="bebe64e9d0d4ae3d44b4dfb3583e0109" category="paragraph">Beide Dateien befinden sich an der Basis des<block ref="c097579d8cb34b312d396304e0b8f487" prefix=" " category="inline-code"></block> Festplattengruppe.</block>
  <block id="558f28628ba8ab4cb8420949524832d3" category="section-title">Dateien kopieren</block>
  <block id="c0c2c850cb11c5bb52abbd55a9578e54" category="paragraph">Kopieren Sie die Dateien mit dem in die neue ASM-Datenträgergruppe<block ref="c0baea639672c84505e661b9e369c68a" prefix=" " category="inline-code"></block> Und<block ref="c7adee97d05083a7b65e4b6953045177" prefix=" " category="inline-code"></block> Befehle. Wenn die neue Laufwerksgruppe vor kurzem erstellt wurde und derzeit leer ist, muss sie möglicherweise zuerst gemountet werden.</block>
  <block id="3ed2faf3e1299edd7d92d6c4064f1579" category="paragraph">Die Dateien wurden nun von kopiert<block ref="c097579d8cb34b312d396304e0b8f487" prefix=" " category="inline-code"></block> Bis<block ref="cb2eb5386826561ac7895f1c878e4252" prefix=" " category="inline-code"></block>.</block>
  <block id="1dba5fc559232a8dae45d9f88ed36030" category="section-title">ASM-Instanz aktualisieren</block>
  <block id="1fe2cc6cf436a4bcc6bdc41f947a416a" category="paragraph">Die ASM-Instanz muss jetzt aktualisiert werden, um die Standortänderung widerzuspiegeln. Der<block ref="c8b1affa9ec15d0c0e79347905e75d3e" prefix=" " category="inline-code"></block> Und<block ref="73fb08020d8535b4123a8968c596038c" prefix=" " category="inline-code"></block> Befehle aktualisieren die zum Starten der ASM-Datenträgergruppe erforderlichen ASM-Metadaten.</block>
  <block id="626759f91a0458b0c068cce8a05a6468" category="section-title">Aktivieren Sie ASM mit aktualisierten Dateien</block>
  <block id="69f08287c6faa04381e5a05f4087abe1" category="paragraph">Zu diesem Zeitpunkt verwendet die ASM-Instanz weiterhin die früheren Speicherorte dieser Dateien. Die Instanz muss neu gestartet werden, um ein erneutes Lesen der Dateien von ihren neuen Speicherorten zu erzwingen und Sperren für die vorherigen Dateien freizugeben.</block>
  <block id="5505cf0c514193b89221f1cef3004c58" category="section-title">Entfernen Sie alte spfile- und Passwortdateien</block>
  <block id="8f2d3ac58be7a91f51d7ce9ffa04e397" category="paragraph">Wenn der Vorgang erfolgreich durchgeführt wurde, sind die vorherigen Dateien nicht mehr gesperrt und können jetzt entfernt werden.</block>
  <block id="cfca2b2eb0d694e70904ec82f5d1c34e" category="section-title">Kopie von Oracle ASM zu ASM</block>
  <block id="12632d520b8d9518b5622ac00dc764a8" category="paragraph">Oracle ASM ist im Grunde ein schlankes kombiniertes Volume-Manager- und Dateisystem. Da das Dateisystem nicht sofort sichtbar ist, muss RMAN für Kopiervorgänge verwendet werden. Ein auf Kopien basierender Migrationsprozess ist zwar sicher und einfach, kann jedoch mit Unterbrechungen verbunden sein. Die Unterbrechung kann minimiert, aber nicht vollständig beseitigt werden.</block>
  <block id="152106ebb2aaa94cc87657d353331189" category="paragraph">Wenn Sie eine unterbrechungsfreie Migration einer ASM-basierten Datenbank wünschen, empfiehlt es sich, die ASM-Fähigkeit zu nutzen, um ASM-Extents auf neue LUNs auszugleichen, während die alten LUNs gelöscht werden. Dies ist im Allgemeinen sicher und unterbrechungsfrei, bietet aber keinen Ausweg. Wenn Funktions- oder Leistungsprobleme auftreten, besteht die einzige Möglichkeit darin, die Daten zurück zur Quelle zu migrieren.</block>
  <block id="81fe23a1ac3b2bb348e579b2c5b0f5d5" category="paragraph">Dieses Risiko kann vermieden werden, indem die Datenbank an den neuen Speicherort kopiert wird, anstatt Daten zu verschieben, sodass die Originaldaten nicht geändert werden. Die Datenbank kann vor der Inbetriebnahme vollständig an ihrem neuen Standort getestet werden, und die ursprüngliche Datenbank steht als Fallback-Option zur Verfügung, wenn Probleme gefunden werden.</block>
  <block id="3028182db088e823770b3d02aa0eac9e" category="paragraph">Dieses Verfahren ist eine von vielen Optionen, die RMAN einbeziehen. Er ermöglicht einen zweistufigen Prozess, bei dem das erste Backup erstellt und später durch die Protokollwiedergabe synchronisiert wird. Dieser Prozess sollte die Downtime minimieren, da die Datenbank betriebsbereit bleibt und während der ersten Basiskopie Daten bereitgestellt werden können.</block>
  <block id="634f50a6c6f112c46150a301d749dbcb" category="section-title">Datenbank kopieren</block>
  <block id="5f0bd85dd65afab1472713e8e034fff7" category="paragraph">Oracle RMAN erstellt eine vollständige Kopie der Quelldatenbank der Ebene 0, die sich derzeit in der ASM-Datenträgergruppe befindet<block ref="c097579d8cb34b312d396304e0b8f487" prefix=" " category="inline-code"></block> An den neuen Standort am<block ref="cb2eb5386826561ac7895f1c878e4252" prefix=" " category="inline-code"></block>.</block>
  <block id="b51dc519efad4b151a05682b68f4b182" category="section-title">Schalter für Archivprotokoll erzwingen</block>
  <block id="10793485747151dfa2eaddb0854c84b2" category="paragraph">Sie müssen einen Schalter für das Archivprotokoll erzwingen, um sicherzustellen, dass die Archivprotokolle alle Daten enthalten, die erforderlich sind, um die Kopie vollständig konsistent zu machen. Ohne diesen Befehl können Schlüsseldaten in den Wiederherstellungsprotokollen weiterhin vorhanden sein.</block>
  <block id="c55a88a6b8fdc5972d4e2a4924f93dbf" category="section-title">Quelldatenbank herunterfahren</block>
  <block id="9ec82bff06c706f42a8170befdd6cf39" category="paragraph">Die Unterbrechung beginnt in diesem Schritt, weil die Datenbank heruntergefahren und in einen schreibgeschützten Modus mit eingeschränktem Zugriff versetzt wird. Um die Quelldatenbank herunterzufahren, führen Sie die folgenden Befehle aus:</block>
  <block id="6ecf02e1343e82cf0cd95d7303346ff1" category="section-title">Backup von Controlfile</block>
  <block id="e7bc82f6d1381df55ace121dc02368e6" category="paragraph">Sie müssen die controlfile sichern, falls Sie die Migration abbrechen und zum ursprünglichen Speicherort zurückkehren müssen. Eine Kopie der Backup-Steuerdatei ist nicht 100% erforderlich, aber es macht den Prozess des Rücksetzens der Datenbank-Speicherorte zurück an den ursprünglichen Speicherort einfacher.</block>
  <block id="4e7a51f2c1ee60c69cd719d415f5e87a" category="paragraph">Der aktuelle spfile enthält Verweise auf die Steuerdateien an ihren aktuellen Speicherorten innerhalb der alten ASM-Datenträgergruppe. Es muss bearbeitet werden, was leicht durch das Bearbeiten einer Zwischenversion von pfile erfolgt.</block>
  <block id="253e30479f86461bc79f2fca58ee4cb1" category="section-title">Aktualisieren Sie pfile</block>
  <block id="35db8a54b9c7581f0f77b059233cc61b" category="paragraph">Aktualisieren Sie alle Parameter, die sich auf alte ASM-Datenträgergruppen beziehen, um die neuen Namen der ASM-Datenträgergruppen wiederzugeben. Speichern Sie dann die aktualisierte Datei pfile. Stellen Sie sicher, dass die<block ref="5fa1d01559c04dcdb3ba05775712b1ab" prefix=" " category="inline-code"></block> Parameter sind vorhanden.</block>
  <block id="bb607fbd46f48c3a4080ecda83288b69" category="paragraph">Im folgenden Beispiel werden die Verweise auf angezeigt<block ref="c097579d8cb34b312d396304e0b8f487" prefix=" " category="inline-code"></block> Die in geändert wurden<block ref="cb2eb5386826561ac7895f1c878e4252" prefix=" " category="inline-code"></block> Sind gelb markiert. Zwei wichtige Parameter sind die<block ref="5fa1d01559c04dcdb3ba05775712b1ab" prefix=" " category="inline-code"></block> Parameter, die neue Dateien am richtigen Speicherort erstellen.</block>
  <block id="5275e85db4d606621d8f074e44b95748" category="section-title">Init.ora-Datei aktualisieren</block>
  <block id="17a2b5d1af48f7f8d1511d5372896f7a" category="paragraph">Die meisten ASM-basierten Datenbanken verwenden einen<block ref="b6bd7c570605affc5b50bbd3e436f0c0" prefix=" " category="inline-code"></block> Datei befindet sich im<block ref="45c7302d9c2e8745b3a2bff0f992b6df" prefix=" " category="inline-code"></block> Verzeichnis, das einen Punkt auf das Spfile auf der ASM-Datenträgergruppe darstellt. Diese Datei muss an einen Speicherort auf der neuen ASM-Datenträgergruppe umgeleitet werden.</block>
  <block id="6ad274256421918b584abff3b9446b09" category="paragraph">Ändern Sie diese Datei wie folgt:</block>
  <block id="b6638505615764c377e2cf2585993fd0" category="section-title">Wiederherstellung der Parameterdatei</block>
  <block id="52b21c2e6103a3b8d888a4185e21a787" category="paragraph">Der spfile kann nun mit den Daten in der bearbeiteten pfile gefüllt werden.</block>
  <block id="37f4738eddd35e77d15af55a40e1ab1c" category="section-title">Starten Sie die Datenbank, um neue spfile zu verwenden</block>
  <block id="79f8a930424fe3b21dc30a72791171d4" category="paragraph">Starten Sie die Datenbank, um sicherzustellen, dass sie jetzt den neu erstellten spfile verwendet und dass alle weiteren Änderungen an den Systemparametern korrekt aufgezeichnet werden.</block>
  <block id="231ef0ea12d142611eeea2fdfad67114" category="section-title">Kontrolldatei wiederherstellen</block>
  <block id="33cdbbeabc7f2c647f20b3f8b924307f" category="paragraph">Die von RMAN erstellte Backup-Controldatei kann auch direkt an dem im neuen spfile angegebenen Speicherort wiederhergestellt werden.</block>
  <block id="ce656b7f351ebdc7c599aff6a99e0f2a" category="paragraph">Mounten Sie die Datenbank und überprüfen Sie die Verwendung der neuen Steuerdatei.</block>
  <block id="fda1e79fe400519496075b7e2871092a" category="section-title">Protokollwiedergabe</block>
  <block id="daaec4441f16b5bac11476d4582a296c" category="paragraph">Die Datenbank verwendet derzeit die Datendateien am alten Speicherort. Bevor die Kopie verwendet werden kann, müssen sie synchronisiert werden. Die Zeit während des ersten Kopiervorgangs ist verstrichen, und die Änderungen wurden hauptsächlich in den Archivprotokollen protokolliert. Diese Änderungen werden wie folgt repliziert:</block>
  <block id="fca6726270a94f51cab3142509d32919" category="list-text">Führen Sie ein inkrementelles RMAN-Backup durch, das die Archivprotokolle enthält.</block>
  <block id="8b8a5667b011b5f37163ff4da1004343" category="list-text">Wiederholen Sie das Protokoll.</block>
  <block id="a9a62e70841c4d06dd16306a85700d36" category="section-title">Aktivierung</block>
  <block id="8fecb0145484a35b1d9d5926c945ca30" category="paragraph">Die wiederhergestellte Steuerdatei verweist weiterhin auf die Datendateien am ursprünglichen Speicherort und enthält auch die Pfadinformationen für die kopierten Datendateien.</block>
  <block id="f590db5fdc10f6942f7cd173c0bb578b" category="list-text">Um die aktiven Datendateien zu ändern, führen Sie den aus<block ref="7705ebb59373d0b1dfc71d86ef726f93" prefix=" " category="inline-code"></block> Befehl.</block>
  <block id="2c5c2b5174eafe321358329636e80f5e" category="paragraph">Die aktiven Datendateien sind nun die kopierten Datendateien, aber es können immer noch Änderungen in den letzten Redo-Protokollen enthalten sein.</block>
  <block id="fe83c5826bec1aa20e26802fa94a4a0f" category="list-text">Um alle verbleibenden Protokolle wiederzugeben, führen Sie den aus<block ref="81168dbef15af03d02c51a7447378e76" prefix=" " category="inline-code"></block> Befehl. Wenn die Meldung angezeigt wird<block ref="cef3519da39a73834e89a18ca91951f1" prefix=" " category="inline-code"></block> Wird angezeigt, der Prozess war erfolgreich.</block>
  <block id="15434ad90ad32205a804fe96f265b91d" category="paragraph">Bei diesem Vorgang wurde nur der Speicherort der normalen Datendateien geändert. Die temporären Datendateien müssen umbenannt werden, müssen aber nicht kopiert werden, da sie nur temporär sind. Die Datenbank ist derzeit nicht verfügbar, sodass es keine aktiven Daten in den temporären Datendateien gibt.</block>
  <block id="2a7c5560f8e4ad220c853666c46eb051" category="list-text">Um die temporären Datendateien zu verschieben, geben Sie zuerst ihren Speicherort an.</block>
  <block id="10b2632cf9c3bf6568b36f6a37e627c2" category="list-text">Verschieben Sie temporäre Datendateien mithilfe eines RMAN-Befehls, der den neuen Namen für jede Datendatei festlegt. Bei Oracle Managed Files (OMF) ist der vollständige Name nicht erforderlich; die ASM-Datenträgergruppe reicht aus. Wenn die Datenbank geöffnet wird, verknüpft OMF mit dem entsprechenden Speicherort in der ASM-Datenträgergruppe. Um Dateien zu verschieben, führen Sie die folgenden Befehle aus:</block>
  <block id="2c592b1e2841899b280deafe27eeefbf" category="section-title">Migration des Wiederherstellungsprotokolls</block>
  <block id="3f53598059c3bf11ff505ad9dc0d6776" category="paragraph">Der Migrationsprozess ist fast abgeschlossen, aber die Wiederherstellungsprotokolle befinden sich immer noch in der ursprünglichen ASM-Laufwerksgruppe. Wiederherstellungsprotokolle können nicht direkt verschoben werden. Stattdessen wird ein neuer Satz von Wiederherstellungsprotokollen erstellt und der Konfiguration hinzugefügt, gefolgt von einem Drop der alten Protokolle.</block>
  <block id="d54f1c98a924385411a8488657b21dad" category="list-text">Erstellen Sie für jedes Redo-Protokoll eine neue Gruppe mit einer passenden Konfiguration. Wenn Sie OMF nicht verwenden, müssen Sie den vollständigen Pfad angeben. Dies ist auch ein Beispiel, das den verwendet<block ref="c9655c773db3ebd5871fbe76c0e38a52" prefix=" " category="inline-code"></block> Parameter. Wie bereits gezeigt, wurde dieser Parameter auf +NEWLOGS gesetzt. Mit dieser Konfiguration können Sie die folgenden Befehle verwenden, um neue Online-Protokolle zu erstellen, ohne einen Dateispeicherort oder sogar eine bestimmte ASM-Datenträgergruppe angeben zu müssen.</block>
  <block id="3e0c7c8ee841e9919343fe45e3e04f2d" category="list-text">Öffnen Sie die Datenbank.</block>
  <block id="7e4b38db083a404d0d628bdb1ad83311" category="list-text">Die alten Protokolle ablegen.</block>
  <block id="37e48dd687a01e58ed329feebebf3198" category="list-text">Wenn ein Fehler auftritt, der verhindert, dass Sie ein aktives Protokoll ablegen, erzwingen Sie einen Wechsel zum nächsten Protokoll, um die Sperre freizugeben und einen globalen Kontrollpunkt zu erzwingen. Ein Beispiel ist unten dargestellt. Der Versuch, die Logfile-Gruppe 3, die sich am alten Speicherort befand, zu löschen, wurde abgelehnt, da noch aktive Daten in dieser Logdatei vorhanden waren. Eine Protokollarchivierung nach einem Kontrollpunkt ermöglicht das Löschen der Protokolldatei.</block>
  <block id="79d08abfbe5fd73dc85c55cd20ad4974" category="list-text">Überprüfen Sie die Umgebung, um sicherzustellen, dass alle standortbasierten Parameter aktualisiert werden.</block>
  <block id="799f5ce04f1d9cad4455676b2fd908d9" category="list-text">Im folgenden Skript wird erläutert, wie dieser Prozess vereinfacht werden kann:</block>
  <block id="9867dfc0eee2725f24f86d8114c356a0" category="list-text">Wenn die ASM-Datenträgergruppen vollständig evakuiert wurden, können sie jetzt mit abgehängt werden<block ref="ae709dc3414219bf2c9c8abc9ff6c67a" prefix=" " category="inline-code"></block>. In vielen Fällen sind jedoch die Dateien, die zu anderen Datenbanken oder der ASM-Datei spfile/passwd gehören, noch vorhanden.</block>
  <block id="49f8dc5754bd205ea0da17521cb89a1f" category="section-title">Kopie von Oracle ASM auf das Dateisystem</block>
  <block id="0f5b1767a58e43cb548226e94441dc07" category="paragraph">Das Verfahren zum Kopieren von Oracle ASM in ein Dateisystem ähnelt dem Verfahren zum Kopieren von ASM zu ASM mit ähnlichen Vorteilen und Einschränkungen. Der Hauptunterschied ist die Syntax der verschiedenen Befehle und Konfigurationsparameter bei der Verwendung eines sichtbaren Dateisystems im Gegensatz zu einer ASM-Datenträgergruppe.</block>
  <block id="b533404c568b600f9fceddeac9253964" category="paragraph">Oracle RMAN wird verwendet, um eine (vollständige) Kopie der Quelldatenbank zu erstellen, die sich derzeit in der ASM-Datenträgergruppe befindet<block ref="c097579d8cb34b312d396304e0b8f487" prefix=" " category="inline-code"></block> An den neuen Standort am<block ref="f006bf17b06c884c47190c4225d3215a" prefix=" " category="inline-code"></block>.</block>
  <block id="f78b0ab71e2863c28dbe331056451135" category="paragraph">Der Wechsel des Archivprotokolls muss erzwungen werden, um sicherzustellen, dass die Archivprotokolle alle erforderlichen Daten enthalten, damit die Kopie vollständig konsistent ist. Ohne diesen Befehl können Schlüsseldaten in den Wiederherstellungsprotokollen weiterhin vorhanden sein. Um einen Archivprotokollschalter zu erzwingen, führen Sie den folgenden Befehl aus:</block>
  <block id="c37c761fc28428b9ead4533aba9ee148" category="paragraph">Die Unterbrechung beginnt in diesem Schritt, weil die Datenbank heruntergefahren und in einen schreibgeschützten Modus mit eingeschränktem Zugriff versetzt wird. Um die Quelldatenbank herunterzufahren, führen Sie die folgenden Befehle aus:</block>
  <block id="3abd8870d8a333edcaec6a1082520c67" category="paragraph">Sichern Sie controlfiles, falls Sie die Migration abbrechen und zum ursprünglichen Speicherort zurückkehren müssen. Eine Kopie der Backup-Steuerdatei ist nicht 100% erforderlich, aber es macht den Prozess des Rücksetzens der Datenbank-Speicherorte zurück an den ursprünglichen Speicherort einfacher.</block>
  <block id="38825215f02e25ca63ff64387b01c1c2" category="paragraph">Alle Parameter, die sich auf alte ASM-Datenträgergruppen beziehen, sollten aktualisiert und in einigen Fällen gelöscht werden, wenn sie nicht mehr relevant sind. Aktualisieren Sie sie, um die neuen Dateisystempfade wiederzugeben, und speichern Sie die aktualisierte Datei pfile. Stellen Sie sicher, dass der vollständige Zielpfad aufgeführt ist. Um diese Parameter zu aktualisieren, führen Sie die folgenden Befehle aus:</block>
  <block id="9d0dce3e570fa0ec18269ca508809401" category="section-title">Deaktivieren Sie die ursprüngliche init.ora-Datei</block>
  <block id="03dda6526778417460fd0bbb7716b373" category="paragraph">Diese Datei befindet sich im<block ref="45c7302d9c2e8745b3a2bff0f992b6df" prefix=" " category="inline-code"></block> Verzeichnis und befindet sich in der Regel in einem pfile, das als Zeiger auf den spfile auf der ASM-Datenträgergruppe dient. Um sicherzustellen, dass der ursprüngliche Spfile nicht mehr verwendet wird, benennen Sie ihn um. Löschen Sie sie jedoch nicht, da diese Datei erforderlich ist, wenn die Migration abgebrochen werden muss.</block>
  <block id="6087f741090627369fd114fbc794abc4" category="paragraph">Dies ist der letzte Schritt bei der Verlagerung von Spfile. Der ursprüngliche spfile wird nicht mehr verwendet und die Datenbank wird derzeit mit der Zwischendatei gestartet (aber nicht gemountet). Der Inhalt dieser Datei kann wie folgt an den neuen Speicherort spfile geschrieben werden:</block>
  <block id="3f62847435205b7b2af1bd5aaf1d3277" category="paragraph">Sie müssen die Datenbank starten, um die Sperren der Zwischendatei freizugeben und die Datenbank nur mit der neuen Datei spfile zu starten. Das Starten der Datenbank beweist auch, dass der neue spfile-Speicherort korrekt ist und seine Daten gültig sind.</block>
  <block id="ea164abfd83078cd71a2f3ba5d237900" category="paragraph">Auf dem Pfad wurde eine Sicherungscontroldatei erstellt<block ref="7a1a1280501cff0194d8ce6e351f2a2c" prefix=" " category="inline-code"></block> Früher im Verfahren. Der neue spfile definiert die Speicherorte der controlfile als <block ref="b4d54c88ccade13a82dcaf8f7c07cac5" prefix="/" category="inline-code"></block> Und<block ref="cd300fb0386526438b368e7a6d1b8ace" prefix=" " category="inline-code"></block>. Diese Dateien sind jedoch noch nicht vorhanden.</block>
  <block id="f7ed72e578ca8d23dd45bfa2c0a61ed8" category="list-text">Mit diesem Befehl werden die controlfile-Daten auf den im spfile definierten Pfaden wiederhergestellt.</block>
  <block id="c947c89cb4c7a595782d0be73bbe8cfa" category="list-text">Geben Sie den Mount-Befehl ein, damit die Steuerdateien korrekt erkannt werden und gültige Daten enthalten.</block>
  <block id="da4ae71a31a1286600968f8a340fddb2" category="paragraph">Um den zu validieren<block ref="d98137bd230dac4372cf20a2e7839463" prefix=" " category="inline-code"></block> Parameter, führen Sie den folgenden Befehl aus:</block>
  <block id="c25a81770c591c46d6909aae84bf5814" category="paragraph">Die Datenbank verwendet derzeit die Datendateien am alten Speicherort. Bevor die Kopie verwendet werden kann, müssen die Datendateien synchronisiert werden. Die Zeit während des ersten Kopiervorgangs ist verstrichen, und Änderungen wurden hauptsächlich in den Archivprotokollen protokolliert. Diese Änderungen werden in den folgenden beiden Schritten repliziert.</block>
  <block id="95ea23a5654037c0ab2a5418478d9da2" category="list-text">Wiederholen Sie die Protokolle.</block>
  <block id="e726fdb8508ef59abc6d8ef533186382" category="list-text">Um die aktiven Datendateien zu ändern, führen Sie den aus<block ref="7705ebb59373d0b1dfc71d86ef726f93" prefix=" " category="inline-code"></block> Befehl:</block>
  <block id="7fdba4cfea0e0d298cda23e7f46e40c4" category="list-text">Obwohl die Datendateien vollständig konsistent sein sollten, ist ein letzter Schritt erforderlich, um die verbleibenden Änderungen, die in den Online-Wiederherstellungsprotokollen aufgezeichnet werden, wiederzugeben. Verwenden Sie die<block ref="81168dbef15af03d02c51a7447378e76" prefix=" " category="inline-code"></block> Befehl, um diese Änderungen erneut einzuspielen und die Kopie 100 % mit dem Original zu identisch zu machen. Die Kopie ist jedoch noch nicht geöffnet.</block>
  <block id="8eb94d53286760da56ca9c892a49cd8b" category="section-title">Temporäre Datendateien Verschieben</block>
  <block id="a1e7f15bee0eaa976661307aa884fd99" category="list-text">Ermitteln Sie den Speicherort der temporären Datendateien, die noch auf der ursprünglichen Laufwerksgruppe verwendet werden.</block>
  <block id="89b79c94ceba42d5bc646c1b4b1acd24" category="list-text">Um die Datendateien zu verschieben, führen Sie die folgenden Befehle aus. Wenn es viele Tempfiles gibt, verwenden Sie einen Texteditor, um den RMAN-Befehl zu erstellen, und schneiden Sie ihn dann aus und fügen Sie ihn ein.</block>
  <block id="81aa65cb154a5f125bbf3ede064d4b3f" category="paragraph">Der Migrationsprozess ist fast abgeschlossen, aber die Wiederherstellungsprotokolle befinden sich immer noch in der ursprünglichen ASM-Laufwerksgruppe. Wiederherstellungsprotokolle können nicht direkt verschoben werden. Stattdessen wird ein neuer Satz von Wiederherstellungsprotokollen erstellt und der Konfiguration hinzugefügt, gefolgt von einem Drop der alten Protokolle.</block>
  <block id="6e46687c6cc5deabe53fa42932683c4d" category="list-text">Erstellen Sie für jedes Wiederherstellungsprotokoll eine neue Gruppe, indem Sie die gleiche Größe wie die aktuelle Wiederherstellungsprotokollgruppe verwenden, die den neuen Speicherort des Dateisystems verwendet.</block>
  <block id="22fd48a866ce4d04edb57c15b2fad5da" category="list-text">Entfernen Sie die alten Logfile-Gruppen, die sich noch im vorherigen Speicher befinden.</block>
  <block id="94158400e59555bf69ecd550887bd9f6" category="list-text">Wenn ein Fehler auftritt, der das Löschen eines aktiven Protokolls blockiert, erzwingen Sie einen Switch zum nächsten Protokoll, um die Sperre freizugeben und einen globalen Kontrollpunkt zu erzwingen. Ein Beispiel ist unten dargestellt. Der Versuch, die Logfile-Gruppe 3, die sich am alten Speicherort befand, zu löschen, wurde abgelehnt, da noch aktive Daten in dieser Logdatei vorhanden waren. Eine Protokollarchivierung, gefolgt von einem Kontrollpunkt, ermöglicht das Löschen von Logdateien.</block>
  <block id="326770a786c5933416b6a167c854e7b8" category="list-text">Das folgende Skript zeigt, wie Sie diesen Prozess vereinfachen können.</block>
  <block id="ffdcdc8ee9653232a906773fa11358fd" category="list-text">Wenn die ASM-Datenträgergruppen vollständig evakuiert wurden, können sie jetzt mit abgehängt werden<block ref="ae709dc3414219bf2c9c8abc9ff6c67a" prefix=" " category="inline-code"></block>. In vielen Fällen können Dateien, die zu anderen Datenbanken oder der ASM-Datei spfile/passwd gehören, weiterhin vorhanden sein.</block>
  <block id="3d8b8f4734039817e35d5fb6993c8d08" category="section-title">Bereinigung der Datendatei</block>
  <block id="637607e2203ac522fc9dbc88deb6c36f" category="inline-link-macro">Bereinigung der ASM-Migration</block>
  <block id="18325b8b206963bc1ae6cca25406f942" category="paragraph">Der Migrationsprozess kann je nach Verwendung von Oracle RMAN zu Datendateien mit langer oder kryptischer Syntax führen. Im hier gezeigten Beispiel wurde das Backup mit dem Dateiformat von durchgeführt<block ref="1da91ad80bcfeb818066022a42cde773" prefix=" " category="inline-code"></block>.<block ref="432459869b7d0085e120ec9454032267" prefix=" " category="inline-code"></block> Gibt an, dass RMAN für jede Datendatei einen eindeutigen Standardnamen erstellen sollte. Das Ergebnis ist ähnlich wie im folgenden Text dargestellt. Die traditionellen Namen der Datendateien sind in die Namen eingebettet. Dies kann mithilfe des in dargestellten skriptgesteuerten Ansatzes bereinigt werden <block ref="bf0c2c579af181f7d1e4d0267c1385fe" category="inline-link-macro-rx"></block>.</block>
  <block id="5e8488a95f83d2789417a1f3298f2f31" category="section-title">Oracle ASM-Ausgleich</block>
  <block id="2768938325db3a71a6713959a0f309d9" category="paragraph">Wie bereits erläutert, kann eine Oracle ASM-Festplattengruppe mithilfe des Ausgleichs transparent auf ein neues Storage-System migriert werden. Zusammenfassend ist zu sagen, dass beim Ausbalancieren der vorhandenen LUN-Gruppe LUNs gleicher Größe hinzugefügt werden müssen, gefolgt von einem Drop-Vorgang der vorherigen LUN. Oracle ASM verlagert die zugrunde liegenden Daten automatisch in einem optimalen Layout auf neuen Speicher und gibt dann die alten LUNs nach Abschluss frei.</block>
  <block id="24a638b29b3c55f26c731aa4b9943a52" category="paragraph">Der Migrationsprozess nutzt effiziente sequenzielle I/O-Vorgänge und führt im Allgemeinen keine Performance-Unterbrechung durch. Bei Bedarf kann die Migrationsrate jedoch gedrosselt werden.</block>
  <block id="25d06b03dd002ad791b19500c58e9f72" category="section-title">Identifizieren Sie die zu migrierenden Daten</block>
  <block id="3709d4034873467e8e6ae138da54a443" category="section-title">Erstellen neuer LUNs</block>
  <block id="32176bd565539329eafb96db9447e517" category="paragraph">Erstellen Sie neue LUNs gleicher Größe und legen Sie die Mitgliedschaft für Benutzer und Gruppen nach Bedarf fest. Die LUNs sollten als angezeigt werden<block ref="bea4821516973214973a70aea8337c38" prefix=" " category="inline-code"></block> Festplatten.</block>
  <block id="6e59535a7b8dd239c67b8d015618f5e1" category="section-title">Neue LUNS hinzufügen</block>
  <block id="71f7e1ff1a6e075f8364d083f145b655" category="paragraph">Während die Add- und Drop-Vorgänge zusammen ausgeführt werden können, ist es in der Regel einfacher, neue LUNs in zwei Schritten hinzuzufügen. Fügen Sie zunächst die neuen LUNs der Festplattengruppe hinzu. Dieser Schritt führt dazu, dass die Hälfte der Extents von den aktuellen ASM-LUNs auf die neuen LUNs migriert wird.</block>
  <block id="7989dfdf722e8bee00001dac817e4833" category="paragraph">Die Ausgleichskraft gibt die Rate an, mit der Daten übertragen werden. Je höher die Zahl, desto höher ist die Parallelität der Datenübertragung. Die Migration erfolgt mit effizienten sequenziellen I/O-Vorgängen, die wahrscheinlich keine Performance-Probleme verursachen. Auf Wunsch kann die Ausgleichskraft einer laufenden Migration jedoch mit dem angepasst werden<block ref="e20ea7dfbbe351f6a5275d7adc71efbd" prefix=" " category="inline-code"></block> Befehl. Für typische Migrationen wird der Wert 5 verwendet.</block>
  <block id="2b69f08c262a142b3fb0868f4d31ab4f" category="section-title">Überwachen Sie den Betrieb</block>
  <block id="a5097a0967c777285f74d63bf0cc26a1" category="paragraph">Ein Ausgleichsoperation kann auf verschiedene Weise überwacht und verwaltet werden. Für dieses Beispiel haben wir den folgenden Befehl verwendet.</block>
  <block id="5d68829c11bdb5ae7a50b847bb126d44" category="paragraph">Nach Abschluss der Migration werden keine Vorgänge zur Ausbalancierung gemeldet.</block>
  <block id="957310262614cddc1992368d2d0b14b3" category="section-title">Alte LUNs ablegen</block>
  <block id="260c16583c8eb689202ed5b4a44708b1" category="paragraph">Die Migration ist nun zur Hälfte abgeschlossen. Einige grundlegende Performance-Tests stellen sicher, dass die Umgebung sich in einem ordnungsgemäßen Zustand befindet. Nach Bestätigung können die verbleibenden Daten durch Löschen der alten LUNs verschoben werden. Beachten Sie, dass dies nicht zur sofortigen Freigabe der LUNs führt. Der Drop-Vorgang signalisiert Oracle ASM, die Extents zuerst zu verschieben und dann die LUN freizugeben.</block>
  <block id="90423249814011b02ba06afd9fb2d617" category="paragraph">Der Ausgleichsoperation kann auf verschiedene Weise überwacht und verwaltet werden. Für dieses Beispiel haben wir den folgenden Befehl verwendet:</block>
  <block id="1544b3a52519b3856f1f7d1e704cb56f" category="section-title">Entfernen Sie alte LUNs</block>
  <block id="983d704730b30910fc03d3ed444278f0" category="paragraph">Bevor Sie die alten LUNs aus der Laufwerksgruppe entfernen, sollten Sie den Header-Status einer letzten Prüfung entnehmen. Nachdem eine LUN aus ASM freigegeben wurde, wird kein Name mehr aufgeführt, und der Kopfzeilenstatus wird als aufgeführt<block ref="e2ffc99cf675f5f0f1a3456438551a9a" prefix=" " category="inline-code"></block>. Dies bedeutet, dass diese LUNs sicher aus dem System entfernt werden können.</block>
  <block id="22a67c375eff91d37f2363ea69b0c41f" category="section-title">LVM-Migration</block>
  <block id="d52f83c78118c243801797ba9b795b72" category="paragraph">Das hier vorgestellte Verfahren zeigt die Prinzipien einer LVM-basierten Migration einer Volume-Gruppe namens<block ref="ec2db4422261eae02091227fb9e53c88" prefix=" " category="inline-code"></block>. Die Beispiele stammen aus Linux LVM, die Prinzipien gelten jedoch gleichermaßen für AIX, HP-UX und VxVM. Die genauen Befehle können variieren.</block>
  <block id="64a375907ede7a86c9e8a1b4b142f22e" category="list-text">Identifizieren Sie die LUNs, die sich derzeit im befinden<block ref="ec2db4422261eae02091227fb9e53c88" prefix=" " category="inline-code"></block> Volume-Gruppe.</block>
  <block id="0ae75856dd12545cdd98b8cad5c12ad8" category="list-text">Erstellen Sie neue LUNs mit derselben oder einer etwas größeren physischen Größe und definieren Sie sie als physische Volumes.</block>
  <block id="2426fa707e9d362c711977b79acd65bb" category="list-text">Fügen Sie die neuen Volumes zur Volume-Gruppe hinzu.</block>
  <block id="02443ebd7f86c3a688a1e3466d3f106a" category="list-text">Stellen Sie das aus<block ref="8167c9a6bd495f7ed235364201039099" prefix=" " category="inline-code"></block> Befehl, um die Extents jeder aktuellen LUN in die neue LUN zu verschieben. Der<block ref="1b464374742cfda0167b82bc6f9462f1" prefix=" " category="inline-code"></block> Argument überwacht den Fortschritt des Vorgangs.</block>
  <block id="ce4a174d56245f6ba5b8808527919921" category="list-text">Wenn dieser Vorgang abgeschlossen ist, löschen Sie die alten LUNs aus der Volume-Gruppe mithilfe von<block ref="3a4d1e7b0af1cf3d80e419c09d238535" prefix=" " category="inline-code"></block> Befehl. Wenn die LUN erfolgreich war, kann sie jetzt sicher aus dem System entfernt werden.</block>
  <block id="599a80136f745b38f35077e1c7eb020e" category="summary">Oracle-Migrationsverfahren</block>
  <block id="329ffa7a9038afa62748f87628b749d5" category="paragraph">Für die Oracle-Migrationsdatenbank sind zahlreiche Verfahren verfügbar. Das richtige hängt von Ihren geschäftlichen Anforderungen ab.</block>
  <block id="48a2d08de8943e7ca4718c02e6b791b6" category="paragraph">In vielen Fällen haben Systemadministratoren und DBAs ihre eigenen bevorzugten Methoden, um physische Volume-Daten zu verschieben, zu spiegeln und zu demirrieren oder Oracle RMAN zum Kopieren von Daten zu nutzen.</block>
  <block id="79d01f6e7efcfef7db530c0b3c5ea516" category="paragraph">Diese Verfahren dienen in erster Linie als Orientierungshilfe für IT-Mitarbeiter, die mit einigen der verfügbaren Optionen nicht vertraut sind. Des Weiteren werden die Aufgaben, der zeitliche Bedarf und der Qualifikationsbedarf für jeden Migrationsansatz dargestellt. Dadurch können auch andere Parteien wie NetApp und Partner Professional Services oder IT-Management die Anforderungen an die einzelnen Verfahren voll einschätzen.</block>
  <block id="5604c71a7d9ae37df511cc85052c1894" category="paragraph">Es gibt keine einzigen Best Practices für die Erstellung einer Migrationsstrategie. Um einen Plan zu erstellen, müssen zunächst die Verfügbarkeitsoptionen verstanden und anschließend die Methode ausgewählt werden, die den Anforderungen des Unternehmens am besten entspricht. Die folgende Abbildung zeigt die grundlegenden Überlegungen und typischen Schlussfolgerungen von Kunden, ist aber nicht universell auf alle Situationen anwendbar.</block>
  <block id="fa2e72668f7d48694bef3ba5474e3967" category="paragraph">Ein Schritt wirft beispielsweise das Problem der Gesamtgröße der Datenbank auf. Der nächste Schritt hängt davon ab, ob die Datenbank mehr oder weniger als 1 TB umfasst. Die empfohlenen Schritte sind genau das – Empfehlungen auf der Basis typischer Kundenpraktiken. Die meisten Kunden würden nicht mit DataGuard eine kleine Datenbank kopieren, aber einige könnten. Die meisten Kunden würden aufgrund der erforderlichen Zeit nicht versuchen, eine 50 TB große Datenbank zu kopieren, aber einige haben möglicherweise ein ausreichend großes Wartungsfenster, um einen solchen Vorgang zu ermöglichen.</block>
  <block id="e38af1623479eee6cfe1782df43819d3" category="paragraph">Bei Oracle 12cR1 und höher kann eine Datendatei verschoben werden, während die Datenbank online bleibt. Es funktioniert außerdem zwischen verschiedenen Dateisystemtypen. Eine Datendatei kann beispielsweise von einem xfs-Dateisystem in ASM verschoben werden. Diese Methode wird im Allgemeinen nicht in der Größenordnung verwendet, da die Anzahl der erforderlichen individuellen Datendateiverschiebungsvorgänge erforderlich wäre. Es ist jedoch eine Option, die es sich lohnt, bei kleineren Datenbanken mit weniger Datendateien in Betracht zu ziehen.</block>
  <block id="5972a038819f045814401b2baea9ee09" category="paragraph">Darüber hinaus ist das einfache Verschieben einer Datendatei eine gute Option für die Migration von Teilen vorhandener Datenbanken. Beispielsweise können weniger aktive Datendateien auf kostengünstigeren Storage verschoben werden, beispielsweise auf ein FabricPool Volume, mit dem ungenutzte Blöcke im Objektspeicher gespeichert werden können.</block>
  <block id="fcdd0d9eae6df775f1bdc52f950a0160" category="section-title">Migration auf Datenbankebene</block>
  <block id="2b619622b2b8505c335acc8be3a2c3fd" category="paragraph">Die Migration auf Datenbankebene bedeutet, dass die Datenbank Daten verschieben kann. Konkret bedeutet dies Protokollversand. Technologien wie RMAN und ASM sind Oracle Produkte. Im Rahmen der Migration arbeiten sie jedoch auf Hostebene, wo sie Dateien kopieren und Volumes managen.</block>
  <block id="1048000638cc5357b8b2b36ab55cbc5a" category="section-title">Protokollversand</block>
  <block id="36f7294e33f88b462199b92362434684" category="paragraph">Die Grundlage für die Migration auf Datenbankebene ist das Oracle Archivprotokoll, das ein Protokoll der Änderungen an der Datenbank enthält. Meistens ist ein Archivprotokoll Bestandteil einer Backup- und Recovery-Strategie. Der Recovery-Prozess beginnt mit der Wiederherstellung einer Datenbank und dann mit der Wiedergabe eines oder mehrerer Archivprotokolle, um die Datenbank in den gewünschten Zustand zu bringen. Mit derselben Basistechnologie kann eine Migration mit nur minimaler bis keiner Unterbrechung des Betriebs durchgeführt werden. Noch wichtiger ist, dass diese Technologie die Migration ermöglicht und gleichzeitig die ursprüngliche Datenbank unberührt lässt. Dabei wird ein Back-Out-Pfad beibehalten.</block>
  <block id="d43de5ded302797939b76670bc488b51" category="paragraph">Der Migrationsprozess beginnt mit der Wiederherstellung eines Datenbank-Backups auf einem sekundären Server. Dies kann auf unterschiedliche Weise erfolgen, doch die meisten Kunden verwenden ihre normale Backup-Applikation, um die Datendateien wiederherzustellen. Nachdem die Datendateien wiederhergestellt sind, legen Benutzer eine Methode für den Protokollversand fest. Das Ziel besteht darin, einen konstanten Feed von Archivprotokollen zu erstellen, die von der primären Datenbank generiert werden, und diese in der wiederhergestellten Datenbank wiederzugeben, um sie nahe am selben Status zu halten. Wenn die Umstellung ankommt, wird die Quelldatenbank vollständig heruntergefahren und die letzten Archivprotokolle sowie in einigen Fällen die Wiederherstellungsprotokolle kopiert und wiedergegeben. Es ist wichtig, dass die Wiederherstellungsprotokolle auch berücksichtigt werden, da sie einige der letzten abgeschlossenen Transaktionen enthalten können.</block>
  <block id="f6fc42763ddbd981d5803ee6676dc7c3" category="paragraph">Nachdem diese Protokolle übertragen und wiedergegeben wurden, sind beide Datenbanken konsistent. Jetzt führen die meisten Kunden einige grundlegende Tests durch. Wenn während des Migrationsprozesses Fehler auftreten, sollte die Protokollwiedergabe Fehler melden und fehlschlagen. Es ist weiterhin ratsam, einige schnelle Tests basierend auf bekannten Abfragen oder applikationsgestützten Aktivitäten durchzuführen, um zu überprüfen, ob die Konfiguration optimal ist. Es ist auch üblich, eine abschließende Testtabelle zu erstellen, bevor die ursprüngliche Datenbank heruntergefahren wird, um zu überprüfen, ob sie in der migrierten Datenbank vorhanden ist. Dieser Schritt stellt sicher, dass während der endgültigen Protokollsynchronisierung keine Fehler gemacht wurden.</block>
  <block id="d8e9d23b4beb4db3da15d77542782747" category="paragraph">Eine einfache Log-Shipping-Migration kann Out-of-Band hinsichtlich der ursprünglichen Datenbank konfiguriert werden, was dies besonders für geschäftskritische Datenbanken nützlich macht. Für die Quelldatenbank sind keine Konfigurationsänderungen erforderlich, und die Wiederherstellung und Erstkonfiguration der Migrationsumgebung haben keine Auswirkungen auf den Produktionsbetrieb. Nachdem der Protokollversand konfiguriert wurde, werden einige I/O-Anforderungen an die Produktionsserver gestellt. Der Protokollversand besteht jedoch aus einfachen sequenziellen Lesevorgängen in den Archivprotokollen, was sich wahrscheinlich nicht auf die Performance der Produktionsdatenbank auswirken wird.</block>
  <block id="2c31403002a9f424c6ef56460c6b3715" category="paragraph">Der Protokollversand hat sich besonders für große Entfernungen bei Migrationen mit hohen Änderungsraten bewährt. In einer Instanz wurde eine einzelne 220 TB Datenbank an einen etwa 500 Meilen entfernten neuen Standort migriert. Die Änderungsrate war extrem hoch und Sicherheitsbeschränkungen verhinderten die Nutzung einer Netzwerkverbindung. Der Protokollversand wurde durch Tape und Kurier durchgeführt. Eine Kopie der Quelldatenbank wurde zunächst mithilfe der unten beschriebenen Verfahren wiederhergestellt. Die Protokolle wurden dann wöchentlich per Kurier bis zum Zeitpunkt der Umstellung versendet, als die endgültigen Tapes zugestellt wurden und die Protokolle auf die Replikatdatenbank angewendet wurden.</block>
  <block id="b8efa655c4deb7af8a24fc241f8b53ff" category="section-title">Oracle DataGuard</block>
  <block id="d3165c4060fc6d19c0a6364e7c4580ee" category="paragraph">In einigen Fällen ist eine vollständige DataGuard Umgebung gerechtfertigt. Es ist falsch, den Begriff DataGuard zu verwenden, um auf eine Protokollversendungs- oder Standby-Datenbankkonfiguration zu verweisen. Oracle DataGuard ist ein umfassendes Framework für das Management der Datenbankreplikation, es handelt sich jedoch nicht um eine Replizierungstechnologie. Der Hauptvorteil einer kompletten DataGuard-Umgebung bei einer Migration ist das transparente Umschalten von einer Datenbank zur anderen. DataGuard ermöglicht außerdem ein transparentes Switchover zurück zur Originaldatenbank, falls ein Problem erkannt wird, beispielsweise ein Problem mit der Performance oder der Netzwerkkonnektivität in der neuen Umgebung. Eine vollständig konfigurierte DataGuard-Umgebung erfordert nicht nur die Konfiguration der Datenbankschicht, sondern auch der Applikationen, damit Applikationen eine Änderung am primären Datenbankstandort erkennen können. Im Allgemeinen ist es nicht notwendig, eine Migration mit DataGuard durchzuführen, aber einige Kunden haben intern umfangreiche DataGuard-Kenntnisse und verlassen sich bei Migrationsaufgaben bereits auf diese.</block>
  <block id="f17c0a1ce359c1a6665412f89234bf41" category="section-title">Neuarchitektur</block>
  <block id="ece8d784e248d63091fc0d248cdeebc3" category="paragraph">Wie bereits erläutert, erfordert die Nutzung der erweiterten Funktionen von Storage Arrays manchmal eine Änderung des Datenbank-Layouts. Darüber hinaus verändert eine Änderung des Storage-Protokolls, wie etwa das Wechsel von ASM zu einem NFS Filesystem, zwangsläufig das Filesystem-Layout.</block>
  <block id="71843fb61639ed5fe216bfebc85af16f" category="paragraph">Einer der Hauptvorteile von Protokollversandmethoden, einschließlich DataGuard, besteht darin, dass das Replizierungsziel nicht mit der Quelle übereinstimmen muss. Bei der Migration von ASM zu einem normalen Dateisystem oder umgekehrt gibt es keine Probleme mit der Verwendung eines Protokollversandansatzes. Das genaue Layout der Datendateien kann am Ziel geändert werden, um die Verwendung der Pluggable Database (PDB)-Technologie zu optimieren oder QoS-Kontrollen für bestimmte Dateien selektiv festzulegen. Mit anderen Worten: Ein Migrationsprozess auf der Basis des Protokollversand ermöglicht Ihnen eine einfache und sichere Optimierung des Datenbank-Storage-Layouts.</block>
  <block id="f00ec1b23f539163f4c748a85e49e2dd" category="section-title">Server-Ressourcen</block>
  <block id="76c5d43a72d856a95b0eed94694ae696" category="paragraph">Eine Einschränkung für die Migration auf Datenbankebene besteht in der Notwendigkeit eines zweiten Servers. Dieser zweite Server kann auf zwei Arten verwendet werden:</block>
  <block id="6cb133f8b5045ef30c17e96ab281749e" category="list-text">Sie können den zweiten Server als permanentes neues Zuhause für die Datenbank verwenden.</block>
  <block id="f5133929790fe62dbc76a73db4c88544" category="list-text">Sie können den zweiten Server als temporären Staging-Server verwenden. Nachdem die Datenmigration zum neuen Storage-Array abgeschlossen und getestet wurde, werden die LUN- oder NFS-Dateisysteme vom Staging-Server getrennt und mit dem ursprünglichen Server verbunden.</block>
  <block id="48a00ba9881b62c610201cd833ee9c88" category="paragraph">Die erste Option ist die einfachste, aber in sehr großen Umgebungen, die sehr leistungsstarke Server erfordern, ist die Verwendung möglicherweise nicht möglich. Die zweite Option erfordert zusätzliche Arbeit, um die Dateisysteme wieder an den ursprünglichen Speicherort zu verschieben. Es kann sich um eine einfache Operation handelt, bei der NFS als Storage-Protokoll verwendet wird, da die File-Systeme vom Staging-Server abgehängt und dann wieder auf dem ursprünglichen Server gemountet werden können.</block>
  <block id="c223252c2d577984b75caddbd1dff9dc" category="paragraph">Blockbasierte Dateisysteme erfordern eine zusätzliche Arbeitsleistung für die Aktualisierung von FC-Zoning oder iSCSI-Initiatoren. Bei den meisten logischen Volume-Managern (einschließlich ASM) werden die LUNs automatisch erkannt und online geschaltet, nachdem sie auf dem ursprünglichen Server verfügbar gemacht wurden. Einige Dateisystem- und LVM-Implementierungen erfordern jedoch möglicherweise mehr Arbeit für den Export und Import der Daten. Die genaue Vorgehensweise kann variieren, es ist jedoch im Allgemeinen einfach, ein einfaches, wiederholbares Verfahren einzurichten, um die Migration abzuschließen und die Daten auf dem ursprünglichen Server wiederherzustellen.</block>
  <block id="2d22ab8be3661ce5dfd8c9910d1cc003" category="paragraph">Es ist zwar möglich, einen Protokollversand einzurichten und eine Datenbank in einer einzigen Server-Umgebung zu replizieren, aber die neue Instanz muss eine andere Prozess-SID haben, um die Protokolle wiederzugeben. Es ist möglich, die Datenbank vorübergehend unter einem anderen Satz von Prozess-IDs mit einer anderen SID zu erstellen und später zu ändern. Dies kann jedoch zu vielen komplizierten Management-Aktivitäten und einem Risiko von Benutzerfehlern führen.</block>
  <block id="2660e825d3a58f68aeec49e1c749f477" category="section-title">Migration auf Host-Ebene</block>
  <block id="fae5bb68519bf78c1b3711566283f4ef" category="paragraph">Bei der Migration von Daten auf Hostebene müssen das Host-Betriebssystem und die zugehörigen Dienstprogramme zum Abschluss der Migration verwendet werden. Dieser Prozess umfasst alle Utilitys zum Kopieren von Daten, darunter Oracle RMAN und Oracle ASM.</block>
  <block id="01070e0746a412afd171ccf162d3a170" category="section-title">Kopieren von Daten</block>
  <block id="180e717e34d5e4bd2991c47960d00f51" category="paragraph">Der Wert einer einfachen Kopieroperation sollte nicht unterschätzt werden. Moderne Netzwerkinfrastrukturen können Daten in Gigabytes pro Sekunde verschieben und Dateikopievorgänge basieren auf effizienten sequenziellen Lese- und Schreib-I/O. Im Vergleich zum Protokollversand lassen sich mehr Unterbrechungen durch Host-Kopien vermeiden, doch bei einer Migration handelt es sich nicht nur um die Datenverschiebung. Sie umfasst im Allgemeinen Änderungen am Netzwerk, den Neustartzeit der Datenbank und Tests nach der Migration.</block>
  <block id="750e04d167938f35054d1c30fd06af58" category="paragraph">Die tatsächlich zum Kopieren der Daten benötigte Zeit ist möglicherweise nicht signifikant. Darüber hinaus behält ein Kopiervorgang einen garantierten Back-out-Pfad bei, da die Originaldaten unverändert bleiben. Sollten während des Migrationsprozesses Probleme auftreten, können die ursprünglichen Dateisysteme mit den Originaldaten wieder aktiviert werden.</block>
  <block id="5a3572c65171e06d99a4e055f9d35d32" category="section-title">Ändern Der Plattform</block>
  <block id="d123cc133e4de7ab3a4b1a5af3bf37a7" category="paragraph">Replatforming bezieht sich auf eine Änderung des CPU-Typs. Wenn eine Datenbank von einer herkömmlichen Solaris-, AIX- oder HP-UX-Plattform zu x86 Linux migriert wird, müssen die Daten aufgrund von Änderungen in der CPU-Architektur neu formatiert werden. SPARC, IA64 und POWER CPUs werden als Big-Endian-Prozessoren bezeichnet, während die x86- und x86_64-Architekturen als Little-Endian bezeichnet werden. Daher werden einige Daten in Oracle-Datendateien je nach verwendetem Prozessor unterschiedlich sortiert.</block>
  <block id="6a03806f9c4b1f0048478bab7863fd2c" category="paragraph">In der Vergangenheit haben Kunden Daten mithilfe von DataPump plattformübergreifend repliziert. DataPump ist ein Dienstprogramm, das einen speziellen Typ des logischen Datenexports erzeugt, der schneller in die Zieldatenbank importiert werden kann. Da es eine logische Kopie der Daten erstellt, lässt DataPump die Abhängigkeiten der Prozessorabhängigkeit hinter sich. DataPump wird von einigen Kunden weiterhin für das Replatforming verwendet, aber mit Oracle 11g ist eine schnellere Option verfügbar: Plattformübergreifende transportable Tablespaces. Mit diesem Vorschub kann ein Tablespace in ein anderes endian-Format konvertiert werden. Dies ist eine physische Transformation, die eine bessere Leistung bietet als ein DataPump-Export, der physische Bytes in logische Daten konvertieren und dann zurück in physische Bytes konvertieren muss.</block>
  <block id="713bf8ff4ea4c4ece861f3345e15e3ea" category="paragraph">Eine vollständige Diskussion über DataPump und transportable Tablespaces geht über den Umfang der NetApp-Dokumentation hinaus. NetApp hat jedoch einige Empfehlungen, die auf unseren Erfahrungen basieren, die Kunden bei der Migration zu einem neuen Storage Array-Protokoll mit einer neuen CPU-Architektur unterstützt haben:</block>
  <block id="86b0ebca28354c7f55b4cc2fe79bee0e" category="list-text">Wenn DataPump verwendet wird, sollte die für den Abschluss der Migration erforderliche Zeit in einer Testumgebung gemessen werden. Kunden sind manchmal überrascht, wie lange sie für die Durchführung der Migration benötigen. Diese unerwartete zusätzliche Ausfallzeit kann zu Unterbrechungen führen.</block>
  <block id="8a592f25a72fd6f3718b1b01869ade30" category="list-text">Viele Kunden glauben irrtümlicherweise, dass plattformübergreifende transportable Tablespaces keine Datenkonvertierung erfordern. Wenn eine CPU mit einem anderen Endian verwendet wird, wird ein RMAN verwendet<block ref="31168275dcaac634489082b54c4c66d0" prefix=" " category="inline-code"></block> Der Betrieb muss zuvor an den Datendateien durchgeführt werden. Dies ist kein sofortiger Vorgang. In einigen Fällen kann der Konvertierungsprozess beschleunigt werden, indem mehrere Threads auf verschiedenen Dateien arbeiten, aber der Konvertierungsprozess kann nicht vermieden werden.</block>
  <block id="db7b3f86f4c9a2a6a76adac81614c03b" category="section-title">Migration über Manager eines logischen Volumes</block>
  <block id="86586aa50b0df80a7c68bcd3e86c8606" category="paragraph">LVMs nehmen eine Gruppe von einer oder mehreren LUNs und zerteilen sie in kleine Einheiten, die im Allgemeinen als Extents bezeichnet werden. Der Pool mit Erweiterungen wird dann als Quelle verwendet, um logische Volumes zu erstellen, die im Wesentlichen virtualisiert sind. Diese Virtualisierungsebene bietet auf verschiedene Weise einen Mehrwert:</block>
  <block id="68d083c44669b8b20f0421b0389c6ded" category="list-text">Logische Volumes können Extents verwenden, die von mehreren LUNs stammen. Wenn ein Filesystem auf einem logischen Volume erstellt wird, können alle Performance-Funktionen aller LUNs genutzt werden. Zudem wird die gleichmäßige Auslastung aller LUNs in der Volume-Gruppe gefördert, wodurch eine besser planbare Performance erzielt wird.</block>
  <block id="10d1e748082641ce33e82f457cabcbe7" category="list-text">Die Größe logischer Volumes kann durch Hinzufügen und in einigen Fällen durch Entfernen von Extents geändert werden. Die Größe eines Filesystems auf einem logischen Volume ist im Allgemeinen unterbrechungsfrei.</block>
  <block id="5efed6e7bf2fda8321882aff35768721" category="list-text">Logische Volumes können unterbrechungsfrei migriert werden, indem die zugrunde liegenden Extents verschoben werden.</block>
  <block id="96371ba57b3d2f331cc9bd7b5e34708b" category="paragraph">Migration mit einer LVM funktioniert auf zwei Arten: Ein Extent verschieben oder ein Extent spiegeln/demirrieren. Bei der LVM-Migration werden effiziente sequenzielle I/O große Blöcke eingesetzt, und es entstehen nur selten Performance-Probleme. Wenn dies zu einem Problem wird, gibt es in der Regel Optionen zur Drosselung der I/O-Rate. Dadurch erhöht sich die für den Abschluss der Migration erforderliche Zeit und gleichzeitig verringert sich die I/O-Last für Host- und Speichersysteme.</block>
  <block id="28084a93e6570f55c3923991e269816e" category="section-title">Spiegel und Demirror</block>
  <block id="f5b05967100f74bb14ae26d5021ef4e3" category="paragraph">Einige Volume-Manager, wie AIX LVM, erlauben dem Benutzer, die Anzahl der Kopien für jedes Extent festzulegen und zu steuern, welche Geräte die einzelnen Kopien hosten. Zur Migration wird ein vorhandenes logisches Volume erstellt, die zugrunde liegenden Extents zu den neuen Volumes gespiegelt, auf eine Synchronisierung der Kopien gewartet und anschließend die alte Kopie verworfen. Wenn ein Back- Out-Pfad gewünscht wird, kann vor dem Zeitpunkt, an dem die Spiegelungskopie abgelegt wird, ein Snapshot der Originaldaten erstellt werden. Alternativ kann der Server kurz heruntergefahren werden, um die ursprünglichen LUNs zu maskieren, bevor die enthaltenen Spiegelkopien erzwungen gelöscht werden. Dabei wird eine wiederherstellbare Kopie der Daten am ursprünglichen Speicherort aufbewahrt.</block>
  <block id="7151373ff69a9fafa4a579b40282beeb" category="section-title">Extent-Migration</block>
  <block id="682ba6adb168511d03877bdbf940a0b7" category="paragraph">Fast alle Volume-Manager erlauben die Migration von Extents, und manchmal gibt es mehrere Optionen. Beispielsweise ermöglichen einige Volume Manager einem Administrator, die einzelnen Extents für ein bestimmtes logisches Volume von altem zu neuem Storage zu verschieben. Volume-Manager wie Linux LVM2 bieten die<block ref="8167c9a6bd495f7ed235364201039099" prefix=" " category="inline-code"></block> Befehl, der alle Extents auf dem angegebenen LUN-Gerät auf eine neue LUN verlagert. Nach der Evakuierung der alten LUN kann sie entfernt werden.</block>
  <block id="8c232bd157fb78062260d11e1ec3fc2c" category="admonition">Das primäre Risiko für den Betrieb ist das Entfernen alter, nicht genutzter LUNs aus der Konfiguration. Beim Ändern des FC-Zoning und beim Entfernen veralteter LUN-Geräte ist besonders darauf zu achten.</block>
  <block id="41175a8b2053bedbf868e340edf92f55" category="section-title">Oracle Automatic Storage Management</block>
  <block id="dce3f0c1b1c85aaee6448197054602d1" category="paragraph">Oracle ASM ist ein kombinierter logischer Volume-Manager und ein Dateisystem. Oracle ASM erstellt eine Sammlung von LUNs, unterteilt sie in kleine Zuweisungseinheiten und präsentiert sie als einzelnes Volume, das als ASM-Festplattengruppe bezeichnet wird. ASM bietet auch die Möglichkeit, die Laufwerksgruppe durch Festlegen des Redundanzniveaus zu spiegeln. Ein Volume kann nicht gespiegelt (externe Redundanz), gespiegelt (normale Redundanz) oder dreifach gespiegelt (hohe Redundanz) werden. Bei der Konfiguration der Redundanzstufe ist darauf zu achten, dass sie nach der Erstellung nicht mehr geändert werden kann.</block>
  <block id="d0ac5c14a6207835f733f4366345089d" category="paragraph">ASM bietet auch Dateisystemfunktionen. Obwohl das Dateisystem nicht direkt vom Host aus sichtbar ist, kann die Oracle-Datenbank Dateien und Verzeichnisse auf einer ASM-Datenträgergruppe erstellen, verschieben und löschen. Außerdem kann die Struktur mit dem Dienstprogramm asmcmd navigiert werden.</block>
  <block id="e72c9da30cbec4daedddc9b6e6fdf03b" category="paragraph">Wie bei anderen LVM-Implementierungen optimiert Oracle ASM die I/O-Performance durch Striping und Lastausgleich der I/O-Vorgänge jeder Datei über alle verfügbaren LUNs. Zweitens können die zugrunde liegenden Extents verschoben werden, um sowohl die Größenänderung der ASM-Datenträgergruppe als auch die Migration zu ermöglichen. Oracle ASM automatisiert den Prozess durch den Rebalancing-Vorgang. Neue LUNs werden einer ASM-Festplattengruppe hinzugefügt und alte LUNs werden verworfen. Dies führt zu einer Extent-Verschiebung und einem nachfolgenden Drop der evakuierten LUN aus der Festplattengruppe. Dieser Prozess ist eine der bewährtesten Migrationsmethoden, und die Zuverlässigkeit von ASM bei der Bereitstellung einer transparenten Migration ist möglicherweise das wichtigste Merkmal.</block>
  <block id="d4ad443888b51ec4e13912c45da68f76" category="admonition">Da die Spiegelungsebene von Oracle ASM fest festgelegt ist, kann sie nicht mit der Mirror- und Demirror-Methode der Migration verwendet werden.</block>
  <block id="048429277e643e20907317612c1f3318" category="section-title">Migration auf Storage-Ebene</block>
  <block id="623bb7ffbb3730716ea39b5086d26f18" category="paragraph">Bei der Migration auf Storage-Ebene wird die Migration sowohl unter der Applikations- als auch unter der Betriebssystemebene durchgeführt. In der Vergangenheit bedeutete dies manchmal, spezialisierte Geräte zu verwenden, auf denen LUNs auf Netzwerkebene kopiert werden konnten. Diese Funktionen finden sich jedoch jetzt nativ in ONTAP.</block>
  <block id="794cb725c5631ad99b5b7c000307f0df" category="section-title">SnapMirror</block>
  <block id="963ce762febea70d619f1fd5c114d85c" category="paragraph">Mit der Datenreplizierungssoftware NetApp SnapMirror erfolgt die Migration von Datenbanken zwischen NetApp Systemen nahezu universell. Der Prozess beinhaltet die Einrichtung einer Spiegelbeziehung für die zu migrierenden Volumes, um sie zu synchronisieren und dann auf das Umstellungsfenster zu warten. Wenn sie eintrifft, wird die Quelldatenbank heruntergefahren, eine letzte Aktualisierung der Spiegelung durchgeführt und die Spiegelung wird unterbrochen. Die Replikatvolumes können dann verwendet werden, indem entweder ein enthaltenes NFS-Dateisystem-Verzeichnis gemountet oder die enthaltenen LUNs ermittelt und die Datenbank gestartet wird.</block>
  <block id="1871452e5ea7e53152b5c874a12a890b" category="paragraph">Das Verschieben von Volumes innerhalb eines einzigen ONTAP Clusters gilt nicht als Migration, sondern als Routine<block ref="0fc2ecb8aa71bddc595fbc39f593496a" prefix=" " category="inline-code"></block> Betrieb. SnapMirror wird als Datenreplizierungs-Engine im Cluster eingesetzt. Dieser Prozess ist vollständig automatisiert. Es gibt keine weiteren Migrationsschritte, die durchgeführt werden müssen, wenn Attribute des Volume, wie z. B. LUN-Zuordnung oder NFS-Exportberechtigungen, mit dem Volume selbst verschoben werden. Die Standortverlagerung hat keine Unterbrechung des Host-Betriebs. In manchen Fällen muss der Netzwerkzugriff aktualisiert werden, um sicherzustellen, dass auf die neu verlagerten Daten so effizient wie möglich zugegriffen wird. Diese Aufgaben sind aber auch unterbrechungsfrei.</block>
  <block id="19fee3ea4b7f4472b5c1853122e8f47b" category="section-title">Import fremder LUNs (FLI)</block>
  <block id="a73cf458eed5aeff5c15aa5d70298cce" category="paragraph">FLI ist eine Funktion, mit der ein Data ONTAP-System mit 8.3 oder höher eine vorhandene LUN von einem anderen Storage-Array migrieren kann. Das Verfahren ist einfach: Das ONTAP-System ist auf das bestehende Speicher-Array abgegrenzt, als ob es sich um einen anderen SAN-Host handelt. Data ONTAP übernimmt dann die Kontrolle über die gewünschten Legacy-LUNs und migriert die zugrunde liegenden Daten. Außerdem kommen bei der Migration von Daten im Importprozess die Effizienzeinstellungen des neuen Volume zum Einsatz, sodass Daten während des Migrationsprozesses inline komprimiert und dedupliziert werden können.</block>
  <block id="0ee4b09881406835f151f103412a2f1e" category="paragraph">Die erste Implementierung von FLI in Data ONTAP 8.3 erlaubte nur Offline-Migration. Dies war ein extrem schneller Transfer, aber trotzdem bedeuteten die LUN-Daten, dass sie erst nach Abschluss der Migration verfügbar waren. Die Online-Migration wurde mit Data ONTAP 8.3 eingeführt. Diese Migration minimiert Unterbrechungen, da ONTAP während der Übertragung LUN-Daten bereitstellen kann. Während die Host-Zone neu aufgeteilt wird, um die LUNs über ONTAP zu verwenden, kommt es zu einer kurzen Unterbrechung. Sobald diese Änderungen jedoch vorgenommen werden, sind die Daten wieder verfügbar und bleiben während des gesamten Migrationsprozesses zugänglich.</block>
  <block id="19d95ee75467fc2f9a06d150cc99d944" category="paragraph">Lese-I/O wird über ONTAP als Proxy übertragen, bis der Kopiervorgang abgeschlossen ist, während Schreib-I/O synchron sowohl auf die fremde als auch auf die ONTAP-LUN geschrieben wird. Die beiden LUN-Kopien werden auf diese Weise synchron gehalten, bis der Administrator eine vollständige Umstellung ausführt, die die fremde LUN freigibt und Schreibvorgänge nicht mehr repliziert.</block>
  <block id="5caad387757ca99ada41709ce30a31a5" category="paragraph">FLI ist für den Einsatz mit FC konzipiert. Wenn jedoch ein Wechsel zu iSCSI gewünscht wird, kann die migrierte LUN nach Abschluss der Migration problemlos als iSCSI-LUN neu zugeordnet werden.</block>
  <block id="7fa6cfc683e0f5f9e3a79f6be14e23f4" category="paragraph">Zu den Merkmalen von FLI gehört die automatische Ausrichtungserkennung und -Einstellung. In diesem Kontext bezieht sich der Begriff „Alignment“ auf eine Partition auf einem LUN-Gerät. Für eine optimale Performance muss der I/O mit 4-KB-Blöcken abgestimmt werden. Wenn eine Partition auf einem Offset platziert wird, der kein Vielfaches von 4K ist, leidet die Performance.</block>
  <block id="7b4ef266729f1ddfbc724cad17af4efc" category="paragraph">Es gibt einen zweiten Aspekt der Ausrichtung, der nicht korrigiert werden kann, indem ein Partitionsoffset angepasst wird: Die Blockgröße des Dateisystems. Ein ZFS-Dateisystem beispielsweise hat in der Regel eine interne Blockgröße von 512 Byte. Andere Kunden, die AIX verwenden, haben gelegentlich jfs2-Dateisysteme mit einer 512- oder 1, 024-Byte-Blockgröße erstellt. Auch wenn das Filesystem an eine 4-KB-Grenze ausgerichtet ist, bleiben die in diesem Filesystem erstellten Dateien jedoch nicht und die Performance leidet.</block>
  <block id="1d6b28ed67b1d554df806b0b5a020711" category="paragraph">FLI sollte unter diesen Umständen nicht verwendet werden. Obwohl nach der Migration auf die Daten zugegriffen werden kann, ergeben sich daraus Filesysteme mit erheblichen Performance-Einschränkungen. Grundsätzlich sollte jedes Filesystem, das einen zufälligen Überschreibvorgang auf ONTAP unterstützt, eine 4-KB-Blockgröße verwenden. Dies gilt insbesondere für Workloads wie Datenbankdateien und VDI-Implementierungen. Die Blockgröße kann mit den entsprechenden Host-Betriebssystembefehlen identifiziert werden.</block>
  <block id="4941562ef813562e0d6aaef673158b99" category="paragraph">Auf AIX kann beispielsweise die Blockgröße mit angezeigt werden<block ref="10c9a985c983a826424b496a708263ec" prefix=" " category="inline-code"></block>. Mit Linux<block ref="ba5265473f00b27178098cc38d3aef24" prefix=" " category="inline-code"></block> Und<block ref="1f8a87190704df357c64a49aee936874" prefix=" " category="inline-code"></block> Kann für verwendet werden<block ref="310201b6353c5f38bc039e0e51b079d3" prefix=" " category="inline-code"></block> Und<block ref="5382133ffbecb9bce9d47f2e44327b16" prefix=" " category="inline-code"></block>. Mit<block ref="8cbc6ba7c8bba133ce2bc44780d88742" prefix=" " category="inline-code"></block>, Der Befehl lautet<block ref="1615297782a0dff59b20451e2ca97ea7" prefix=" " category="inline-code"></block>.</block>
  <block id="033a4cb04b5225e2b7cf966f2ce16598" category="paragraph">Der Parameter, der die Blockgröße steuert, ist<block ref="81df978a8e3da4bee74b135548f390f3" prefix=" " category="inline-code"></block> Und im Allgemeinen ist der Standardwert 9, was 2^9 oder 512 Byte bedeutet. Für eine optimale Leistung, die<block ref="81df978a8e3da4bee74b135548f390f3" prefix=" " category="inline-code"></block> Wert muss 12 (2^12=4K) sein. Dieser Wert wird zum Zeitpunkt der Erstellung des zpool gesetzt und kann nicht geändert werden, was bedeutet, dass Data zpools mit einem<block ref="81df978a8e3da4bee74b135548f390f3" prefix=" " category="inline-code"></block> Andere als 12 sollten durch Kopieren der Daten in einen neu erstellten zpool migriert werden.</block>
  <block id="8bd9661d8c8eb9924c62b8242191af4e" category="paragraph">Oracle ASM hat keine grundlegende Blockgröße. Die einzige Voraussetzung ist, dass die Partition, auf der die ASM-Festplatte erstellt wird, ordnungsgemäß ausgerichtet sein muss.</block>
  <block id="ea7f7e1e8119ee64585b6173afe720e8" category="section-title">7-Mode Transition Tool</block>
  <block id="75df51cc2f43b8dde951695f97b838c0" category="paragraph">Bei dem 7-Mode Transition Tool (7MTT) handelt es sich um ein Automatisierungstool zur Migration großer 7-Mode Konfigurationen zu ONTAP. Die meisten Datenbankkunden finden andere Methoden einfacher, zum Teil, da sie in der Regel ihre Umgebungen einer Datenbank nach Datenbank migrieren, anstatt den gesamten Storage-Platzbedarf zu verschieben. Zudem sind Datenbanken häufig nur ein Teil einer größeren Storage-Umgebung. Daher werden Datenbanken oft einzeln migriert und die restliche Umgebung kann mit 7MTT verschoben werden.</block>
  <block id="85fd7dcbc2206c27059ae7f88712154d" category="paragraph">Es gibt eine kleine aber beträchtliche Anzahl von Kunden, die Storage-Systeme haben, die komplizierten Datenbankumgebungen gewidmet sind. Diese Umgebungen können viele Volumes, Snapshots und zahlreiche Konfigurationsdetails wie Exportberechtigungen, LUN-Initiatorgruppen, Benutzerberechtigungen und die Konfiguration des Lightweight Directory Access Protocol enthalten. In diesen Fällen können die Automatisierungsfunktionen von 7MTT die Migration vereinfachen.</block>
  <block id="2c87fbdecbf0dfe299f9f89958d61085" category="paragraph">7MTT kann in einem der beiden Modi ausgeführt werden:</block>
  <block id="dc818e71a5519d1b99c7e996cf21fd74" category="list-text">*Copy- Based Transition (CBT).* 7MTT mit CBT richtet SnapMirror Volumes aus einem bestehenden 7-Mode System in der neuen Umgebung ein. Nachdem die Daten synchronisiert sind, orchestriert 7MTT den Umstellungsprozess.</block>
  <block id="453bbc96e365e25cc2d091dced3565a6" category="list-text">*Copy- Free Transition (CFT).* 7MTT mit CFT basiert auf der in-Place Konvertierung vorhandener 7-Mode Platten-Shelfs. Es werden keine Daten kopiert und die vorhandenen Festplatten-Shelfs können wieder verwendet werden. Die vorhandene Konfiguration für Datensicherung und Storage-Effizienz bleibt erhalten.</block>
  <block id="16262236c3981cb3310b9320b5a67fd0" category="paragraph">Der primäre Unterschied zwischen diesen beiden Optionen ist der Copy-Free Transition. Er ist ein „Big-Bang“-Ansatz, bei dem alle mit dem ursprünglichen 7-Mode HA-Paar verbundenen Platten-Shelfs in die neue Umgebung verschoben werden müssen. Eine Untergruppe von Shelfs lässt sich nicht verschieben. Durch den Copy-basierten Ansatz können ausgewählte Volumes verschoben werden. Es besteht auch die Möglichkeit, dass ein längeres Umstellungsfenster mit Copy-Free Transition möglich ist, da für die Neuerstellung von Festplatten-Shelfs und die Konvertierung von Metadaten eine Verbindung erforderlich ist. Je nach Praxiserfahrung empfiehlt NetApp, für die Verlagerung und Neuverkabelung von Festplatten-Shelfs eine Stunde und für die Metadatenkonvertierung zwischen 15 Minuten und 2 Stunden zu verwenden.</block>
  <block id="795bbfbd054829fad905065087ba2d5a" category="paragraph">Aus Sicht der Datenbank und des Hosts sind keine besonderen Schritte erforderlich. Nachdem die FC-Zonen aktualisiert wurden und die LUNs auf ONTAP verfügbar werden, sollte die LVM in der Lage sein, die LVM-Metadaten von den LUNs zu lesen. Außerdem sind die Volume-Gruppen ohne weitere Konfigurationsschritte einsatzbereit. In seltenen Fällen können Umgebungen Konfigurationsdateien enthalten, die hartcodiert waren und Verweise auf das vorherige Storage-Array enthalten. Zum Beispiel ein Linux-System, das enthalten<block ref="dd5ff67ba72768d33f75e645aa2b8e56" prefix=" " category="inline-code"></block> Regeln, die auf einen WWN eines bestimmten Geräts verwiesen haben, müssen aktualisiert werden, um die von FLI eingeführten Änderungen wiederzugeben.</block>
  <block id="9e7ddc2ddefcd6f202fb11a82a80bc7e" category="admonition">Informationen zu unterstützten Konfigurationen finden Sie in der NetApp Kompatibilitätsmatrix. Falls Ihr System nicht im Lieferumfang enthalten ist, wenden Sie sich an Ihren NetApp Ansprechpartner.</block>
  <block id="0f5b3758229b3e28c8605b6c42398115" category="paragraph">Dieses Beispiel zeigt die Migration von ASM- und LVM-LUNs, die auf einem Linux-Server gehostet werden. FLI wird auf anderen Betriebssystemen unterstützt, und obwohl die Host-seitigen Befehle unterschiedlich sein können, sind die Prinzipien identisch, und die ONTAP-Verfahren sind identisch.</block>
  <block id="f1036cd97e461d07351c9df32fc5948d" category="section-title">LVM-LUNs identifizieren</block>
  <block id="88f7413f9621a816ee3d370703168647" category="paragraph">Der erste Schritt zur Vorbereitung besteht darin, die zu migrierenden LUNs zu identifizieren. In dem hier gezeigten Beispiel werden zwei SAN-basierte Dateisysteme in gemountet<block ref="572f241ef88fdb17d16eba97133d861f" prefix=" " category="inline-code"></block> Und<block ref="d14bc7787c7396144583e99a7df52f67" prefix=" " category="inline-code"></block>.</block>
  <block id="ecad615a52fda392a9b7c1075d2cb2c5" category="paragraph">Der Name der Volume-Gruppe kann aus dem Gerätenamen extrahiert werden, der das Format (Name der Volume-Gruppe)-(Name des logischen Volumes) verwendet. In diesem Fall wird die Volume-Gruppe aufgerufen<block ref="bb97320b8c517da828eae4ec543113db" prefix=" " category="inline-code"></block>.</block>
  <block id="b08986bb1757d5ae8de06d523c71803d" category="paragraph">Der<block ref="f1e2c495108b111c930735e3a0f9a04e" prefix=" " category="inline-code"></block> Mit dem Befehl können Sie die LUNs identifizieren, die diese Volume-Gruppe unterstützen. In diesem Fall sind 10 LUNs vorhanden<block ref="bb97320b8c517da828eae4ec543113db" prefix=" " category="inline-code"></block> Volume-Gruppe.</block>
  <block id="8b47fac6803dfff2e8d2cdad6f297cfa" category="section-title">ASM-LUNs identifizieren</block>
  <block id="a23d0124954ecdbfe9d0f3aceb9e17a2" category="paragraph">ASM-LUNs müssen ebenfalls migriert werden. Um die Anzahl der LUNs und LUN-Pfade von sqlplus als sysasm-Benutzer zu erhalten, führen Sie den folgenden Befehl aus:</block>
  <block id="918841992da84786f00de7c4c7d83dbf" category="paragraph">Die aktuelle Umgebung enthält 20 zu migrierende LUNs. Aktualisieren Sie das aktuelle SAN, damit ONTAP auf die aktuellen LUNs zugreifen kann. Daten werden noch nicht migriert, aber ONTAP muss die Konfigurationsinformationen der aktuellen LUNs lesen, um das neue Zuhause für diese Daten zu erstellen.</block>
  <block id="5816768a5d83977f34ffa5025c14f430" category="paragraph">Mindestens ein HBA-Port auf dem All Flash FAS/FAS System muss als Initiator-Port konfiguriert sein. Zudem müssen die FC-Zonen aktualisiert werden, damit ONTAP auf die LUNs auf dem fremden Storage Array zugreifen können. Bei einigen Speicher-Arrays ist die LUN-Maskierung konfiguriert, wodurch WWNs auf eine bestimmte LUN zugreifen können. In diesen Fällen muss die LUN-Maskierung ebenfalls aktualisiert werden, um Zugriff auf die ONTAP-WWNs zu gewähren.</block>
  <block id="030a992b1340f67584d67ef6230e9adf" category="paragraph">Nach Abschluss dieses Schritts sollte ONTAP in der Lage sein, das fremde Speicher-Array mit dem anzuzeigen<block ref="d90ba20b63acab53952d68665c50ec47" prefix=" " category="inline-code"></block> Befehl. Das Schlüsselfeld, das zurückgegeben wird, ist das Präfix, das zur Identifizierung der fremden LUN auf dem System verwendet wird. Im folgenden Beispiel werden die LUNs auf dem Fremdarray angezeigt<block ref="25103eba8d70e82b5bd837be0d2f63c4" prefix=" " category="inline-code"></block> Wird in ONTAP mit dem Präfix von angezeigt<block ref="6b35470d99880c4630ed3ca7b4c842e7" prefix=" " category="inline-code"></block>.</block>
  <block id="c1a0f5174323df40a8b659eb65ac5155" category="section-title">Identifizierung von Fremdarrays</block>
  <block id="6c1b8919e2e706972b3ec48cb7f014ba" category="section-title">Identifizierung fremder LUNs</block>
  <block id="63841e312d3a8531331e35bd826268d1" category="paragraph">Die LUNs können durch Bestehen des aufgelistet werden<block ref="907702f5103a7823393b8dd296a75c26" prefix=" " category="inline-code"></block> Bis zum<block ref="242f4ce1948273386f8bb487bdd3732f" prefix=" " category="inline-code"></block> Befehl. Die zurückgegebenen Daten werden während des Migrationsvorgangs mehrfach referenziert.</block>
  <block id="e5e94f7e2098883f0d621e064b1104f4" category="section-title">Registrieren Sie LUNs für Fremdarrays als Importkandidaten</block>
  <block id="c968ec41c1c0c449b0263962966d8293" category="paragraph">Die ausländischen LUNs werden zunächst als jeder bestimmte LUN-Typ klassifiziert. Bevor Daten importiert werden können, müssen die LUNs als fremd gekennzeichnet werden und daher als Kandidat für den Importprozess. Um diesen Schritt abzuschließen, geben Sie die Seriennummer an den weiter<block ref="93c633f7fa188f1b27df574c0e5e929a" prefix=" " category="inline-code"></block> Wie im folgenden Beispiel gezeigt. Beachten Sie, dass bei diesem Prozess nur die LUN als fremd innerhalb von ONTAP markiert wird. Es werden keine Daten auf die fremde LUN selbst geschrieben.</block>
  <block id="33380a523841f23815f2f83c9de1a628" category="section-title">Erstellung von Volumes zum Hosten migrierter LUNs</block>
  <block id="2c2ba8ca6bdd77da1da981b91a3fae4a" category="paragraph">Ein Volume ist erforderlich, um die migrierten LUNs zu hosten. Die genaue Volume-Konfiguration hängt von der Planung der Nutzung von ONTAP Funktionen ab. In diesem Beispiel werden die ASM-LUNs in einem Volume platziert und die LVM-LUNs in einem zweiten Volume platziert. Auf diese Weise können Sie die LUNs als unabhängige Gruppen managen, beispielsweise für Tiering, die Erstellung von Snapshots oder die Einstellung von QoS-Kontrollen.</block>
  <block id="8bc7d64e68e4be8f4908effd57293a2a" category="paragraph">Stellen Sie die ein<block ref="fb983ebeedc63d8723c0d4aa558a4c02" prefix=" " category="inline-code"></block>. Der Migrationsprozess kann sehr viel Datenfluktuation beinhalten. Daher kann es zu einem starken Anstieg des Platzverbrauchs kommen, wenn Snapshots versehentlich erstellt werden, weil unerwünschte Daten in den Snapshots erfasst werden.</block>
  <block id="8c0675d07e44f55f1f5aa6d45abe0cdb" category="section-title">Erstellen Sie ONTAP-LUNs</block>
  <block id="0b799020dac78a5e12a3d1a6400fd0c8" category="paragraph">Nach der Erstellung der Volumes müssen die neuen LUNs erstellt werden. Normalerweise erfordert die Erstellung einer LUN, dass der Benutzer Informationen wie die LUN-Größe angeben muss. In diesem Fall wird jedoch das Argument für eine fremde Festplatte an den Befehl übergeben. Infolgedessen repliziert ONTAP die aktuellen LUN-Konfigurationsdaten von der angegebenen Seriennummer. Außerdem werden die LUN-Geometrie und Partitionstabellen-Daten verwendet, um die LUN-Ausrichtung anzupassen und eine optimale Performance herzustellen.</block>
  <block id="5272a1dc3854f1eddc353c8073234e03" category="paragraph">In diesem Schritt müssen die Seriennummern mit dem Fremdarray verglichen werden, um sicherzustellen, dass die richtige fremde LUN mit der richtigen neuen LUN abgeglichen wird.</block>
  <block id="f6ee70ec9fce7bd34fa5a33d8969fb5e" category="section-title">Erstellen Sie Importbeziehungen</block>
  <block id="94700a911b95416221efe4064acbc2ba" category="paragraph">Die LUNs wurden jetzt erstellt, sind aber nicht als Replikationsziel konfiguriert. Bevor dieser Schritt durchgeführt werden kann, müssen die LUNs zunächst in den Offline-Modus versetzt werden. Dieser zusätzliche Schritt dient dem Schutz von Daten vor Benutzerfehlern. Wenn ONTAP die Durchführung einer Migration auf einer Online-LUN zulässt, besteht das Risiko, dass durch einen typografischen Fehler aktive Daten überschrieben werden. Durch den zusätzlichen Schritt, den Benutzer zum ersten Mal offline zu schalten, wird überprüft, ob die richtige Ziel-LUN als Migrationsziel verwendet wird.</block>
  <block id="85255e52054af9c6ffd3c4e79c723a34" category="paragraph">Nachdem die LUNs offline sind, können Sie die Importbeziehung wiederherstellen, indem Sie die Seriennummer der fremden LUN an den übergeben<block ref="04c2f87857699971f5aedd525e65690a" prefix=" " category="inline-code"></block> Befehl.</block>
  <block id="8e3c921431d2b7f66fa0cc85f3ae5c9d" category="paragraph">Nachdem alle Importbeziehungen eingerichtet sind, können die LUNs wieder online geschaltet werden.</block>
  <block id="ea382cbfdb353cee0cf25eaae9bba150" category="section-title">Erstellen einer Initiatorgruppe</block>
  <block id="152c4728a5fb152525a639439b9d7cb9" category="inline-link-macro">Protokollkonvertierung</block>
  <block id="298a1c3c736859f2cfa3cb8eb1f3fdcc" category="paragraph">In diesem Beispiel wird eine Initiatorgruppe erstellt, die zwei WWNs enthält, die den beiden auf dem HBA des Hosts verfügbaren Ports entsprechen.</block>
  <block id="4f6f03762bfcdc7174cdd30240ce45b3" category="section-title">Ordnen Sie neue LUNs dem Host zu</block>
  <block id="c22fee2d07a149be91ae1ae2ada1cb57" category="paragraph">Nach der Erstellung der Initiatorgruppe werden die LUNs dann der definierten Initiatorgruppe zugeordnet. Diese LUNs sind nur für die WWNs dieser Initiatorgruppe verfügbar. NetApp geht in dieser Phase des Migrationsprozesses davon aus, dass der Host nicht auf ONTAP abgegrenzt wurde. Dies ist wichtig, denn wenn der Host gleichzeitig auf das fremde Array und das neue ONTAP-System begrenzt ist, besteht das Risiko, dass LUNs mit derselben Seriennummer auf jedem Array erkannt werden können. Diese Situation kann zu Fehlfunktionen des Multipfad-Funktionszubers oder zu Schäden an Daten führen.</block>
  <block id="ecca392290c36269e0489c0b1cf36f50" category="summary">Beispielskripts für die Automatisierung von Oracle-Migrationsvorgängen</block>
  <block id="fc53c036602508ed6c2afc18e2fbdf51" category="doc">Beispielskripts</block>
  <block id="6f78af00b87bcad5d08c95c7b6066ad0" category="paragraph">Die vorgestellten Skripte werden als Beispiele für das Skript verschiedener Betriebssystem- und Datenbankaufgaben bereitgestellt. Sie werden wie sie sind geliefert. Wenn für eine bestimmte Vorgehensweise Support erforderlich ist, wenden Sie sich an NetApp oder einen NetApp Reseller.</block>
  <block id="3146889e40b3ca2b78c56a19178bff84" category="section-title">Datenbank wird heruntergefahren</block>
  <block id="5b98b9877c0c74dc3a3daf46cf943d3f" category="paragraph">Das folgende Perl-Skript nimmt ein einziges Argument der Oracle SID und fährt eine Datenbank herunter. Sie kann als Oracle-Benutzer oder als root ausgeführt werden.</block>
  <block id="7df41a3619af59182fb5df6d1920d11a" category="section-title">Starten der Datenbank</block>
  <block id="57249fb9cdac8ddc9ea8843636c4e088" category="section-title">Konvertieren Sie das Dateisystem in schreibgeschützt</block>
  <block id="f2977a22ba44544ca8921e2d75bce435" category="paragraph">Das folgende Skript nimmt ein Dateisystemargument an und versucht, es als schreibgeschützt zu entfernen und wieder zu mounten. Dies ist bei Migrationsprozessen sinnvoll, bei denen ein Dateisystem für die Datenreplikation verfügbar gehalten werden muss und dennoch vor versehentlichen Schäden geschützt werden muss.</block>
  <block id="4eac395595a8d266839db0b88fae31e4" category="section-title">Ersetzen Sie das Dateisystem</block>
  <block id="988d7030a130298e1641007b5b0aae20" category="paragraph">Das folgende Skriptbeispiel wird verwendet, um ein Dateisystem durch ein anderes zu ersetzen. Da die Datei `/etc/fstab `bearbeitet wird, muss sie als root ausgeführt werden. Es akzeptiert ein einzelnes kommagetrenntes Argument des alten und des neuen Dateisystems.</block>
  <block id="149c75d086adf78a50a16bcb5e349158" category="list-text">Führen Sie zum Ersetzen des Dateisystems das folgende Skript aus:</block>
  <block id="67a4c588434c781febb12c165c3e860a" category="paragraph">Nehmen Sie als Beispiel für die Verwendung dieses Skripts an, dass die Daten in enthalten sind<block ref="f006bf17b06c884c47190c4225d3215a" prefix=" " category="inline-code"></block> Wird auf migriert<block ref="27df80f143a5812bda1553d09c712efc" prefix=" " category="inline-code"></block> Und<block ref="0e62afc91abe157ef67895cca0a7f912" prefix=" " category="inline-code"></block> Wird auf migriert<block ref="1c988fc7a325635f00f9b55992128a08" prefix=" " category="inline-code"></block>. Eine der einfachsten Methoden, um diese Aufgabe durchzuführen, besteht darin, das neue Gerät mit einem einfachen Dateikopiervorgang wieder in den ursprünglichen Bereitstellungspunkt zu verschieben.</block>
  <block id="4a3d90c30a5e921650fe8ad0decf4827" category="list-text">Gehen Sie davon aus, dass die alten und die neuen Dateisysteme im vorhanden sind<block ref="816a3ef77b62ade41c3f936c958aa555" prefix=" " category="inline-code"></block> Datei wie folgt:</block>
  <block id="4588a4e3d2f8d0f7c036f59c1be2163f" category="list-text">Wenn dieses Skript ausgeführt wird, wird das aktuelle Dateisystem abgehängt und durch das neue ersetzt:</block>
  <block id="2d808cb44231d0a80164567e7220fc44" category="list-text">Das Skript aktualisiert auch die<block ref="816a3ef77b62ade41c3f936c958aa555" prefix=" " category="inline-code"></block> Entsprechende Datei erstellen. Im hier gezeigten Beispiel sind folgende Änderungen enthalten:</block>
  <block id="840967a448e36b9efeaacf7673f825aa" category="section-title">Automatisierte Datenbankmigration</block>
  <block id="1107dd36fe8ba0c18ea8abc8734986e2" category="paragraph">Dieses Beispiel zeigt, wie Skripts zum Herunterfahren, Starten und Ersetzen von Dateisystemen genutzt werden können, um eine Migration vollständig zu automatisieren.</block>
  <block id="36881541e7d9c7bced65fcac337ac327" category="section-title">Dateispeicherorte anzeigen</block>
  <block id="1e9a1acc40d4943a73e6865cdebaf560" category="paragraph">Dieses Skript sammelt eine Reihe wichtiger Datenbankparameter und druckt sie in einem leicht lesbaren Format aus. Dieses Skript kann bei der Überprüfung von Datenlayouts nützlich sein. Darüber hinaus kann das Skript für andere Zwecke geändert werden.</block>
  <block id="6172b49b7fdf806b59af0aeef8de71e3" category="section-title">Bereinigung der ASM-Migration</block>
  <block id="dd8d75d6766af495b5442e67eb27d387" category="section-title">Namenskonvertierung von ASM in Dateisystem</block>
  <block id="abc3fc91423668bb4bd49e6e837fa3e4" category="section-title">Wiedergabe von Protokollen in der Datenbank</block>
  <block id="cc7b9a518bb2532f61662a9893248133" category="paragraph">Dieses Skript akzeptiert ein einzelnes Argument einer Oracle SID für eine Datenbank, die sich im Mount-Modus befindet, und versucht, alle derzeit verfügbaren Archivprotokolle wiederzugeben.</block>
  <block id="2a5bd3a209a1ed33b81c5306780227ce" category="section-title">Wiedergabe von Protokollen in der Standby-Datenbank</block>
  <block id="ea7bebed0b4151a9b28acbde55fc0eb3" category="paragraph">Dieses Skript ist identisch mit dem vorhergehenden Skript, außer dass es für eine Standby-Datenbank konzipiert ist.</block>
  <block id="2adcfd589637337984e39e94dd14ae33" category="paragraph">Das Ändern des Protokolls für den Zugriff auf eine LUN ist eine gängige Anforderung.</block>
  <block id="38fe6838f11a6023172d11d806adbfcf" category="paragraph">In einigen Fällen ist die Migration der Daten in die Cloud Teil einer Gesamtstrategie. TCP/IP ist das Protokoll der Cloud, und der Wechsel von FC zu iSCSI ermöglicht eine einfachere Migration in verschiedene Cloud-Umgebungen. In anderen Fällen kann iSCSI wünschenswert sein, die gesunkenen Kosten eines IP SAN zu nutzen. Gelegentlich kann eine Migration ein anderes Protokoll als temporäre Maßnahme verwenden. Wenn beispielsweise ein fremdes Array und ONTAP-basierte LUNs nicht auf denselben HBAs koexistieren können, können Sie iSCSI-LUNs verwenden, die lang genug sind, um Daten vom alten Array zu kopieren. Nachdem die alten LUNs aus dem System entfernt wurden, können Sie sie wieder zu FC konvertieren.</block>
  <block id="832c81bd0db3ad98a3804ff5ce996a58" category="paragraph">Das folgende Verfahren zeigt die Konvertierung von FC zu iSCSI, jedoch gelten die allgemeinen Prinzipien für eine umgekehrte iSCSI- zu FC-Konvertierung.</block>
  <block id="e21180783b17ea695cf9275fce8a0bc7" category="section-title">Installieren Sie den iSCSI-Initiator</block>
  <block id="0e1ddcc32a3ef636c9ca084a3d7706f5" category="paragraph">Die meisten Betriebssysteme enthalten standardmäßig einen Software-iSCSI-Initiator, aber wenn dieser nicht enthalten ist, kann er problemlos installiert werden.</block>
  <block id="289fe013df899f5f74fb362d4efcdd5d" category="section-title">Identifizieren Sie den iSCSI-Initiatornamen</block>
  <block id="8242e97b190184a6c7b8ec96c5662886" category="paragraph">Während der Installation wird ein eindeutiger iSCSI-Initiatorname generiert. Unter Linux befindet sie sich im<block ref="22565a44e8f73044eb2029acbb069639" prefix=" " category="inline-code"></block> Datei: Dieser Name dient zur Identifizierung des Hosts auf dem IP-SAN.</block>
  <block id="721a96eb0b922b337fa815839839b328" category="section-title">Erstellen Sie eine neue Initiatorgruppe</block>
  <block id="cda1311d977b7b77ad3aebe4a813b3bd" category="paragraph">Eine Initiatorgruppe (Initiatorgruppe) ist Teil der ONTAP LUN-Masking-Architektur. Auf eine neu erstellte LUN kann nur dann zugegriffen werden, wenn einem Host der erste Zugriff gewährt wurde. Hierzu wird eine Initiatorgruppe erstellt, die entweder die FC-WWNs oder die iSCSI-Initiatornamen enthält, die Zugriff erfordern.</block>
  <block id="6548788f050643897f7354b6df4488db" category="paragraph">In diesem Beispiel wird eine Initiatorgruppe erstellt, die den iSCSI-Initiator des Linux Hosts enthält.</block>
  <block id="da8490d81c1ef6b559b09c2ab94631d2" category="section-title">Fahren Sie die Umgebung herunter</block>
  <block id="0bf211160ad57fedca3499176f213b11" category="paragraph">Vor dem Ändern des LUN-Protokolls müssen die LUNs vollständig stillgelegt werden. Jede Datenbank auf einer der zu konvertierenden LUNs muss heruntergefahren, die File-Systeme deaktiviert und die Volume-Gruppen deaktiviert werden. Wenn ASM verwendet wird, stellen Sie sicher, dass die ASM-Laufwerksgruppe getrennt ist und fahren Sie alle Netzdienste herunter.</block>
  <block id="2ba1c509ee5f5e31f05b24aed2d23054" category="section-title">LUN-Zuordnungen zum FC-Netzwerk aufheben</block>
  <block id="2f9e42c479a76dd54d7946c181049dd4" category="paragraph">Nachdem die LUNs vollständig stillgelegt sind, entfernen Sie die Zuordnungen von der ursprünglichen FC-Initiatorgruppe.</block>
  <block id="5f889f52ef85be290c3e5c0211bbb8fa" category="section-title">LUN-Zuordnung zum IP-Netzwerk neu</block>
  <block id="4ac84701530b31d8d33d9aa742715d25" category="paragraph">Gewähren Sie der neuen iSCSI-basierten Initiatorgruppe Zugriff auf jede LUN.</block>
  <block id="3d59eaa280c8b75d8e997eac2cf5b3e6" category="section-title">ISCSI-Ziele erkennen</block>
  <block id="981b782c3eacf8b87d8231b887873f03" category="paragraph">Die iSCSI-Erkennung besteht aus zwei Phasen. Zum einen werden die Ziele ermittelt. Dies ist nicht dasselbe wie beim Erkennen einer LUN. Der<block ref="f88921f58beaaae5bcb9dffac54bf5ad" prefix=" " category="inline-code"></block> Der unten abgebildete Befehl prüft die vom angegebene Portalgruppe<block ref="24e12535882a30ad33a21d76d76bd0ff" prefix=" " category="inline-code"></block> Und speichert eine Liste aller IP-Adressen und Ports, die iSCSI-Dienste anbieten. In diesem Fall gibt es vier IP-Adressen, die iSCSI-Dienste auf dem Standardport 3260 haben.</block>
  <block id="0ab5dd035c7f92f4dce1026744c01604" category="admonition">Dieser Befehl kann mehrere Minuten dauern, wenn eine der Ziel-IP-Adressen nicht erreicht werden kann.</block>
  <block id="1dba8f39914c5f44e3cb635e7ac7563e" category="section-title">ISCSI-LUNs erkennen</block>
  <block id="68307bef4b9dc4071d875a0c2aadd321" category="paragraph">Nachdem die iSCSI-Ziele erkannt wurden, starten Sie den iSCSI-Dienst neu, um die verfügbaren iSCSI-LUNs zu ermitteln und zugehörige Geräte wie Multipath- oder ASMlib-Geräte zu erstellen.</block>
  <block id="25d811252f87ae85dda97f2ab9dd1f3a" category="section-title">Starten Sie die Umgebung neu</block>
  <block id="8a9aae851d31dae8baf96171f114955e" category="paragraph">Starten Sie die Umgebung neu, indem Sie Volume-Gruppen erneut aktivieren, Dateisysteme neu mounten, RAC-Dienste neu starten usw. Als Vorsichtsmaßnahme empfiehlt NetApp, den Server nach Abschluss des Konvertierungsprozesses neu zu starten, um sicherzustellen, dass alle Konfigurationsdateien korrekt sind und alle veralteten Geräte entfernt werden.</block>
  <block id="00f4c8378b0bf837cee4b0aed8886f31" category="paragraph">Achtung: Bevor Sie einen Host neu starten, stellen Sie sicher, dass alle Einträge in sind<block ref="816a3ef77b62ade41c3f936c958aa555" prefix=" " category="inline-code"></block> Diese Referenz migrierte SAN-Ressourcen werden kommentiert. Wenn dieser Schritt nicht durchgeführt wird und Probleme mit dem LUN-Zugriff auftreten, kann es zu einem Betriebssystem kommen, das nicht gebootet wird. Dieses Problem beschädigt die Daten nicht. Es kann jedoch sehr unbequem sein, in den Rettungsmodus oder einen ähnlichen Modus zu starten und zu korrigieren<block ref="816a3ef77b62ade41c3f936c958aa555" prefix=" " category="inline-code"></block> Damit das Betriebssystem gestartet werden kann, um die Fehlerbehebung zu ermöglichen.</block>
  <block id="08e3cfb3c3dfa08d34bcbbfe07fe5f28" category="paragraph">Für die sichere und effiziente Verwaltung mehrerer Oracle Datenbanken ist eine effektive QoS-Strategie erforderlich. Der Grund dafür sind die stetig wachsenden Performance-Möglichkeiten eines modernen Storage-Systems.</block>
  <block id="3408fe4b6e4b2ed9366356f08e3b9ab0" category="summary">Oracle Datenbanken und Storage-Effizienz</block>
  <block id="151654fbc652cca26707bd7a060073d3" category="doc">Oracle Database und ONTAP-Effizienzfunktionen</block>
  <block id="4564218d346b0ef8263512ebe659337d" category="paragraph">Die Funktionen zur Steigerung der Speicherplatzeffizienz von ONTAP sind für Oracle Datenbanken optimiert. In fast allen Fällen besteht der beste Ansatz darin, die Standardeinstellungen bei aktivierten Effizienzfunktionen zu belassen.</block>
  <block id="794c6d3bb54da92d9a4a4022bc5c4e94" category="summary">RAID und Oracle Datenbanken</block>
  <block id="e959fe6bb794f67f31a0d4e6bc5d5bf4" category="paragraph">RAID bezieht sich auf den Einsatz von Redundanz, um Daten vor dem Verlust eines Laufwerks zu schützen.</block>
  <block id="b60b0e7c75501fe2f322675f2e6da6d7" category="paragraph">Gelegentlich stellen sich Fragen zu RAID-Levels bei der Konfiguration von NetApp-Speicher, der für Oracle-Datenbanken und andere Enterprise-Applikationen verwendet wird. Viele, von Oracle bewährte Verfahren zur Storage Array-Konfiguration enthalten Warnungen über die Verwendung von RAID-Spiegelung und/oder Vermeidung bestimmter Arten von RAID. Obwohl in ihnen gültige Punkte aufgeführt sind, gelten diese Quellen nicht für RAID 4 und die in ONTAP verwendeten NetApp RAID DP und RAID-TEC Technologien.</block>
  <block id="6de6bcceac7c1a53ecb774d42b8d5c9a" category="summary">Oracle und ONTAP Thin Provisioning</block>
  <block id="26c71bfc140fc9f2d56f83f7aedffd53" category="paragraph">Thin Provisioning für eine Oracle-Datenbank erfordert eine sorgfältige Planung, da im Ergebnis mehr Speicherplatz auf einem Storage-System konfiguriert wird, als unbedingt physisch verfügbar ist. Dieser Aufwand lohnt sich wirklich, denn bei korrekter Umsetzung ergeben sich erhebliche Kosteneinsparungen und Verbesserungen beim Management.</block>
  <block id="c9a6446e7a1a2aa2d3d06c76b3dd02ae" category="summary">SVM-Bereitstellung für Oracle-Datenbanken</block>
  <block id="eaa6380760494a867d1be23523aaba3e" category="doc">Oracle-Datenbanken und Storage Virtual Machines</block>
  <block id="ffa2dec61afa63c15c9906328e62e2e8" category="paragraph">Das Storage-Management für Oracle Datenbanken wird auf einer Storage Virtual Machine (SVM) zentralisiert.</block>
  <block id="ba0fd7b6bfa61ad7f7ad5326a0934d01" category="summary">Kenntnisse über Storage-Takeover- und Switchover-Funktionen sind erforderlich, damit Oracle Datenbankvorgänge nicht durch diese Vorgänge unterbrochen werden. Darüber hinaus können die Argumente für Takeover- und Switchover-Vorgänge die Datenintegrität beeinträchtigen, wenn sie falsch verwendet werden.</block>
  <block id="999548558d2844d78a3b5f9186c231a4" category="summary">Datenbanken und ONTAP Storage-Kapazität und freien Speicherplatz</block>
  <block id="360612d74cdd78953036946ec943f795" category="paragraph">Für das Management von Datenbanken oder anderen Enterprise-Applikationen mit vorhersehbarem, leicht verwaltbarem, hochperformantem Enterprise-Storage ist auf den Laufwerken freier Speicherplatz für das Daten- und Metadaten-Management erforderlich. Die Menge des freien Speicherplatzes hängt vom Typ des verwendeten Laufwerks und von den Geschäftsprozessen ab.</block>
  <block id="d3cc759510a3aba837fc8efeb2e24b91" category="doc">db_File_Multiblock_read_count</block>
  <block id="32faf0a922da10425f2be9f86a5f5e18" category="paragraph">Der<block ref="d3cc759510a3aba837fc8efeb2e24b91" prefix=" " category="inline-code"></block> Der Parameter steuert die maximale Anzahl von Oracle-Datenbankblöcken, die Oracle während sequenzieller I/O-Vorgänge als Einzelvorgang liest</block>
  <block id="36cb03af3dc688208e0e94fda31ca7a4" category="paragraph">Dieser Parameter wirkt sich jedoch weder auf die Anzahl der Blöcke aus, die Oracle während aller Lesevorgänge liest, noch auf zufälligen I/O. Nur die Blockgröße sequenzieller I/O ist betroffen.</block>
  <block id="2565c992b3732464484104f4c846d9b4" category="paragraph">Oracle empfiehlt dem Benutzer, diesen Parameter nicht festzulegen. Dadurch kann die Datenbanksoftware automatisch den optimalen Wert einstellen. Das bedeutet im Allgemeinen, dass dieser Parameter auf einen Wert gesetzt wird, der eine I/O-Größe von 1 MB ergibt. Zum Beispiel würde ein Lesevorgang von 1 MB mit 8-KB-Blöcken 128 Blöcke erfordern, und der Standardwert für diesen Parameter wäre daher 128.</block>
  <block id="0fd26e725a6c1a81508e013094776e2c" category="paragraph">Bei den meisten von NetApp an Kundenstandorten festgestellten Performance-Problemen bei Datenbanken handelt es sich um eine falsche Einstellung für diesen Parameter. Es gab triftige Gründe, diesen Wert mit den Oracle-Versionen 8 und 9 zu ändern. Daher kann der Parameter in vorhanden sein, ohne dass dies bekannt ist<block ref="b6bd7c570605affc5b50bbd3e436f0c0" prefix=" " category="inline-code"></block> Dateien, da die Datenbank auf Oracle 10 und höher aktualisiert wurde. Eine ältere Einstellung von 8 oder 16 beeinträchtigt im Vergleich zu einem Standardwert von 128 erheblich die sequenzielle I/O-Performance.</block>
  <block id="3f594bb922365c7a9f095f44822fa22e" category="admonition">*NetApp empfiehlt* die Einstellung<block ref="d3cc759510a3aba837fc8efeb2e24b91" prefix=" " category="inline-code"></block> Der Parameter darf nicht im vorhanden sein<block ref="b6bd7c570605affc5b50bbd3e436f0c0" prefix=" " category="inline-code"></block> Datei: NetApp hat noch nie eine Situation erlebt, in der sich durch die Änderung dieses Parameters die Performance verbesserte. In vielen Fällen wurde jedoch der sequenzielle I/O-Durchsatz deutlich beeinträchtigt.</block>
  <block id="0eec903bb9d87349e379892ca99ef9ec" category="paragraph">Oracle RAC ist ein Clusterware-Produkt mit verschiedenen Arten von internen Heartbeat-Prozessen, die den Zustand des Clusters überwachen.</block>
  <block id="7c13d022c9fda6e792c4349a043a6c74" category="inline-link-macro">Fehlzählung</block>
  <block id="bab4ae46619fcbbb84e3375ef205e61c" category="admonition">Die Informationen im <block ref="9cd17d807316d35ef47b12cbecab4ec8" category="inline-link-macro-rx"></block> Der Abschnitt enthält wichtige Informationen für Oracle RAC-Umgebungen, die Netzwerkspeicher verwenden. In vielen Fällen müssen die standardmäßigen Oracle RAC-Einstellungen geändert werden, um sicherzustellen, dass der RAC-Cluster Netzwerkpfadänderungen und Speicher-Failover-/Switchover-Vorgänge überlebt.</block>
  <block id="0494651671c3dc7f1ccdf99a4368a2cf" category="section-title">Festplatten-Timeout</block>
  <block id="1973cfff3b7c6d828ede9d420b8481b5" category="paragraph">Der primäre speicherbezogene RAC-Parameter lautet<block ref="0494651671c3dc7f1ccdf99a4368a2cf" prefix=" " category="inline-code"></block>. Dieser Parameter steuert den Schwellenwert, innerhalb dessen die Abstimmungsdatei E/A abgeschlossen werden muss. Wenn der<block ref="0494651671c3dc7f1ccdf99a4368a2cf" prefix=" " category="inline-code"></block> Parameter überschritten wird, dann wird der RAC-Knoten aus dem Cluster entfernt. Der Standardwert für diesen Parameter ist 200. Dieser Wert sollte für standardmäßige Storage-Takeover- und Giveback-Verfahren ausreichen.</block>
  <block id="4aa33fc988e36b9531d36da80d35ec2f" category="paragraph">NetApp empfiehlt, die RAC-Konfigurationen vor ihrer Inbetriebnahme sorgfältig zu testen, da sich viele Faktoren auf einen Takeover oder Giveback auswirken. Neben der Zeit, die für den Abschluss des Storage-Failovers benötigt wird, ist auch für die Verbreitung der Änderungen des Link Aggregation Control Protocol (LACP) zusätzliche Zeit erforderlich. Darüber hinaus muss die SAN-Multipathing-Software eine I/O-Zeitüberschreitung erkennen und einen alternativen Pfad erneut versuchen. Wenn eine Datenbank extrem aktiv ist, muss eine große Menge an I/O-Vorgängen in die Warteschlange gestellt und erneut versucht werden, bevor die Abstimmungs-E/A-Vorgänge verarbeitet werden.</block>
  <block id="655a10764a0781d26dca3bfb939457b7" category="paragraph">Wenn ein tatsächlicher Storage Takeover oder Giveback nicht möglich ist, kann der Effekt durch Cable Pull-Tests auf dem Datenbankserver simuliert werden.</block>
  <block id="b2a9e8dd67b2d9a7333cb2c2495c2aa5" category="list-text">Verlassen des<block ref="0494651671c3dc7f1ccdf99a4368a2cf" prefix=" " category="inline-code"></block> Parameter mit dem Standardwert 200.</block>
  <block id="971a52f4b805c654dd133cf142457c53" category="list-text">Testen Sie eine RAC-Konfiguration immer gründlich.</block>
  <block id="cd7a91511dbfbe5c79de12fa7d394dc0" category="paragraph">Der<block ref="7c13d022c9fda6e792c4349a043a6c74" prefix=" " category="inline-code"></block> Der Parameter wirkt sich normalerweise nur auf den Netzwerk-Heartbeat zwischen RAC-Knoten aus. Die Standardeinstellung ist 30 Sekunden. Wenn sich die Grid-Binärdateien auf einem Storage Array befinden oder das Boot-Laufwerk des Betriebssystems nicht lokal ist, kann dieser Parameter wichtig werden. Dazu gehören Hosts mit Boot-Laufwerken in einem FC-SAN, über NFS gestartete Betriebssysteme und Boot-Laufwerke in Virtualisierungs-Datastores, beispielsweise eine VMDK-Datei.</block>
  <block id="9c1fd24d7d8e04847dadb944a6643121" category="paragraph">Wird der Zugriff auf ein Boot-Laufwerk durch eine Storage-Übernahme oder -Rückgabe unterbrochen, kann es sein, dass der Binärstandort des Grid oder das gesamte Betriebssystem vorübergehend nicht verfügbar ist. Die Zeit, die ONTAP bis zum Abschluss des Storage-Vorgangs und zum Ändern von Pfaden und zum Fortsetzen der I/O benötigt, kann größer sein als die<block ref="7c13d022c9fda6e792c4349a043a6c74" prefix=" " category="inline-code"></block> Schwellenwert. Infolgedessen wird ein Node sofort entfernt, nachdem die Verbindung zur Boot-LUN oder zu den Grid-Binärdateien wiederhergestellt wurde. In den meisten Fällen werden Entfernung und anschließende Neustarts ohne Protokollmeldungen durchgeführt, um den Grund für das Neubooten zu geben. Da nicht alle Konfigurationen betroffen sind, sollten Sie jeden SAN-Booting, NFS-Booting oder Datastore-basierten Host in einer RAC-Umgebung testen, damit RAC stabil bleibt, wenn die Kommunikation zum Startlaufwerk unterbrochen wird.</block>
  <block id="2b875dbca82565f3d7e2fa51633bc2c9" category="paragraph">Bei nicht-lokalen Startlaufwerken oder einem nicht lokalen Dateisystem, das hostet<block ref="ff4a008470319a22d9cf3d14af485977" prefix=" " category="inline-code"></block> Binärdateien, die<block ref="7c13d022c9fda6e792c4349a043a6c74" prefix=" " category="inline-code"></block> Muss entsprechend geändert werden<block ref="0494651671c3dc7f1ccdf99a4368a2cf" prefix=" " category="inline-code"></block>. Wenn dieser Parameter geändert wird, führen Sie weitere Tests durch, um auch alle Auswirkungen auf das RAC-Verhalten zu identifizieren, z. B. die Node-Failover-Zeit.</block>
  <block id="08d7a86ed9292993deaa28bee18d5ce5" category="list-text">Verlassen Sie den<block ref="7c13d022c9fda6e792c4349a043a6c74" prefix=" " category="inline-code"></block> Parameter mit dem Standardwert 30, sofern keine der folgenden Bedingungen zutrifft:</block>
  <block id="5b46e861366ffd98cd94c26abf2bf550" category="list-text"><block ref="ff4a008470319a22d9cf3d14af485977" prefix="" category="inline-code"></block> Binärdateien befinden sich auf einem Network-Attached-Laufwerk, einschließlich NFS-, iSCSI-, FC- und Datastore-basierten Laufwerken.</block>
  <block id="7ace6d79e63cb29c86093526fbddf845" category="list-text">Das Betriebssystem wird über SAN gebootet.</block>
  <block id="985511d6258bbf659d35bd76e0b0cea1" category="list-text">Prüfen Sie in solchen Fällen die Auswirkungen von Netzwerkunterbrechungen, die den Zugriff auf das Betriebssystem oder beeinträchtigen<block ref="9599929f8663b5e2093f7bc5cb20b0c2" prefix=" " category="inline-code"></block> File-Systeme. In einigen Fällen führen solche Unterbrechungen dazu, dass die Oracle RAC-Daemons abgewürgt werden, was zu einem führen kann<block ref="7c13d022c9fda6e792c4349a043a6c74" prefix=" " category="inline-code"></block>-Based Timeout und Entfernung. Die Zeitüberschreitung beträgt standardmäßig 27 Sekunden. Dies ist der Wert von<block ref="7c13d022c9fda6e792c4349a043a6c74" prefix=" " category="inline-code"></block> Minus<block ref="c9333b0a26b9f9e9fd808e6238d613b0" prefix=" " category="inline-code"></block>. In solchen Fällen erhöhen<block ref="7c13d022c9fda6e792c4349a043a6c74" prefix=" " category="inline-code"></block> Bis 200, um zu entsprechen<block ref="0494651671c3dc7f1ccdf99a4368a2cf" prefix=" " category="inline-code"></block>.</block>
  <block id="a3f7c31b8d967e0ac9f9dd30fc81c061" category="paragraph">Der Oracle-Initialisierungsparameter<block ref="f1110589be196c2310808482067fce1b" prefix=" " category="inline-code"></block> Steuert die Verwendung von asynchronem und direktem I/O.</block>
  <block id="d1fa3441fd96418a75a509ac2907d644" category="paragraph">Entgegen der allgemeinen Auffassung schließen sich asynchroner und direkter I/O nicht gegenseitig aus. NetApp hat festgestellt, dass dieser Parameter in Kundenumgebungen häufig falsch konfiguriert ist und dass diese Fehlkonfiguration direkt für viele Performance-Probleme verantwortlich ist.</block>
  <block id="0775b72ddf51e2a16e466ee2df8f75ab" category="paragraph">Asynchroner I/O bedeutet, dass Oracle-I/O-Vorgänge parallelisiert werden können. Bevor asynchroner I/O auf verschiedenen Betriebssystemen verfügbar war, konfigurierten Anwender zahlreiche dbwriter-Prozesse und änderten die Serverprozesskonfiguration. Bei asynchronem I/O führt das Betriebssystem selbst I/O im Auftrag der Datenbanksoftware hocheffizient und parallel aus. Dieser Prozess gefährdet keine Daten, und kritische Vorgänge wie die Oracle-Wiederherstellungsprotokollierung werden weiterhin synchron ausgeführt.</block>
  <block id="6b72950206966c1983e559eb6886ec69" category="paragraph">Direkter I/O umgeht den Puffercache des Betriebssystems. I/O auf einem UNIX-System durchläuft normalerweise den Puffercache des Betriebssystems. Dies ist nützlich für Applikationen, die keinen internen Cache verwalten, aber Oracle hat einen eigenen Puffer-Cache innerhalb des SGA. In fast allen Fällen ist es besser, direkten I/O zu ermöglichen und dem SGA Server-RAM zuzuweisen, anstatt sich auf den Puffercache des Betriebssystems zu verlassen. Oracle SGA nutzt den Speicher effizienter. Wenn I/O den Puffer des Betriebssystems durchläuft, finden weitere Verarbeitungsschritte statt, wodurch die Latenzen erhöht werden. Die erhöhten Latenzen sind besonders bei umfangreichen I/O-Schreibvorgängen spürbar, bei denen eine niedrige Latenz eine wichtige Anforderung ist.</block>
  <block id="d8590a940a5bb30d8a845f4359f3c779" category="paragraph">Die Optionen für<block ref="f1110589be196c2310808482067fce1b" prefix=" " category="inline-code"></block> Sind:</block>
  <block id="0aea10c6719256027228a859db38f291" category="list-text">*Async.* Oracle sendet I/O-Anfragen zur Verarbeitung an das Betriebssystem. Mit diesem Prozess kann Oracle andere Aufgaben ausführen, anstatt auf den I/O-Abschluss zu warten. Dadurch wird die I/O-Parallelisierung erhöht.</block>
  <block id="00cb0cc309682c42b3b936a19a042434" category="list-text">*Directio.* Oracle führt I/O direkt auf physische Dateien aus, anstatt I/O über den Host-BS-Cache zu leiten.</block>
  <block id="f9941e1b6cc84711e95db0e757efc36a" category="list-text">*None.* Oracle verwendet synchrone und gepufferte I/O. In dieser Konfiguration ist die Wahl zwischen Shared-Server- und dedizierten Server-Prozessen und der Anzahl der dbwriters wichtiger.</block>
  <block id="f0939ff6231565c83c5c8c9ce1eddc9d" category="list-text">*setall.* Oracle verwendet sowohl asynchrone als auch direkte I/O. In fast allen Fällen, die Verwendung von<block ref="aef8f91b1e026cc1bb55e8b08c491d35" prefix=" " category="inline-code"></block> Ist optimal.</block>
  <block id="8647b2eaf101a9425f69e6f2e67aeb9d" category="admonition">Der<block ref="f1110589be196c2310808482067fce1b" prefix=" " category="inline-code"></block> Parameter hat keine Auswirkungen in DNFS- und ASM-Umgebungen. Die Verwendung von DNFS oder ASM führt automatisch zur Verwendung von asynchronem und direktem I/O.</block>
  <block id="6dd5604aa0be2956f8b1b41c74fa648e" category="paragraph">Einige Kunden sind in der Vergangenheit auf Probleme mit asynchronem I/O gestoßen, insbesondere mit früheren Versionen von Red hat Enterprise Linux 4 (RHEL4). Einige veraltete Ratschläge im Internet deuten immer noch darauf hin, dass asynchrone IO aufgrund veraktueller Informationen vermieden wird. Asynchrone I/O-Vorgänge sind auf allen aktuellen Betriebssystemen stabil. Es gibt keinen Grund, es zu deaktivieren, ohne einen bekannten Fehler mit dem Betriebssystem.</block>
  <block id="1ea377466376b194b2d1bfc3ab2fa4c4" category="paragraph">Wenn in einer Datenbank gepufferte I/O verwendet wurden, könnte ein Wechsel zu direkten I/O auch eine Änderung der SGA-Größe rechtfertigen. Durch die Deaktivierung gepufferter I/O-Vorgänge werden die Performance-Vorteile eliminiert, die der Host-BS-Cache für die Datenbank bietet. Durch das Hinzufügen von RAM zum SGA wird dieses Problem behoben. Das Nettoergebnis sollte eine Verbesserung der I/O-Performance sein.</block>
  <block id="787b3d8b62af05ac3efd1c9ca7a8fc7e" category="paragraph">Obwohl es fast immer besser ist, RAM für Oracle SGA zu verwenden statt für das Zwischenspeichern von BS-Puffern, ist es unter Umständen nicht möglich, den besten Wert zu ermitteln. Es könnte beispielsweise besser sein, gepufferten I/O mit sehr kleinen SGA-Größen auf einem Datenbankserver mit vielen intermittierend aktiven Oracle-Instanzen zu verwenden. Diese Anordnung ermöglicht die flexible Nutzung des verbleibenden freien RAM auf dem Betriebssystem durch alle ausgeführten Datenbankinstanzen. Dies ist eine äußerst ungewöhnliche Situation, die jedoch an einigen Kundenstandorten beobachtet wurde.</block>
  <block id="d4157ab77eb31459bd406aa0f873e20c" category="admonition">*NetApp empfiehlt* Einstellung<block ref="f1110589be196c2310808482067fce1b" prefix=" " category="inline-code"></block> Bis<block ref="aef8f91b1e026cc1bb55e8b08c491d35" prefix=" " category="inline-code"></block>, Aber beachten Sie, dass unter bestimmten Umständen der Verlust des Host-Puffer-Caches eine Erhöhung des Oracle SGA erfordern kann.</block>
  <block id="f8b42f9efd1a05ede10b0e0c11dfa8b0" category="paragraph">ONTAP verwendet intern eine variable Blockgröße, d. h. Oracle Datenbanken können mit beliebigen Blockgrößen konfiguriert werden. Allerdings können Blockgrößen des Dateisystems die Performance beeinträchtigen und in einigen Fällen kann eine größere Blockgröße für Wiederherstellungen die Performance verbessern.</block>
  <block id="0944aedb4703c4a5e94fb90de9d7a22f" category="section-title">Blockgrößen der Datendatei</block>
  <block id="cc4662c047050e0082701ce6f052dcb7" category="paragraph">Einige Betriebssysteme bieten eine Auswahl an Filesystem-Blockgrößen. Bei Filesystemen, die Oracle Datendateien unterstützen, sollte die Blockgröße bei Verwendung der Komprimierung 8 KB betragen. Wenn keine Komprimierung erforderlich ist, kann eine Blockgröße von 8 KB oder 4 KB verwendet werden.</block>
  <block id="e1a001cf8c7e2072c7a44ee65e8fee11" category="paragraph">Wenn eine Datendatei auf einem Dateisystem mit einem 512-Byte-Block abgelegt wird, sind falsch ausgerichtete Dateien möglich. Die LUN und das Filesystem sind möglicherweise basierend auf Empfehlungen von NetApp richtig ausgerichtet, der Datei-I/O wäre jedoch falsch ausgerichtet. Eine solche Fehlausrichtung würde zu schwerwiegenden Leistungsproblemen führen.</block>
  <block id="9514c8638f798f68108498dd218ec793" category="paragraph">Dateisysteme, die Redo-Protokolle unterstützen, müssen eine Blockgröße verwenden, die ein Vielfaches der Redo-Blockgröße ist. Dies erfordert in der Regel, dass sowohl das Redo-Log-Dateisystem als auch das Redo-Protokoll selbst eine Blockgröße von 512 Byte verwenden.</block>
  <block id="cac7d87a1164763b666ca7d02109d153" category="section-title">Wiederholen Sie die Blockgrößen</block>
  <block id="fa06b9584ea36af182742c07894220f0" category="paragraph">Bei sehr hohen Wiederherstellungsraten ist es möglich, dass 4-KB-Blockgrößen die Performance verbessern, da hohe Wiederherstellungsraten es ermöglichen, I/O in weniger und effizienteren Operationen auszuführen. Wenn Redo-Raten größer als 50 Mbit/s sind, sollten Sie eine 4-KB-Blockgröße testen.</block>
  <block id="781000593b4913b5625dd5ed9eebe87e" category="paragraph">Einige Kundenprobleme wurden mit Datenbanken identifiziert, die Wiederherstellungsprotokolle mit 512-Byte-Blockgröße auf einem Dateisystem mit 4-KB-Blockgröße und vielen sehr kleinen Transaktionen verwenden. Der Mehraufwand, der an der Anwendung mehrerer 512-Byte-Änderungen auf einen einzigen 4-KB-Dateisystemblock beteiligt war, führte zu Performance-Problemen, die behoben wurden, indem das Dateisystem auf eine Blockgröße von 512 Byte geändert wurde.</block>
  <block id="a02416fde36b174cf1196213fcf1bef6" category="admonition">*NetApp empfiehlt*, dass Sie die Größe des Redo-Blocks nicht ändern, es sei denn, Sie werden von einem zuständigen Kundensupport oder einer professionellen Serviceorganisation beraten oder die Änderung basiert auf der offiziellen Produktdokumentation.</block>
  <block id="65a6ece62f52a5e2ff74baaae821ae86" category="paragraph">Disaster Recovery bezieht sich auf die Wiederherstellung von Datenservices nach einem schwerwiegenden Ereignis, beispielsweise durch einen Brand, der ein Storage-System oder sogar einen kompletten Standort zerstört.</block>
  <block id="e168b3b0b2f075f39012df52c5ea3c0a" category="admonition">Diese Dokumentation ersetzt zuvor veröffentlichte technische Berichte _TR-4591: Oracle Data Protection_ und _TR-4592: Oracle on MetroCluster._</block>
  <block id="a91982f09e6a0341a90dae002865ea11" category="paragraph">Disaster Recovery kann durch eine einfache Datenreplizierung mit SnapMirror durchgeführt werden, wobei viele Kunden gespiegelte Replikate natürlich so oft wie stündlich aktualisieren.</block>
  <block id="ad2c5e9521f13e89cd84fdd65ec96c7d" category="paragraph">MetroCluster bezieht sich auf ONTAP in einer Hardwarekonfiguration mit synchron gespiegelten Storage auf niedriger Ebene und zahlreichen zusätzlichen Funktionen. Integrierte Lösungen wie MetroCluster vereinfachen die heutigen komplizierten, horizontal skalierbaren Datenbanken, Applikationen und Virtualisierungsinfrastrukturen. Sie ersetzt mehrere externe Datensicherungsprodukte und -Strategien durch ein einfaches, zentrales Storage-Array. Sie bietet außerdem integriertes Backup, Recovery, Disaster Recovery und Hochverfügbarkeit (HA) in einem einzigen geclusterten Storage-System.</block>
  <block id="4278792a47efb1e599476fe07109dca4" category="inline-link-macro">Oracle RAC auf MetroCluster</block>
  <block id="d12be40db836559c628e8024f5d6ff56" category="paragraph">Um zu verstehen, wie Oracle Datenbanken in einer MetroCluster-Umgebung arbeiten, ist eine Erläuterung des physischen Designs eines MetroCluster-Systems erforderlich.</block>
  <block id="5e33c0d8a2417ce5fb6e7a211ad063bb" category="admonition">Diese Dokumentation ersetzt den zuvor veröffentlichten technischen Bericht _TR-4592: Oracle on MetroCluster._</block>
  <block id="37fa92fc2662529308c62c84eb1b38ea" category="paragraph">Um zu verstehen, wie Oracle-Datenbanken in einer MetroCluster-Umgebung funktionieren, bedarf es einer Erklärung der logischen Funktionalität eines MetroCluster-Systems.</block>
  <block id="2623b589ddbc1cdb6184ba32aa06e83f" category="inline-link-macro">NV-FEHLER</block>
  <block id="a4feacea8b05d0f24e5a4612a164a055" category="paragraph">Die üblichen Best Practices gelten weiterhin, und wenn Ihre Bedürfnisse nur RPO=0 Datensicherung erfordern, wird diese Anforderung mit MetroCluster erfüllt. Die meisten Kunden verwenden MetroCluster jedoch nicht nur für die RPO=0-Datensicherung, sondern auch zur Verbesserung der RTO in Notfallszenarien sowie zur Gewährleistung eines transparenten Failovers im Rahmen der Wartungsarbeiten an den Standorten.</block>
  <block id="3ff8bcf18c694581aa25b0a347508e0f" category="section-title">Failover mit einem vorkonfigurierten Betriebssystem</block>
  <block id="442f34633164f1be348a442b2c62d29d" category="list-text">Erzwingen einer MetroCluster-Umschaltung</block>
  <block id="e081e79ea1b1040c51d4a452e46de76d" category="list-text">Durchführen der Erkennung von FC-LUNs (nur SAN)</block>
  <block id="e60946bf73021e18706a6ac33386b376" category="paragraph">Die eigentliche Aktivierung ist einfach. Befehle wie die LUN-Erkennung erfordern nur einige wenige Befehle pro FC-Port. Das Mounten des Filesystems ist nichts anderes als ein<block ref="19822b1b15d9eefc54c07ab49f87b100" prefix=" " category="inline-code"></block> Befehl, und sowohl Datenbanken als auch ASM können über die CLI mit einem einzigen Befehl gestartet und gestoppt werden. Wenn die Volumes und Dateisysteme vor dem Switchover nicht am Disaster-Recovery-Standort verwendet werden, müssen Sie sie nicht festlegen<block ref="3b93661df04b698b1340ff2da3238d16" prefix=" " category="inline-code"></block> Auf Volumes.</block>
  <block id="486eed21f55b6fb544db5504294592e6" category="section-title">Failover mit einem virtualisierten Betriebssystem</block>
  <block id="bf65e943f86f26b49698bfdac9b95f81" category="paragraph">Der Failover von Datenbankumgebungen kann auf das Betriebssystem selbst erweitert werden. In der Theorie kann dieses Failover mit Boot-LUNs durchgeführt werden, meistens erfolgt es jedoch mit einem virtualisierten Betriebssystem. Das Verfahren ähnelt den folgenden Schritten:</block>
  <block id="7c37589384e83d2b43bba430c18a45aa" category="list-text">Mounten der Datenspeicher, die die virtuellen Maschinen des Datenbankservers hosten</block>
  <block id="3d4d04d6cdab90b7ca0deb0344f04eec" category="list-text">Starten der virtuellen Maschinen</block>
  <block id="c702c387f07bcb4f18482e68d69a155a" category="paragraph">Die Grundlage für die Oracle Datensicherung mit einem MetroCluster System ist SyncMirror, eine Technologie für die synchrone Spiegelung, die maximale Performance und horizontale Skalierbarkeit bietet.</block>
  <block id="9028b52caaa7bdbfb9be779eeb1fe00e" category="summary">Oracle Extended RAC mit MetroCluster</block>
  <block id="4d4d8a609e5b2ac58e622eb1e90f9e39" category="paragraph">Viele Kunden optimieren ihre RTO, indem sie einen Oracle RAC Cluster über mehrere Standorte verteilen und damit eine vollständig aktiv/aktiv-Konfiguration erzielen. Das gesamte Design wird komplizierter, da es die Quorumverwaltung von Oracle RAC beinhalten muss. Außerdem erfolgt der Datenzugriff von beiden Standorten aus. Ein forcierter Switchover kann dazu führen, dass eine veraltete Kopie der Daten verwendet wird.</block>
  <block id="89ffaa1a7d089080e9a1ce912bb8c2d3" category="paragraph">Obwohl eine Kopie der Daten auf beiden Standorten vorhanden ist, kann nur der Controller, der derzeit Eigentümer eines Aggregats ist, Daten bereitstellen. Daher müssen bei erweiterten RAC-Clustern die Remote-Knoten I/O über eine Site-to-Site-Verbindung durchführen. Es kommt zu zusätzlicher I/O-Latenz, aber diese Latenz ist im Allgemeinen kein Problem. Das RAC Interconnect-Netzwerk muss auch über mehrere Standorte verteilt sein, was bedeutet, dass ohnehin ein High-Speed-Netzwerk mit niedriger Latenz erforderlich ist. Falls die zusätzliche Latenz ein Problem verursacht, kann das Cluster aktiv/Passiv betrieben werden. I/O-intensive Vorgänge müssten dann zu den RAC-Knoten geleitet werden, die lokal zu dem Controller sind, der die Aggregate besitzt. Die Remote-Knoten führen dann weniger I/O-Vorgänge aus oder werden ausschließlich als Warm-Standby-Server verwendet.</block>
  <block id="f441d5fdc388f3419fb87e071534f3ce" category="inline-link-macro">Oracle RAC mit ONTAP</block>
  <block id="58285fb769eb7fb05ab12d9e0efb3e50" category="section-title">Konfiguration an zwei Standorten</block>
  <block id="af1184f29d838fbfec8d5b5bf5225f66" category="paragraph">Eine erweiterte RAC-Konfiguration mit zwei Standorten kann aktiv/aktiv-Datenbankservices bereitstellen, die viele, aber nicht alle Ausfallszenarien unterbrechungsfrei überstehen.</block>
  <block id="7544489c7854fcae445508015240a865" category="section-title">RAC-Abstimmungsdateien</block>
  <block id="8b6e46422f88dab6ea82f2dbcadfc76b" category="paragraph">Die erste Überlegung bei der Implementierung von Extended RAC auf MetroCluster sollte das Quorum-Management sein. Oracle RAC verfügt über zwei Mechanismen zur Verwaltung des Quorums: Disk Heartbeat und Netzwerk Heartbeat. Der Disk Heartbeat überwacht den Speicherzugriff mithilfe der Abstimmungsdateien. Bei einer RAC-Konfiguration an einem Standort ist eine einzelne Abstimmressource ausreichend, solange das zugrunde liegende Storage-System HA-Funktionen bietet.</block>
  <block id="fafd618b82d1827eeb8197e1c38722bc" category="paragraph">In früheren Versionen von Oracle wurden die Abstimmungsdateien auf physischen Speichergeräten abgelegt, aber in aktuellen Versionen von Oracle werden die Abstimmungsdateien in ASM-Diskgroups gespeichert.</block>
  <block id="aeab2391a07321ac474989b9c9047226" category="admonition">Oracle RAC wird von NFS unterstützt. Während der Grid-Installation wird eine Reihe von ASM-Prozessen erstellt, um den für Grid-Dateien verwendeten NFS-Speicherort als ASM-Diskgruppe darzustellen. Der Prozess ist für den Endbenutzer nahezu transparent und erfordert nach Abschluss der Installation keine laufende ASM-Verwaltung.</block>
  <block id="42aca4aae768d5ec1cb0c50d41b7b8b7" category="paragraph">In einer Konfiguration mit zwei Standorten ist es als erstes erforderlich, sicherzustellen, dass jeder Standort immer auf mehr als die Hälfte der Abstimmungsdateien zugreifen kann und so einen unterbrechungsfreien Disaster Recovery-Prozess garantiert. Diese Aufgabe war einfach, bevor die Abstimmungsdateien in ASM-Diskgroups gespeichert wurden, aber heute müssen Administratoren grundlegende Prinzipien der ASM-Redundanz verstehen.</block>
  <block id="60a723853bb2c173648452baf2199e73" category="paragraph">ASM-Diskgruppen haben drei Optionen für Redundanz<block ref="6a21b6995a068148bbb65c8f949b3fb2" prefix=" " category="inline-code"></block>,<block ref="fea087517c26fadd409bd4b9dc642555" prefix=" " category="inline-code"></block>, und<block ref="8d966b2253a917086c8604959e152243" prefix=" " category="inline-code"></block>. Mit anderen Worten: Nicht gespiegelt, gespiegelt und 3-fach gespiegelt. Eine neuere Option namens<block ref="09d2bd168181f1e64c2b1651a80a8aa8" prefix=" " category="inline-code"></block> Ist auch verfügbar, aber nur selten verwendet. Die Redundanzstufe und die Platzierung der redundanten Geräte steuern, was in Ausfallszenarien geschieht. Beispiel:</block>
  <block id="32cdeb2c0245b51a0b590e6054f03c21" category="list-text">Platzieren der Abstimmungsdateien auf einem<block ref="d4f266c5956ee60df748738c483f3dc0" prefix=" " category="inline-code"></block> Mit<block ref="6a21b6995a068148bbb65c8f949b3fb2" prefix=" " category="inline-code"></block> Bei Ausfall der Verbindung zwischen den Standorten wird durch Redundanzressource die Entfernung eines Standorts garantiert.</block>
  <block id="a549b4f1fc0c2d14bc333c70bbde73dc" category="list-text">Platzieren der Abstimmungsdateien auf einem<block ref="d4f266c5956ee60df748738c483f3dc0" prefix=" " category="inline-code"></block> Mit<block ref="fea087517c26fadd409bd4b9dc642555" prefix=" " category="inline-code"></block> Redundanz mit nur einer ASM-Festplatte pro Standort garantiert die Entfernung von Knoten auf beiden Standorten, wenn die Verbindung zwischen Standorten verloren geht, da keiner der Standorte ein mehrheitlich Quorum hätte.</block>
  <block id="c15f1c43a368db0ab3dde90eca52582d" category="list-text">Platzieren der Abstimmungsdateien auf einem<block ref="d4f266c5956ee60df748738c483f3dc0" prefix=" " category="inline-code"></block> Mit<block ref="8d966b2253a917086c8604959e152243" prefix=" " category="inline-code"></block> Redundanz mit zwei Festplatten an einem Standort und einer einzigen Festplatte am anderen Standort ermöglicht aktiv-aktiv-Vorgänge, wenn beide Standorte betriebsbereit sind und beide Seiten miteinander erreichbar sind. Wenn der Standort mit einer Festplatte jedoch vom Netzwerk isoliert ist, wird dieser Standort entfernt.</block>
  <block id="478ced47a1ea1d6f0db02af749bfdaec" category="section-title">RAC-Netzwerk-Heartbeat</block>
  <block id="2432333dc52d6150fb3d58051bc8b0c2" category="paragraph">Der Heartbeat des Oracle RAC-Netzwerks überwacht die Erreichbarkeit des Knotens über den Cluster-Interconnect hinweg. Damit ein Node im Cluster verbleiben kann, muss er sich mit mehr als der Hälfte der anderen Nodes in Verbindung setzen können. In einer Architektur mit zwei Standorten werden folgende Auswahlmöglichkeiten für die Anzahl der RAC-Knoten erstellt:</block>
  <block id="3e36a59c8a51ee9cc2cba0e7051ed35f" category="list-text">Die Platzierung einer gleichen Anzahl von Nodes pro Standort führt zu einer Entfernung an einem Standort, falls die Netzwerkverbindung unterbrochen wird.</block>
  <block id="4955aede919912b3c62fadbc1039671b" category="list-text">Die Platzierung von N Nodes auf einem Standort und N+1 Nodes auf dem anderen Standort garantiert, dass der Verlust der Verbindung zwischen den Standorten zu einer größeren Anzahl von Knoten führt, die im Netzwerk-Quorum verbleiben, und zu einem Standort mit weniger Knoten.</block>
  <block id="80bd651a66488201c928634689e2e5cc" category="paragraph">Vor der Einführung von Oracle 12cR2 war es nicht praktikabel zu kontrollieren, auf welcher Seite bei einem Standortausfall eine Entfernung auftreten würde. Wenn jeder Standort über eine gleiche Anzahl von Knoten verfügt, wird die Entfernung vom Master-Knoten gesteuert, der im Allgemeinen der erste RAC-Knoten ist, der gestartet wird.</block>
  <block id="d128c05245b2e633856a5061db176b7e" category="paragraph">Oracle 12cR2 bietet Funktionen zur Knotengewichtung. Diese Funktion gibt einem Administrator mehr Kontrolle darüber, wie Oracle Split-Brain-Bedingungen löst. Der folgende Befehl legt als einfaches Beispiel die Präferenz für einen bestimmten Knoten in einem RAC fest:</block>
  <block id="104c5747f0d9dbb47b0d09953c709a8d" category="paragraph">Nach dem Neustart von Oracle High-Availability Services sieht die Konfiguration wie folgt aus:</block>
  <block id="693cce334ab6658c73ea67674775156b" category="paragraph">Knoten<block ref="a31853cdea7badfc68b560e70cbbeb02" prefix=" " category="inline-code"></block> Ist jetzt als kritischer Server festgelegt. Wenn die beiden RAC-Knoten isoliert sind,<block ref="a31853cdea7badfc68b560e70cbbeb02" prefix=" " category="inline-code"></block> Überlebt, und<block ref="bf38e7432ba60433c4c8967fc8da0258" prefix=" " category="inline-code"></block> Wird entfernt.</block>
  <block id="386839b09451962d03e26b7bbfabce74" category="admonition">Ausführliche Informationen finden Sie im Oracle Whitepaper „Oracle Clusterware 12c Release 2 Technical Overview“. „</block>
  <block id="11f65689342acc8f71c8014eeb7945ee" category="paragraph">Bei Versionen von Oracle RAC vor 12cR2 kann der Master-Knoten identifiziert werden, indem die CRS-Protokolle wie folgt geprüft werden:</block>
  <block id="c8b8cd80b31dac17fb85459c341042bd" category="paragraph">Dieses Protokoll gibt an, dass der Master-Node ist<block ref="c81e728d9d4c2f636f067f89cc14862c" prefix=" " category="inline-code"></block> Und dem Knoten<block ref="a31853cdea7badfc68b560e70cbbeb02" prefix=" " category="inline-code"></block> Hat eine ID von<block ref="c4ca4238a0b923820dcc509a6f75849b" prefix=" " category="inline-code"></block>. Diese Tatsache bedeutet das<block ref="a31853cdea7badfc68b560e70cbbeb02" prefix=" " category="inline-code"></block> Ist nicht der Master-Knoten. Die Identität des Master-Knotens kann mit dem Befehl bestätigt werden<block ref="08e7c60bbf8289a563b25cc033d431f7" prefix=" " category="inline-code"></block>.</block>
  <block id="fdf6179b59d3f4873b042609a5f09bc4" category="paragraph">Der Knoten mit der ID von<block ref="c81e728d9d4c2f636f067f89cc14862c" prefix=" " category="inline-code"></block> Ist<block ref="bf38e7432ba60433c4c8967fc8da0258" prefix=" " category="inline-code"></block>, Das ist der Master-Knoten. In einer Konfiguration mit gleicher Anzahl von Knoten an jedem Standort, der Standort mit<block ref="bf38e7432ba60433c4c8967fc8da0258" prefix=" " category="inline-code"></block> Ist der Standort, der überlebt, wenn die beiden Sets aus irgendeinem Grund die Netzwerkverbindung verlieren.</block>
  <block id="f2932b43ad604fa01d57212558608ed6" category="paragraph">Der Protokolleintrag, der den Master-Knoten identifiziert, kann möglicherweise aus dem System altern. In diesem Fall können die Zeitstempel der Oracle Cluster Registry (OCR) Backups verwendet werden.</block>
  <block id="50cd29958db0c1dcb6505d470f553e54" category="paragraph">Dieses Beispiel zeigt, dass der Master-Knoten ist<block ref="bf38e7432ba60433c4c8967fc8da0258" prefix=" " category="inline-code"></block>. Sie zeigt auch eine Änderung im Master-Knoten von an<block ref="a31853cdea7badfc68b560e70cbbeb02" prefix=" " category="inline-code"></block> Bis<block ref="bf38e7432ba60433c4c8967fc8da0258" prefix=" " category="inline-code"></block> Am 4. Mai zwischen 2:05 und 21:39 Uhr. Diese Methode zur Identifizierung des Master-Knotens ist nur dann sicher zu verwenden, wenn die CRS-Protokolle ebenfalls geprüft wurden, da sich der Master-Knoten möglicherweise seit der vorherigen OCR-Sicherung geändert hat. Wenn diese Änderung stattgefunden hat, sollte sie in den OCR-Protokollen sichtbar sein.</block>
  <block id="df044a3c3d7f7f244db7c42b347b9a90" category="paragraph">Die meisten Kunden wählen eine einzelne Abstimmdiskette, die die gesamte Umgebung und eine gleiche Anzahl von RAC-Knoten an jedem Standort unterstützt. Die Datenträgergruppe sollte auf dem Standort platziert werden, der die Datenbank enthält. Das Ergebnis ist, dass der Verlust der Verbindung zu einer Entfernung am Remote-Standort führt. Der Remote-Standort hätte weder Quorum noch würde er Zugriff auf die Datenbankdateien haben, aber der lokale Standort läuft weiterhin wie gewohnt. Wenn die Konnektivität wiederhergestellt ist, kann die Remote-Instanz wieder online geschaltet werden.</block>
  <block id="84a24276bfdfbe485b9d961c1a57a830" category="paragraph">Bei einem Notfall ist eine Umschaltung erforderlich, um die Datenbankdateien und die abstimmende Diskgruppe am verbleibenden Standort online zu schalten. Wenn AUSO die Umschaltung auslösen kann, wird das NVFAIL nicht ausgelöst, da bekannt ist, dass das Cluster synchron ist und die Speicherressourcen ordnungsgemäß online gehen. AUSO ist ein sehr schneller Vorgang und sollte vor dem abgeschlossen werden<block ref="0494651671c3dc7f1ccdf99a4368a2cf" prefix=" " category="inline-code"></block> Zeitraum läuft ab.</block>
  <block id="6244ee13e435dcc768edbc9b0b38b73e" category="paragraph">Da es nur zwei Standorte gibt, ist es nicht möglich, eine automatisierte externe Tiebreaking-Software zu verwenden, was bedeutet, dass die erzwungene Umschaltung eine manuelle Operation sein muss.</block>
  <block id="568c8c12fdb7c74f65125732ee717ad3" category="section-title">Konfigurationen mit drei Standorten</block>
  <block id="2c7696fb04f52eb5687d059415066fa7" category="paragraph">Ein erweiterter RAC-Cluster lässt sich mit drei Standorten viel einfacher erstellen. Die beiden Standorte, die jeweils die Hälfte des MetroCluster Systems hosten, unterstützen auch die Datenbank-Workloads, während der dritte Standort als Tiebreaker für die Datenbank und das MetroCluster System dient. Die Oracle Tiebreaker-Konfiguration kann so einfach sein, als ob ein Mitglied der ASM-Diskgroup, die für die Abstimmung an einem dritten Standort verwendet wird, platziert werden könnte, und kann auch eine Betriebsinstanz am dritten Standort enthalten, um sicherzustellen, dass es eine ungerade Anzahl von Knoten im RAC-Cluster gibt.</block>
  <block id="6265611ca77e9ad52249c5fdd4780da9" category="admonition">Wichtige Informationen zur Verwendung von NFS in einer erweiterten RAC-Konfiguration finden Sie in der Oracle Dokumentation zum Thema „Quorum-Fehlergruppe“. Zusammenfassend kann es sein, dass die NFS-Mount-Optionen geändert werden müssen, um sicherzustellen, dass der Verlust der Verbindung zum dritten Standort, der Quorumressourcen hostet, nicht die primären Oracle-Server oder Oracle RAC-Prozesse hängt.</block>
  <block id="d84d0eea463e72a6e15018c70a45af46" category="doc">Oracle, MetroCluster und NVFAIL</block>
  <block id="9dc6a9986107298da2a22e7c5a3efb00" category="summary">Einzelne Oracle Instanz auf MetroCluster</block>
  <block id="f8b28e31633ae72c2971ee8b815ed4af" category="paragraph">Wie bereits erwähnt, trägt das Vorhandensein eines MetroCluster-Systems nicht notwendigerweise zur Ergänzung oder Änderung von Best Practices für den Betrieb einer Datenbank bei. Bei den meisten Datenbanken, die derzeit auf MetroCluster Kundensystemen ausgeführt werden, handelt es sich um eine Einzelinstanz, und befolgen Sie die Empfehlungen in der Dokumentation zu Oracle auf ONTAP.</block>
  <block id="f4872c843923dd8d8731ef66462b5a99" category="paragraph">SyncMirror liefert eine synchrone Kopie der Daten am Disaster Recovery-Standort. Um diese Daten verfügbar zu machen, sind jedoch ein Betriebssystem und die zugehörigen Applikationen erforderlich. Eine grundlegende Automatisierung kann die Failover-Zeit der gesamten Umgebung deutlich verbessern. Clusterware Produkte wie Veritas Cluster Server (VCS) werden oft verwendet, um einen Cluster standortübergreifend zu erstellen, in vielen Fällen kann der Failover-Prozess mit einfachen Skripten angetrieben werden.</block>
  <block id="c50820a41eb4f23d0a3acc1eeddb391b" category="paragraph">Wenn die primären Knoten verloren gehen, ist die Clusterware (oder Skripte) so konfiguriert, dass die Datenbanken am alternativen Standort online geschaltet werden. Eine Option besteht darin, Standby-Server zu erstellen, die für die NFS- oder SAN-Ressourcen, aus denen die Datenbank besteht, vorkonfiguriert sind. Wenn der primäre Standort ausfällt, führt die Clusterware- oder skriptbasierte Alternative eine Abfolge von Aktionen durch, die der folgenden ähneln:</block>
  <block id="2a6f458321ddf666cb0c876ee39255f3" category="list-text">Mounten von Dateisystemen und/oder Mounten von ASM-Datenträgergruppen</block>
  <block id="3dee93bd9bc9b8b50dcdd2066a86009f" category="list-text">Die Datenbank wird gestartet</block>
  <block id="620515885465a26daea6cc6441403e34" category="paragraph">Die primäre Anforderung dieses Ansatzes ist ein Betriebssystem, das am Remote Standort ausgeführt wird. Sie muss mit Oracle-Binärdateien vorkonfiguriert sein, was auch bedeutet, dass Aufgaben wie das Patching von Oracle am primären Standort und am Standby-Standort durchgeführt werden müssen. Alternativ können die Oracle Binärdateien auf den Remote-Standort gespiegelt und gemountet werden, wenn ein Notfall deklariert wird.</block>
  <block id="edb5332feef69c0aa60ea10ed5d31a9f" category="list-text">Manuelles Starten von Datenbanken oder Konfigurieren der virtuellen Maschinen, um die Datenbanken automatisch zu starten, z. B. kann ein ESX-Cluster mehrere Standorte umfassen. Bei einem Notfall können die Virtual Machines nach dem Switchover am Disaster Recovery-Standort online geschaltet werden. Solange die Datastores, die die virtualisierten Datenbankserver hosten, zum Zeitpunkt des Ausfalls nicht verwendet werden, ist keine Einstellung erforderlich<block ref="3b93661df04b698b1340ff2da3238d16" prefix=" " category="inline-code"></block> Auf zugeordneten Volumes.</block>
  <block id="64bba30b717cb45555458da4fed880a1" category="paragraph">Nahezu jede Applikation erfordert Datenreplizierung.</block>
  <block id="25fb1fa02f326334533a22c105ad3e69" category="paragraph">Auf der einfachsten Ebene kann Replikation eine Kopie auf einem externen Band oder eine Replikation auf Anwendungsebene an einem Standby-Standort bedeuten. Bei Disaster Recovery werden diese Replikatkopien verwendet, um bei einem katastrophalen Serviceausfall einen Service online zu schalten.</block>
  <block id="eab6cb3a1168ef2311865e6155b8272c" category="paragraph">ONTAP bietet mehrere Replizierungsoptionen, um eine Vielzahl von Anforderungen nativ innerhalb des Storage Array zu erfüllen und so ein breites Spektrum an Anforderungen abzudecken. Diese Optionen können die einfache Replizierung von Backups an einen Remote-Standort bis hin zu einer synchronen, voll automatisierten Lösung umfassen, die sowohl Disaster Recovery als auch Hochverfügbarkeit auf derselben Plattform bietet.</block>
  <block id="32df0c1cabf839652741a56bdaac7def" category="paragraph">Die primären ONTAP Replizierungstechnologien, die für Applikationen anwendbar sind, sind NetApp SnapMirror und NetApp SyncMirror. Es handelt sich nicht um Add-on-Produkte, sondern sie sind vollständig in ONTAP integriert und werden durch das einfache Hinzufügen eines Lizenzschlüssels aktiviert. Auch die Replizierung auf Storage-Ebene ist nicht die einzige Option. Replizierung auf Applikationsebene wie Oracle DataGuard kann ebenfalls in eine auf ONTAP basierende Datensicherungsstrategie integriert werden.</block>
  <block id="a2259fa074bae962e46ca7fc7aa31be1" category="paragraph">Die richtige Wahl hängt von den spezifischen Replizierungs-, Recovery- und Aufbewahrungsanforderungen ab.</block>
  <block id="0d3b37ff5855bf299f3c4a154373c1e2" category="section-title">ONTAP SnapMirror</block>
  <block id="296e89e943f5160d338484b3b705a44e" category="paragraph">SnapMirror ist die asynchrone Replizierungslösung von NetApp und eignet sich ideal für die Sicherung großer, komplizierter und dynamischer Datensätze wie Datenbanken und zugehöriger Applikationen. Die wichtigsten Werte lauten wie folgt:</block>
  <block id="d9f36dae2818ace926c4259271530724" category="list-text">*Manageability.* SnapMirror ist einfach zu konfigurieren und zu verwalten, da es ein nativer Teil der Speichersoftware ist. Es sind keine Add-on-Produkte erforderlich. Replizierungsbeziehungen lassen sich innerhalb von Minuten einrichten und direkt auf dem Storage-System managen.</block>
  <block id="8f761eff026903aa0140319f597d1fb0" category="list-text">*Einfachheit.* die Replikation basiert auf FlexVol-Volumes, die Container von LUNs oder Dateien sind, die als einheitliche konsistente Gruppe repliziert werden.</block>
  <block id="8b737d6a9db3de6436966db508cb51d4" category="list-text">*Effizienz.* Nachdem die erste Replikationsbeziehung hergestellt ist, werden nur Änderungen repliziert. Darüber hinaus bleiben Effizienzfunktionen wie Deduplizierung und Komprimierung erhalten und verringern die zu übertragende Datenmenge an einen Remote-Standort weiter.</block>
  <block id="0783dadb67ff77ed26bbfc5d80fa0cbd" category="list-text">*Flexibilität.* Spiegelungen können vorübergehend unterbrochen werden, um das Testen von Disaster-Recovery-Verfahren zu ermöglichen. Anschließend kann die Spiegelung problemlos wiederhergestellt werden, ohne dass eine vollständige Neuspiegelung erforderlich ist. Nur die geänderten Daten müssen angewendet werden, um die Spiegelungen wieder zu synchronisieren. Die Spiegelung kann auch umgekehrt werden, um eine schnelle Resynchronisierung nach dem Zwischenfall und nach der Rückkehr des ursprünglichen Standorts möglich zu machen. Schließlich stehen Lese- und Schreibklone replizierter Daten für Test und Entwicklung zur Verfügung.</block>
  <block id="8a00f09d8937e890f2cfb4d7acf72733" category="paragraph">ONTAP bietet zwar mehrere verschiedene Replizierungstechnologien, die größte Flexibilität ist jedoch SnapMirror, eine asynchrone Spiegelungsoption von Volume zu Volume.</block>
  <block id="80c8fae3d0b5dd100cc984d9ba3eced7" category="paragraph">Wie bereits erwähnt, ist ein FlexVol Volume die grundlegende Managementeinheit für Snapshot-basierte Backups und SnapRestore-basierte Recovery. Ein FlexVol Volume ist außerdem die Basiseinheit für die SnapMirror-basierte Replizierung. Der erste Schritt ist die Erstellung der Basisspiegelung des Quell-Volume auf das Ziel-Volume. Nach der Initialisierung dieser Spiegelbeziehung basieren alle nachfolgenden Vorgänge allein auf der Replikation der geänderten Daten.</block>
  <block id="c1020b1755f65f23643158ce6a4d22cc" category="paragraph">Aus Sicht der Recovery gelten die folgenden zentralen Werte für SnapMirror:</block>
  <block id="50bde20b37cd4d57fbff2b95f2f8ec97" category="list-text">SnapMirror Vorgänge lassen sich leicht verstehen und einfach automatisieren.</block>
  <block id="66162be45733f65fee4abb22e989eb78" category="list-text">Bei einer einfachen Aktualisierung eines SnapMirror Replikats müssen nur die Delta-Änderungen repliziert werden. Dies reduziert die Bandbreitenanforderungen und ermöglicht häufigere Updates.</block>
  <block id="a3b4fbe416387a4991ccaa8b325a68be" category="list-text">SnapMirror bietet hohe Granularität. Er basiert auf einfachen Volume-to-Volume-Beziehungen und ermöglicht die Erstellung von Hunderten von unabhängig gemanagten Replikaten und Replikationsintervallen. Die Replikation muss nicht eine Einheitslösung sein.</block>
  <block id="4e3785883ef3ccad9938d7e35e493057" category="list-text">Die Spiegelungsrichtung kann leicht umgekehrt werden, wobei die Fähigkeit erhalten bleibt, die Beziehung allein auf der Grundlage der Änderungen zu aktualisieren. Dies ermöglicht ein schnelles Failback, nachdem der primäre Standort nach einem Ausfall, wie z. B. einem Stromausfall, wieder in Betrieb genommen wurde. Nur die Änderungen müssen zurück zur Quelle synchronisiert werden.</block>
  <block id="8b8325381cf528eff12c1e678a3112f9" category="list-text">Spiegelungen können einfach beschädigt und effizient neu synchronisiert werden, um eine Wiederholung des Disaster Recovery-Verfahrens zu ermöglichen.</block>
  <block id="b1f45b7d95bfc815351fb649fa94b671" category="list-text">SnapMirror, das im vollen Replizierungsmodus auf Blockebene arbeitet, repliziert nicht nur die Daten eines Volumes, sondern auch die Snapshots. Diese Funktion bietet sowohl eine Kopie der Daten als auch einen vollständigen Satz von Backups am Disaster-Recovery-Standort.</block>
  <block id="ffafd9ae31dee51209961eb2453ac806" category="paragraph">Im versionsflexiblen Modus von SnapMirror können spezifische Snapshots repliziert und unterschiedliche Aufbewahrungszeiten am primären und sekundären Standort ermöglicht werden.</block>
  <block id="974658d7ac018cb945e78365b0a760fb" category="section-title">SnapMirror Synchronous</block>
  <block id="ffbf9d0ce29a631b6eae8b22c41ec95e" category="paragraph">SnapMirror Synchronous (SM-S) ist eine Erweiterung von SnapMirror, die eine synchrone RPO=0-Replizierung bietet. Sie wird besonders häufig in Storage-Architekturen eingesetzt, in denen nur ein Teil der gesamten Daten eine synchrone Spiegelung erfordert.</block>
  <block id="e8a45c4c0391895df78421904703c933" category="paragraph">SM-S kann in zwei leicht unterschiedlichen Modi, Sync und StrictSync, betrieben werden.</block>
  <block id="15a4e16195fb9d30c5df0e11df5c2166" category="paragraph">Im synchronen Modus werden Änderungen repliziert, bevor sie bestätigt werden. Dadurch wird ein RPO von Null garantiert, sofern die Replizierung betriebsbereit ist. Wenn die Änderung nicht repliziert werden kann, kann SM-S den synchronen Modus beenden und den Betrieb fortsetzen. Dies ermöglicht RPO=0 unter normalen Umständen, aber die Datenprozesse werden nicht vollständig angehalten, wenn das Replikationsziel nicht verfügbar ist.</block>
  <block id="115455025d4664c0dd6deaaf3c1fc93c" category="paragraph">StrictSync garantiert ein RPO=0. Ein Fehler beim Replizieren von Änderungen führt zu einem I/O-Fehler, der in der Regel zum Herunterfahren der Applikation führt.</block>
  <block id="1b1ea0d1ad6e5cfb06253a14fcdcfe71" category="inline-link">TR-4733</block>
  <block id="9ccd61d6efc86cc37743fdff385832e8" category="paragraph">Eine vollständige Erläuterung von SM-S finden Sie unter<block ref="4459b84726c651a8047a6486e87e48fb" category="inline-link-rx"></block> Und die offizielle ONTAP-Dokumentation. Die Funktionen werden kontinuierlich mit neuen Versionen von ONTAP ergänzt.</block>
  <block id="6165c4352e1504dbaeef34d0cc50c187" category="section-title">Konsistenzgruppen</block>
  <block id="0651e813e385dbd62a9bca9baee23c6c" category="paragraph">ONTAP ermöglicht die Erstellung von Konsistenzgruppen-Snapshots. Ab 9.13.1 kann ONTAP Gruppen von Volumes (wobei zu beachten ist, dass ein Volume in der ONTAP-Terminologie keine LUN, sondern ein Management-Container aus einer oder mehreren Dateien oder LUNs ist) als konsistente Gruppe replizieren.</block>
  <block id="455c7758003d6ebd974e8858f5ef1d7d" category="paragraph">Das Ergebnis: Sie können einen Datensatz mit mehreren Volumes replizieren und sicherstellen, dass alle Volumes Cross-konsistent sind. Dadurch werden u. a. die Spiegelungen unterbrochen und DR-Vorgänge unterbrochen, ohne dass zusätzliche Schritte zur Applikations- oder Datenbank-Recovery nötig sind.</block>
  <block id="c44a2da164b7b770fce6338a987670ee" category="section-title">MetroCluster und SyncMirror</block>
  <block id="b1a1cfbf3988d8abdfdedaabc1742ae9" category="paragraph">MetroCluster ist außerdem eine Lösung für synchrone Replizierung für umfangreiche geschäftskritische Workloads. Die Replizierung basiert auf SyncMirror. SyncMirror erstellt auf der einfachsten Ebene zwei vollständige Sätze RAID-geschützter Daten an zwei verschiedenen Orten. Sie könnten sich in angrenzenden Räumen innerhalb eines Datacenters oder sogar viele Kilometer voneinander entfernt befinden.</block>
  <block id="89b8adf28db7abb0febecafdca5871ad" category="paragraph">SyncMirror ist vollständig in ONTAP integriert und wird knapp über dem RAID Level ausgeführt. Daher funktionieren alle üblichen ONTAP-Funktionen wie Snapshot Kopien, SnapRestore und NetApp FlexClone nahtlos. Es handelt sich noch immer um ONTAP und umfasst nur eine zusätzliche Ebene der synchronen Datenspiegelung.</block>
  <block id="75c677fa68f5d15c17efbf6fd1fe1765" category="paragraph">Eine Sammlung von ONTAP Controllern, die SyncMirror-Daten managen, wird als NetApp MetroCluster-Konfiguration bezeichnet. Der Hauptzweck von MetroCluster ist es, Hochverfügbarkeit auf synchron gespiegelte Daten in einer Vielzahl von typischen und Disaster-Recovery-Ausfallszenarien zu bieten.</block>
  <block id="af845652f0234e1f71bc37e241ab63dc" category="paragraph">Die Datensicherung mit MetroCluster und SyncMirror hat folgende zentrale Vorteile:</block>
  <block id="9ab80c5e33e343c3415c21be433c4447" category="list-text">Im normalen Betrieb ermöglicht SyncMirror standortübergreifendes, synchrones Spiegeln. Ein Schreibvorgang wird erst dann bestätigt, wenn er auf nicht-flüchtigen Medien an beiden Standorten vorhanden ist.</block>
  <block id="680ccf94624c9f922ed236c8cc0c1af2" category="list-text">Wenn die Verbindung zwischen Standorten ausfällt, wechselt SyncMirror automatisch in den asynchronen Modus, damit der primäre Standort Daten bereitstellt, bis die Verbindung wiederhergestellt ist. Bei einer Wiederherstellung ermöglicht es eine schnelle Neusynchronisierung, indem die am primären Standort angesammelten Änderungen effizient aktualisiert werden. Eine vollständige Neuinitialisierung ist nicht erforderlich.</block>
  <block id="c18ec55908c635b68abe8c6edbf6476e" category="paragraph">SnapMirror ist zudem vollständig mit SyncMirror-basierten Systemen kompatibel. Beispielsweise kann eine primäre Datenbank auf einem MetroCluster Cluster ausgeführt werden, das über zwei geografische Standorte verteilt ist. Diese Datenbank kann Backups auch als langfristige Archive an einem dritten Standort oder zur Erstellung von Klonen in einer DevOps-Umgebung replizieren.</block>
  <block id="3a8111f0b903472ec78531619bf984f6" category="paragraph">Die Replikationsverfahren für eine Oracle-Datenbank sind im Wesentlichen dieselben wie die Backup-Verfahren. Die primäre Anforderung besteht darin, dass die Snapshots, die ein wiederherstellbares Backup darstellen, auf das Remote-Speichersystem repliziert werden müssen.</block>
  <block id="c62b3abab4fdbaf8f0a84e8c3559b5fd" category="paragraph">Wie bereits in der Dokumentation zur lokalen Datensicherheit beschrieben, kann ein wiederherstellbares Backup mit dem Hot-Backup-Prozess oder mithilfe von Snapshot-optimierten Backups erstellt werden.</block>
  <block id="3d8f8c6a5c90bcd85a22fbd17390a864" category="inline-link-macro">Datenlayout</block>
  <block id="f3f227cc5c07a040d61c36679053a58a" category="paragraph">Angenommen, die Datendateien sind in dedizierten Volumes eingekapselt. Die nächste Frage ist, wie die Wiederherstellungsprotokolle, Archivprotokolle und Steuerdateien gemanagt werden. Am einfachsten ist es, alle diese Datentypen in einem einzelnen Volume zu platzieren. Der Vorteil dabei ist, dass replizierte Wiederherstellungsprotokolle, Archivprotokolle und Steuerdateien perfekt synchronisiert sind. Es besteht keine Notwendigkeit für unvollständige Wiederherstellung oder die Verwendung einer Backup-controlfile, obwohl es wünschenswert sein könnte, auch Skript-Erstellung von Backup-controlfiles für andere potenzielle Recovery-Szenarien.</block>
  <block id="927992cbeccc8211e382c97e81514ef9" category="section-title">Zwei-Volumes-Layout</block>
  <block id="fe1cf96940a710dc95fd30bbb7533b52" category="paragraph">Das einfachste Layout ist in der folgenden Abbildung dargestellt.</block>
  <block id="57f1db4bdf2ce49a98f375d8393f02bf" category="paragraph">Dies ist der häufigste Ansatz. Aus der Perspektive eines DBAs mag es ungewöhnlich erscheinen, alle Kopien der Wiederherstellungs- und Archivprotokolle auf demselben Volume zu vermengen. Allerdings bietet Trennung keinen besonderen Schutz, wenn die Dateien und LUNs sich alle noch auf derselben zugrunde liegenden Laufwerkgruppe befinden.</block>
  <block id="bc46e4831ef5654d084e456c4c544a42" category="section-title">Layout mit drei Volumes</block>
  <block id="f6e969769d9d166122489505d676769b" category="paragraph">Gelegentlich ist eine Trennung von Wiederherstellungsprotokollen erforderlich, da Bedenken hinsichtlich der Datensicherung bestehen oder Redo-Log-I/O über Controller verteilt werden müssen. In diesem Fall wird das in der Abbildung unten abgebildete Layout mit drei Volumes für die Replikation verwendet, während gleichzeitig die Notwendigkeit einer unvollständigen Wiederherstellung vermieden oder auf Backup-Controller-Dateien angewiesen ist.</block>
  <block id="4957c2feaa2cb72d80a0ab65228ad43f" category="paragraph">Dies ermöglicht das Striping der Redo-Protokolle und Steuerdateien über unabhängige Sätze von Spindeln und Controllern auf der Quelle. Die Archivprotokolle und ein Satz von Steuerdateien und Wiederherstellungsprotokollen können jedoch weiterhin in einem synchronisierten Zustand mit den Archivprotokollen repliziert werden.</block>
  <block id="39128da64bc55500b4b11391aa54a011" category="paragraph">In diesem Modell wird das Redo-Protokoll-B-Volume nicht repliziert.</block>
  <block id="c0ce858e99375deac8274c96b7e5d0ee" category="section-title">Disaster-Recovery-Verfahren – Hot-Backups</block>
  <block id="6993729228512b7e23e3ff3ec9099f33" category="paragraph">Gehen Sie wie folgt vor, um eine Disaster Recovery mithilfe von Hot Backups durchzuführen:</block>
  <block id="ee68e5b99222bbc29a480fcb0d1d6ee2" category="section-title">Voraussetzungen</block>
  <block id="41bde9c586a860264c425700c48bd375" category="list-text">Oracle Binärdateien werden auf dem Disaster Recovery Server installiert.</block>
  <block id="cebf7cec81152da2cadffd580d81937f" category="list-text">Datenbankinstanzen sind in aufgeführt<block ref="2c0b0433aa1c96a2be6f40d9e71bc6af" prefix=" " category="inline-code"></block>.</block>
  <block id="17f8e5eda67237f2ca94c7dcce3e4022" category="list-text">Der<block ref="76a2173be6393254e72ffa4d6df1030a" prefix=" " category="inline-code"></block> Und<block ref="6b2f852f7f63f2e5d4be597bbca97bb6" prefix=" " category="inline-code"></block> Oder<block ref="1737629d60b511cf45957529a8f046e2" prefix=" " category="inline-code"></block> Die Instanz muss sich im befinden<block ref="45c7302d9c2e8745b3a2bff0f992b6df" prefix=" " category="inline-code"></block> Verzeichnis. .</block>
  <block id="645a84975202e30700b572b49a5ee29d" category="section-title">Disaster Recovery</block>
  <block id="25d18766964d7515e1f5acd893094f6e" category="list-text">Brechen Sie die Spiegelungen für die Datendateien und das gemeinsame Protokoll-Volume auf.</block>
  <block id="8fc655e4752d521563db3bfa138ba62e" category="list-text">Stellen Sie die Datendatei-Volumes auf den neuesten Hot-Backup-Snapshot der Datendateien wieder her.</block>
  <block id="68a9c75816a16f6d4f7572fe287961bf" category="list-text">Wenn SAN verwendet wird, aktivieren Sie Volume-Gruppen und/oder mounten Sie Dateisysteme.</block>
  <block id="312007510a0f0dc7878d4ed6a7a01554" category="list-text">Geben Sie Archivprotokolle bis zum gewünschten Punkt wieder.</block>
  <block id="e54b793d8bfa83bbb1bd965992c3474e" category="list-text">Wiederholen Sie die aktuellen Wiederherstellungsprotokolle, wenn eine vollständige Wiederherstellung gewünscht wird.</block>
  <block id="72e5734d53581f86093de5c97d8a3a24" category="paragraph">Die Verwendung von NFS vereinfacht diesen Vorgang erheblich, da die NFS-Filesysteme für Datendateien und Protokolldateien jederzeit auf den Disaster Recovery-Server gemountet werden können. Wenn die Spiegelungen beschädigt sind, wird es zu Lesen/Schreiben.</block>
  <block id="de4735bc5bbfdcae3d690313c739ad67" category="section-title">Disaster Recovery-Verfahren – Snapshot optimierte Backups</block>
  <block id="6b2b8ef11e5ded8a083372ba49d488d2" category="paragraph">Die Wiederherstellung von Snapshot-optimierten Backups ist mit dem Hot-Backup-Recovery-Verfahren mit den folgenden Änderungen fast identisch:</block>
  <block id="af1e49e7bc2b25ce36bd93df09433fb4" category="list-text">Stellen Sie die Datendatei-Volumes auf einem Snapshot wieder her, der vor dem aktuellen Replikat des Protokollvolumes erstellt wurde.</block>
  <block id="5e09669c98226c5d3a748f7e0f0333ad" category="paragraph">Diese Unterschiede vereinfachen das gesamte Recovery-Verfahren, da nicht sichergestellt werden muss, dass ein Snapshot ordnungsgemäß auf der Quelle erstellt wurde, während sich die Datenbank im Hot Backup-Modus befand. Das Disaster-Recovery-Verfahren basiert auf den Zeitstempeln der Snapshots auf dem Disaster-Recovery-Standort. Der Zustand der Datenbank, zu dem die Snapshots erstellt wurden, ist nicht wichtig.</block>
  <block id="81d6beef7eaf28bbce692d4b59888f3e" category="section-title">Disaster Recovery mit Hot-Backup-Snapshots</block>
  <block id="dc711b77f0d6a9c9ccf88b424509440a" category="paragraph">Dies ist ein Beispiel für eine Disaster-Recovery-Strategie, die auf der Replizierung von Hot-Backup-Snapshots basiert. Er ist auch ein Beispiel für eine einfache und skalierbare lokale Backup-Strategie.</block>
  <block id="edaef18e1c956b9284405fd664c4ca4a" category="paragraph">Die Beispieldatenbank befindet sich in einer grundlegenden Architektur mit zwei Volumes.<block ref="f006bf17b06c884c47190c4225d3215a" prefix=" " category="inline-code"></block> Enthält Datendateien und<block ref="894db1fc1592356880976f1b92c3e6a2" prefix=" " category="inline-code"></block> Wird für kombinierte Redo-Logs, Archivprotokolle und Steuerdateien verwendet.</block>
  <block id="fc0e7b6cd7cd15929987b1429d1c3a7e" category="paragraph">Es sind zwei Zeitpläne erforderlich, einer für die nächtlichen Datendatei-Backups und einer für die Protokolldatei-Backups. Diese werden Mitternacht bzw. 15 Minuten genannt.</block>
  <block id="44c0956347a70c78d2124c1041321049" category="paragraph">Diese Zeitpläne werden dann innerhalb der Snapshot-Richtlinien verwendet<block ref="44dc83cda2d6b3ce99cc25803e824061" prefix=" " category="inline-code"></block> Und<block ref="9b055600529d69c4ae0d2a9408dc7aa3" prefix=" " category="inline-code"></block>, Wie unten gezeigt:</block>
  <block id="180e86b8e3c5049779c98c2c1006bb15" category="paragraph">Diese Snapshot-Richtlinien werden schließlich auf die Volumes angewendet.</block>
  <block id="2887f2c14c7efbc97d53ac2f914c5657" category="paragraph">Dadurch wird der Backup-Zeitplan der Volumes definiert. Datendatei-Snapshots werden um Mitternacht erstellt und für 60 Tage aufbewahrt. Das Protokollvolumen enthält 72 Snapshots, die in 15-Minuten-Intervallen erstellt wurden, was bis zu 18 Stunden Abdeckung ergibt.</block>
  <block id="e1e5ef0e7100311d950b4ffebe040670" category="paragraph">Stellen Sie dann sicher, dass sich die Datenbank im Hot-Backup-Modus befindet, wenn ein Datendatei-Snapshot erstellt wird. Dies wird mit einem kleinen Skript gemacht, das einige grundlegende Argumente akzeptiert, die den Backup-Modus auf der angegebenen SID starten und stoppen.</block>
  <block id="683cdd7abd993faf3ace355cd6870b6f" category="paragraph">Dieser Schritt stellt sicher, dass sich die Datenbank während eines vierminütigen Fensters um den Mitternacht-Snapshot im Hot Backup-Modus befindet.</block>
  <block id="ccff7eb3ad0059a184886709c87f4889" category="paragraph">Die Replikation zum Disaster Recovery-Standort ist wie folgt konfiguriert:</block>
  <block id="42d3752a745fb0cdae528f6cc425b569" category="paragraph">Das Ziel des Protokollvolumes wird alle 15 Minuten aktualisiert. Somit wird eine RPO von etwa 15 Minuten erzielt. Das genaue Update-Intervall variiert ein wenig abhängig vom Gesamtvolumen der Daten, die während der Aktualisierung übertragen werden müssen.</block>
  <block id="e272424001d72919ad498e0e9d3c2cc4" category="paragraph">Das Ziel des Datendatei-Volumes wird alle sechs Stunden aktualisiert. Dies hat keine Auswirkung auf RPO oder RTO. Wenn eine Disaster-Recovery erforderlich ist, besteht einer der ersten Schritte darin, das Datendateivolume wieder auf einen Hot-Backup-Snapshot wiederherzustellen. Der Zweck des häufigeren Aktualisierungsintervalls besteht darin, die Übertragungsrate dieses Volumens zu glätten. Wenn die Aktualisierung einmal pro Tag geplant ist, müssen alle Änderungen, die während des Tages angesammelt wurden, gleichzeitig übertragen werden. Bei häufigeren Updates werden die Änderungen schrittweise im Laufe des Tages repliziert.</block>
  <block id="ce176ff569d716c1fa73d0754aba684b" category="paragraph">Im Falle eines Ausfalls besteht der erste Schritt darin, die Spiegelungen für beide Volumes zu unterbrechen:</block>
  <block id="4b9cdb6e2096c0828aeccd2ee5eecf42" category="paragraph">Die Replikate sind jetzt Lese-/Schreibzugriff. Im nächsten Schritt wird der Zeitstempel des Protokoll-Volumes überprüft.</block>
  <block id="b4d461e48f5be5d08f677045d11e8200" category="paragraph">Die neueste Kopie des Logvolumens ist der 14. März um 13:30:00.</block>
  <block id="96f75a4b63f4377efdf64d571d71caec" category="paragraph">Identifizieren Sie als Nächstes den Hot-Backup-Snapshot, der unmittelbar vor dem Status des Protokollvolumes erstellt wurde. Dies ist erforderlich, da die Protokollwiedergabe alle Archivprotokolle erfordert, die im Hot Backup-Modus erstellt wurden. Das Replikat des Protokollvolumes muss daher älter als die Hot-Backup-Images sein, da sonst die erforderlichen Protokolle nicht enthalten wären.</block>
  <block id="d358e4eb10a2f4e0b591e16941d7782c" category="paragraph">Der zuletzt erstellte Snapshot ist<block ref="c001e27abf0e66d13790bb8a7e00da54" prefix=" " category="inline-code"></block>. Hierbei handelt es sich um das neueste Hot-Backup-Image der Datendateien, das wie folgt wiederhergestellt wird:</block>
  <block id="d2037035a976c9be71cf6f372300c8c3" category="paragraph">In dieser Phase kann die Datenbank nun wiederhergestellt werden. Wenn es sich um eine SAN-Umgebung handelt, würde der nächste Schritt die Aktivierung von Volume-Gruppen und das Mounten von Dateisystemen umfassen, ein einfach automatisierter Prozess. Da in diesem Beispiel NFS verwendet wird, sind die Dateisysteme bereits gemountet und wurden in Schreib- und Lesezugriff eingebunden, ohne dass in dem Moment, in dem die Spiegelungen beschädigt wurden, eine weitere Bereitstellung oder Aktivierung erforderlich war.</block>
  <block id="2edce22eade45b42bc6b2665057fa082" category="paragraph">Die Datenbank kann jetzt bis zum gewünschten Zeitpunkt wiederhergestellt werden, oder sie kann in Bezug auf die Kopie der replizierten Wiederherstellungsprotokolle vollständig wiederhergestellt werden. Dieses Beispiel zeigt den Wert des kombinierten Archivprotokolls, der Steuerdatei und des Wiederherstellungsprotokolls. Der Recovery-Prozess ist drastisch einfacher, da es keine Notwendigkeit, auf Backup-Steuerdateien oder Reset-Protokolldateien verlassen.</block>
  <block id="51f74a82c7125b3b288755b6668091b1" category="section-title">Disaster Recovery mit Snapshot-optimierten Backups</block>
  <block id="3680a4757ddad43934237ab6a0ca955c" category="paragraph">Der Disaster-Recovery-Vorgang mithilfe von Snapshot optimierten Backups ist nahezu identisch mit dem Disaster-Recovery-Verfahren für Hot Backups. Wie beim Hot Backup Snapshot Verfahren ist es auch im Grunde eine Erweiterung einer lokalen Backup-Architektur, in der die Backups für die Disaster Recovery repliziert werden. Das folgende Beispiel zeigt das detaillierte Konfigurations- und Wiederherstellungsverfahren. Dieses Beispiel nennt auch die wichtigsten Unterschiede zwischen Hot Backups und Snapshot optimierten Backups.</block>
  <block id="330dadae5e2585726f41ed1f90e88f9f" category="paragraph">Die Beispieldatenbank befindet sich in einer grundlegenden Architektur mit zwei Volumes.<block ref="f006bf17b06c884c47190c4225d3215a" prefix=" " category="inline-code"></block> Enthält Datendateien, und<block ref="894db1fc1592356880976f1b92c3e6a2" prefix=" " category="inline-code"></block> Wird für kombinierte Redo-Logs, Archivprotokolle und Steuerdateien verwendet.</block>
  <block id="1669f47e046ce7e069e9d143f42e025d" category="paragraph">Es sind zwei Zeitpläne erforderlich: Eine für die nächtlichen Datendatei-Backups und eine für die Protokolldatei-Backups. Diese werden Mitternacht bzw. 15 Minuten genannt.</block>
  <block id="ef4adcab3e9bd572d0670f9f2073c311" category="paragraph">Dadurch wird der ultimative Backup-Plan der Volumes gesteuert. Snapshots werden um Mitternacht erstellt und 60 Tage aufbewahrt. Das Protokollvolumen enthält 72 Snapshots, die in 15-Minuten-Intervallen erstellt wurden, was bis zu 18 Stunden Abdeckung ergibt.</block>
  <block id="f9d5149b630a43ff5996adc26c6e1f08" category="paragraph">Das Ziel des Protokollvolumes wird alle 15 Minuten aktualisiert. Dadurch wird ein RPO von ca. 15 Minuten erreicht, wobei das genaue Update-Intervall etwas variiert, je nach dem Gesamtvolumen der Daten, die während der Aktualisierung übertragen werden müssen.</block>
  <block id="992398ebd9e9f825f0aa4415519abdba" category="paragraph">Das Datendatei-Volume-Ziel wird alle 6 Stunden aktualisiert. Dies hat keine Auswirkung auf RPO oder RTO. Wenn eine Disaster Recovery erforderlich ist, müssen Sie das Datendatei-Volume zunächst auf einem Hot-Backup-Snapshot wiederherstellen. Der Zweck des häufigeren Aktualisierungsintervalls besteht darin, die Übertragungsrate dieses Volumens zu glätten. Wenn die Aktualisierung einmal pro Tag geplant wurde, müssen alle Änderungen, die sich während des Tages angesammelt haben, gleichzeitig übertragen werden. Bei häufigeren Updates werden die Änderungen schrittweise im Laufe des Tages repliziert.</block>
  <block id="9e85b8590ac2f9ff5ebc8d1def51ae82" category="paragraph">Im Falle eines Ausfalls besteht der erste Schritt darin, die Spiegelungen für alle Volumes zu unterbrechen:</block>
  <block id="fd2834dcadb39f00eb897fdcdc07a03d" category="paragraph">Die neueste Kopie des Logvolumens ist der 14. März um 13:30. Identifizieren Sie als nächstes den Datendatei-Snapshot, der unmittelbar vor dem Status des Protokoll-Volumes erstellt wurde. Dies ist erforderlich, da für die Protokollwiedergabe alle Archivprotokolle von kurz vor dem Snapshot zum gewünschten Wiederherstellungspunkt erforderlich sind.</block>
  <block id="810e47566b9cd5aa3b28e6514ec95735" category="paragraph">Der zuletzt erstellte Snapshot ist<block ref="c001e27abf0e66d13790bb8a7e00da54" prefix=" " category="inline-code"></block>. Diesen Snapshot wiederherstellen.</block>
  <block id="87d83a0406ac6622efcdf91c42784ecd" category="paragraph">Die Datenbank kann nun wiederhergestellt werden. Wenn es sich um eine SAN-Umgebung handelt, würden Sie dann Volume-Gruppen aktivieren und Filesysteme mounten, ein einfach automatisierter Prozess. In diesem Beispiel wird jedoch NFS verwendet, d. h. die Dateisysteme sind bereits gemountet und wurden in Lese- und Schreibzugriff überführt. In dem Moment, in dem die Spiegelungen beschädigt wurden, ist keine weitere Bereitstellung oder Aktivierung erforderlich.</block>
  <block id="7a2e97f17454be76b7e522ce612ff961" category="paragraph">Die Datenbank kann jetzt bis zum gewünschten Zeitpunkt wiederhergestellt werden, oder sie kann in Bezug auf die Kopie der replizierten Wiederherstellungsprotokolle vollständig wiederhergestellt werden. Dieses Beispiel zeigt den Wert des kombinierten Archivprotokolls, der Steuerdatei und des Wiederherstellungsprotokolls. Der Wiederherstellungsvorgang ist wesentlich einfacher, da keine Notwendigkeit besteht, sich auf Backup-Steuerdateien oder Reset-Protokolldateien zu verlassen.</block>
  <block id="5eda890cc07e4b7cd409730d8af8ddc3" category="paragraph">Die Replizierung von Konsistenzgruppen kann so einfach wie die Terminierung der Replizierung eines einzelnen Volumes über SnapMirror erfolgen. Dazu gehören Datendateien, Steuerdateien, Archivprotokolle und Wiederherstellungsprotokolle. Jedes SnapMirror Update führt zu einer neuen Kopie der Datenbank am Zielstandort, die konsistent und einsatzbereit ist, indem sie die Spiegelung bricht.</block>
  <block id="89cb1432594ce7c25dadf229401874e7" category="paragraph">Wenn eine Datenbank Volumes umfassen muss, ist ein Snapshot einer Konsistenzgruppe (cg-Snapshot) erforderlich.</block>
  <block id="f96b125d1bacc785389957041be148e9" category="paragraph">Ein weiterer Vorteil dieser Strategie bei Verwendung mit SnapMirror im Replizierungsmodus auf Blockebene ist die vollständige Replizierung aller Snapshots auf dem Quell-Storage-System. Neben der Disaster-Recovery-Kopie wird die volle Anzahl der Backups repliziert.</block>
  <block id="77fda44088666155052f9d4224878cbc" category="summary">Tiering von Oracle Archivprotokollen</block>
  <block id="8625af69ad0b5f76799d87df777e3c86" category="paragraph">Die wahrscheinlich wichtigste Verwendung für FabricPool ist die Verbesserung der Effizienz bekannter, kalter Daten, wie z. B. Transaktions-Logs der Datenbank.</block>
  <block id="a98d546484d238b37b7e2584ae8d399d" category="summary">Backup-Tiering für Oracle</block>
  <block id="9f409a250e4be99904c9e90259cb747d" category="paragraph">Zu den herkömmlichen Applikations-Backups gehören Produkte wie der Oracle Recovery Manager, die dateibasierte Backups außerhalb des Standorts der Originaldatenbank erstellen.</block>
  <block id="4627801bdbb24a1991e64f30a2d80cb1" category="paragraph">In ONTAP stehen vier Richtlinien zur Verfügung, die steuern, wie Oracle-Daten auf der Performance-Tier zu einem Kandidaten für die Verlagerung auf die Kapazitäts-Tier werden.</block>
  <block id="2eea2725f31529888b5e57b36492819f" category="paragraph">FabricPool Tiering wird zwar auf Block-Ebene ausgeführt, kann jedoch in einigen Fällen für Tiering auf Dateiebene verwendet werden.</block>
  <block id="f935a501eae76f9c13d430bc3879cd0d" category="paragraph">Da FabricPool auf Block-Ebene arbeitet, können geänderte Dateien teilweise auf Objekt-Storage verschoben werden und dennoch nur teilweise auf Performance-Tier verbleiben.</block>
  <block id="cc4e17c26faf6b70b383529b6215ba0f" category="doc">Unterbrechungen des Zugriffs auf Objektspeicher</block>
  <block id="ec35a176f3e7c086273baee0c7f374d5" category="paragraph">Um zu verstehen, wie sich FabricPool Tiering auf Oracle und andere Datenbanken auswirkt, benötigen Sie ein Verständnis der Low-Level-FabricPool-Architektur.</block>
  <block id="6edf1821ddc55278229c2156f42905f0" category="paragraph">Die Tiering-Richtlinien steuern, welche Oracle-Datenbankblöcke von der Performance-Tier auf die Kapazitäts-Tier verschoben werden. Abrufrichtlinien steuern, was passiert, wenn ein gestaffeltes Block gelesen wird.</block>
  <block id="d01cbb2be77343ccdb1d3372c4b9891e" category="summary">Richtlinien zur Größenanpassung von Oracle Datenbanken und zur LUN-Anzahl</block>
  <block id="51f96d1f8b50084e3a2ddc21ceef8b2f" category="paragraph">Die Auswahl der optimalen LUN-Größe und der Anzahl der zu verwendenden LUNs ist für optimale Performance und einfaches Management der Oracle-Datenbanken von entscheidender Bedeutung.</block>
  <block id="3d1bd23e74923cdc48fe992d0687809f" category="summary">Oracle-Datenbanken und NFS Leasing und Locks</block>
  <block id="cec6f8a988408ba212798e1cc32658c7" category="paragraph">NFSv3 ist statusfrei. Das bedeutet effektiv, dass der NFS-Server (ONTAP) nicht verfolgt, welche Dateisysteme gemountet sind, von wem oder welche Sperren tatsächlich vorhanden sind.</block>
  <block id="7c96331bd03e63c4eb3505c526b42244" category="paragraph">ONTAP verfügt über einige Funktionen, die Mount-Versuche aufzeichnen, sodass Sie eine Vorstellung davon haben, welche Clients möglicherweise auf Daten zugreifen, und es gibt möglicherweise Hinweissperren, aber diese Informationen sind nicht garantiert zu 100% vollständig. Es kann nicht vollständig sein, da die Nachverfolgung des NFS-Client-Status nicht Teil des NFSv3-Standards ist.</block>
  <block id="20733ad7135aa0ede95fd8637fe05cf6" category="section-title">Status der NFSv4-Daten</block>
  <block id="3f7c11f662fc6e07a91179fdf2322ef5" category="paragraph">Im Gegensatz dazu ist NFSv4 zustandsbehafteten. Der NFSv4-Server verfolgt, welche Clients welche Dateisysteme verwenden, welche Dateien existieren, welche Dateien und/oder Regionen von Dateien gesperrt sind usw. Dies bedeutet, dass eine regelmäßige Kommunikation zwischen einem NFSv4-Server erforderlich ist, um die Statusdaten auf dem aktuellen Stand zu halten.</block>
  <block id="6f7e4a1e570f45fa3db0b8a0ed80a38f" category="paragraph">Die wichtigsten Zustände, die vom NFS-Server verwaltet werden, sind NFSv4-Locks und NFSv4-Leases, und sie sind sehr miteinander verflochten. Sie müssen verstehen, wie jede einzelne von sich aus funktioniert und wie sie miteinander in Beziehung stehen.</block>
  <block id="0d55fd4062f29237d0fb3b7f666fb8eb" category="section-title">NFSv4-Sperren</block>
  <block id="bf4139ff42e411ae5483e062387b3f70" category="paragraph">Bei NFSv3 sind Sperren Empfehlung. Ein NFS-Client kann weiterhin „gesperrte“ Dateien ändern oder löschen. Eine NFSv3-Sperre läuft nicht von selbst ab, sie muss entfernt werden. Dies führt zu Problemen. Wenn Sie beispielsweise über eine geclusterte Applikation verfügen, die NFSv3-Sperren erstellt, und einer der Nodes ausfällt, wie gehen Sie vor? Sie können die Anwendung auf den verbleibenden Knoten codieren, um die Sperren zu entfernen, aber wie können Sie wissen, dass das sicher ist? Vielleicht ist der „ausgefallene“ Knoten funktionsfähig, kommuniziert aber nicht mit dem Rest des Clusters?</block>
  <block id="11d1c33ac6b190a233c00cdbdd6a355b" category="paragraph">Mit NFSv4 haben Sperren eine begrenzte Dauer. Solange der Client mit den Sperren weiterhin mit dem NFSv4-Server eincheckt, darf kein anderer Client diese Sperren erwerben. Wenn ein Client nicht mit dem NFSv4 eincheckt, werden die Sperren schließlich vom Server widerrufen und andere Clients können Sperren anfordern und erhalten.</block>
  <block id="c17985911e0182dc5e96c4d60aadf139" category="section-title">NFSv4-Leasing</block>
  <block id="596bcfef3aaccfe299bb45a969afcaf0" category="paragraph">NFSv4-Sperren sind einem NFSv4-Leasing zugeordnet. Wenn ein NFSv4-Client eine Verbindung mit einem NFSv4-Server herstellt, erhält er eine Leasing-Option. Wenn der Kunde eine Sperre erhält (es gibt viele Arten von Sperren), dann ist die Sperre mit dem Leasing verbunden.</block>
  <block id="b5248856b1835e14738aa32e053e30e0" category="paragraph">Diese Lease hat ein definiertes Timeout. Standardmäßig setzt ONTAP den Timeout-Wert auf 30 Sekunden:</block>
  <block id="7e2a66190f452806ac1183721a43eaa1" category="paragraph">Dies bedeutet, dass ein NFSv4-Client alle 30 Sekunden mit dem NFSv4-Server einchecken muss, um seine Mietverträge zu erneuern.</block>
  <block id="809c552ae0a4d377e87f495b4e3ac62b" category="paragraph">Der Lease wird automatisch durch jede Aktivität erneuert, sodass, wenn der Kunde arbeitet, keine zusätzlichen Operationen durchgeführt werden müssen. Wenn eine Anwendung still wird und keine echte Arbeit macht, muss sie stattdessen eine Art Keep-Alive-Vorgang (SEQUENZ genannt) durchführen. Es ist im Grunde nur sagen: "Ich bin immer noch hier, bitte aktualisieren Sie meine Mietverträge."</block>
  <block id="70cb78bb7e91dcafab559f9b72e40bb6" category="paragraph">NFSv3 ist statusfrei. Es wird keine Kommunikation der Clients erwartet. NFSv4 ist zustandsbehaftet. Sobald dieser Leasingzeitraum verstrichen ist, läuft der Leasingvertrag ab, Sperren werden aufgehoben und die gesperrten Dateien werden anderen Clients zur Verfügung gestellt.</block>
  <block id="2ee5d81cfa6306ae71ce534ed9b31431" category="paragraph">Mit NFSv3 können Sie Netzwerkkabel umlegen, Netzwerk-Switches neu booten, Konfigurationsänderungen vornehmen und ziemlich sicher sein, dass nichts Schlimmes passiert. Anwendungen würden normalerweise nur geduldig warten, bis die Netzwerkverbindung wieder funktioniert.</block>
  <block id="8900ff182e4197f8eb2acd7d2b7fd902" category="paragraph">Mit NFSv4 haben Sie 30 Sekunden (es sei denn, Sie haben den Wert dieses Parameters innerhalb von ONTAP erhöht), um Ihre Arbeit abzuschließen. Wenn Sie das überschreiten, Ihre Leasing-Zeit aus. Normalerweise führt dies zu einem Absturz der Anwendung.</block>
  <block id="ffa8c7744b3e2d43b18c67aec0fe7743" category="paragraph">Wenn Sie beispielsweise über eine Oracle-Datenbank verfügen und die Netzwerkverbindung (manchmal auch als „Netzwerkpartition“ bezeichnet) unterbrochen wird, die das Lease-Timeout überschreitet, stürzt die Datenbank ab.</block>
  <block id="33b9897348039594b4b17bf1e1629c74" category="paragraph">Dies ist ein Beispiel dafür, was im Oracle-Alarmprotokoll passiert, wenn dies geschieht:</block>
  <block id="85608585bf8e790a579c49ba9af705c6" category="paragraph">Wenn Sie sich die Syslogs ansehen, sollten Sie mehrere der folgenden Fehler sehen:</block>
  <block id="e19b338de53dada4cf4da3837aaba76d" category="paragraph">Die Protokollmeldungen sind in der Regel das erste Anzeichen eines Problems, das nicht durch das Einfrieren der Anwendung verursacht wird. In der Regel sehen Sie während des Netzwerkausfalls überhaupt nichts, da Prozesse und das Betriebssystem selbst blockiert sind und versuchen, auf das NFS-Dateisystem zuzugreifen.</block>
  <block id="7a3c8c17a9dd10fa565f79e0ebb5bba7" category="paragraph">Die Fehler werden angezeigt, nachdem das Netzwerk wieder betriebsbereit ist. Im obigen Beispiel hat das Betriebssystem versucht, die Sperren nach der Wiederherstellung der Verbindung erneut zu erfassen, aber es war zu spät. Der Mietvertrag war abgelaufen und die Schlösser wurden entfernt. Dies führt zu einem Fehler, der sich auf die Oracle-Ebene ausbreitet und die Meldung im Alarmprotokoll verursacht. Je nach Version und Konfiguration der Datenbank können Sie Abweichungen von diesen Mustern sehen.</block>
  <block id="4bec5e793e34a5819aefb68e4f65db26" category="paragraph">Zusammenfassend lässt sich sagen, dass NFSv3 eine Netzwerkunterbrechung toleriert, aber NFSv4 ist sensibler und sieht einen definierten Leasing-Zeitraum vor.</block>
  <block id="069de2d717910b2677190b9a9b5ed1cf" category="paragraph">Was ist, wenn eine 30-Sekunden-Zeitüberschreitung nicht akzeptabel ist? Was tun Sie, wenn Sie ein dynamisch verändertes Netzwerk verwalten, in dem Switches neu gestartet oder Kabel verlegt werden, und das Ergebnis ist eine gelegentliche Netzwerkunterbrechung? Sie könnten die Leasingdauer verlängern, aber ob Sie dies tun möchten, erfordert eine Erklärung der NFSv4-Kulanzzeiträume.</block>
  <block id="4b84416146795544deb5c9de89187f91" category="section-title">NFSv4-Kulanzzeiträume</block>
  <block id="7d98e091c5f7be92cd5fb85b985d8d20" category="paragraph">Wenn ein NFSv3 Server neu gestartet wird, ist er fast sofort in der Lage, I/O zu bedienen. Es war nicht die Aufrechterhaltung einer Art von Zustand über Kunden. Dies führt dazu, dass ein ONTAP-Übernahmevorgang oft fast unmittelbar zu erfolgen scheint. Sobald ein Controller bereit ist, mit der Datenbereitstellung zu beginnen, sendet er ein ARP an das Netzwerk, das die Änderung der Topologie signalisiert. Clients erkennen dies normalerweise nahezu sofort, und die Daten werden wieder fließend gespeichert.</block>
  <block id="173d4fd05086441236dbee4710639d10" category="paragraph">NFSv4 erzeugt jedoch eine kurze Pause. Nur ein Teil davon, wie NFSv4 funktioniert.</block>
  <block id="21483115213b27583a07dc2ca994671c" category="paragraph">NFSv4-Server müssen die Leasing-Optionen, Sperren und die Verwendung welcher Daten verfolgen. Wenn ein NFS-Server in Panik Gerät und neu startet oder einen Moment lang Strom verliert oder während der Wartungsaktivitäten neu gestartet wird, führt dies zu einer Lease/Sperre und zum Verlust anderer Clientinformationen. Der Server muss herausfinden, welcher Client welche Daten verwendet, bevor er den Betrieb wiederaufnehmen kann. Hier kommt die Kulanzzeit ins Spiel.</block>
  <block id="09b6f386094bd895d3c9694f28bfdf65" category="paragraph">Wenn Sie Ihren NFSv4-Server plötzlich aus- und wieder einschalten. Wenn es wieder verfügbar ist, erhalten Kunden, die versuchen, die E/A-Vorgänge fortzusetzen, eine Antwort, die im Wesentlichen besagt: „Ich habe die Leasing-/Sperrdaten verloren. Möchten Sie Ihre Sperren erneut registrieren?“ Das ist der Anfang der Gnadenfrist. Die Standardeinstellung ist 45 Sekunden bei ONTAP:</block>
  <block id="e4f8aa90af36d76254e2f02e56232aed" category="paragraph">Das Ergebnis ist, dass ein Controller nach einem Neustart I/O-Vorgänge pausiert, während alle Clients ihre Mietverträge und Sperren zurückfordern. Nach Ablauf der Kulanzzeit nimmt der Server die E/A-Vorgänge wieder auf.</block>
  <block id="bb2b26fdcb4e947cda6f8e38724ecfc9" category="section-title">Leasing-Timeouts im Vergleich zu Kulanzzeiträumen</block>
  <block id="c4fb6f7d31d205bc07dbff47d0ae4bb3" category="summary">Oracle Direct NFS</block>
  <block id="1bd6ec1185ba67395dc6f9cd2b458ec8" category="paragraph">Oracle Databases können NFS auf zweierlei Weise verwenden.</block>
  <block id="96997ca5c577dd19a3ba5f7ff771f27a" category="paragraph">Zunächst kann es ein Dateisystem verwenden, das mit dem nativen NFS-Client gemountet ist, der Teil des Betriebssystems ist. Dies wird manchmal Kernel NFS oder kNFS genannt. Das NFS-Dateisystem ist gemountet und von der Oracle-Datenbank genau so verwendet wie jede andere Anwendung ein NFS-Dateisystem verwenden würde.</block>
  <block id="95c17c63df7ceea39a175b1d96955bb7" category="paragraph">Die zweite Methode ist Oracle Direct NFS (dNFS). Hierbei handelt es sich um eine Implementierung des NFS-Standards in der Oracle Datenbanksoftware. Die Art und Weise, wie Oracle-Datenbanken vom DBA konfiguriert oder verwaltet werden, bleibt unverändert. Sofern das Storage-System selbst die richtigen Einstellungen hat, sollte die Verwendung von dNFS für das DBA-Team und die Endanwender transparent sein.</block>
  <block id="43539d2fca21e5b644484efb94614a7b" category="paragraph">Eine Datenbank mit aktivierter dNFS-Funktion hat noch die üblichen NFS-Dateisysteme gemountet. Sobald die Datenbank geöffnet ist, öffnet die Oracle-Datenbank eine Reihe von TCP/IP-Sitzungen und führt NFS-Vorgänge direkt aus.</block>
  <block id="26e7ebad3936387b4b3f7c4f5688ef27" category="section-title">Direktes NFS</block>
  <block id="6d431a52c8a41a7c2335f98e2ae8c454" category="paragraph">Der Hauptwert von Direct NFS von Oracle besteht darin, den NFS-Client des Hosts zu umgehen und NFS-Dateivorgänge direkt auf einem NFS-Server auszuführen. Wenn Sie diese Option aktivieren, muss nur die Oracle Disk Manager (ODM)-Bibliothek geändert werden. Anweisungen zu diesem Prozess finden Sie in der Oracle-Dokumentation.</block>
  <block id="8401d91735209aeccbe56f33b4e2db73" category="paragraph">Die Verwendung von dNFS führt zu einer deutlichen Verbesserung der I/O-Performance und verringert die Last auf dem Host und dem Storage-System, da I/O so effizient wie möglich ausgeführt wird.</block>
  <block id="b0dedecb00787180a9e0c657cfb278c9" category="paragraph">Darüber hinaus enthält Oracle dNFS eine *Option* für Multipathing und Fehlertoleranz der Netzwerkschnittstelle. Beispielsweise können zwei 10-GB-Schnittstellen verbunden werden, um eine Bandbreite von 20 GB bereitzustellen. Ein Ausfall einer Schnittstelle führt dazu, dass die I/O-Vorgänge auf der anderen Schnittstelle wiederholt werden. Der gesamte Vorgang ähnelt dem FC-Multipathing. Multipathing war schon vor Jahren üblich, als 1 GB ethernet der häufigste Standard war. Für die meisten Oracle Workloads ist eine 10-Gbit-NIC ausreichend. Wird jedoch mehr benötigt, können 10-Gbit-NICs verbunden werden.</block>
  <block id="11e50b3c816105439151db3a3b4fdda3" category="paragraph">Wenn dNFS verwendet wird, ist es wichtig, dass alle Patches, die in Oracle Doc 1495104.1 beschrieben werden, installiert sind. Wenn ein Patch nicht installiert werden kann, muss die Umgebung überprüft werden, um sicherzustellen, dass die in diesem Dokument beschriebenen Fehler keine Probleme verursachen. In manchen Fällen kann dNFS nicht verwendet werden, da die erforderlichen Patches nicht installiert werden können.</block>
  <block id="8f98002fee392ad6151679875de1a7cc" category="paragraph">Verwenden Sie dNFS nicht mit Round-Robin-Namensauflösungen wie DNS, DDNS, NIS oder anderen Methoden. Dazu gehört auch die in ONTAP verfügbare DNS-Lastausgleichsfunktion. Wenn eine Oracle-Datenbank mit dNFS einen Hostnamen in eine IP-Adresse auflöst, darf sie sich bei nachfolgenden Suchen nicht ändern. Dies kann zu Abstürzen der Oracle-Datenbank und einer möglichen Beschädigung von Daten führen.</block>
  <block id="e80ad3efb180dc2d016b27506b7b24ce" category="section-title">Direkter NFS- und Host-Filesystem-Zugriff</block>
  <block id="4c38a8c0a49e721bf2a378d0b4fb2010" category="paragraph">Die Verwendung von dNFS kann gelegentlich Probleme für Applikationen oder Benutzeraktivitäten verursachen, die auf den sichtbaren Filesystemen basieren, die auf dem Host gemountet sind, da der dNFS-Client vom Host-Betriebssystem aus auf das Filesystem zugreift. Der dNFS-Client kann Dateien ohne Kenntnis des Betriebssystems erstellen, löschen und ändern.</block>
  <block id="c03389fdb295f2acb079603123e6d0e3" category="paragraph">Wenn die Mount-Optionen für Single-Instance-Datenbanken verwendet werden, ermöglichen sie das Caching von Datei- und Verzeichnisattributen, was auch bedeutet, dass der Inhalt eines Verzeichnisses zwischengespeichert wird. Daher kann dNFS eine Datei erstellen, und es gibt eine kurze Verzögerung, bevor das Betriebssystem den Verzeichnisinhalt erneut liest und die Datei für den Benutzer sichtbar wird. Dies ist in der Regel kein Problem, aber in seltenen Fällen können Dienstprogramme wie SAP BR*Tools Probleme haben. Beheben Sie in diesem Fall das Problem, indem Sie die Mount-Optionen ändern, um die Empfehlungen für Oracle RAC zu verwenden. Mit dieser Änderung wird das gesamte Host-Caching deaktiviert.</block>
  <block id="a370a203383b565c3fb746c1c2e1a64b" category="paragraph">Mount-Optionen nur ändern, wenn (a) dNFS verwendet wird und (b) ein Problem auf eine Verzögerung bei der Dateisichtbarkeit zurückzuführen ist. Wenn dNFS nicht verwendet wird, führt die Verwendung der Oracle RAC Mount-Optionen auf einer Single-Instance-Datenbank zu einer verminderte Performance.</block>
  <block id="22a7a03175addfd9aa5b1fa2c351e833" category="summary">NFS-Konfiguration für Oracle Datenbanken</block>
  <block id="6cb2edd2369a17890dce007723d98a6f" category="paragraph">NetApp bietet seit über 30 Jahren NFS-Storage der Enterprise-Klasse. Seine Einsatzbereich wächst aufgrund der Einfachheit mit dem Trend zu Cloud-basierten Infrastrukturen.</block>
  <block id="0f8a941fcc56687ad8e3914162c19a41" category="section-title">NFS-Versionen</block>
  <block id="d0972be450ccf257fb73d4878bce204e" category="paragraph">Der NFS-Client des Betriebssystems muss von NetApp unterstützt werden.</block>
  <block id="7722a2ae10cef9bccea115dfdfe78a99" category="list-text">NFSv3 wird von Betriebssystemen unterstützt, die dem NFSv3 Standard folgen.</block>
  <block id="a3550edc6962850398d217f78cc4ad0d" category="list-text">NFSv3 wird vom Oracle dNFS-Client unterstützt.</block>
  <block id="89c45ebba54478419aa679a7ece8593d" category="list-text">NFSv4 wird von allen Betriebssystemen unterstützt, die dem NFSv4-Standard entsprechen.</block>
  <block id="471ff51c89daf6e35e8e2b5111039744" category="inline-link-macro">NetApp IMT</block>
  <block id="8594659e8c4292db778973052a30ab86" category="list-text">Für NFSv4.1 und NFSv4.2 ist ein spezieller Support für das Betriebssystem erforderlich. Konsultieren Sie die <block ref="d98f827de8e80b3365d5f4160c243cdc" category="inline-link-macro-rx"></block> Für unterstützte Betriebssysteme.</block>
  <block id="d1eb4c2a4d74705a5aa7945f734eff7c" category="list-text">Oracle dNFS Unterstützung für NFSv4.1 erfordert Oracle 12.2.0.2 oder höher.</block>
  <block id="d8bacc1a7a9d6baaeb962e367920593b" category="inline-link-macro">NetApp Support-Matrix</block>
  <block id="f16dcce07ca44aa38c7ebe3016b981b2" category="admonition">Der <block ref="f1e4f72fb4419d4270270c5ed1e0d9f6" category="inline-link-macro-rx"></block> Für NFSv3 und NFSv4 sind keine spezifischen Betriebssysteme enthalten. Alle Betriebssysteme, die der RFC entsprechen, werden in der Regel unterstützt. Wenn Sie die Online-IMT nach Unterstützung für NFSv3 oder NFSv4 suchen, wählen Sie kein bestimmtes Betriebssystem aus, da keine Treffer angezeigt werden. Alle Betriebssysteme werden implizit von der allgemeinen Richtlinie unterstützt.</block>
  <block id="26ed9768d6a659a9768842d903d0025f" category="section-title">Linux NFSv3 TCP-Slot-Tabellen</block>
  <block id="c56ec9836fbbd9c0a426a40fc71c265c" category="paragraph">TCP-Slot-Tabellen sind das NFSv3 Äquivalent zur Warteschlangentiefe des Host Bus Adapters (HBA). Diese Tabellen steuern die Anzahl der NFS-Vorgänge, die zu einem beliebigen Zeitpunkt ausstehen können. Der Standardwert ist normalerweise 16, was für eine optimale Performance viel zu niedrig ist. Das entgegengesetzte Problem tritt auf neueren Linux-Kerneln auf, die automatisch die Begrenzung der TCP-Slot-Tabelle auf ein Niveau erhöhen können, das den NFS-Server mit Anforderungen sättigt.</block>
  <block id="5270700baa50d07af667096293a27564" category="paragraph">Um eine optimale Performance zu erzielen und Performance-Probleme zu vermeiden, passen Sie die Kernel-Parameter an, die die TCP-Slot-Tabellen steuern.</block>
  <block id="5b29db1d99e19013580fa375a5f87fa5" category="paragraph">Führen Sie die aus<block ref="1a1e44f7a3390ed4647a7d5b7e8b08ed" prefix=" " category="inline-code"></block> Und beobachten Sie die folgenden Parameter:</block>
  <block id="8821adf56df4952370cfaa57ccaac68d" category="paragraph">Alle Linux-Systeme sollten enthalten<block ref="c5f48b899461d4c2b9ddc0b24ea87744" prefix=" " category="inline-code"></block>, Aber nur einige enthalten<block ref="6f72acaebcb9a0de6696aaf3508a6144" prefix=" " category="inline-code"></block>. Beide sollten auf 128 gesetzt werden.</block>
  <block id="3a954c4e5e8d5ac3af590651efc5a2cb" category="cell">Wenn diese Parameter nicht eingestellt werden, kann dies erhebliche Auswirkungen auf die Leistung haben. In einigen Fällen ist die Performance eingeschränkt, da das linux-Betriebssystem nicht genügend I/O ausgibt In anderen Fällen erhöht sich die I/O-Latenz, wenn das linux Betriebssystem versucht, mehr I/O-Vorgänge auszustellen, als gewartet werden kann.</block>
  <block id="837f157b2f309f5415420467f7643e3a" category="section-title">AdR und NFS</block>
  <block id="c0e7bb1383be0bf1268d1d70280bef6d" category="paragraph">Einige Kunden haben Performance-Probleme gemeldet, die auf übermäßig viele I/O-Vorgänge für Daten im führen<block ref="18ca3ed022c239421418de608ceb49c5" prefix=" " category="inline-code"></block> Standort. Das Problem tritt in der Regel erst auf, wenn sich viele Performance-Daten angesammelt haben. Der Grund für den übermäßigen I/O ist unbekannt, aber dieses Problem scheint darauf zurückzuführen zu sein, dass Oracle-Prozesse das Zielverzeichnis wiederholt auf Änderungen scannen.</block>
  <block id="99be2ae95d50120a7c33c818afda1834" category="paragraph">Entfernen des<block ref="79fea224ffd9e8111e4e87cab732464b" prefix=" " category="inline-code"></block> Und/oder<block ref="3504e2b0e7c968444314433be465518d" prefix=" " category="inline-code"></block> Mount-Optionen ermöglichen das Caching des Host-Betriebssystems und reduzieren die Storage-I/O-Level.</block>
  <block id="8950578c587f9503f949dfc6a274b611" category="admonition">*NetApp empfiehlt* nicht zu platzieren<block ref="18ca3ed022c239421418de608ceb49c5" prefix=" " category="inline-code"></block> Daten auf einem Filesystem mit<block ref="79fea224ffd9e8111e4e87cab732464b" prefix=" " category="inline-code"></block> Oder<block ref="3504e2b0e7c968444314433be465518d" prefix=" " category="inline-code"></block> Weil Performance-Probleme wahrscheinlich sind. Trennen<block ref="18ca3ed022c239421418de608ceb49c5" prefix=" " category="inline-code"></block> Daten an einen anderen Bereitstellungspunkt, falls erforderlich.</block>
  <block id="cdb92b4a73ae99cde4df55c06e20dd3c" category="section-title">Nur nfs-Rootonly und Mount-Rootonly</block>
  <block id="c1f0a21ee026237bef11139a30b07040" category="paragraph">ONTAP enthält die NFS-Option<block ref="77743dec92afc92eca95c255fd9333d7" prefix=" " category="inline-code"></block> Damit wird gesteuert, ob der Server NFS-Datenverkehrsverbindungen von hohen Ports akzeptiert. Als Sicherheitsmaßnahme ist es nur dem Root-Benutzer erlaubt, TCP/IP-Verbindungen über einen Quellport unter 1024 zu öffnen, da solche Ports normalerweise für die Verwendung durch das Betriebssystem und nicht für Benutzerprozesse reserviert sind. Durch diese Einschränkung wird sichergestellt, dass NFS-Datenverkehr von einem tatsächlichen Betriebssystem-NFS-Client stammt und kein schädlicher Prozess, der einen NFS-Client emuliert. Der Oracle dNFS-Client ist ein Benutzerspeichertreiber, aber der Prozess läuft als root, daher ist es in der Regel nicht erforderlich, den Wert von zu ändern<block ref="77743dec92afc92eca95c255fd9333d7" prefix=" " category="inline-code"></block>. Die Verbindungen werden von niedrigen Ports hergestellt.</block>
  <block id="832b4524bcec11d28ca777ef9cdb4084" category="paragraph">Der<block ref="eb6c1a5ecd1e17d4cd46fa5c9d415816" prefix=" " category="inline-code"></block> Die Option gilt nur für NFSv3. Er steuert, ob der RPC-MOUNT-Aufruf von Ports über 1024 akzeptiert wird. Wenn dNFS verwendet wird, läuft der Client wieder als root, so dass er Ports unter 1024 öffnen kann. Dieser Parameter hat keine Auswirkung.</block>
  <block id="57c47a1451485bec1ebb060afdf65cb4" category="paragraph">Prozesse, die Verbindungen mit dNFS über NFS Version 4.0 und höher öffnen, laufen nicht als Root und erfordern daher Ports über 1024. Der<block ref="77743dec92afc92eca95c255fd9333d7" prefix=" " category="inline-code"></block> Der Parameter muss auf disabled gesetzt werden, damit dNFS die Verbindung herstellen kann.</block>
  <block id="4dd73247734d116e5039cc04c4979f2c" category="paragraph">Wenn<block ref="77743dec92afc92eca95c255fd9333d7" prefix=" " category="inline-code"></block> Ist aktiviert, ist das Ergebnis ein Hängezustand während der Mount-Phase beim Öffnen von dNFS-Verbindungen. Der sqlplus-Ausgang sieht ähnlich aus wie folgt:</block>
  <block id="f62d72479920ff0f8dc9a3ca6f301b5d" category="paragraph">Der Parameter kann wie folgt geändert werden:</block>
  <block id="47fc6c6691aed3ba0dbdc66c7896c61b" category="admonition">In seltenen Fällen müssen Sie möglicherweise sowohl nfs-rootonly als auch Mount-rootonly auf disabled ändern. Wenn ein Server eine extrem große Anzahl von TCP-Verbindungen verwaltet, ist es möglich, dass keine Ports unter 1024 verfügbar sind und das Betriebssystem gezwungen ist, höhere Ports zu verwenden. Diese beiden ONTAP-Parameter müssen geändert werden, damit die Verbindung abgeschlossen werden kann.</block>
  <block id="10b4eb170fe7bd035f21e7d863796683" category="section-title">NFS-Export-Richtlinien: Superuser und setuid</block>
  <block id="98bfec37486e95443730cc3d462022fb" category="paragraph">Wenn sich Oracle-Binärdateien auf einer NFS-Freigabe befinden, muss die Exportrichtlinie Superuser- und setuid-Berechtigungen enthalten.</block>
  <block id="3c0699ba2c56cb17c145bbda5580ed67" category="paragraph">Für allgemeine Fileservices wie Home Directories der Benutzer verwendete Shared NFS-Exporte vernichten normalerweise den Root-Benutzer. Dies bedeutet, dass eine Anfrage des Root-Benutzers auf einem Host, der ein Dateisystem gemountet hat, als anderer Benutzer mit niedrigeren Berechtigungen neu zugeordnet wird. Dies hilft, Daten zu sichern, indem ein Root-Benutzer auf einem bestimmten Server daran gehindert wird, auf Daten auf dem freigegebenen Server zuzugreifen. Das setuid-Bit kann auch ein Sicherheitsrisiko in einer gemeinsam genutzten Umgebung darstellen. Mit dem setuid-Bit kann ein Prozess als ein anderer Benutzer ausgeführt werden als der Benutzer, der den Befehl aufruft. Beispielsweise wird ein Shell-Skript, das im Besitz von root war, mit dem setuid-Bit als root ausgeführt. Wenn dieses Shell-Skript von anderen Benutzern geändert werden könnte, könnte jeder Benutzer, der nicht root ist, einen Befehl als root ausgeben, indem er das Skript aktualisiert.</block>
  <block id="e341736b7d51c23cb7ef95615832e502" category="paragraph">Die Oracle-Binärdateien enthalten Dateien im Besitz von root und verwenden das setuid-Bit. Wenn Oracle-Binärdateien auf einer NFS-Freigabe installiert sind, muss die Exportrichtlinie die entsprechenden Superuser- und setuid-Berechtigungen enthalten. Im folgenden Beispiel enthält die Regel beides<block ref="1efaeada4e3307369aca511f5da0498a" prefix=" " category="inline-code"></block> Und Genehmigungen<block ref="0baea2f0ae20150db78f58cddac442a9" prefix=" " category="inline-code"></block> (Root)-Zugriff für NFS-Clients unter Verwendung der Systemauthentifizierung.</block>
  <block id="f1a0fa258c785ac546835a1a75017faf" category="section-title">Konfiguration von NFSv4/4.1</block>
  <block id="52f9ad2e353a42e61d9bf7dd9f59d0cb" category="paragraph">Für die meisten Applikationen gibt es kaum einen Unterschied zwischen NFSv3 und NFSv4. Applikations-I/O ist in der Regel sehr einfach I/O und nicht von einigen der erweiterten Funktionen, die in NFSv4 verfügbar sind, erheblich profitieren. Höhere Versionen von NFS sollten nicht aus Sicht des Datenbank-Storage als „Upgrade“ betrachtet werden, sondern als Versionen von NFS, die zusätzliche Features enthalten. Wenn beispielsweise die End-to-End-Sicherheit des kerberos Datenschutzmodus (krb5p) erforderlich ist, ist NFSv4 erforderlich.</block>
  <block id="a1dc4350330c061ae52050f5a3400e25" category="section-title">NFSv4-Domäne</block>
  <block id="c30669603c950b31f6093d8beaf8060a" category="paragraph">Eine vollständige Erklärung der NFSv4/4.1-Konfiguration geht über den Umfang dieses Dokuments hinaus, aber ein häufig aufgetretendes Problem ist eine Diskrepanz bei der Domänenzuordnung. Aus Sicht von sysadmin scheinen sich die NFS-Dateisysteme normal zu verhalten, aber Anwendungen melden Fehler über Berechtigungen und/oder setuid auf bestimmte Dateien. In einigen Fällen haben Administratoren fälschlicherweise festgestellt, dass die Berechtigungen der Anwendungsbinärdateien beschädigt wurden und chown- oder chmod-Befehle ausgeführt haben, wenn das eigentliche Problem der Domänenname war.</block>
  <block id="f0c6c8a632eeaa954640907ee1da277d" category="paragraph">Der NFSv4-Domänenname wird auf der ONTAP SVM festgelegt:</block>
  <block id="97a98ebe6ac1f4a8eaf69158419dd818" category="paragraph">Der NFSv4-Domänenname auf dem Host wird in festgelegt<block ref="60969252b5b8391a3378335ae420d2e8" prefix=" " category="inline-code"></block></block>
  <block id="4ade50e8135e817e75aa3086851eaaa8" category="paragraph">Die Domänennamen müssen übereinstimmen. Wenn dies nicht der Fall ist, werden ähnliche Zuordnungsfehler wie die folgenden in angezeigt<block ref="452905a167cf4509fd08acb964fdb20c" prefix=" " category="inline-code"></block>:</block>
  <block id="c0eb73bb35a4c58d87b330cd00f1986a" category="paragraph">Anwendungsbinärdateien, wie z. B. Oracle-Datenbank-Binärdateien, enthalten Dateien im Besitz von root mit dem setuid-Bit, was bedeutet, dass eine Diskrepanz in den NFSv4-Domänennamen Fehler beim Starten von Oracle verursacht und eine Warnung über die Eigentumsrechte oder Berechtigungen einer Datei namens enthält<block ref="dd440398b298ff16eb1188ba8afca6df" prefix=" " category="inline-code"></block>, Die sich im befindet<block ref="002f94d606ac06f290f33a5fcc67b264" prefix=" " category="inline-code"></block> Verzeichnis. Sie sollte wie folgt aussehen:</block>
  <block id="21cc6f8790de2a7022b344914915604e" category="paragraph">Wenn diese Datei mit der Eigentümerschaft von Niemand angezeigt wird, kann es ein Problem mit der NFSv4-Domänenzuordnung geben.</block>
  <block id="203f1cdb2421d34e9eb7391e03ffcaf6" category="paragraph">Um dies zu beheben, überprüfen Sie die<block ref="60969252b5b8391a3378335ae420d2e8" prefix=" " category="inline-code"></block> Datei mit der v4-id-Domain-Einstellung auf ONTAP und stellen Sie sicher, dass sie konsistent sind. Wenn dies nicht der Fall ist, nehmen Sie die erforderlichen Änderungen vor, und führen Sie aus<block ref="1c7ebfbcaa2681599e41320ed491ca91" prefix=" " category="inline-code"></block>, Und warten Sie einen Moment, bis sich die Änderungen fortpflanzen. Die Dateieigentümerschaft sollte dann ordnungsgemäß als root erkannt werden. Wenn ein Benutzer versucht hatte, ausgeführt zu werden<block ref="05f0da969f2e0e6ecf3ab445e1e665ad" prefix=" " category="inline-code"></block> Vor der Korrektur der Konfiguration der NFS-Domänen in dieser Datei muss möglicherweise ausgeführt werden<block ref="05f0da969f2e0e6ecf3ab445e1e665ad" prefix=" " category="inline-code"></block> Ein weiteres Jahr in der</block>
  <block id="748f6387bf49161b9a69b7bfd691dcd2" category="paragraph">Das Vorhandensein einer der folgenden Mount-Optionen bewirkt, dass das Host-Caching deaktiviert wird:</block>
  <block id="9f75df502ef11eaef4292c94110cb673" category="paragraph">Diese Einstellungen können sich stark negativ auf die Geschwindigkeit der Softwareinstallation, des Patches und der Backup-/Wiederherstellungsvorgänge auswirken. In manchen Fällen, insbesondere bei geclusterten Applikationen, sind diese Optionen unweigerlich erforderlich, weil die Cache-Kohärenz über alle Nodes im Cluster hinweg gewährleistet werden muss. In anderen Fällen verwenden Kunden diese Parameter irrtümlich und das Ergebnis ist ein unnötiger Leistungsschaden.</block>
  <block id="8d23519d53d6611bc42fef8dc4a432df" category="paragraph">Viele Kunden entfernen diese Mount-Optionen vorübergehend während der Installation oder dem Patching der Binärdateien der Anwendung. Diese Entfernung kann sicher durchgeführt werden, wenn der Benutzer überprüft, dass während der Installation oder des Patching-Prozesses keine anderen Prozesse aktiv das Zielverzeichnis verwenden.</block>
  <block id="f73bede4039d47664f70de3c2d1f2b46" category="summary">LVM Stripe-Konfiguration für Oracle-Datenbanken</block>
  <block id="1b5681ffc153e25d2a48ae3f3f01b9f5" category="paragraph">LVM-Striping bezieht sich auf die Verteilung von Daten über mehrere LUNs. So lässt sich die Performance vieler Datenbanken deutlich steigern.</block>
  <block id="358b9a846de3e04b423c68bac9bc6d53" category="summary">LUN-Ausrichtung mit Oracle Datenbanken</block>
  <block id="3e00e4d28b17eedd6f6c2c38233e06a8" category="paragraph">LUN-Ausrichtung bezieht sich auf die I/O-Optimierung in Bezug auf das zugrunde liegende Filesystem-Layout.</block>
  <block id="a6c7e85d51ae3b98197ceb3eafa3686a" category="inline-link-macro">Effizienz</block>
  <block id="6e2524d5663b2bdc0c7985fa3e57cc55" category="section-title">Warnungen wegen Falschausrichtung</block>
  <block id="79e721357e358d77c4700c25c863fc37" category="inline-link">ONTAP SAN-Host-Konfiguration</block>
  <block id="268b1da3f1ff11d32e385d0223101718" category="paragraph">Die Ausrichtung in Solaris-Umgebungen ist komplizierter. Siehe<block ref="2d33e0364c07dc165b4079892340d4fe" category="inline-link-rx"></block> Finden Sie weitere Informationen.</block>
  <block id="6d017248f1e4f0ec7e2cd19d967b6bee" category="cell">Achten Sie in Solaris x86-Umgebungen besonders auf die richtige Ausrichtung, da die meisten Konfigurationen mehrere Ebenen von Partitionen haben. Solaris x86-Partitionsschichten befinden sich in der Regel oben auf einer Standard-Master-Bootdatensammelpartitionstabelle.</block>
  <block id="b1230060a0220caf9d68d64b9dd85e6f" category="summary">NFS-Transfergröße mit Oracle</block>
  <block id="fabe34ed4a5e432f4d1c4f6584956d8d" category="doc">NFS-Transfergröße mit Oracle Datenbanken</block>
  <block id="1e207db9a7c8127a6c43c771b89254fb" category="paragraph">Standardmäßig beschränkt ONTAP die NFS-I/O-Größe auf 64K.</block>
  <block id="72d22e3b9af95e272859e64eb1d82df7" category="paragraph">Zufälliger I/O mit den meisten Applikationen und Datenbanken verwendet eine viel kleinere Blockgröße, die weit unter dem 64K-Maximum liegt. Der I/O großer Blöcke wird in der Regel parallelisiert, sodass die 64K-Maximalgröße auch keine Einschränkung für die Erzielung der maximalen Bandbreite darstellt.</block>
  <block id="ebf61afddb34d070825eb696bea48813" category="paragraph">Es gibt einige Workloads, bei denen das 64K-Maximum eine Einschränkung darstellt. Insbesondere Vorgänge in einem einzigen Thread, wie Backup- oder Recovery-Vorgänge oder ein vollständiger Tabellenscan in einer Datenbank, laufen schneller und effizienter, wenn die Datenbank weniger, aber größere I/OS ausführen kann. Die optimale I/O-Handhabungsgröße für ONTAP beträgt 256 KB.</block>
  <block id="b0a5b394f3a29e64573049cda19fbacc" category="paragraph">Die maximale Übertragungsgröße für eine bestimmte ONTAP SVM kann wie folgt geändert werden:</block>
  <block id="b070b82d66be7116d5fd6dfd8172351b" category="cell">Verringern Sie niemals die maximal zulässige Übertragungsgröße auf ONTAP unter den Wert rsize/wsize der aktuell gemounteten NFS-Dateisysteme. Dies kann bei einigen Betriebssystemen zu Hängebleiben oder sogar Datenbeschädigungen führen. Wenn beispielsweise NFS-Clients derzeit auf 65536 rsize/wsize gesetzt sind, dann könnte die maximale Übertragungsgröße für ONTAP ohne Auswirkung auf die Clients selbst begrenzt werden, zwischen 65536 und 1048576 angepasst werden. Wenn Sie die maximale Übertragungsgröße unter 65536 verringern, können die Verfügbarkeit oder die Daten beeinträchtigt werden.</block>
  <block id="8eb703f2756cafd21dbac4f23c0d6416" category="paragraph">Bei aktivierter Inline-Komprimierung entfernt ONTAP effizient Blöcke, die auf Dateien oder LUNs geschrieben werden, auf Null gesetzt. Dienstprogramme wie das Oracle ASM Reclamation Utility (ASRU) schreiben Nullen in ungenutzte ASM-Extents.</block>
  <block id="9de53c14f2bae0c5d4b6e19be495ee94" category="paragraph">Aus der Datenbankperspektive enthält die ASM-Datenträgergruppe Nullen, und das Lesen dieser Bereiche der LUNs würde zu einem Strom von Nullen führen, ONTAP speichert die Nullen jedoch nicht auf Laufwerken. Stattdessen werden einfache Metadatenänderungen vorgenommen, die intern die Bereiche, in denen der Wert auf Null gesetzt wurde, als leer von Daten markieren.</block>
  <block id="06348c6e795160187e298a91b25a37a3" category="paragraph">Aus ähnlichen Gründen sind Performance-Tests mit gelöschten Daten nicht gültig, da Blöcke mit Nullen tatsächlich nicht als Schreibvorgänge innerhalb des Storage-Arrays verarbeitet werden.</block>
  <block id="cee584f1e6223f6dbb73c0d64d599228" category="admonition">Stellen Sie bei der Verwendung von ASRU sicher, dass alle von Oracle empfohlenen Patches installiert sind.</block>
  <block id="17ff263bc6eb701aecb8e79f9ef53efd" category="summary">LUN- und LVM-Größe mit Oracle-Datenbanken</block>
  <block id="ff15e8b125dae74c1231f09380e48886" category="paragraph">Wenn ein SAN-basiertes Dateisystem seine Kapazitätsgrenze erreicht hat, gibt es zwei Möglichkeiten, den verfügbaren Speicherplatz zu erhöhen:</block>
  <block id="4182200437c090276a01ad8ca54076b0" category="summary">Konfigurieren von NVFAIL zum Schutz von Oracle-Datenbanken</block>
  <block id="60e002164f8988e984cf4a95b240f19d" category="doc">Oracle und NVFAIL</block>
  <block id="370bdf5dd39278264774ea595054e243" category="paragraph">NVFAIL ist eine Funktion von ONTAP, die in katastrophalen Failover-Szenarien die Integrität sicherstellt.</block>
  <block id="80250c1f1d70fc693877be6f22cc134e" category="paragraph">Datenbanken sind bei Storage Failover-Ereignissen anfällig für Beschädigungen, da große interne Caches verfügbar sind. Wenn ein katastrophales Ereignis das Erzwingen eines ONTAP-Failovers oder das Erzwingen einer MetroCluster-Umschaltung erfordert, kann das Ergebnis, unabhängig vom Zustand der Gesamtkonfiguration, effektiv verworfen werden. Der Inhalt des Storage-Arrays springt zurück in die Zeit, und der Status des Datenbank-Cache entspricht nicht mehr dem Status der Daten auf der Festplatte. Diese Inkonsistenz führt zu Datenbeschädigung.</block>
  <block id="68114defc717054e7e3b02df438cdbe1" category="paragraph">Caching kann auf Applikations- oder Serverebene erfolgen. Beispielsweise werden in einer Oracle RAC-Konfiguration (Real Application Cluster) mit Servern, die sowohl auf einem primären Standort als auch an einem Remote-Standort aktiv sind, Daten innerhalb des Oracle SGA zwischengespeichert. Bei einem erzwungenen Switchover-Vorgang, der zu einem Datenverlust führte, würde die Datenbank beschädigt werden, da die im SGA gespeicherten Blöcke möglicherweise nicht mit den Blöcken auf der Festplatte übereinstimmen.</block>
  <block id="162de6d405c04ce8f152f1d246ed9187" category="paragraph">Eine weniger offensichtliche Verwendung von Caching erfolgt auf der Ebene des Betriebssystems. Blöcke aus einem gemounteten NFS-Filesystem können im OS zwischengespeichert werden. Alternativ kann ein geclustertes Filesystem, das auf LUNs am primären Standort basiert, auf Servern am Remote-Standort gemountet werden, und wieder einmal konnten Daten zwischengespeichert werden. Ein Ausfall von NVRAM oder eine erzwungene Übernahme oder ein erzwungenes Switchover kann in diesen Situationen zu einer Beschädigung des File-Systems führen.</block>
  <block id="04ca8d9ad570a3bdf1d76e23bc108ae5" category="paragraph">ONTAP schützt Datenbanken und Betriebssysteme vor diesem Szenario mit NVFAIL und den zugehörigen Einstellungen.</block>
  <block id="82ef718f4ebe0c276814ee3d51e33361" category="doc">SnapRestore</block>
  <block id="863696626734d6591db8be57b41c7eec" category="paragraph">Die schnelle Datenwiederherstellung in ONTAP anhand eines Snapshots wird durch die NetApp SnapRestore Technologie ermöglicht.</block>
  <block id="9099b669f87419bc03914f11f5779c60" category="paragraph">Wenn ein kritischer Datensatz nicht verfügbar ist, laufen die geschäftskritischen Prozesse ab. Tapes können beschädigt werden und selbst Restores aus festplattenbasierten Backups können die Übertragung über das Netzwerk verlangsamen. SnapRestore vermeidet diese Probleme durch eine nahezu sofortige Wiederherstellung der Datensätze. Selbst Datenbanken im Petabyte-Bereich lassen sich in wenigen Minuten vollständig wiederherstellen.</block>
  <block id="b02fb7dec22d0e276ca01cc60b9efb48" category="paragraph">Es gibt zwei Arten von SnapRestore: Datei-/LUN-basiert und Volume-basiert.</block>
  <block id="520e4b70b10aa2755a82b77e438a510f" category="list-text">Einzelne Dateien oder LUNs lassen sich innerhalb von Sekunden wiederherstellen, egal ob es sich um eine 2-TB-LUN oder eine 4-KB-Datei handelt.</block>
  <block id="45e8a84fe4ac58c8b30a7bdaf17c0b7f" category="list-text">Der Container von Dateien oder LUNs kann innerhalb von Sekunden wiederhergestellt werden, egal ob es sich um 10 GB oder 100 TB an Daten handelt.</block>
  <block id="4be4677be8cd434956b9b9af515b971a" category="paragraph">Ein „Container mit Dateien oder LUNs“ würde sich normalerweise auf ein FlexVol Volume beziehen. Beispielsweise können Sie 10 LUNs aufweisen, aus denen sich eine LVM-Festplattengruppe in einem einzelnen Volume befindet. Alternativ kann ein Volume die NFS-Home-Verzeichnisse von 1000 Benutzern speichern. Anstatt für jede einzelne Datei oder jedes LUN einen Wiederherstellungsvorgang auszuführen, können Sie das gesamte Volume als einzelnen Vorgang wiederherstellen. Der Prozess funktioniert auch mit horizontal skalierbaren Containern, die mehrere Volumes enthalten, wie z. B. eine FlexGroup oder eine ONTAP-Konsistenzgruppe.</block>
  <block id="55abea92d64d6738f9e0ed48799696e7" category="paragraph">ONTAP erlaubt nur schreibgeschützten Zugriff auf Snapshot-Daten, die Daten können jedoch mit SnapRestore reaktiviert werden. Der Snapshot wird als Lese-/Schreibansicht der Daten wieder aktiviert und gibt die Daten in ihren vorherigen Zustand zurück. SnapRestore kann auf Volume- oder Dateiebene betrieben werden. Die Technologie ist im Wesentlichen die gleiche mit ein paar geringfügigen Unterschiede im Verhalten.</block>
  <block id="f10f2af75e2770a8ed3c2fde594e986d" category="section-title">Volume SnapRestore</block>
  <block id="ca9846346bc3b03cdec10f9fff456866" category="paragraph">Volume-basierte SnapRestore stellt das gesamte Datenvolumen in einen früheren Zustand zurück. Dieser Vorgang erfordert keine Datenverschiebung, d. h., der Wiederherstellungsprozess erfolgt im Wesentlichen unmittelbar, obwohl die Verarbeitung der API- oder CLI-Vorgänge einige Sekunden dauern kann. Die Wiederherstellung von 1 GB Daten ist nicht komplizierter und zeitaufwändiger als die Wiederherstellung von 1 PB Daten. Diese Funktion ist der Hauptgrund dafür, dass viele Enterprise-Kunden zu ONTAP Storage-Systemen migrieren. Die RTO wird in Sekunden für selbst größte Datensätze gemessen.</block>
  <block id="627436e77bb011a6a312ae87b12f98ea" category="paragraph">Ein Nachteil von Volume-basierten SnapRestore ist die Tatsache, dass Änderungen innerhalb eines Volumes im Laufe der Zeit kumuliert werden. Daher sind jeder Snapshot und die Daten der aktiven Datei von den bis zu diesem Zeitpunkt vorgenommenen Änderungen abhängig. Das Zurücksetzen eines Volumes in einen früheren Zustand bedeutet, dass alle nachfolgenden Änderungen, die an den Daten vorgenommen wurden, verworfen werden. Weniger offensichtlich ist jedoch, dass dies nachträglich erstellte Snapshots einschließt. Das ist nicht immer wünschenswert.</block>
  <block id="5c956f2c1cfc5b9e5bac32ad1acba314" category="paragraph">Beispielsweise kann in einem SLA für die Datenaufbewahrung eine nächtliche Sicherung von 30 Tagen festgelegt werden. Wenn ein Datensatz auf einen vor fünf Tagen mit Datenträger SnapRestore erstellten Snapshot wiederhergestellt wird, werden alle in den letzten fünf Tagen erstellten Snapshots verworfen und dies verstößt gegen den SLA.</block>
  <block id="8bf9e697004f47ed6133aef724a8a42c" category="paragraph">Es gibt eine Reihe von Optionen, um diese Einschränkung zu beheben:</block>
  <block id="ad3b75456e625de5fe347fb918f59a22" category="list-text">Daten können von einem früheren Snapshot kopiert werden, anstatt eine SnapRestore des gesamten Volumes durchzuführen. Diese Methode eignet sich am besten für kleinere Datensätze.</block>
  <block id="90541eb74c7c56cadc2f90ec6fb6f0c0" category="list-text">Ein Snapshot kann geklont und nicht wiederhergestellt werden. Die Einschränkung dieses Ansatzes besteht darin, dass der Quell-Snapshot eine Abhängigkeit des Klons ist. Daher kann sie nur gelöscht oder in ein unabhängiges Volume aufgesplittet werden.</block>
  <block id="ab6045d3463c0f68ce2fa405dce9d554" category="list-text">Verwendung von dateibasiertem SnapRestore.</block>
  <block id="7f7599a8b92846b691484dc91e090164" category="section-title">File SnapRestore</block>
  <block id="f03eebe4f07d362dbfe4c1d4e76c724d" category="paragraph">Bei File-basierten SnapRestore handelt es sich um einen granulareren Snapshot-basierten Wiederherstellungsprozess. Anstatt den Status eines gesamten Volume zurückzusetzen, wird der Status einer einzelnen Datei oder LUN zurückgesetzt. Es müssen keine Snapshots gelöscht werden. Durch diesen Vorgang wird auch keine Abhängigkeit von einem vorherigen Snapshot erzeugt. Die Datei oder LUN ist im aktiven Volume sofort verfügbar.</block>
  <block id="e432fdcc277fc74a1b2b94cfeec8a7d2" category="paragraph">Bei einem SnapRestore Restore einer Datei oder eines LUN sind keine Datenverschiebungen erforderlich. Einige interne Metadaten-Updates sind jedoch erforderlich, um abzubilden, dass die zugrunde liegenden Blöcke in einer Datei oder einem LUN jetzt sowohl in einem Snapshot als auch in dem aktiven Volume vorhanden sind. Die Performance sollte sich nicht auswirken, doch bei diesem Prozess wird die Erstellung von Snapshots blockiert, bis dieser abgeschlossen ist. Die Verarbeitungsrate beträgt ca. 5 Gbit/s (18 TB/Stunde), basierend auf der Gesamtgröße der wiederhergestellten Dateien.</block>
  <block id="45c46768a1cb073f0d13245a0dc816f2" category="paragraph">Die richtige Datensicherungsarchitektur hängt von den geschäftlichen Anforderungen für die Datenaufbewahrung, Recovery-Fähigkeit und Ausfalltoleranz bei verschiedenen Ereignissen ab.</block>
  <block id="46e1b2893bdb002c0ede448cbfe1cb14" category="paragraph">Betrachten Sie beispielsweise die Anzahl der im Umfang enthaltenen Applikationen, Datenbanken und wichtigen Datensätze. Die Entwicklung einer Backup-Strategie für einen einzelnen Datensatz gewährleistet, dass die Compliance mit typischen SLAs relativ unkompliziert ist, da nicht viele Objekte zu managen sind. Je mehr Datensätze es gibt, desto komplizierter wird das Monitoring und Administratoren müssen sich zunehmend mit dem Vermeiden von Backup-Fehlern befassen. Wenn eine Umgebung also die Skalierung von Cloud- und Service-Provider-Umgebungen erreicht, braucht es einen ganz anderen Ansatz.</block>
  <block id="b70d233428e03d072b1134a97d853ab2" category="paragraph">Die Datensatzgröße wirkt sich auch auf die Strategie aus. Es gibt beispielsweise viele Optionen für Backup und Recovery mit einer Datenbank mit 100 GB, da die Datenmenge so klein ist. Das einfache Kopieren der Daten von Backup-Medien mit herkömmlichen Tools bietet normalerweise eine ausreichende RTO für die Recovery. Eine 100-TB-Datenbank benötigt normalerweise eine komplett andere Strategie, es sei denn, das RTO erlaubt einen mehrtägigen Ausfall. In diesem Fall könnte ein herkömmliches Backup und Recovery auf Basis von Kopien akzeptabel sein.</block>
  <block id="64848fee9c168231f5d0f13322ed0a87" category="paragraph">Schließlich gibt es Faktoren, die nicht dem Backup- und Recovery-Prozess selbst unterliegen. Gibt es zum Beispiel Datenbanken, die kritische Produktionsaktivitäten unterstützen, was das Recovery zu einem seltenen Ereignis macht, das nur von erfahrenen DBAs durchgeführt wird? Sind Datenbanken alternativ Teil einer großen Entwicklungsumgebung, in der häufig ein Recovery erfolgt und von einem IT-Team mit Generalisten gemanagt wird?</block>
  <block id="e26b57887de9a65c0316a87d49bf6c44" category="section-title">Ist ein Snapshot eine Sicherung?</block>
  <block id="c3fd105ab426caff0a0ade2e93476543" category="paragraph">Ein häufig vorgebrachter Einwand gegen die Verwendung von Snapshots als Datensicherungsstrategie ist die Tatsache, dass sich die „echten“ Daten und die Snapshot-Daten auf denselben Laufwerken befinden. Der Verlust dieser Laufwerke würde sowohl zum Verlust der Primärdaten als auch des Backups führen.</block>
  <block id="12d1adb01d24027626b2a47660fb5b81" category="paragraph">Das ist ein berechtigte Anliegen. Lokale Snapshots werden für tägliche Backup- und Recovery-Anforderungen verwendet, in dieser Hinsicht ist der Snapshot ein Backup. Beinahe 99 % aller Recovery-Szenarien in NetApp Umgebungen basieren auf Snapshots, um selbst die anspruchsvollsten RTO-Anforderungen zu erfüllen.</block>
  <block id="fc34c9966c8ecb7307468637d9e79933" category="paragraph">Lokale Snapshots sollten jedoch nie die einzige Backup-Strategie sein. Aus diesem Grund bietet NetApp Technologien wie SnapMirror Replizierung, mit denen Snapshots schnell und effizient auf einen unabhängigen Laufwerkssatz repliziert werden können. In einer richtig konzipierten Lösung mit Snapshots und Snapshot-Replikation kann die Verwendung von Tapes auf ein vierteljährliches Archiv minimiert oder ganz eliminiert werden.</block>
  <block id="1ea948130a9dbbc02e3a80648a99ff57" category="section-title">Backups von Konsistenzgruppen</block>
  <block id="9b234a126e57777ab83807827d4e7078" category="paragraph">Bei einem Konsistenzgruppenbackup wird der Status eines Datensatzes (oder mehrerer Datensätze) zu einem einzelnen atomischen Zeitpunkt erfasst. Als Datenbankbeispiel umfasst dies alle Datenbankkomponenten, z. B. Datendateien, Protokolldateien und andere Dateien, die direkt mit der Datenbank verknüpft sind. Dies funktioniert mit fast allen relationalen Datenbankprodukten, einschließlich Oracle RDBMS, Microsoft SQL Server, SAP HANA, PostgreSQL, MySQL und MariaDB. Der Schutz einer VMware-Konfiguration mit einer Consistency Group-Sicherung wäre ähnlich - Erfassung aller Datenspeicher und potenziell der ESX-Boot-LUNs in einem einzigen atomaren Point-in-Time.</block>
  <block id="5a1f7038c74e54c08669efa6722f1091" category="paragraph">Die Erstellung eines solchen Snapshots einer Konsistenzgruppe simuliert im Wesentlichen einen Absturz. Aus diesem Grund werden solche Backups häufig als Crash-konsistente Backups bezeichnet. Es gibt manchmal Bedenken bei der Unterstützung für Recovery-Szenarien, aber es ist wichtig zu verstehen, dass in der Regel kein Recovery-Verfahren erforderlich ist. Wenn die Anwendung nach dem Wiederherstellen einer Consistency Group-Sicherung gestartet wird, führt sie die üblichen Protokollwiederherstellungsprozesse, Wiederholungen von Dateisystemjournalen und andere Aufgaben aus, um alle I/O-Vorgänge, die am Zeitpunkt des Backups im laufenden Vorgang waren, wiederzugeben. Die Anwendung startet dann wie gewohnt.</block>
  <block id="aebdbddec25f7eba6434244a989fb659" category="paragraph">Somit können grundsätzlich alle Applikationen geschützt werden, die einem Stromausfall oder Serverabsturz ohne Datenbeschädigung standhalten. Dass dies funktioniert, zeigt sich auch an der großen Zahl an Applikationen, die mit synchronen und asynchronen Spiegelungsprodukten vieler verschiedener Anbieter geschützt sind. Wenn plötzlich am primären Standort ein Notfall eintritt, enthält der Replikatstandort zum Zeitpunkt des Ausfalls ein konsistentes Abbild der ursprünglichen Umgebung. Auch hier ist kein besonderes Recovery-Verfahren erforderlich.</block>
  <block id="e085ffefd89f8fb66260a9720e3fb6f8" category="paragraph">Der RPO für diesen Ansatz ist in der Regel auf den Punkt des Backups begrenzt. Im Allgemeinen beträgt der RPO-Mindestwert für Snapshots mit einem einzigen Volume eine Stunde. Zum Beispiel sind 48 stündliche Schnappschüsse plus weitere 30 Tage nächtlicher Schnappschüsse vernünftig und würden nicht die Aufbewahrung einer übermäßigen Anzahl von Snapshots erfordern. Ein RPO von unter einer Stunde ist schwieriger zu erreichen. Es wird nicht empfohlen, ohne zuvor die NetApp Professional Services einzuarbeiten, um sich mit den Anforderungen in Bezug auf Umgebung, Skalierung und Datensicherung vertraut zu machen.</block>
  <block id="1fe119a790b393b7dd7e79b63f15f52b" category="paragraph">Die RTO kann in der Regel in Sekunden gemessen werden. Eine Anwendung wird heruntergefahren, die Volumes wiederhergestellt und die Anwendung neu gestartet.</block>
  <block id="444cd304758b7607c8f7a3f627a5bc1f" category="paragraph">Am einfachsten legen Sie alle Dateien oder LUNs in einer einzelnen Volume-Konsistenzgruppe ab, wodurch die Erstellung von Snapshots direkt in ONTAP geplant werden kann. Wenn ein Datensatz über mehrere Volumes erstrecken muss, ist ein KonsistenzgruppenSnapshot (cg-Snapshot) erforderlich. Dies kann mit System Manager oder RESTful API-Aufrufen konfiguriert werden. Außerdem ist SnapCenter in der Lage, einen einfachen KonsistenzgruppenSnapshot auf einer definierten Liste von Volumes zu erstellen.</block>
  <block id="689b0155b42a554594a44cddc736eb67" category="section-title">Architektur für Replizierung und Disaster Recovery</block>
  <block id="1749d12df3df2319d3eee218a277ff83" category="inline-link-macro">MetroCluster</block>
  <block id="f443592081a2b3ddc55b94abe49ba128" category="paragraph">Das DR RPO wird durch die verfügbare Netzwerkbandbreite und die Gesamtgröße der zu sichernden Daten begrenzt. Nach der Erstellung des ersten Basistransfers basieren die Updates nur auf den geänderten Daten. Dies ist in der Regel ein geringer Prozentsatz des gesamten Datacenter-Platzbedarfs, obwohl es Ausnahmen gibt.</block>
  <block id="89e87a831dae60244ba34496d70fd45d" category="paragraph">Beispielsweise weist eine 10-TB-Datenbank mit einer wöchentlichen Änderungsrate von 10 % im Schnitt eine Gesamtänderung von 6 GB pro Stunde auf. Mit 10 GB Konnektivität benötigt diese Datenbank etwa sechs Minuten für die Übertragung. Die Änderungsrate unterliegt Schwankungen der Änderungsrate der Datenbank, sollte jedoch insgesamt ein Update-Intervall von 15 Minuten und somit ein RPO von 15 Minuten erreichbar sein. Wenn es 100 solche Datenbanken gibt, dann sind 600 Minuten erforderlich, um die Daten zu übertragen. Daher ist ein RPO von einer Stunde nicht möglich. Ebenso kann ein Replikat einer einzelnen Datenbank mit einer Größe von 100 TB mit einer wöchentlichen Änderungsrate von 10 % nicht innerhalb einer Stunde zuverlässig aktualisiert werden.</block>
  <block id="3c886a803aef21f5ac9f7bcea7408cbb" category="paragraph">Zusätzliche Faktoren können sich auf die Replikation auswirken, wie etwa der Overhead für die Replikation und Einschränkungen bei der Anzahl gleichzeitiger Replikationsvorgänge. Die Gesamtplanung einer Replikationsstrategie für ein einzelnes Volume kann jedoch auf der verfügbaren Bandbreite basieren, und ein Replikations-RPO von einer Stunde ist im Allgemeinen erreichbar. Ein RPO von unter einer Stunde ist schwieriger zu erreichen und sollte erst nach Rücksprache mit den NetApp Professional Services durchgeführt werden. In einigen Fällen sind 15 Minuten mit einer sehr guten Site-to-Site-Netzwerkverbindung möglich. Wenn jedoch insgesamt ein RPO von unter einer Stunde erforderlich ist, liefert die Architektur für die Wiedergabe mehrerer Volumes bessere Ergebnisse.</block>
  <block id="5e2cbdd214e7a73bca1af8ed5cc04626" category="paragraph">Das RTO mit Konsistenzgruppenreplikation in einem Disaster Recovery-Szenario ist ausgezeichnet, das aus Storage-Sicht in Sekunden gemessen wird. Der übersichtlichste Ansatz besteht darin, die Spiegelung zu zerbrechen und die Datenbank zu starten. Der Start der Datenbank dauert in der Regel ca. 10 Sekunden, aber sehr große Datenbanken mit vielen protokollierten Transaktionen können einige Minuten dauern.</block>
  <block id="1c18c939e73ddb1ceee34e8a33079d1a" category="paragraph">Der wichtigere Faktor bei der Bestimmung der RTO ist nicht das Speichersystem, sondern die Anwendung und das Host-Betriebssystem, auf dem es ausgeführt wird. Die replizierten Daten können beispielsweise in ein oder zwei Sekunden verfügbar gemacht werden, aber dies stellt nur die Daten dar. Es muss auch ein korrekt konfiguriertes Betriebssystem mit Anwendungsbinärdateien vorhanden sein, um die Daten verwenden zu können.</block>
  <block id="6fab898feea25b9d7fd4477911b90b31" category="paragraph">In einigen Fällen haben Kunden Disaster-Recovery-Instanzen schon vor der Zeit vorbereitet, da der Speicher auf Betriebssystemen vorerkannt wurde. In diesen Fällen erfordert die Aktivierung des Disaster-Recovery-Szenarios nicht mehr als das Brechen einer Spiegelung und das Starten der Anwendung. In anderen Fällen können das Betriebssystem und die zugehörigen Applikationen neben der Datenbank als ESX Virtual Machine Disk (VMDK) gespiegelt werden. In diesen Fällen hängt der RPO davon ab, wie viel ein Kunde in die Automatisierung investiert hat, um die VMDK schnell zu booten und damit die Applikationen zu starten.</block>
  <block id="c7d5d755b896edf43189fbb020202913" category="paragraph">Die Aufbewahrungszeit wird zum Teil durch das Snapshot Limit gesteuert. Volumes in ONTAP haben beispielsweise eine Grenze von 1024 Snapshots. In einigen Fällen haben Kunden die Replikation multipliziert, um das Limit zu erhöhen. Wenn zum Beispiel 2000 Tage Backups erforderlich sind, kann eine Quelle auf zwei Volumes repliziert werden, wobei Aktualisierungen an alternativen Tagen stattfinden. Dies erfordert eine Erhöhung des anfänglichen Platzbedarfs, stellt aber dennoch einen wesentlich effizienteren Ansatz dar als ein herkömmliches Backup-System, bei dem mehrere vollständige Backups durchgeführt werden müssen.</block>
  <block id="7ec0f114b69016879106b5018cdf10b9" category="section-title">Konsistenzgruppe in einem einzelnen Volume</block>
  <block id="cd70f3e596e275fe80fbbe5d7858ce1a" category="paragraph">Am einfachsten werden alle Dateien oder LUNs in einer einzigen Volume-Konsistenzgruppe abgelegt, wodurch SnapMirror und SnapVault Updates direkt im Storage-System geplant werden können. Es ist keine externe Software erforderlich.</block>
  <block id="909e8df57072d0b3c6d7d64b11acb571" category="section-title">Konsistenzgruppe mit mehreren Volumes</block>
  <block id="c7371eded617935ace6c71a33cfbf3fd" category="paragraph">Wenn eine Datenbank über mehrere Volumes hinweg erstellt werden muss, ist ein KonsistenzgruppenSnapshot (cg-Snapshot) erforderlich. Wie oben erwähnt, kann dies mit System Manager- oder RESTful-API-Aufrufen konfiguriert werden. Außerdem kann SnapCenter einen einfachen KonsistenzgruppenSnapshot auf einer definierten Liste von Volumes erstellen.</block>
  <block id="9c991b38c054d7bec0632efd6667540c" category="paragraph">Des Weiteren sollte die Verwendung von konsistenten Snapshots mit mehreren Volumes für Disaster Recovery zusätzlich berücksichtigt werden. Bei der Aktualisierung mehrerer Volumes kann es zu einer Katastrophe kommen, während noch ein Transfer durchgeführt wird. Das Ergebnis wäre ein Satz von Volumes, die nicht konsistent sind. In diesem Fall müssen einige Volumes in einen früheren Snapshot-Zustand zurückgesetzt werden, um ein Datenbank-Image zu liefern, das ausfallkonsistent und einsatzbereit ist.</block>
  <block id="fad8a5f46ebd89f74383b461e32dd6b9" category="section-title">Disaster Recovery: Aktivierung</block>
  <block id="5ac319591298468b0af89c2c469201f8" category="paragraph">Der Prozess zur Aktivierung der Disaster Recovery-Kopie hängt vom Speichertyp ab. Mit NFS können die Dateisysteme auf dem Disaster Recovery-Server vorgemountet werden. Sie befinden sich im schreibgeschützten Zustand und werden Lese- und Schreibzugriff, wenn die Spiegelung beschädigt ist. Dadurch verkürzen sich die RPO-Werte, und der gesamte Disaster Recovery-Prozess ist zuverlässiger, da weniger Teile gemanagt werden müssen.</block>
  <block id="b829fc11ff11b57af8d4632d38d7d7e8" category="paragraph">Die Aktivierung von SAN-Konfigurationen im Falle einer Disaster Recovery wird komplizierter. Die einfachste Option besteht im Allgemeinen darin, die Spiegelungen vorübergehend zu unterbrechen und die SAN-Ressourcen zu mounten, einschließlich Schritte wie das Erkennen der LVM-Konfiguration (einschließlich anwendungsspezifischer Funktionen wie Oracle Automatic Storage Management [ASM]) und das Hinzufügen von Einträgen zu /etc/fstab.</block>
  <block id="335a5796fb106a877544d121062534be" category="paragraph">Dies führt dazu, dass die LUN-Gerätepfade, Namen von Volume-Gruppen und andere Gerätepfade dem Zielserver bekannt werden. Diese Ressourcen können dann heruntergefahren und anschließend die Spiegelungen wiederhergestellt werden. Als Folge dessen befindet sich ein Server, durch den die Applikation schnell online geschaltet werden kann. Die einzelnen Schritte zur Aktivierung von Volume-Gruppen, zum Mounten von Dateisystemen oder zum Starten von Datenbanken und Anwendungen lassen sich einfach automatisieren.</block>
  <block id="b1a175c5f42da29f6c27f4ea29684b63" category="paragraph">Es ist unbedingt zu beachten, dass die Disaster-Recovery-Umgebung auf dem neuesten Stand ist. Beispielsweise werden neue LUNs wahrscheinlich dem Quellserver hinzugefügt. Das bedeutet, dass die neuen LUNs auf dem Ziel vorab erkannt werden müssen, damit der Disaster-Recovery-Plan wie erwartet funktioniert.</block>
  <block id="90426ba53f908049843b3ea392578d04" category="doc">Disaster Recovery mit Oracle</block>
  <block id="2a4fa9d9434561574119e3086112023f" category="paragraph">Der Schutz von Datendateien, Archivprotokollen, Wiederherstellungsprotokollen und Steuerdateien mit einem einzelnen Snapshot ist eine gültige Backup-, Restore- und Replizierungsmethode.  Das RPO ist jedoch auf den Punkt des Backups selbst beschränkt. Er eignet sich für einen RPO von einer Stunde oder länger. Wenn eine Datenbank mehrere Volumes umfasst, sind cg-Snapshots mit einem der zuvor beschriebenen Tools erforderlich.</block>
  <block id="93310f4cebc752304c164f00d67e0bca" category="paragraph">Beispielsweise kann sich die gesamte Datenbank in einem einzigen Volume mit dem folgenden Snapshot-Zeitplan befinden:</block>
  <block id="a33821be4b3522d65daed1ec317299f6" category="list-text">72 stündliche Snapshots</block>
  <block id="0eea0679ac7bf43c19d603cd78e81de9" category="list-text">30 nächtliche Schnappschüsse</block>
  <block id="c4546c401776d279c5577011584577fa" category="list-text">12 monatliche Snapshots</block>
  <block id="b63db1b650d128c342617b377baf928a" category="paragraph">Dadurch wird ein RPO von einer Stunde für den festgelegten Zeitraum von 72 Stunden sowie zusätzliche nächtliche und monatliche Backups erzielt. Mehrere Datenbanken oder Applikationsdateien können auch in das einzelne Volume oder den Satz von cg-Snapshots aufgenommen werden, um konsistente Backups in einer größeren Umgebung zu ermöglichen.</block>
  <block id="686540783626106489734a1990d4930d" category="paragraph">ONTAP wurde für eine maximale Verfügbarkeit von Oracle-Datenbanken konzipiert. Eine vollständige Beschreibung der Hochverfügbarkeitsfunktionen von ONTAP übersteigt den Rahmen dieses Dokuments. Wie bei der Datensicherheit ist jedoch ein grundlegendes Verständnis dieser Funktionalität bei der Entwicklung einer Datenbankinfrastruktur wichtig.</block>
  <block id="b049fcbe8198301adc6bf87634a02845" category="section-title">HA-Paare</block>
  <block id="1a7c11d1e2d03d3d1b57cff284ebd761" category="paragraph">Die Basiseinheit der Hochverfügbarkeit ist das HA-Paar. Jedes Paar enthält redundante Links, um die Replikation von Daten in NVRAM zu unterstützen. NVRAM ist kein Schreib-Cache. Der RAM im Controller dient als Schreib-Cache. Der Zweck von NVRAM besteht darin, Daten vorübergehend zu protokollieren, um Schutz vor unerwarteten Systemausfällen zu bieten. In dieser Hinsicht ähnelt es einem Datenbank-Redo-Protokoll.</block>
  <block id="cd2e54027dfcfeddd07ddd580db527c0" category="paragraph">Sowohl NVRAM als auch ein Datenbank-Wiederherstellungsprotokoll werden verwendet, um Daten schnell zu speichern, sodass Datenänderungen so schnell wie möglich vorgenommen werden können. Die Aktualisierung der persistenten Daten auf Laufwerken (oder Datendateien) findet erst später bei einem Prozess statt, der sowohl auf ONTAP- als auch auf den meisten Datenbankplattformen als Checkpoint bezeichnet wird. Weder NVRAM-Daten noch Datenbank-Wiederherstellungsprotokolle werden im normalen Betrieb gelesen.</block>
  <block id="4e044ec0c36e513506c77c0a97d53539" category="paragraph">Wenn ein Controller abrupt ausfällt, sind in NVRAM wahrscheinlich noch nicht gespeicherte Änderungen zu erwarten, die noch nicht auf die Laufwerke geschrieben wurden. Der Partner-Controller erkennt den Ausfall, übernimmt die Kontrolle über die Laufwerke und wendet die erforderlichen Änderungen an, die im NVRAM gespeichert wurden.</block>
  <block id="310e46747ebab6a43d5a15998f6e2b33" category="section-title">Takeover und Giveback</block>
  <block id="1d739263e44469ad28f77fe7329fa78d" category="paragraph">Takeover und Giveback beziehen sich auf den Prozess, bei dem die Verantwortung für Storage-Ressourcen zwischen Nodes in einem HA-Paar übertragen wird. Takeover und Giveback sind zweierlei Aspekte:</block>
  <block id="f2d43d90397aa8140414437eda14febb" category="list-text">Verwaltung der Netzwerkverbindung, die den Zugriff auf die Laufwerke ermöglicht</block>
  <block id="6e1c794c81468e3286a23138460bb3fb" category="list-text">Verwaltung der Antriebe selbst.</block>
  <block id="89d3f6c60c53a11e16a6756bde03ca00" category="paragraph">Die Netzwerkschnittstellen, die CIFS- und NFS-Datenverkehr unterstützen, werden sowohl mit dem Home-Standort als auch mit dem Failover-Standort konfiguriert. Eine Übernahme umfasst das Verschieben der Netzwerkschnittstellen zu ihrem temporären Home-Standort auf einer physischen Schnittstelle, die sich in denselben Subnetzen befindet wie der ursprüngliche Standort. Bei einem Giveback werden die Netzwerkschnittstellen zurück an ihre ursprünglichen Standorte verschoben. Das genaue Verhalten kann nach Bedarf angepasst werden.</block>
  <block id="108018656caf59cc565f1172047dab38" category="paragraph">Netzwerkschnittstellen, die SAN-Blockprotokolle wie iSCSI und FC unterstützen, werden während des Takeover und Giveback nicht verlagert. Stattdessen sollten LUNs mit Pfaden bereitgestellt werden, die ein vollständiges HA-Paar enthalten, was zu einem primären Pfad und einem sekundären Pfad führt.</block>
  <block id="80795d5dd590e432b4f5945aba47caf4" category="admonition">Zusätzliche Pfade zu zusätzlichen Controllern können auch konfiguriert werden, um das Verschieben von Daten zwischen Nodes in einem größeren Cluster zu unterstützen. Dies ist jedoch nicht Teil des HA-Prozesses.</block>
  <block id="aae63904d16fd1c522dc5ff53eb695b5" category="paragraph">Der zweite Aspekt von Takeover und Giveback ist die Übertragung der Eigentumsrechte an den Festplatten. Der genaue Prozess hängt von mehreren Faktoren ab, einschließlich dem Grund für das Takeover/Giveback und den ausgegebenen Befehlszeilenoptionen. Das Ziel ist es, die Operation so effizient wie möglich durchzuführen. Obwohl der Gesamtprozess möglicherweise mehrere Minuten in Anspruch nimmt, kann der tatsächliche Zeitpunkt, in dem die Eigentumsrechte an dem Laufwerk von einem Node auf einen Node übertragen werden, in der Regel in Sekunden gemessen werden.</block>
  <block id="61775b9e8f759f26eafd6b03448d1f30" category="section-title">Takeover-Zeit</block>
  <block id="3224b51e62790c4ce988d8c1876fb36e" category="paragraph">Host I/O durchläuft zwar eine kurze I/O-Pause bei Takeover- und Giveback-Vorgängen, jedoch sollte in einer korrekt konfigurierten Umgebung keine Applikationsunterbrechung auftreten. Der eigentliche Transitionsprozess, bei dem I/O verzögert wird, wird in der Regel in Sekunden gemessen. Der Host benötigt jedoch möglicherweise zusätzliche Zeit, um die Änderung der Datenpfade zu erkennen und die I/O-Vorgänge erneut auszuführen.</block>
  <block id="40f291752a9848bfecb7264cfd9aa8ee" category="paragraph">Die Art der Störung hängt vom Protokoll ab:</block>
  <block id="cb44db7841f8fb68230a260c4c68bfe0" category="list-text">Eine Netzwerkschnittstelle, die NFS- und CIFS-Datenverkehr unterstützt, stellt nach dem Übergang zu einem neuen physischen Standort eine ARP-Anforderung (Address Resolution Protocol) an das Netzwerk aus. Dies führt dazu, dass die Netzwerk-Switches ihre MAC-Adresstabellen (Media Access Control) aktualisieren und die I/O-Verarbeitung fortsetzen Im Falle von geplanten Takeover und Giveback werden Störungen in der Regel in Sekunden gemessen und oftmals nicht feststellbar. Einige Netzwerke sind möglicherweise langsamer, um die Änderung des Netzwerkpfads vollständig zu erkennen, und einige Betriebssysteme können in sehr kurzer Zeit viele I/O-Vorgänge in Warteschlange stellen, die erneut versucht werden müssen. Dadurch kann die für die I/O-Wiederaufnahme erforderliche Zeit verlängert werden</block>
  <block id="6a410522094e0b0f874dab5784e81ab8" category="list-text">Eine Netzwerkschnittstelle, die SAN-Protokolle unterstützt, kann nicht an einen neuen Speicherort verschoben werden. Ein Host-Betriebssystem muss den oder die verwendeten Pfade ändern. Die vom Host beobachtete I/O-Pause hängt von mehreren Faktoren ab. Aus Sicht des Storage-Systems beträgt der Zeitraum, in dem I/O nicht mehr ausgeführt werden kann, nur wenige Sekunden. Verschiedene Host-Betriebssysteme erfordern jedoch möglicherweise eine zusätzliche Zeit, damit eine I/O-Dauer vor einem erneuten Versuch wieder aberkannt wird. Neuere Betriebssysteme können eine Pfadänderung viel schneller erkennen, aber ältere Betriebssysteme benötigen in der Regel bis zu 30 Sekunden, um eine Änderung zu erkennen.</block>
  <block id="cabc4f9cda7d4bfb99181166c863a374" category="paragraph">Die zu erwartenden Übernahmezeiten, während denen das Storage-System keine Daten für eine Applikationsumgebung bereitstellen kann, sind in der folgenden Tabelle aufgeführt. Es sollte keine Fehler in einer Applikationsumgebung geben, das Takeover sollte stattdessen als kurze Pause bei der I/O-Verarbeitung erscheinen.</block>
  <block id="c8984126713ed072b9cb0a44a162be4d" category="cell">AFF</block>
  <block id="b7b46ce5d1a547db89c0bc615ff272a5" category="cell">ASA</block>
  <block id="ed3467fac861f6a32cd66093ac415805" category="cell">Geplante Übernahme</block>
  <block id="57a915dfa0e8469b25c1b8b947f7ee2c" category="cell">15 Sek.</block>
  <block id="cd5dce62294e9a79e9c2165ee0903f5f" category="cell">6-10 Sek.</block>
  <block id="cbfdbb11fd5eab0d9fdd078ae66a1c69" category="cell">2-3 Sek.</block>
  <block id="1a066f0304c1d3a279466c70359c5103" category="cell">Ungeplante Übernahme</block>
  <block id="0f4a6e109a34d3483995e427b6581130" category="cell">30 Sek.</block>
  <block id="dbfb054670bc82c6e08a7e101c0f0ef4" category="paragraph">Die logische Datensicherung innerhalb von ONTAP setzt sich aus drei Kernanforderungen zusammen:</block>
  <block id="b6cc6762545d78ab663e39c37d172bd2" category="list-text">Daten müssen vor Datenbeschädigung geschützt werden.</block>
  <block id="e7c7e14f4b6ca1b3ae32b2279243dba6" category="list-text">Die Daten müssen vor Laufwerksausfällen geschützt werden.</block>
  <block id="1168469b387027a850496f6cac9f3529" category="list-text">Änderungen an Daten müssen vor Verlust geschützt werden.</block>
  <block id="864a6ca5da4b36656beba11c90f4c4e5" category="paragraph">Diese drei Anforderungen werden in den folgenden Abschnitten erläutert.</block>
  <block id="a05b54004946aa523bfe7c1e92a52f52" category="section-title">Netzwerkkorruption: Prüfsummen</block>
  <block id="ed95581a613a4f19be367c10cd063cc2" category="paragraph">Die grundlegendste Stufe des Datenschutzes ist die Prüfsumme, die einen speziellen Fehler erkennenden Code ist, der neben den Daten gespeichert wird. Eine Beschädigung der Daten bei der Netzwerkübertragung wird mit Hilfe einer Prüfsumme und in einigen Fällen mehreren Prüfsummen erkannt.</block>
  <block id="c1343e51090396cf011509389bc0adab" category="paragraph">Ein FC-Frame enthält beispielsweise eine Form der Prüfsumme, die als zyklische Redundanzprüfung (CRC, Cyclic Redundancy Check) bezeichnet wird, um sicherzustellen, dass die Nutzlast während der Übertragung nicht beschädigt ist. Der Sender sendet sowohl die Daten als auch den CRC der Daten. Der Empfänger eines FC-Frames berechnet den CRC der empfangenen Daten neu, um sicherzustellen, dass er mit dem übertragenen CRC übereinstimmt. Wenn der neu berechnete CRC nicht mit dem CRC übereinstimmt, der dem Frame zugeordnet ist, sind die Daten beschädigt und der FC-Frame wird verworfen oder abgelehnt. Eine iSCSI-I/O-Operation umfasst Prüfsummen auf TCP/IP- und Ethernet-Ebenen und kann für zusätzlichen Schutz optional auch den CRC-Schutz auf der SCSI-Schicht beinhalten. Jede Bit-Beschädigung auf dem Kabel wird von der TCP-Schicht oder IP-Schicht erkannt, was zu einer erneuten Übertragung des Pakets führt. Wie bei FC führen Fehler im SCSI CRC zu einem Verwerfen oder Zurückweisen des Vorgangs.</block>
  <block id="1f08639c540e47068c16c3ac48ea7bbe" category="section-title">Laufwerkbeschädigungen: Prüfsummen</block>
  <block id="c5d50bc8d916a4b26166bbda091cd6d4" category="paragraph">Mit Prüfsummen wird auch die Integrität der auf Laufwerken gespeicherten Daten überprüft. Auf Laufwerke geschriebene Datenblöcke werden mit einer Prüfsummenfunktion gespeichert, die eine unvorhersehbare Anzahl ergibt, die mit den Originaldaten verknüpft ist. Wenn Daten vom Laufwerk gelesen werden, wird die Prüfsumme neu berechnet und mit der gespeicherten Prüfsumme verglichen. Wenn sie nicht übereinstimmt, sind die Daten beschädigt und müssen von der RAID-Schicht wiederhergestellt werden.</block>
  <block id="8edc44db01815f20f6508db8b5630cfe" category="section-title">Datenbeschädigung: Verlorene Schreibvorgänge</block>
  <block id="972eab86b2aecd693914feabc5a3b65c" category="paragraph">Eine der schwierigsten Arten von Korruption ist ein verlorenes oder falsch geschaltenes Schreiben. Wenn ein Schreibvorgang bestätigt wird, muss er an der richtigen Stelle auf das Medium geschrieben werden. Datenbeschädigungen lassen sich mithilfe einer einfachen Prüfsumme, die mit den Daten gespeichert wurde, relativ einfach erkennen. Wenn der Schreibvorgang jedoch einfach verloren geht, dann könnte die vorherige Version der Daten noch existieren und die Prüfsumme wäre korrekt. Wenn der Schreibvorgang an einem falschen physischen Speicherort platziert wird, ist die zugehörige Prüfsumme erneut für die gespeicherten Daten gültig, auch wenn der Schreibvorgang andere Daten zerstört hat.</block>
  <block id="53830741c46fbc560962b086f582e79a" category="paragraph">Die Lösung für diese Herausforderung ist wie folgt:</block>
  <block id="656d064400822ca2ffb25f52c9fc95db" category="list-text">Ein Schreibvorgang muss Metadaten enthalten, die den Speicherort angeben, an dem der Schreibvorgang erwartungsgemäß gefunden werden soll.</block>
  <block id="37a60b1c7392a26270880a361f85db55" category="list-text">Ein Schreibvorgang muss eine Art Versionskennung enthalten.</block>
  <block id="4275b2e68761baa23b112833facf4ca1" category="paragraph">Wenn ONTAP einen Block schreibt, schließt er Daten ein, zu denen der Block gehört. Wenn ein nachfolgender Lesezugriff einen Block identifiziert, der jedoch aufgrund der Metadaten zu Standort 123 gehört, als er an Position 456 gefunden wurde, wurde der Schreibvorgang fehlgestellt.</block>
  <block id="009b13bf07c7db21c1e4769526e60694" category="paragraph">Es ist schwieriger, einen vollständig verlorenen Schreibvorgang zu erkennen. Die Erklärung ist sehr kompliziert, aber im Wesentlichen speichert ONTAP Metadaten so, dass ein Schreibvorgang zu Updates an zwei verschiedenen Orten auf den Laufwerken führt. Wenn ein Schreibvorgang verloren geht, werden bei einem nachfolgenden Lesen der Daten und der zugehörigen Metadaten zwei unterschiedliche Versionsidentitäten angezeigt. Dies zeigt an, dass der Schreibvorgang vom Laufwerk nicht abgeschlossen wurde.</block>
  <block id="3b04a99c209a31ad69e7865ea94e5c94" category="paragraph">Verloren gegangene und falsch verlegte Schreibvorgänge sind äußerst selten, doch steigt mit zunehmendem Laufwerksanzahl und steigenden Datenmengen der Datensätze das Risiko. Jedes Storage-System, das Datenbank-Workloads unterstützt, sollte die verlorener Schreibschutz enthalten.</block>
  <block id="e978ccc010ec4d37148d52e2fbf439dd" category="section-title">Laufwerksausfälle: RAID, RAID DP und RAID-TEC</block>
  <block id="a9fffaaf2b1b518afb56ef273736c0c2" category="paragraph">Wenn ein Datenblock auf einem Laufwerk erkannt wird, dass er beschädigt ist oder das gesamte Laufwerk ausfällt und nicht verfügbar ist, müssen die Daten wiederhergestellt werden. Dies wird in ONTAP mithilfe von Paritätslaufwerken durchgeführt. Die Daten werden auf mehreren Datenlaufwerken verteilt und anschließend Paritätsdaten generiert. Diese wird getrennt von den Originaldaten gespeichert.</block>
  <block id="54abdf00b7122c16619266482eb09f43" category="paragraph">ONTAP verwendete ursprünglich RAID 4, das für jede Gruppe von Datenlaufwerken ein Single-Parity-Laufwerk verwendet. Das Ergebnis war, dass ein Laufwerk in der Gruppe ausfallen konnte, ohne dass es zu Datenverlust kam. Bei einem Ausfall des Paritätslaufwerks wurden keine Daten beschädigt und ein neues Paritätslaufwerk erstellt. Wenn ein einzelnes Datenlaufwerk ausfällt, können die verbleibenden Laufwerke zusammen mit dem Paritätslaufwerk verwendet werden, um die fehlenden Daten neu zu generieren.</block>
  <block id="6d1ccb512ce2b25e53d90e6d8d459e18" category="paragraph">Bei geringen Laufwerksanzahl war die statistische Wahrscheinlichkeit, dass zwei Laufwerke gleichzeitig ausfallen, vernachlässigbar. Mit wachsenden Laufwerkskapazitäten hat sich auch die Zeit entwickelt, die für die Wiederherstellung von Daten nach einem Laufwerksausfall benötigt wird. Dadurch erhöht sich das Zeitfenster, in dem ein zweiter Laufwerksausfall zum Datenverlust führen würde. Darüber hinaus erzeugt der Neuerstellungsvorgang eine Menge zusätzlicher I/O auf den verbleibenden Laufwerken. Mit zunehmendem Festplattenalter steigt auch das Risiko, dass die zusätzliche Last zu einem zweiten Laufwerksausfall führt. Selbst wenn das Risiko eines Datenverlusts mit der fortgesetzten Nutzung von RAID 4 nicht Anstieg, würden die Folgen eines Datenverlusts schwerwiegender. Je mehr Daten im Falle eines Ausfalls einer RAID-Gruppe verloren gehen würden, desto länger würde die Wiederherstellung der Daten dauern, wodurch die Unterbrechung des Geschäftsbetriebs käme.</block>
  <block id="caef9e75bcd55d5a9a4ec855b5a6385c" category="paragraph">Aus diesen Problemen entwickelte NetApp die NetApp RAID DP-Technologie, eine Variante von RAID 6. Diese Lösung umfasst zwei Paritätslaufwerke, d. h., zwei beliebige Laufwerke einer RAID-Gruppe können ohne Datenverlust ausfallen. Die Größe der Laufwerke wurde weiter vergrößert, wodurch NetApp schließlich die NetApp RAID-TEC-Technologie entwickelt hat, wodurch ein drittes Paritätslaufwerk eingeführt wird.</block>
  <block id="704c2697ee323569003b7b7ea203ae0f" category="paragraph">Einige bewährte Verfahren für historische Datenbanken empfehlen die Verwendung von RAID-10, auch als Striped Mirroring bekannt. Dies bietet weniger Datensicherheit als RAID DP, da mehrere zwei-Festplatten-Fehlerszenarien auftreten, während es in RAID DP keine gibt.</block>
  <block id="3d0147402d7506d1d3a5227fe447427a" category="paragraph">Es gibt auch einige historische Best Practices für Datenbanken, die darauf hinweisen, dass RAID-10 aufgrund von Performance-Bedenken den Optionen RAID-4/5/6 vorzuziehen ist. Diese Empfehlungen beziehen sich manchmal auf einen RAID-Abzug. Obwohl diese Empfehlungen in der Regel richtig sind, gelten sie nicht für die Implementierungen von RAID innerhalb von ONTAP. Die Leistungsbedenken beziehen sich auf die Paritäts-Regeneration. Bei herkömmlichen RAID-Implementierungen müssen bei der Verarbeitung der routinemäßigen, zufälligen Schreibvorgänge durch eine Datenbank mehrere Lesezugriffe auf die Festplatte durchgeführt werden, um die Paritätsdaten neu zu generieren und den Schreibvorgang abzuschließen. Der Abzug wird definiert als die zusätzlichen Lese-IOPS, die zum Ausführen von Schreibvorgängen erforderlich sind.</block>
  <block id="84ff015d2df40fc0ebb34485173f39e4" category="paragraph">Bei ONTAP kommt es nicht zu RAID-Einbußen, da Schreibvorgänge in den Speicher ausgelagert werden, wo Parität erzeugt wird und dann als einzelner RAID-Stripe auf die Festplatte geschrieben wird. Zum Abschließen des Schreibvorgangs sind keine Lesevorgänge erforderlich.</block>
  <block id="b7fe11ecbd8b434a231195335b4894ac" category="paragraph">Zusammengefasst bieten RAID DP und RAID-TEC im Vergleich zu RAID 10 viel mehr nutzbare Kapazität, besseren Schutz vor Festplattenausfällen und keine Performance-Einbußen.</block>
  <block id="e5e0dc0629a299f8e56ebe7de38f49f9" category="section-title">Schutz vor Hardware-Ausfällen: NVRAM</block>
  <block id="1db5fde40e489d805029f9d5be6375e7" category="paragraph">Jedes Storage-Array für Datenbank-Workloads muss Schreibvorgänge so schnell wie möglich durchführen. Darüber hinaus muss ein Schreibvorgang vor einem Verlust durch unerwartete Ereignisse, wie z. B. einen Stromausfall, geschützt werden. Das bedeutet, dass jeder Schreibvorgang sicher an mindestens zwei Orten gespeichert werden muss.</block>
  <block id="ff540b857af868ec85f8f53ddc1f1bd1" category="paragraph">AFF und FAS Systeme vertrauen zur Erfüllung dieser Anforderungen auf NVRAM. Der Schreibvorgang funktioniert wie folgt:</block>
  <block id="049f2df0de743a6be8d3ec69864f234a" category="list-text">Die eingehenden Schreibdaten werden im RAM gespeichert.</block>
  <block id="976be2293c2f4499d079224003644acf" category="list-text">Die Änderungen, die an Daten auf Festplatte vorgenommen werden müssen, werden sowohl auf dem lokalen Node als auch auf dem Partner-Node in NVRAM eingetragen. NVRAM ist kein Schreib-Cache, sondern ein Journal, das einem Datenbank-Wiederherstellungsprotokoll ähnelt. Unter normalen Bedingungen wird sie nicht gelesen. Sie wird nur für die Wiederherstellung verwendet, z. B. nach einem Stromausfall während der I/O-Verarbeitung.</block>
  <block id="8ae87ed4a421bec56e020cd0e83bd748" category="list-text">Der Schreibvorgang wird dann dem Host bestätigt.</block>
  <block id="3a7682bcbb490353074b5ee69e18f01c" category="paragraph">Der Schreibvorgang in dieser Phase ist aus Sicht der Applikation abgeschlossen, und die Daten sind vor Verlust geschützt, da sie an zwei verschiedenen Standorten gespeichert werden. Schließlich werden die Änderungen auf die Festplatte geschrieben, doch dieser Prozess ist aus Sicht der Applikation bandextern, da er nach dem Quittieren des Schreibvorgangs auftritt und sich somit nicht auf die Latenz auswirkt. Dieser Prozess ist wieder ähnlich wie die Datenbankprotokollierung. Eine Änderung an der Datenbank wird so schnell wie möglich in den Wiederherstellungsprotokollen aufgezeichnet und die Änderung wird dann als festgeschrieben bestätigt. Die Updates der Datendateien erfolgen viel später und haben keinen direkten Einfluss auf die Geschwindigkeit der Verarbeitung.</block>
  <block id="c17f09622da5f1e0064f7b6a65cb8b01" category="paragraph">Bei einem Controller-Ausfall übernimmt der Partner-Controller die erforderlichen Festplatten und gibt die protokollierten Daten im NVRAM wieder, um I/O-Vorgänge, die beim Ausfall gerade ausgeführt wurden, wiederherzustellen.</block>
  <block id="24e6592d53b3bd56ed8827d2f51bb1ab" category="section-title">Schutz vor Hardware-Ausfällen: NVFAIL</block>
  <block id="7965eb54acae120d25d0c77b012b8f90" category="paragraph">Wie zuvor bereits erläutert, wird ein Schreibvorgang erst bestätigt, wenn er in lokalem NVRAM und NVRAM auf mindestens einem anderen Controller angemeldet wurde. Dieser Ansatz stellt sicher, dass ein Hardware-Ausfall oder ein Stromausfall nicht zum Verlust der aktiven I/O führen Wenn der lokale NVRAM ausfällt oder die Verbindung zum HA-Partner ausfällt, werden diese aktiven Daten nicht mehr gespiegelt.</block>
  <block id="307b6d3e63c12ada7fbcf75cdda0b574" category="paragraph">Wenn der lokale NVRAM einen Fehler meldet, wird der Node heruntergefahren. Dieses Herunterfahren führt zu einem Failover auf einen HA-Partner-Controller. Es gehen keine Daten verloren, da der Controller den Schreibvorgang nicht bestätigt hat.</block>
  <block id="6e0c6c8fe69a50581a4ac18acb2c04dd" category="paragraph">ONTAP lässt kein Failover zu, wenn die Daten nicht synchron sind, es sei denn, das Failover wird erzwungen. Durch das Erzwingen einer solchen Änderung der Bedingungen wird bestätigt, dass Daten im ursprünglichen Controller zurückgelassen werden können und dass ein Datenverlust akzeptabel ist.</block>
  <block id="a3961862ed464a06ef6fa4e23935fef2" category="paragraph">Datenbanken sind besonders anfällig für Beschädigungen, wenn ein Failover erzwungen wird, da Datenbanken große interne Daten-Caches auf der Festplatte aufbewahren. Wenn ein erzwungenes Failover auftritt, werden zuvor bestätigte Änderungen effektiv verworfen. Der Inhalt des Storage Arrays springt effektiv zurück in die Zeit, und der Zustand des Datenbank-Cache entspricht nicht mehr dem Status der Daten auf der Festplatte.</block>
  <block id="b46ce3019d702ee3ece327e5df990be0" category="paragraph">Um Daten aus dieser Situation zu schützen, können mit ONTAP Volumes für speziellen Schutz vor NVRAM-Ausfällen konfiguriert werden. Wenn dieser Schutzmechanismus ausgelöst wird, gelangt ein Volume in den Status „NVFAIL“. Dieser Status führt zu I/O-Fehlern, die dazu führen, dass Applikationen heruntergefahren werden, sodass keine veralteten Daten verwendet werden. Daten sollten nicht verloren gehen, da alle bestätigten Schreibvorgänge auf dem Speicher-Array vorhanden sein sollten.</block>
  <block id="322e1f85910dfc206274f8bd58a54a9d" category="list-text">Jeder Laufwerkssatz an einem bestimmten Standort wird automatisch als eine oder mehrere vollständig redundante RAID-DP- oder RAID-TEC-Gruppen konfiguriert, und zwar unabhängig vom Einsatz der Spiegelung. So wird eine kontinuierliche Datensicherung auch nach dem Verlust eines Standorts gewährleistet.</block>
  <block id="dee2dfce5ab0c47301b3c12970a57aa6" category="paragraph">Die Abbildung oben zeigt eine Beispiel-SyncMirror-Konfiguration. Es wurde ein Aggregat mit 24 Laufwerken auf dem Controller mit 12 Laufwerken aus einem an Standort A zugewiesenen Shelf und 12 Laufwerken aus einem an Standort B zugewiesenen Shelf erstellt Die Laufwerke wurden in zwei gespiegelte RAID-Gruppen gruppiert. RAID-Gruppe 0 enthält einen Plex mit 6 Laufwerken an Standort A, der auf einen Plex mit 6 Laufwerken an Standort B gespiegelt wird Ebenso enthält RAID-Gruppe 1 einen Plex mit 6 Laufwerken an Standort A, der auf einen Plex mit 6 Laufwerken an Standort B gespiegelt wird</block>
  <block id="4fbc0abe91c0c1381d0a742f58b4e537" category="paragraph">Normalerweise wird SyncMirror für die Remote-Spiegelung bei MetroCluster Systemen verwendet, wobei eine Kopie der Daten an jedem Standort vorhanden ist. Gelegentlich wurde es verwendet, um eine zusätzliche Redundanz in einem einzigen System bereitzustellen. Insbesondere bietet sie Redundanz auf Shelf-Ebene. Ein Festplatten-Shelf enthält bereits duale Netzteile und Controller und ist im Großen und Ganzen etwas mehr als Bleche, doch in einigen Fällen ist möglicherweise der zusätzliche Schutz gewährleistet. Ein NetApp Kunde beispielsweise hat SyncMirror für eine mobile Echtzeitanalyse-Plattform für Automobiltests implementiert. Das System wurde in zwei physische Racks getrennt, die von unabhängigen USV-Systemen mit Strom versorgt wurden.</block>
  <block id="0cac56685de94a3ef0e1fd2bfca0c112" category="paragraph">Das Thema Prüfsummen ist von besonderem Interesse für DBAs, die es gewohnt sind, Oracle RMAN Streaming Backups zu Snapshot-basierten Backups zu verwenden. Eine Funktion von RMAN besteht darin, dass es während der Backups Integritätsprüfungen durchführt. Auch wenn dieses Feature einen gewissen Wert bietet, ist der Hauptvorteil für eine Datenbank, die nicht in einem modernen Storage-Array verwendet wird. Wenn physische Laufwerke für eine Oracle-Datenbank verwendet werden, ist es fast sicher, dass eine Beschädigung irgendwann auftritt, wenn die Laufwerke altern, ein Problem, das durch Array-basierte Prüfsummen in echten Storage-Arrays behoben wird.</block>
  <block id="8b7b33533c2b36c157a0a482bdd52a8e" category="paragraph">Die Architektur der Oracle-Datendatei- und des Wiederherstellungsprotokolls wurde auch für höchste Datenintegrität entwickelt, selbst unter extremen Bedingungen. Auf der einfachsten Ebene enthalten Oracle-Blöcke Prüfsumme und grundlegende logische Prüfungen mit fast jedem I/O Wenn Oracle nicht abgestürzt ist oder einen Tablespace offline genommen hat, sind die Daten intakt. Der Grad der Datenintegritätsprüfung ist einstellbar und Oracle kann auch zur Bestätigung von Schreibvorgängen konfiguriert werden. Dadurch können fast alle Crash- und Ausfallszenarien wiederhergestellt werden. Im äußerst seltenen Fall einer nicht wiederherstellbaren Situation wird eine Beschädigung umgehend erkannt.</block>
  <block id="4066c36b811913c85aa86346e30cee4d" category="paragraph">Die meisten NetApp-Kunden, die Oracle-Datenbanken einsetzen, beenden die Nutzung von RMAN und anderen Backup-Produkten nach der Migration zu Snapshot-basierten Backups. Es gibt nach wie vor Optionen, mit RMAN Recovery auf Blockebene mit SnapCenter durchgeführt werden kann. Allerdings werden RMAN, NetBackup und andere Produkte täglich nur gelegentlich verwendet, um monatliche oder vierteljährliche Archivkopien zu erstellen.</block>
  <block id="d0cf0e689669c61d564607d2ef7deb79" category="paragraph">Einige Kunden wählen zu laufen<block ref="d308cbfce7e270e5b41a84d84385645f" prefix=" " category="inline-code"></block> Regelmäßige Integritätsprüfungen der vorhandenen Datenbanken durchführen. NetApp rät von dieser Vorgehensweise ab, da dadurch unnötige I/O-Last erzeugt werden. Wie oben erwähnt, wenn die Datenbank zuvor keine Probleme hatte, die Chance von<block ref="d308cbfce7e270e5b41a84d84385645f" prefix=" " category="inline-code"></block> Das Erkennen eines Problems ist nahezu gleich null, und dieses Dienstprogramm erzeugt eine sehr hohe sequenzielle I/O-Last auf dem Netzwerk und dem Speichersystem. Es sei denn, es gibt Grund zu der Annahme, dass Korruption vorhanden ist, wie die Offenlegung eines bekannten Oracle-Fehlers, gibt es keinen Grund, ausgeführt zu werden<block ref="d308cbfce7e270e5b41a84d84385645f" prefix=" " category="inline-code"></block>.</block>
  <block id="431bbcf828b8d3513280cb1207824991" category="doc">Oracle Storage Snapshot optimierte Backups</block>
  <block id="c9447fe684870cf68e9cb4f23f44db43" category="paragraph">Obwohl DBAs mit der Hot-Backup-Wiederherstellung vertrauter sind, ist es seit langem möglich, Snapshots zu verwenden, die nicht erstellt wurden, während sich die Datenbank im Hot-Backup-Modus befand. Für Oracle 10g und 11g waren während der Recovery zusätzliche manuelle Schritte erforderlich, um die Datenbankkonsistenz zu gewährleisten. Mit Oracle 12c,<block ref="a84f4f3da79386c29ab6dfa560af786e" prefix=" " category="inline-code"></block> Und<block ref="075b7d3bd3bbe7767f7ef62f430bc22b" prefix=" " category="inline-code"></block> Enthalten die zusätzliche Logik zur Wiedergabe von Archivprotokollen für Datendatei-Backups, die sich nicht im Hot-Backup-Modus befanden.</block>
  <block id="5c7dba68615d3fa5c4c90a36dab57bcb" category="paragraph">Wie bereits erwähnt, erfordert die Wiederherstellung eines Snapshot-basierten Hot-Backups zwei Datensätze:</block>
  <block id="50a7086972cada154c8164dc1c6b3539" category="list-text">Ein Snapshot der Datendateien, der im Backup-Modus erstellt wurde</block>
  <block id="c72d009669a638aab75c21a28a2101de" category="list-text">Die Archivprotokolle, die generiert wurden, während sich die Datendateien im Hot-Backup-Modus befanden</block>
  <block id="b8de4b1201e5595bc3de878f2bf59d9b" category="paragraph">Während der Recovery liest die Datenbank Metadaten aus den Datendateien, um die erforderlichen Archivprotokolle für die Recovery auszuwählen.</block>
  <block id="66ee0a6fd90f3cfab8320e1de1341cbd" category="paragraph">Storage Snapshot optimierte Recovery erfordert geringfügig unterschiedliche Datensätze, um die gleichen Ergebnisse zu erzielen:</block>
  <block id="6030a1d9761fca3806ef0a0329438071" category="list-text">Ein Snapshot der Datendateien und eine Methode zur Identifizierung des Zeits, zu dem der Snapshot erstellt wurde</block>
  <block id="4e822d4c19600b654f176d335c158829" category="list-text">Archivieren Sie Protokolle vom Zeitpunkt des letzten Datendatei-Kontrollpunkts bis zum genauen Zeitpunkt des Snapshots</block>
  <block id="604432b50055c69f34df17a8ed5befb5" category="paragraph">Während der Recovery liest die Datenbank Metadaten aus den Datendateien, um das früheste erforderliche Archivprotokoll zu identifizieren. Eine vollständige oder zeitpunktgenaue Recovery kann durchgeführt werden. Bei einer zeitpunktgenauen Recovery ist es wichtig, die Zeit des Snapshots der Datendateien zu kennen. Der angegebene Wiederherstellungspunkt muss nach der Erstellungszeit der Snapshots liegen. NetApp empfiehlt, die Snapshot-Zeit um mindestens einige Minuten zu erweitern, um Uhrschwankungen zu berücksichtigen.</block>
  <block id="08bfe39c49a2dc07b08bd2ccd77d8281" category="paragraph">Ausführliche Informationen finden Sie in der Oracle-Dokumentation zum Thema „Recovery Using Storage Snapshot Optimization“, die in verschiedenen Versionen der Oracle 12c-Dokumentation verfügbar ist. Weitere Informationen zur Snapshot-Unterstützung von Drittanbietern finden Sie unter Oracle Document ID Doc ID 604683.1.</block>
  <block id="d22a890163762fcf7a4cb7605c29b7e6" category="section-title">Datenlayout</block>
  <block id="6a7fc1f74262f6973420f4fc849ba7f7" category="paragraph">Am einfachsten ist es, die Datendateien in einem oder mehreren dedizierten Volumes zu isolieren. Sie müssen durch einen anderen Dateityp nicht kontaminiert sein. Dadurch soll sichergestellt werden, dass die Datendatei-Volumes mit einem SnapRestore-Vorgang schnell wiederhergestellt werden können, ohne dass ein wichtiges Wiederherstellungsprotokoll, eine Steuerdatei oder ein Archivprotokoll zerstört werden.</block>
  <block id="ff6fb5d2136ab516e182f4b23f45886d" category="paragraph">SAN hat ähnliche Anforderungen für die Isolation von Datendateien in dedizierten Volumes. Bei einem Betriebssystem wie Microsoft Windows kann ein einzelnes Volume mehrere Datendatei-LUNs mit jeweils einem NTFS-Filesystem enthalten. Bei anderen Betriebssystemen gibt es in der Regel auch einen logischen Volume Manager. Mit Oracle ASM wäre es beispielsweise am einfachsten, Laufwerksgruppen auf ein einzelnes Volume zu beschränken, das als Einheit gesichert und wiederhergestellt werden kann. Wenn aus Gründen der Performance oder des Kapazitätsmanagements zusätzliche Volumes erforderlich sind, erleichtert die Erstellung einer zusätzlichen Laufwerksgruppe auf dem neuen Volume das Management.</block>
  <block id="a06956ba6de69aec2dc2b015074e5821" category="paragraph">Wenn diese Richtlinien befolgt werden, können Snapshots direkt auf ONTAP geplant werden, ohne dass ein Snapshot einer Konsistenzgruppe erforderlich ist. Der Grund hierfür liegt darin, dass Snapshot-optimierte Backups keine gleichzeitige Sicherung von Datendateien erfordern.</block>
  <block id="9e422c12420f1bd8c73e3c0f63da55e9" category="paragraph">Eine Komplikation entsteht in Situationen wie einer ASM-Datenträgergruppe, die auf Volumes verteilt ist. In diesen Fällen muss ein cg-Snapshot ausgeführt werden, um sicherzustellen, dass die ASM-Metadaten über alle zusammengehörigen Volumes hinweg konsistent sind.</block>
  <block id="7a7969d6a47ed242c25f6a38f2da0e36" category="paragraph">[Hinweis]Vergewissern Sie sich, dass sich die ASM-Spfile- und Passwd-Dateien nicht in der Festplattengruppe befinden, die die Datendateien hostet. Dies beeinträchtigt die Fähigkeit, Datendateien und nur Datendateien selektiv wiederherzustellen.</block>
  <block id="12eb3ceede91a01c74368e0fab03dbe4" category="section-title">Verfahren zur lokalen Wiederherstellung – NFS</block>
  <block id="e1a03fcc478ae212f07b5044b11ff369" category="paragraph">Dieses Verfahren kann manuell oder über eine Anwendung wie SnapCenter gesteuert werden. Das Grundverfahren ist wie folgt:</block>
  <block id="abb0083d6ab87bbe9d906632fea121ae" category="list-text">Stellen Sie die Datendatei-Volumes unmittelbar vor dem gewünschten Wiederherstellungspunkt auf den Snapshot wieder her.</block>
  <block id="3450fe1e1eea00fa55cfb5cb835a3d80" category="paragraph">Bei diesem Verfahren wird davon ausgegangen, dass die gewünschten Archivprotokolle noch im aktiven Dateisystem vorhanden sind. Wenn dies nicht der Fall ist, müssen die Archivprotokolle wiederhergestellt werden, oder<block ref="075b7d3bd3bbe7767f7ef62f430bc22b" prefix=" " category="inline-code"></block> Oder<block ref="a84f4f3da79386c29ab6dfa560af786e" prefix=" " category="inline-code"></block> Kann auf die Daten im weitergeleitet werden<block ref="a15da8cecb1f1192c4d913b8ba676121" prefix=" " category="inline-code"></block> Verzeichnis.</block>
  <block id="e43e33171833b26f6b024d28cb8b1afd" category="paragraph">Außerdem können Datendateien bei kleineren Datenbanken von einem Endbenutzer direkt aus wiederhergestellt werden<block ref="a15da8cecb1f1192c4d913b8ba676121" prefix=" " category="inline-code"></block> Directory ohne Unterstützung durch Automatisierungs-Tools oder einen Storage-Administrator, um einen SnapRestore-Befehl auszuführen.</block>
  <block id="097230f6ad5345abbe261eabbf533a68" category="section-title">Verfahren zur lokalen Wiederherstellung – SAN</block>
  <block id="4e3110543ebf2a771c16b3b2101f1f88" category="list-text">Legen Sie die Festplattengruppe(n), die die Datendateien hosten, still. Die Vorgehensweise hängt vom gewählten Logical Volume Manager ab. Bei ASM muss die Datenträgergruppe demontieren. Bei Linux müssen die Dateisysteme getrennt und die logischen Volumes und Volume-Gruppen deaktiviert werden. Ziel ist es, alle Aktualisierungen auf der Zieldatentengruppe zu stoppen, die wiederhergestellt werden sollen.</block>
  <block id="1d0ecd206531f96737f8e8002be4a047" category="list-text">Stellen Sie die Datendatei-Datenträgergruppen auf dem Snapshot unmittelbar vor dem gewünschten Wiederherstellungspunkt wieder her.</block>
  <block id="946fe41f610ab658e270a1844c992bcc" category="list-text">Reaktivieren Sie die neu wiederhergestellten Datenträgergruppen.</block>
  <block id="c21d8f41541fc2a0259e7ea5e51edac3" category="paragraph">Bei diesem Verfahren wird davon ausgegangen, dass die gewünschten Archivprotokolle noch im aktiven Dateisystem vorhanden sind. Wenn dies nicht der Fall ist, müssen die Archivprotokolle wiederhergestellt werden, indem die Archivprotokoll-LUNs offline geschaltet und eine Wiederherstellung durchgeführt wird. Dies ist ebenfalls ein Beispiel, bei dem sich Archivprotokolle in dedizierte Volumes aufteilen lassen. Wenn die Archivprotokolle eine Volume-Gruppe mit Wiederherstellungsprotokollen gemeinsam nutzen, müssen die Wiederherstellungsprotokolle vor der Wiederherstellung des gesamten LUN-Satzes an eine andere Stelle kopiert werden, damit die letzten aufgezeichneten Transaktionen nicht verloren gehen.</block>
  <block id="3fd5b2a08b3d7c4db29da8590ef6013f" category="section-title">Beispiel für eine vollständige Wiederherstellung</block>
  <block id="31679daa394b7091acfd728d940b7322" category="paragraph">Angenommen, die Datendateien wurden beschädigt oder zerstört, und eine vollständige Recovery ist erforderlich. Das Verfahren ist wie folgt:</block>
  <block id="aa09ca247daac25fc18c2633124ca9b6" category="section-title">Beispiel für eine zeitpunktgenaue Recovery</block>
  <block id="935ff627038d1f5001220c8df7f0e1b9" category="paragraph">Der gesamte Wiederherstellungsvorgang erfolgt über einen einzigen Befehl:<block ref="3df448f0a5ec82cac76d91ba14c0fa1d" prefix=" " category="inline-code"></block>.</block>
  <block id="dcd27cf8b8ceae2cd84c898d3c1b9fdb" category="paragraph">Wenn eine Point-in-Time-Recovery erforderlich ist, muss der Zeitstempel der Snapshots bekannt sein und kann wie folgt identifiziert werden:</block>
  <block id="17108ef3ad2e1e613b844beb5f2c6d29" category="paragraph">Die Erstellungszeit für Snapshots wird als 9. März und 10:10:06 aufgeführt. Um sicher zu sein, wird der Snapshot-Zeit eine Minute hinzugefügt:</block>
  <block id="904b0a17b003c70e398ea85996f0f9ba" category="paragraph">Die Wiederherstellung ist nun gestartet. Es gab eine Snapshot-Zeit von 10:11:00, eine Minute nach der aufgezeichneten Zeit, um mögliche Taktabweichungen zu berücksichtigen, und eine Ziel-Recovery-Zeit von 10:44 an. Als Nächstes fordert sqlplus die Archivprotokolle an, die benötigt werden, um die gewünschte Wiederherstellungszeit von 10:44 zu erreichen.</block>
  <block id="91f99c5ad04365fb4d00e509963e2a0c" category="admonition">Führen Sie die Wiederherstellung einer Datenbank mithilfe von Snapshots mit dem durch<block ref="3df448f0a5ec82cac76d91ba14c0fa1d" prefix=" " category="inline-code"></block> Für Befehl ist keine spezifische Lizenzierung erforderlich, aber die zeitpunktgenaue Recovery mit<block ref="72104026d9850e8c478c23b2e2e4e8e9" prefix=" " category="inline-code"></block> Erfordert die Oracle Advanced Compression-Lizenz.</block>
  <block id="9173402b63dc5f55506a692258f185e8" category="doc">SnapCenter und andere Tools</block>
  <block id="5459c1727c923baa303ff472498dbe97" category="paragraph">In manchen Fällen erfüllt eine einfache Konfiguration dieser Kernfunktionen direkt in ONTAP die Anforderungen, für kompliziertere Anforderungen ist jedoch eine Orchestrierungsschicht erforderlich.</block>
  <block id="aeeb3212c35c26d321183e81d4926e57" category="paragraph">SnapCenter ist das Vorzeigeprodukt für die Datensicherung von NetApp. Sie ähnelt im Hinblick auf die Durchführung von Datenbank-Backups den SnapManager Produkten. Sie wurde jedoch von Grund auf entwickelt, um bei NetApp Storage-Systemen eine zentrale Konsole für das Management der Daten zu bieten.</block>
  <block id="50780f47f6839d47d60bc4555ee00c3f" category="section-title">RUHE</block>
  <block id="a0bac638f30c704da8e03aba136da86d" category="paragraph">ONTAP enthält außerdem einen umfangreichen RESTful API-Satz. Drittanbieter sind so in der Lage, Datensicherungs- und Management-Applikationen mit enger Integration in ONTAP zu erstellen. Darüber hinaus kann die RESTful API von Kunden genutzt werden, die ihre eigenen Automatisierungs-Workflows und Dienstprogramme erstellen möchten.</block>
  <block id="a3b43ae3d4028e3c7ccac3d5720e9fb0" category="paragraph">Zwei Datensätze sind erforderlich, um eine Oracle Datenbank im Backup-Modus zu schützen und wiederherzustellen. Beachten Sie, dass dies nicht die einzige Oracle-Backup-Option ist, aber es ist die häufigste.</block>
  <block id="8e523970ff7c41dba74e723511d14717" category="list-text">Ein Snapshot der Datendateien im Backup-Modus</block>
  <block id="deeddf207b1a29c63ae7c1943f90f3d9" category="list-text">Die Archivprotokolle, die erstellt wurden, während sich die Datendateien im Backup-Modus befanden</block>
  <block id="fa1229103a2e02a4ed8790098c98f71f" category="paragraph">Wenn eine vollständige Recovery einschließlich aller festgeschriebenen Transaktionen notwendig ist, ist ein dritter Artikel erforderlich:</block>
  <block id="7c11b244c2549a2cc477c134ec4c5635" category="list-text">Ein Satz aktueller Wiederherstellungsprotokolle</block>
  <block id="8522faf14bc9d4ab497945bb48adc870" category="paragraph">Es gibt eine Reihe von Möglichkeiten, die Recovery eines Online-Backups zu fördern. Viele Kunden stellen Snapshots mithilfe der ONTAP CLI wieder her und verwenden dann Oracle RMAN oder sqlplus, um die Recovery abzuschließen. Dies ist besonders bei großen Produktionsumgebungen der Fall, in denen die Wahrscheinlichkeit und Häufigkeit der Wiederherstellung von Datenbanken äußerst gering ist und alle Wiederherstellungsverfahren von einem erfahrenen DBA durchgeführt werden. Für die vollständige Automatisierung verfügen Lösungen wie NetApp SnapCenter über ein Oracle Plug-in mit Befehlszeile und grafischer Benutzeroberfläche.</block>
  <block id="f88f41d4801183a5d53a4b71f383d144" category="paragraph">Einige große Kunden haben einen einfacheren Ansatz verfolgt, indem sie einfache Skripte auf den Hosts konfigurieren, um die Datenbanken zu einem bestimmten Zeitpunkt in den Backup-Modus zu versetzen, um einen geplanten Snapshot vorzubereiten. Planen Sie beispielsweise den Befehl<block ref="56f72994c4cc84b13a4c20dab49fea35" prefix=" " category="inline-code"></block> Um 23:58<block ref="84c8391d82cbfe300a9615844e0a167f" prefix=" " category="inline-code"></block> Um 00:02 Uhr, und planen Sie dann Snapshots direkt auf dem Speichersystem um Mitternacht. Das Ergebnis ist eine einfache, hochgradig skalierbare Backup-Strategie, für die keine externe Software oder Lizenzen erforderlich sind.</block>
  <block id="719fbeb74939b8c065321f175de3d222" category="paragraph">Am einfachsten ist es, Datendateien in einem oder mehreren dedizierten Volumes zu isolieren. Sie müssen durch einen anderen Dateityp nicht kontaminiert sein. Dadurch soll sichergestellt werden, dass die Datendatei-Volumes über einen SnapRestore-Vorgang schnell wiederhergestellt werden können, ohne dass ein wichtiges Wiederherstellungsprotokoll, eine Steuerdatei oder ein Archivprotokoll zerstört werden.</block>
  <block id="19156bdcbf322e8e3189909bbb7a69e8" category="paragraph">SAN hat ähnliche Anforderungen für die Isolation von Datendateien in dedizierten Volumes. Bei einem Betriebssystem wie Microsoft Windows kann ein einzelnes Volume mehrere Datendatei-LUNs mit jeweils einem NTFS-Filesystem enthalten. Bei anderen Betriebssystemen gibt es in der Regel einen logischen Volume Manager. Mit Oracle ASM wäre es beispielsweise am einfachsten, die LUNs einer ASM-Laufwerksgruppe auf ein einzelnes Volume zu beschränken, das als Einheit gesichert und wiederhergestellt werden kann. Wenn aus Gründen der Performance oder des Kapazitätsmanagements zusätzliche Volumes erforderlich sind, vereinfacht sich das Management durch die Erstellung einer zusätzlichen Festplattengruppe auf dem neuen Volume.</block>
  <block id="993afa8643b7200c5c9e527c24c1d8b2" category="paragraph">Wenn diese Richtlinien befolgt werden, können Snapshots direkt auf dem Speichersystem geplant werden, ohne dass ein Snapshot einer Konsistenzgruppe erforderlich ist. Der Grund hierfür liegt darin, dass für Oracle-Backups keine Datendateien gleichzeitig gesichert werden müssen. Das Online-Backup-Verfahren wurde entwickelt, damit Datendateien weiterhin aktualisiert werden können, da sie im Laufe der Stunden langsam auf Tape gestreamt werden.</block>
  <block id="a2ea5ea024e4a68e4d7f4abeca3441a1" category="paragraph">Eine Komplikation entsteht in Situationen wie der Verwendung einer ASM-Datenträgergruppe, die auf Volumes verteilt ist. In diesen Fällen muss ein cg-Snapshot ausgeführt werden, um sicherzustellen, dass die ASM-Metadaten über alle zusammengehörigen Volumes hinweg konsistent sind.</block>
  <block id="0a3ecb97c1b7c97e5075bae0daaa45aa" category="paragraph">*Achtung:* Überprüfen Sie, dass der ASM<block ref="1737629d60b511cf45957529a8f046e2" prefix=" " category="inline-code"></block> Und<block ref="76a2173be6393254e72ffa4d6df1030a" prefix=" " category="inline-code"></block> Die Dateien befinden sich nicht in der Festplattengruppe, in der die Datendateien gehostet werden. Dies beeinträchtigt die Fähigkeit, Datendateien und nur Datendateien selektiv wiederherzustellen.</block>
  <block id="2f8a5e254c86a42ebcc8b7d3c84cd22e" category="paragraph">Bei diesem Verfahren wird davon ausgegangen, dass die gewünschten Archivprotokolle noch im aktiven Dateisystem vorhanden sind. Ist dies nicht der Fall, müssen die Archivprotokolle wiederhergestellt werden oder rman/sqlplus kann zu den Daten im Snapshot-Verzeichnis geleitet werden.</block>
  <block id="a67a69f712a877950e94edd27310b40e" category="paragraph">Außerdem können Datendateien bei kleineren Datenbanken von einem Endbenutzer direkt aus wiederhergestellt werden<block ref="a15da8cecb1f1192c4d913b8ba676121" prefix=" " category="inline-code"></block> Verzeichnis ohne die Unterstützung von Automatisierungs-Tools oder Storage-Administratoren, ein auszuführen<block ref="a4c0ce4da6fb04943b499690fb3afd6e" prefix=" " category="inline-code"></block> Befehl.</block>
  <block id="6da8ebf3245e4fbe4654e7f92f0330d7" category="list-text">Legen Sie die Festplattengruppe(n), die die Datendateien hosten, still. Die Vorgehensweise hängt vom gewählten Logical Volume Manager ab. Bei ASM muss die Datenträgergruppe demontieren. Bei Linux müssen die Dateisysteme demontiert und die logischen Volumes und Volume-Gruppen deaktiviert werden. Ziel ist es, alle Aktualisierungen auf der Zieldatentengruppe zu stoppen, die wiederhergestellt werden sollen.</block>
  <block id="931ab211ac6773d20c03a998f9f921ce" category="list-text">Wiederholen Sie alle Wiederherstellungsprotokolle, wenn eine vollständige Wiederherstellung gewünscht wird.</block>
  <block id="b6575c3fca5eaaa56eeb8e9ce8867f05" category="paragraph">Bei diesem Verfahren wird davon ausgegangen, dass die gewünschten Archivprotokolle noch im aktiven Dateisystem vorhanden sind. Wenn dies nicht der Fall ist, müssen die Archivprotokolle wiederhergestellt werden, indem die Archivprotokoll-LUNs offline geschaltet und eine Wiederherstellung durchgeführt wird. Dies ist ebenfalls ein Beispiel, bei dem sich Archivprotokolle in dedizierte Volumes aufteilen lassen. Wenn die Archivprotokolle eine Volume-Gruppe mit Wiederherstellungsprotokollen gemeinsam nutzen, müssen die Wiederherstellungsprotokolle vor der Wiederherstellung des gesamten LUN-Satzes an eine andere Stelle kopiert werden. Dieser Schritt verhindert den Verlust dieser letzten aufgezeichneten Transaktionen.</block>
  <block id="65fd6f7f97414140b4d6060b743dc645" category="paragraph">Zu diesen Anforderungen gehören Faktoren wie die Geschwindigkeit der Recovery, der maximal zulässige Datenverlust und die Anforderungen an die Aufbewahrung von Backups. Der Datensicherungsplan muss zudem verschiedene gesetzliche Vorgaben für die Datenaufbewahrung und -Wiederherstellung berücksichtigen. Schließlich müssen verschiedene Datenwiederherstellungsszenarien in Betracht gezogen werden, von der typischen und vorhersehbaren Wiederherstellung aufgrund von Benutzer- oder Applikationsfehlern bis hin zu Disaster Recovery-Szenarien, die den vollständigen Ausfall eines Standorts beinhalten.</block>
  <block id="2bd1d35ff88de136066c33905c0fd677" category="paragraph">Kleine Änderungen an Richtlinien zur Datensicherung und Wiederherstellung können sich erheblich auf die Gesamtarchitektur von Storage, Backup und Recovery auswirken. Es ist wichtig, Standards zu definieren und zu dokumentieren, bevor mit dem Design begonnen wird, um eine Verkomplizierung einer Datensicherungsarchitektur zu vermeiden. Unnötige Schutzfunktionen oder -Ebenen führen zu unnötigen Kosten und Management-Overhead. Eine zunächst übersehene Anforderung kann ein Projekt in die falsche Richtung führen oder kurzfristig Designänderungen erfordern.</block>
  <block id="a60e6b87ede45aef49d8d095435db22b" category="section-title">Recovery-Zeitvorgabe</block>
  <block id="3d3b341e14c584d1edcea9fe13619ee1" category="paragraph">Die Recovery-Zeitvorgabe (Recovery Time Objective, RTO) definiert die maximal zulässige Zeit für die Recovery eines Services. Eine Personaldatenbank könnte beispielsweise eine RTO von 24 Stunden haben, da, obwohl es sehr unpraktisch wäre, den Zugriff auf diese Daten während der Arbeitszeit zu verlieren, das Unternehmen dennoch arbeiten kann. Im Gegensatz dazu würde bei einer Datenbank, die das Hauptbuch einer Bank unterstützt, eine RTO in Minuten oder sogar Sekunden gemessen werden. Ein RTO von null ist nicht möglich, da es eine Möglichkeit geben muss, zwischen einem tatsächlichen Serviceausfall und einem Routineereignis wie einem verlorenen Netzwerkpaket zu unterscheiden. Typische Anforderungen sind jedoch ein RTO von nahezu null.</block>
  <block id="5a27873285a0397cc06f9f47280c9426" category="section-title">Recovery-Zeitpunkt</block>
  <block id="f3ae061e139f47d793058ee596a5243f" category="paragraph">Der Recovery Point Objective (RPO) definiert den maximal tolerierbaren Datenverlust. In vielen Fällen wird der RPO lediglich durch die Häufigkeit von Snapshots oder snapmirror Updates bestimmt.</block>
  <block id="2d68fbd3cefeb34bfd64c8f89caba64d" category="paragraph">In manchen Fällen lässt sich der RPO-Wert aggressiver einsetzen, da er bestimmte Daten selektiv häufiger schützt. Im Datenbankkontext ist der RPO in der Regel eine Frage, wie viele Protokolldaten in einer bestimmten Situation verloren gehen können. In einem typischen Recovery-Szenario, bei dem eine Datenbank aufgrund eines Produktfehlers oder eines Benutzerfehlers beschädigt wird, sollte der RPO gleich null sein, d. h. es darf keine Daten verloren gehen. Bei der Wiederherstellung wird eine frühere Kopie der Datenbankdateien wiederhergestellt und anschließend die Protokolldateien wiedergegeben, um den Datenbankstatus auf den gewünschten Zeitpunkt zu bringen. Die für diesen Vorgang erforderlichen Protokolldateien sollten sich bereits am ursprünglichen Speicherort befinden.</block>
  <block id="57df8b025934bcd0f7e96bae19cf0688" category="paragraph">In ungewöhnlichen Szenarien können Protokolldaten verloren gehen. Zum Beispiel eine versehentliche oder böswillige<block ref="4609acba202877f99742342f4195d95e" prefix=" " category="inline-code"></block> Der Datenbankdateien können zum Löschen aller Daten führen. Die einzige Option wäre die Wiederherstellung aus dem Backup, einschließlich Protokolldateien, und einige Daten würden unweigerlich verloren gehen. Die einzige Option zur Verbesserung des RPO in einer herkömmlichen Backup-Umgebung besteht in der Durchführung wiederholter Backups der Protokolldaten. Dies hat jedoch Einschränkungen aufgrund der ständigen Datenverschiebung und der Schwierigkeiten, ein Backup-System als ständig laufenden Service zu warten. Einer der Vorteile erweiterter Storage-Systeme besteht in der Möglichkeit, Daten vor versehentlichen oder böswilligen Schäden an Dateien zu schützen und somit ein besseres RPO ohne Datenverschiebung zu ermöglichen.</block>
  <block id="3d08c71e26ac92e34925e02570c4bd22" category="paragraph">Disaster Recovery umfasst die IT-Architektur, Richtlinien und Verfahren, die zur Wiederherstellung eines Services bei einem physischen Ausfall erforderlich sind. Dies kann Überschwemmungen, Brände oder Personen sein, die mit böswilliger oder fahrlässiger Absicht handeln.</block>
  <block id="78f4e749ec0c6bb2627abbbc00bce296" category="paragraph">Disaster Recovery ist mehr als nur eine Reihe von Recovery-Verfahren. Der gesamte Prozess umfasst die Identifizierung der verschiedenen Risiken, die Definition der Anforderungen an die Datenwiederherstellung und die Servicekontinuität sowie die Bereitstellung der richtigen Architektur mit den zugehörigen Verfahren.</block>
  <block id="75bcd60b234aba82b5827cd7a5171563" category="paragraph">Bei der Festlegung von Datensicherungsanforderungen ist es entscheidend, zwischen den typischen RPO- und RTO-Anforderungen und den für die Disaster Recovery erforderlichen RPO- und RTO-Anforderungen zu unterscheiden. Einige Applikationsumgebungen erfordern einen RPO von null und ein RTO von nahezu null für Datenverluste – von einem relativ normalen Benutzerfehler bis hin zu einem Brand, der ein Datacenter zerstört. Für diese hohen Schutzniveaus gibt es jedoch Kosten- und administrative Konsequenzen.</block>
  <block id="756651fea87d05811d7a42451d074b60" category="paragraph">Im Allgemeinen sollten die Anforderungen an die nicht-Disaster-Recovery aus zwei Gründen strikt erfüllt werden. Zunächst sind Anwendungsfehler und Benutzerfehler, die zu Datenschäden führen, bis zu dem Punkt vorhersehbar, an dem sie fast unvermeidlich sind. Zweitens ist es nicht schwierig, eine Backup-Strategie zu entwickeln, die einen RPO von null und ein RTO von niedrigen Vorgaben liefern kann, solange das Storage-System nicht zerstört wird. Es gibt keinen Grund, ein erhebliches Risiko, das leicht behoben werden kann, nicht anzugehen. Deshalb sollten die RPO- und RTO-Ziele für die lokale Recovery aggressiv sein.</block>
  <block id="cb6faee28dd34d25f06bd0b33abe1c2f" category="paragraph">Disaster Recovery-RTO- und RPO-Anforderungen variieren stärker, je nach Wahrscheinlichkeit eines Ausfalls und den Folgen des damit verbundenen Datenverlusts oder der Unterbrechung des Geschäftsbetriebs. RPO- und RTO-Anforderungen sollten auf den tatsächlichen geschäftlichen Anforderungen basieren und nicht auf allgemeinen Prinzipien. Sie müssen mehrere logische und physische Ausfallszenarien berücksichtigen.</block>
  <block id="68f31611fc8a5e6a20793905280a7001" category="section-title">Logische Ausfälle</block>
  <block id="07f5727077caaf8dedf895542181d26b" category="paragraph">Zu logischen Katastrophen gehören Datenbeschädigungen durch Benutzer, Applikations- oder Betriebssystemfehler und Fehlfunktionen. Zu logischen Katastrophen können auch böswillige Angriffe durch externe Parteien mit Viren oder Würmern gehören oder die Ausnutzung von Schwachstellen von Applikationen. In diesen Fällen wird die physische Infrastruktur unbeschädigt, die zugrunde liegenden Daten sind jedoch nicht mehr gültig.</block>
  <block id="3cb5ee5f24e42f56ba6dac48b7b046d2" category="paragraph">Eine immer häufiger vorauftretende logische Katastrophe wird als Ransomware bezeichnet. Bei ihr werden Daten mit einem Angriffsvektor verschlüsselt. Die Verschlüsselung schädigt die Daten nicht, macht sie jedoch erst verfügbar, wenn die Zahlung an einen Dritten erfolgt. Immer mehr Unternehmen sind gezielt auf Ransomware-Hacks ausgerichtet. Für diese Bedrohung bietet NetApp manipulationssichere Snapshots, bei denen nicht einmal der Storage-Administrator geschützte Daten vor dem konfigurierten Ablaufdatum ändern kann.</block>
  <block id="91c87f53ce0a1f416308a67ce119c623" category="section-title">Physische Ausfälle</block>
  <block id="79055ca11f54a97d22617a656eab8b9b" category="paragraph">Zu physischen Ausfällen gehört der Ausfall von Komponenten einer Infrastruktur, die die Redundanzmerkmale übertreffen und zu einem Datenverlust oder erweitertem Service-Verlust führen. Der RAID-Schutz bietet beispielsweise Redundanz für Laufwerke, und die Verwendung von HBAs bietet Redundanz für FC-Port und FC-Kabel. Hardwareausfälle solcher Komponenten sind vorhersehbar und beeinträchtigen nicht die Verfügbarkeit.</block>
  <block id="3f7c55fe5f35f94cd2c4bd73b9cbb11b" category="paragraph">In einer Unternehmensumgebung ist es in der Regel möglich, die Infrastruktur eines gesamten Standorts mit redundanten Komponenten so weit zu schützen, dass das einzige vorhersehbare physische Ausfallszenario ein vollständiger Verlust des Standorts ist. Die Planung des Disaster Recovery hängt dann von der Site-to-Site-Replizierung ab.</block>
  <block id="7f3dd7bdd41f7af6853732a383bbd7e2" category="section-title">Synchrone und asynchrone Datensicherung</block>
  <block id="c5ac17d5693914368f39b40a07af23d4" category="paragraph">Im Idealfall würden alle Daten zwischen geografisch verteilten Standorten synchron repliziert werden. Eine solche Replikation ist nicht immer möglich oder sogar aus mehreren Gründen möglich:</block>
  <block id="28c59a675d03e1f62a48958bb53ae523" category="list-text">Die synchrone Replikation erhöht zwangsläufig die Schreiblatenz, da alle Änderungen an beiden Standorten repliziert werden müssen, bevor die Applikation/Datenbank mit der Verarbeitung fortfahren kann. Der daraus resultierende Performance-Effekt ist manchmal nicht akzeptabel, sodass die Verwendung von synchroner Spiegelung ausgeschlossen wird.</block>
  <block id="50a05ceeb998c4dac7e8b5d706fe2029" category="list-text">Die zunehmende Einführung von 100 % SSD-Storage bedeutet, dass zusätzliche Schreiblatenz mit größerer Wahrscheinlichkeit zu verzeichnen ist, da die Performance-Erwartungen Hunderttausende IOPS und eine Latenz von unter einer Millisekunde umfassen. Um das volle Potenzial von 100 % SSDs auszuschöpfen, kann ein erneuter Besuch der Disaster-Recovery-Strategie erforderlich sein.</block>
  <block id="04d90fe6288ce6cceba54a2b71e55727" category="list-text">Die Anzahl der Datensätze nimmt weiterhin an Byte zu. Dies stellt Unternehmen vor Herausforderungen, wenn es darum geht, genügend Bandbreite für eine synchrone Replizierung sicherzustellen.</block>
  <block id="d3aaae220bd5b3d68dd4ccec3bf78197" category="list-text">Die Komplexität der Datensätze nimmt zu und führt zu Herausforderungen beim Management einer umfassenden synchronen Replizierung.</block>
  <block id="c3a841e0bfc640f91b0a00c4af8c6f5c" category="list-text">Cloud-basierte Strategien sind häufig mit höheren Replizierungsentfernungen und Latenz verbunden, wodurch die Nutzung einer synchronen Spiegelung weiterhin ausgeschlossen wird.</block>
  <block id="7df1515e247dfa2063c433b9339c2d4a" category="paragraph">NetApp bietet Lösungen, die sowohl synchrone Replikation für höchste Anforderungen an die Datenwiederherstellung als auch asynchrone Lösungen für eine bessere Performance und Flexibilität beinhalten. Darüber hinaus lässt sich die NetApp Technologie nahtlos in viele Replizierungslösungen von Drittanbietern integrieren, wie z. B. Oracle DataGuard</block>
  <block id="4d2c802af835583121763e1a6acc3f9b" category="section-title">Aufbewahrungszeit</block>
  <block id="123dbb4c09a5c8fb993a856cf65c9d25" category="paragraph">Der letzte Aspekt einer Datensicherungsstrategie ist die Zeit für die Datenaufbewahrung, die sehr unterschiedlich sein kann.</block>
  <block id="d3a424909a8559a2d076e5d9a28d834f" category="list-text">Eine typische Anforderung sind nächtliche Backups von 14 Tagen auf dem primären Standort und 90 Tage Backups auf einem sekundären Standort.</block>
  <block id="1d7c2d97c3bfd5ca9489e0ffeb06b150" category="list-text">Viele Kunden erstellen vierteljährliche eigenständige Archive, die auf unterschiedlichen Medien gespeichert sind.</block>
  <block id="61ae175e4a0c88c4624184a0cb9b04d2" category="list-text">Eine ständig aktualisierte Datenbank benötigt möglicherweise keine Verlaufsdaten, und Backups müssen nur für einige Tage aufbewahrt werden.</block>
  <block id="8bb85c2149e1e808032e7025b0a27b80" category="list-text">Gesetzliche Vorschriften erfordern möglicherweise die Wiederherstellbarkeit bis zu einem beliebigen Zeitpunkt jeder beliebigen Transaktion innerhalb eines Zeitfensters von 365 Tagen.</block>
  <block id="05280349760feacdd6227fb8012e39d7" category="doc">Snapshot basierte Backups</block>
  <block id="a4c2aeedbfeeda21f232cbf45be91693" category="paragraph">Die wichtigsten Werte sind:</block>
  <block id="d06c89cff665ecdde6eded3c9d44cd2b" category="list-text">*Einfachheit.* Ein Snapshot ist eine schreibgeschützte Kopie des Inhalts eines Datencontainers zu einem bestimmten Zeitpunkt.</block>
  <block id="175888ba89c31a851f719bdd271cae9c" category="list-text">*Effizienz.* Snapshots benötigen zum Zeitpunkt der Erstellung keinen Platz. Der Speicherplatz wird nur dann verbraucht, wenn Daten geändert werden.</block>
  <block id="d853e9e275a6e58b3a9a56fa641cbb9f" category="list-text">*Verwaltbarkeit.* Eine auf Snapshots basierende Backup-Strategie lässt sich einfach konfigurieren und verwalten, da Snapshots ein nativer Teil des Storage-Betriebssystems sind. Wenn das Speichersystem eingeschaltet ist, kann es Backups erstellen.</block>
  <block id="b564e5fbab71af776ed54a19d09b268c" category="list-text">*Skalierbarkeit.* bis zu 1024 Backups eines einzigen Dateicontainers und LUNs können beibehalten werden. Bei komplexen Datensätzen können diverse Daten-Container durch einen einzelnen, konsistenten Satz von Snapshots gesichert werden.</block>
  <block id="7159872a568b97a971b9ea182fb23b7c" category="list-text">Die Performance bleibt davon unberührt, ob ein Volume 1024 Snapshots enthält oder keine.</block>
  <block id="1a54f7e14e6e652bb32b12df22faf387" category="paragraph">Viele Storage-Anbieter liefern zwar Snapshot-Technologie, doch ist die Snapshot Technologie bei ONTAP einzigartig und bietet in Enterprise-Applikations- und Datenbankumgebungen deutliche Vorteile:</block>
  <block id="d77b23e355dc2f65b90fb52ed9a90f9c" category="list-text">Snapshot Kopien sind Teil des zugrunde liegenden Write-Anywhere-Dateilayouts (WAFL). Es handelt sich nicht um ein Add-on oder eine externe Technologie. Dies vereinfacht das Management, da das Storage-System das Backup-System ist.</block>
  <block id="8b888e6a2d883c0111428c101532294a" category="list-text">Snapshot-Kopien beeinträchtigen die Performance nicht. Ausnahmen bilden Edge-Fälle, in denen so viele Daten in Snapshots gespeichert werden, dass sich das zugrunde liegende Storage-System füllt.</block>
  <block id="2283ea50a243a5d1ad551f7d21f39895" category="list-text">Der Begriff „Konsistenzgruppe“ wird häufig verwendet, um eine Gruppierung von Storage-Objekten zu referenzieren, die als konsistente Sammlung von Daten gemanagt werden. Ein Snapshot eines bestimmten ONTAP Volumes stellt ein Konsistenzgruppenbackup dar.</block>
  <block id="ac72c83a8204d600d567261b41c5488c" category="paragraph">ONTAP Snapshots lassen sich auch besser skalieren als bei Technologien von Mitbewerbern. Kunden können ohne Beeinträchtigung der Performance 5, 50 oder 500 Snapshots speichern. Derzeit sind in einem Volume maximal 1024 Snapshots zulässig. Wenn eine zusätzliche Snapshot-Aufbewahrung erforderlich ist, gibt es Optionen, die Snapshots an zusätzliche Volumes zu übergeben.</block>
  <block id="3b7ff39ba50f1bc3f56d38c41ba51855" category="paragraph">Daher ist die Sicherung eines auf ONTAP gehosteten Datensatzes einfach und hochskalierbar. Backups erfordern keine Verschiebung von Daten. Daher kann eine Backup-Strategie auf die Bedürfnisse des Unternehmens zugeschnitten werden und nicht auf die Beschränkungen der Netzwerkübertragungsraten, der großen Anzahl von Bandlaufwerken oder der Bereiche, in denen Festplatten bereitgestellt werden.</block>
  <block id="55c5bbb3aad88266600eaa1a526406d4" category="paragraph">Eine häufig gestellte Frage zur Verwendung von Snapshots als Datensicherungsstrategie ist die Tatsache, dass sich die „echten“ Daten und Snapshot-Daten auf denselben Laufwerken befinden. Der Verlust dieser Laufwerke würde sowohl zum Verlust der Primärdaten als auch des Backups führen.</block>
  <block id="68870deb35eeb53373325a314030bd11" category="paragraph">Lokale Snapshots sollten jedoch nie die einzige Backup-Strategie sein. Deshalb bietet NetApp Technologien wie SnapMirror und SnapVault-Replizierung, um Snapshots schnell und effizient auf einen unabhängigen Laufwerkssatz zu replizieren. In einer richtig konzipierten Lösung mit Snapshots und Snapshot-Replikation kann die Verwendung von Tapes auf ein vierteljährliches Archiv minimiert oder ganz eliminiert werden.</block>
  <block id="73d3fd255835ca3dc926f59d50223f91" category="paragraph">Für die Sicherung Ihrer Daten gibt es viele Optionen für den Einsatz von ONTAP Snapshots. Snapshots bilden die Basis vieler anderer ONTAP Funktionen wie Replizierung, Disaster Recovery und Klonen. Eine vollständige Beschreibung der Snapshot-Technologie geht über den Umfang dieses Dokuments hinaus. Die folgenden Abschnitte bieten jedoch einen allgemeinen Überblick.</block>
  <block id="7441577327d1d49b71cc2f6403e17e7b" category="paragraph">Es gibt zwei primäre Ansätze zum Erstellen eines Snapshots eines Datensatzes:</block>
  <block id="b38ba43128207852ef5149e6c578ba89" category="list-text">Absturzkonsistente Backups</block>
  <block id="dc64a2f6075820724476e435a1ebd7fa" category="list-text">Applikationskonsistente Backups</block>
  <block id="32561979f55bc68c4f028fd26a9975f6" category="paragraph">Absturzkonsistente Backups kommen vor allem dann zum Einsatz, wenn die Recovery am Point-of-the-Backup ausreichend ist. Wenn ein granulareres Recovery erforderlich ist, sind in der Regel applikationskonsistente Backups erforderlich.</block>
  <block id="367735591e5e6e04c17b7930acb29b7d" category="paragraph">Das Wort „konsistent“ in „anwendungskonsistent“ ist oft eine Fehlbezeichnung. Das Platzieren einer Oracle-Datenbank in den Backup-Modus wird beispielsweise als applikationskonsistentes Backup bezeichnet, die Daten werden jedoch in keiner Weise konsistent oder stillgelegt. Die Daten ändern sich während des Backups weiterhin. Im Gegensatz dazu machen die meisten MySQL und Microsoft SQL Server Backups die Daten tatsächlich stillgelegt, bevor sie das Backup ausführen. VMware kann bestimmte Dateien konsistent machen oder auch nicht.</block>
  <block id="ebf90723e7659cb5381545104b618612" category="paragraph">Der Begriff „Konsistenzgruppe“ bezieht sich auf die Fähigkeit eines Speicherarrays, mehrere Speicherressourcen als ein einziges Image zu verwalten. Beispielsweise kann eine Datenbank aus 10 LUNs bestehen. Das Array muss in der Lage sein, diese 10 LUNs konsistent zu sichern, wiederherzustellen und zu replizieren. Eine Wiederherstellung ist nicht möglich, wenn die Images der LUNs zum Zeitpunkt des Backups nicht konsistent waren. Die Replikation dieser 10 LUNs erfordert, dass alle Replikate perfekt miteinander synchronisiert sind.</block>
  <block id="3f8a43f1defaa984435092f616876545" category="paragraph">Der Begriff „Konsistenzgruppe“ wird nicht oft verwendet, wenn es um ONTAP geht, da Konsistenz immer eine Grundfunktion der Volume- und Aggregat-Architektur in ONTAP war. Viele andere Storage Arrays managen LUNs oder File-Systeme als einzelne Einheiten. Sie könnten aus Datenschutzgründen optional als „Konsistenzgruppe“ konfiguriert werden, dies ist jedoch ein zusätzlicher Schritt in der Konfiguration.</block>
  <block id="74e816c3c784f5c30c79dca4590d7f59" category="paragraph">ONTAP war schon immer in der Lage, konsistente lokale und replizierte Images von Daten zu erfassen. Auch wenn die verschiedenen Volumes auf einem ONTAP-System normalerweise nicht formal als Konsistenzgruppe beschrieben werden, so sind sie doch das. Ein Snapshot dieses Volumes ist ein Konsistenzgruppenabbild, die Wiederherstellung dieses Snapshots ist eine Wiederherstellung der Konsistenzgruppe, und sowohl SnapMirror als auch SnapVault bieten Konsistenzgruppenreplikation.</block>
  <block id="15cc4389a08c3f4748e98622e630ad3e" category="section-title">Snapshots von Konsistenzgruppen</block>
  <block id="d286b747757ef3d8d3abac27bfae65f9" category="paragraph">Konsistenzgruppen-Snapshots (cg-Snapshots) sind eine Erweiterung der grundlegenden ONTAP-Snapshot-Technologie. Bei einem standardmäßigen Snapshot-Vorgang wird ein konsistentes Image aller Daten innerhalb eines einzelnen Volumes erstellt. In manchen Fällen ist es jedoch erforderlich, einen konsistenten Satz von Snapshots über mehrere Volumes und sogar über mehrere Storage-Systeme hinweg zu erstellen. Das Ergebnis ist ein Satz von Snapshots, die auf die gleiche Weise wie ein Snapshot von nur einem einzelnen Volume verwendet werden können. Sie können für die lokale Datenwiederherstellung verwendet, für Disaster Recovery-Zwecke repliziert oder als einheitliche konsistente Einheit geklont werden.</block>
  <block id="7c199465bc6344e648e2f73c88131605" category="paragraph">Die größte Verwendung von cg-Snapshots ist eine Datenbankumgebung mit einer Größe von ca. 1 PB und 12 Controllern. Die cg-Snapshots, die auf diesem System erstellt wurden, werden für Backups, Wiederherstellungen und Klonvorgänge verwendet.</block>
  <block id="e5c199b4a97409f33d5caae984957744" category="paragraph">Wenn ein Datensatz über mehrere Volumes verteilt und die Schreibreihenfolge beibehalten werden muss, wird meist automatisch ein cg-Snapshot von der ausgewählten Managementsoftware verwendet. Es besteht in solchen Fällen nicht die Notwendigkeit, die technischen Details von cg-Snapshots zu verstehen. Allerdings gibt es Situationen, in denen komplizierte Datensicherungsanforderungen eine detaillierte Kontrolle über den Datenschutz- und Replizierungsprozess erfordern. Einige Optionen sind Automatisierungs-Workflows oder der Einsatz benutzerdefinierter Skripte, um cg-Snapshot-APIs aufzurufen. Das Verständnis der besten Option und der Rolle von cg-Snapshot erfordert eine detailliertere Erläuterung der Technologie.</block>
  <block id="cf3c30ea5c240a8aef02d240f42cdff7" category="paragraph">Die Erstellung eines Satzes von cg-Snapshots erfolgt in zwei Schritten:</block>
  <block id="f30455406a91bd776afbc88e60b9697e" category="list-text">Erstellung von Write Fencing auf allen Ziel-Volumes</block>
  <block id="c7afe45d94f2fd172a01e851d7f92a2f" category="list-text">Erstellen Sie Snapshots dieser Volumes im abgetrennten Zustand.</block>
  <block id="c464d4671e489eaad9a8cbf0a8086ea9" category="paragraph">Schreibzaun wird seriell hergestellt. Das bedeutet, dass bei der Einrichtung des Fencing-Prozesses über mehrere Volumes hinweg die I/O-Schreibvorgänge auf dem ersten Volume in der Sequenz eingefroren werden, da sie weiterhin auf Volumes übertragen werden, die später angezeigt werden. Dies mag anfänglich möglicherweise gegen die Vorgabe verstoßen, die Schreibreihenfolge zu erhalten, gilt aber nur für I/O-Vorgänge, die asynchron auf dem Host ausgegeben werden und nicht von anderen Schreibvorgängen abhängen.</block>
  <block id="688656b1f8644ee299edf7be87a8fc75" category="paragraph">Beispielsweise kann eine Datenbank eine Vielzahl asynchroner Datendatei-Updates ausgeben und dem Betriebssystem ermöglichen, die I/O-Vorgänge neu zu ordnen und sie gemäß seiner eigenen Scheduler-Konfiguration abzuschließen. Die Reihenfolge dieser E/A-Typen kann nicht garantiert werden, da die Anwendung und das Betriebssystem bereits die Anforderung zur Wahrung der Schreibreihenfolge freigegeben haben.</block>
  <block id="599ce07d1bae0fa6169959e2bdb97ada" category="paragraph">Als Zählerbeispiel sind die meisten Datenbankprotokollierungsaktivitäten synchron. Die Datenbank fährt erst mit weiteren Protokollschreibvorgängen fort, nachdem der I/O-Vorgang bestätigt wurde und die Reihenfolge dieser Schreibvorgänge erhalten bleiben muss. Wenn ein Protokoll-I/O auf einem Volume mit Fencing ankommt, wird dies nicht bestätigt, und die Applikation blockiert weitere Schreibvorgänge. Ebenso ist der I/O der Filesystem-Metadaten in der Regel synchron. Beispielsweise darf ein Dateilösch nicht verloren gehen. Wenn ein Betriebssystem mit einem xfs-Dateisystem eine Datei und den I/O gelöscht hat, der die xfs-Dateisystemmetadaten aktualisiert hat, um den Verweis auf diese Datei zu entfernen, der auf einem umzäunten Volume gelandet ist, wird die Dateisystemaktivität angehalten. Dies garantiert die Integrität des Dateisystems während cg-Snapshot-Vorgängen.</block>
  <block id="0f0c0f05310f9cf257d65bfd936f15c5" category="paragraph">Nach der Einrichtung von Write Fencing über die Ziel-Volumes hinweg sind sie für die Snapshot-Erstellung bereit. Die Snapshots müssen nicht genau zur gleichen Zeit erstellt werden, da der Zustand der Volumes aus einer abhängigen Schreibweise eingefroren wird. Um sich vor einem Fehler in der Anwendung zu schützen, die cg-Snapshots erstellt, enthält das anfängliche Write Fencing ein konfigurierbares Timeout, bei dem ONTAP die Fencing automatisch freigibt und die Schreibverarbeitung nach einer definierten Anzahl von Sekunden wieder aufnimmt. Wenn alle Snapshots erstellt werden, bevor die Zeitüberschreitung abgelaufen ist, dann ist der resultierende Snapshot-Satz eine gültige Konsistenzgruppe.</block>
  <block id="df4925d1c6c3f3258e80ff35ce62b17d" category="section-title">Abhängige Schreibreihenfolge</block>
  <block id="6c78ee478b93fbb573157b85fb1114db" category="paragraph">Aus technischer Sicht ist der Schlüssel zu einer Konsistenzgruppe die Aufrechterhaltung der Schreibreihenfolge und insbesondere der abhängigen Schreibreihenfolge. Beispielsweise wird eine Datenbank, die in 10 LUNs schreibt, gleichzeitig auf alle geschrieben. Viele Schreibvorgänge werden asynchron ausgegeben. Dies bedeutet, dass die Reihenfolge ihrer Fertigstellung unwichtig ist und die Reihenfolge ihrer Fertigstellung je nach Betriebssystem und Netzwerkverhalten variiert.</block>
  <block id="69d87c00869001f3f06ad4a1494eac21" category="paragraph">Einige Schreibvorgänge müssen auf der Festplatte vorhanden sein, bevor die Datenbank mit zusätzlichen Schreibvorgängen fortfahren kann. Diese kritischen Schreibvorgänge werden als abhängige Schreibvorgänge bezeichnet. Nachfolgende Schreib-I/O hängt davon ab, ob diese Schreibvorgänge auf der Festplatte vorhanden sind. Jeder Snapshot, jede Wiederherstellung oder Replikation dieser 10 LUNs muss sicherstellen, dass die abhängige Schreibreihenfolge gewährleistet ist. Dateisystemaktualisierungen sind ein weiteres Beispiel für Schreibvorgänge in Schreibreihenfolge. Die Reihenfolge, in der Dateisystemänderungen vorgenommen werden, muss beibehalten werden, oder das gesamte Dateisystem kann beschädigt werden.</block>
  <block id="74fbae5d0b3c16e1774c5f4dce2c1c36" category="section-title">Strategien</block>
  <block id="53e858edb9bd9ff3f9ead52cebf5731d" category="paragraph">Es gibt zwei primäre Ansätze bei Snapshot-basierten Backups:</block>
  <block id="f87f61f625a005bd32c53f808d235eea" category="list-text">Snapshot geschützte Hot-Backups</block>
  <block id="c32eba270db95747b471fd53e203a47b" category="paragraph">Absturzkonsistente Snapshot Backups werden in erster Linie verwendet, wenn die Recovery eines bestimmten Backup ausreichend ist. Archivprotokolle können unter bestimmten Umständen eingesetzt werden. Wenn jedoch eine granularere zeitpunktgenaue Recovery erforderlich ist, ist ein Online-Backup vorzuziehen.</block>
  <block id="82cc23cd5dc667cc9fb78ff6e76ac15b" category="paragraph">Das grundlegende Verfahren für ein Snapshot-basiertes Online-Backup ist wie folgt:</block>
  <block id="dbaf4855045e45fbcf678fa4cd1ab2db" category="list-text">Platzieren Sie die Datenbank in<block ref="402051f4be0cc3aad33bcf3ac3d6532b" prefix=" " category="inline-code"></block> Modus.</block>
  <block id="c3e07fb8aef84ef3d94e3f9d5376c091" category="list-text">Erstellen Sie einen Snapshot aller Volumes, die Datendateien hosten.</block>
  <block id="2d0e58ff1a25bd9fd8a85ad8ce6f2665" category="list-text">Beenden<block ref="402051f4be0cc3aad33bcf3ac3d6532b" prefix=" " category="inline-code"></block> Modus.</block>
  <block id="4196c5674aaa9e4317b2b4c54f94a905" category="list-text">Führen Sie den Befehl aus<block ref="9d2840394bc37850da4e360dbf60c0dc" prefix=" " category="inline-code"></block> So erzwingen Sie die Protokollarchivierung.</block>
  <block id="f2736e795871e2fc2d36addb9436f337" category="list-text">Erstellen Sie Snapshots aller Volumes, die die Archivprotokolle hosten.</block>
  <block id="f24880a3fe8b242fdd1aba5a2d11196f" category="paragraph">Dieses Verfahren ergibt einen Satz von Snapshots, die Datendateien im Backup-Modus enthalten, und die kritischen Archivprotokolle, die im Backup-Modus generiert wurden. Dies sind die beiden Anforderungen für das Recovery einer Datenbank. Dateien wie Kontrolldateien sollten ebenfalls aus Gründen der Bequemlichkeit geschützt werden, aber die einzige absolute Anforderung ist die Sicherung von Datendateien und Archivprotokollen.</block>
  <block id="0c32649da44277db19fcfa3fe4dac28f" category="paragraph">Auch wenn unterschiedliche Kunden möglicherweise sehr unterschiedliche Strategien verfolgen, basieren fast alle diese Strategien letztendlich auf den unten erläuterten Prinzipien.</block>
  <block id="88f451afed04f47352987f617f805826" category="section-title">Snapshot-basierte Recovery</block>
  <block id="c092a9763068c8382be82f9c77bb9658" category="paragraph">Beim Entwurf von Volume-Layouts für Oracle-Datenbanken ist die erste Entscheidung, ob die Volume-basierte VBSR-Technologie (NetApp SnapRestore) verwendet wird.</block>
  <block id="15a0b8dc4d1b340d403bf6515f7e61b8" category="paragraph">Mit Volume-basierten SnapRestore kann ein Volume fast sofort auf einen früheren Zeitpunkt zurückgesetzt werden. Da alle Daten auf dem Volume zurückgesetzt werden, ist VBSR möglicherweise nicht für alle Anwendungsfälle geeignet. Wenn beispielsweise eine gesamte Datenbank, einschließlich Datendateien, Wiederherstellungs- und Archivprotokolle, auf einem einzelnen Volume gespeichert ist und dieses Volume mit VBSR wiederhergestellt wird, gehen Daten verloren, da das neuere Archivprotokoll und die Wiederherstellungsdaten verworfen werden.</block>
  <block id="e63c0ca12b07a654ccadd92527a95c47" category="paragraph">VBSR ist für die Wiederherstellung nicht erforderlich. Viele Datenbanken können mithilfe von dateibasiertem Single-File SnapRestore (SFSR) oder einfach durch Kopieren von Dateien aus dem Snapshot zurück in das aktive Dateisystem wiederhergestellt werden.</block>
  <block id="b38bd603a952c93872eae2ea858cb15f" category="paragraph">Werden Datendateien auf diese Weise isoliert, können sie in einen früheren Zustand zurückgesetzt werden, ohne andere Filesysteme zu beschädigen.</block>
  <block id="5755a6275c3ecd99e8159efc58a8ff6c" category="section-title">Snapshot Reserve</block>
  <block id="957abb72f2db3a20e0b570a5fa3a1dcc" category="paragraph">Für jedes Volume mit Oracle-Daten in einer SAN-Umgebung die<block ref="9ca25b30dcc119f9115aad6d85fd12ce" prefix=" " category="inline-code"></block> Sollte auf null gesetzt werden, da das Reservieren von Speicherplatz für einen Snapshot in einer LUN-Umgebung nicht nützlich ist. Wenn die fraktionale Reserve auf 100 eingestellt ist, benötigt ein Snapshot eines Volumes mit LUNs genug freien Platz im Volumen, ausgenommen die Snapshot-Reserve, um 100% Umsatz aller Daten aufzunehmen. Wenn die fraktionale Reserve auf einen niedrigeren Wert eingestellt ist, dann ist entsprechend weniger freier Speicherplatz erforderlich, schließt jedoch immer die Snapshot Reserve aus. Das bedeutet, dass der Speicherplatz der Snapshot-Reserve in einer LUN-Umgebung verschwendet wird.</block>
  <block id="d5a06adf11e69f265f3ee3277212764e" category="paragraph">In einer NFS-Umgebung gibt es zwei Optionen:</block>
  <block id="7631336a8e2e03f6ac500c145445b2d7" category="list-text">Stellen Sie die ein<block ref="9ca25b30dcc119f9115aad6d85fd12ce" prefix=" " category="inline-code"></block> Basiert auf dem erwarteten Snapshot-Speicherplatzverbrauch.</block>
  <block id="acc4c5d06fc4a9ac93880c9c2d0a6e76" category="list-text">Stellen Sie die ein<block ref="9ca25b30dcc119f9115aad6d85fd12ce" prefix=" " category="inline-code"></block> Zur gemeinsamen Nutzung von Speicherplatz und Snapshots sowie zur Vermeidung und zum Management dieser Kapazitäten.</block>
  <block id="44806986c5abfcd6261803f4f75a7b56" category="paragraph">Mit der ersten Option<block ref="9ca25b30dcc119f9115aad6d85fd12ce" prefix=" " category="inline-code"></block> Wird auf einen Wert ungleich Null gesetzt, normalerweise etwa 20 %. Dieser Raum wird dann vor dem Benutzer ausgeblendet. Dieser Wert schafft jedoch keine Begrenzung der Auslastung. Wenn bei einer Datenbank mit einer Reservierung von 20 % 30 % anfällt, kann der Snapshot-Platz über die Grenze der 20-prozentigen Reserve hinauswachsen und nicht reservierten Speicherplatz belegen.</block>
  <block id="c8910acb0bfa30f82b423ff09a9710ca" category="paragraph">Der Hauptvorteil, wenn Sie eine Reserve auf einen Wert wie 20% setzen, besteht darin zu überprüfen, ob etwas Speicherplatz für Snapshots immer verfügbar ist. Bei einem 1-TB-Volume mit einer Reserve von 20 % wäre es beispielsweise nur einem Datenbankadministrator (DBA) möglich, 800 GB an Daten zu speichern. Diese Konfiguration garantiert mindestens 200 GB Speicherplatz für den Snapshot-Verbrauch.</block>
  <block id="be233226a8cb890c0a8bbf041c06ac6b" category="paragraph">Wenn<block ref="9ca25b30dcc119f9115aad6d85fd12ce" prefix=" " category="inline-code"></block> Ist auf null festgelegt, sodass der gesamte Speicherplatz im Volume für den Endbenutzer verfügbar ist, sodass bessere Sichtbarkeit gewährleistet wird. Ein DBA muss verstehen, dass ein 1-TB-Volume, das Snapshots nutzt, 1 TB Speicherplatz zwischen aktiven Daten und dem Snapshot-Umsatz gemeinsam genutzt wird.</block>
  <block id="b10efee713968b90d211d08ec8345d20" category="paragraph">Es gibt keine klare Präferenz zwischen Option 1 und Option 2 unter den Endbenutzern.</block>
  <block id="8112d08173571f2d01372cadea3ea76d" category="section-title">Snapshots von ONTAP und Drittanbietern</block>
  <block id="0cc082f7cc7618a904f6cde556295cbd" category="paragraph">Oracle Doc ID 604683.1 erläutert die Anforderungen für die Snapshot-Unterstützung von Drittanbietern und die verschiedenen verfügbaren Optionen für Backup- und Wiederherstellungsvorgänge.</block>
  <block id="130a02f8381e6c54b2ea07d2bdaf6f0b" category="paragraph">Der Drittanbieter muss sicherstellen, dass die Snapshots des Unternehmens den folgenden Anforderungen entsprechen:</block>
  <block id="97eb59a0f249226523b31dd52d557e00" category="list-text">Snapshots müssen sich in die von Oracle empfohlenen Restore- und Recovery-Vorgänge integrieren.</block>
  <block id="083e9332dc2b6bf064139ac2e670eee9" category="list-text">Snapshots müssen zum Zeitpunkt des Snapshots auch beim Absturz einer Datenbank konsistent sein.</block>
  <block id="9dae8634b638a6949dab0f7eaf7988bd" category="list-text">Die Schreibreihenfolge wird für jede Datei in einem Snapshot beibehalten.</block>
  <block id="7bc868657652757ca8505ddcd297456f" category="paragraph">Die Oracle Managementprodukte von ONTAP und NetApp erfüllen diese Anforderungen.</block>
  <block id="9e19caeb431a5e29a1a63e3cee4b36c9" category="doc">Oracle Datensicherung mit ONTAP</block>
  <block id="848aab87f65bfade12f80d4e864ec046" category="paragraph">Ein Unternehmen kann nicht ohne Zugriff auf seine Daten arbeiten, und manchmal definieren die Daten das Unternehmen. Diese Daten müssen geschützt werden. Bei der Datensicherung geht es jedoch mehr als nur um das Sicherstellen eines nutzbaren Backups. Es geht darum, die Backups schnell und zuverlässig durchzuführen und diese sicher zu speichern.</block>
  <block id="28a0304f97f876b27cf541f4ad9a30d7" category="paragraph">Die andere Seite der Datensicherung ist die Datenwiederherstellung. Wenn auf Daten nicht zugegriffen werden kann, ist das Unternehmen betroffen und kann nicht mehr in Betrieb sein, bis die Daten wiederhergestellt werden. Dieser Prozess muss schnell und zuverlässig sein. Schließlich müssen die meisten Datenbanken vor Ausfällen geschützt werden, was bedeutet, dass ein Replikat der Datenbank beibehalten wird. Das Replikat muss ausreichend aktuell sein. Außerdem muss es schnell und einfach sein, das Replikat zu einer voll funktionsfähigen Datenbank zu machen.</block>
  <block id="3d601b26d05eac41fc6a7eba92fdc8b9" category="admonition">Diese Dokumentation ersetzt den zuvor veröffentlichten technischen Bericht _TR-4591: Oracle Data Protection: Backup, Recovery und Replication._</block>
  <block id="21d28aa03f67eb242b1c95a6a0594ff5" category="section-title">Planung</block>
  <block id="355ba59494fea8b20be391a73cc755eb" category="paragraph">Das genaue Testen der Datenbank-Storage-Performance ist dabei ein extrem kompliziertes Thema. Sie müssen die folgenden Probleme verstehen:</block>
  <block id="b57041cc3b57577b5c21eff44739e3f9" category="list-text">IOPS und Durchsatz</block>
  <block id="236e7fd957ada95e8094e302623980d3" category="list-text">Der Unterschied zwischen Vorder- und Hintergrund-I/O-Vorgängen</block>
  <block id="902f90cc4ab59808cac5606f767f61c4" category="list-text">Auswirkungen der Latenz auf die Datenbank</block>
  <block id="196198dbaa1d1ea17cfc478fbb2f419f" category="list-text">Zahlreiche Betriebssystem- und Netzwerkeinstellungen, die ebenfalls die Storage-Performance beeinträchtigen</block>
  <block id="5ff18eadb2ef3fc20e13bed3e5b61d79" category="paragraph">Darüber hinaus müssen Aufgaben außerhalb von Storage-Datenbanken berücksichtigt werden. An diesem Punkt kann die Optimierung der Storage-Performance keine nützlichen Vorteile ergeben, da die Storage-Performance keinen einschränkenden Faktor mehr für die Performance darstellt.</block>
  <block id="32a993d3f497a6dd4392d3348b05ebc3" category="list-text">Die Netzwerkbandbreite ist eine immer häufiger auftretende Ursache für Leistungseinschränkungen. Lösungen mit rotierenden Festplatten sind beispielsweise häufig Engpässe in der Datenbank-Performance, da die I/O-Latenz sehr hoch ist. Wenn Latenzbeschränkungen von einem All-Flash-Array beseitigt werden, verschiebt sich die Barriere häufig in das Netzwerk. Dies ist insbesondere bei virtualisierten Umgebungen und Blade-Systemen so bemerkenswert, dass sich die tatsächliche Netzwerkverbindung nur schwer visualisieren lässt. Das kann Performance-Tests erschweren, wenn das Storage-System selbst aufgrund von Bandbreiteneinschränkungen nicht vollständig ausgelastet werden kann.</block>
  <block id="8caa382fb0f5f7a8279424d306dd003d" category="list-text">Der Vergleich der Performance eines All-Flash-Arrays mit einem Array mit rotierenden Festplatten ist im Allgemeinen aufgrund der deutlich verbesserten Latenz von All-Flash-Arrays nicht möglich. Die Testergebnisse sind in der Regel nicht aussagekräftig.</block>
  <block id="bc2b8b37bdda467f2ce639ab28bf4646" category="list-text">Der Vergleich der IOPS-Spitzenperformance mit einem All-Flash-Array ist häufig kein nützlicher Test, da Datenbanken nicht durch Storage-I/O eingeschränkt werden Angenommen, ein Array unterstützt 500.000 zufällige IOPS, ein anderes dagegen 300.000. Der Unterschied ist in der Praxis irrelevant, wenn eine Datenbank 99 % ihrer Zeit für die CPU-Verarbeitung aufwendet. Die Workloads schöpfen dabei niemals alle Kapazitäten des Storage-Arrays aus. Demgegenüber sind IOPS-Spitzenfunktionen bei einer Konsolidierungsplattform durchaus von großer Bedeutung, bei der das Storage-Array voraussichtlich auf seine Spitzenfunktionen ausgelastet ist.</block>
  <block id="0c2cbb26e6baa58b7d3a3309f95a2786" category="list-text">Berücksichtigen Sie bei jedem Storage-Test sowohl Latenz als auch IOPS. Viele Storage Arrays auf dem Markt bieten angeblich ein äußerst extremes IOPS-Niveau, doch aufgrund der Latenz sind diese IOPS in einem solchen Maß nutzlos. Ein typisches Ziel mit All-Flash-Arrays ist die Marke von 1 ms. Ein besserer Testansatz ist nicht die Messung der maximal möglichen IOPS, sondern die Ermittlung der IOPS-Anzahl, die ein Storage-Array verarbeiten kann, bevor die durchschnittliche Latenz größer als 1 ms ist.</block>
  <block id="e6d8362588e550c19912b0f3f0e1a129" category="section-title">Oracle Automatic Workload Repository und Benchmarking</block>
  <block id="d035df494751722dd56d4cf4a8d1f795" category="paragraph">Der Goldstandard für die Performance-Vergleiche mit Oracle ist ein Oracle Automatic Workload Repository (AWR) Bericht.</block>
  <block id="95f7e55383674168e90b01b708bc88db" category="paragraph">Es gibt mehrere Typen von AWR-Berichten. Aus Sicht des Speicherpunkts ein Bericht, der durch Ausführen des generiert wird<block ref="5053a9940effa118d38161a53f3b80f8" prefix=" " category="inline-code"></block> Der Befehl ist der umfassendste und wertvollste, da er auf eine bestimmte Datenbankinstanz abzielt und einige detaillierte Histogramme enthält, die Speicher-I/O-Ereignisse basierend auf Latenz aufteilen.</block>
  <block id="7b79802e5668e6e5b990081db481505d" category="paragraph">Um zwei Performance-Arrays zu vergleichen, wird idealerweise derselbe Workload auf jedem Array ausgeführt und ein AWR-Bericht erstellt, der genau auf den Workload abzielt. Bei einer sehr langen Arbeitslast kann ein einzelner AWR-Bericht mit einer verstrichenen Zeit, die die Start- und Stoppzeit umfasst, verwendet werden. Es ist jedoch vorzuziehen, die AWR-Daten als mehrere Berichte auszuteilen. Wenn beispielsweise ein Batch-Job von Mitternacht bis 6 Uhr ausgeführt wurde, erstellen Sie eine Reihe einstündiger AWR-Berichte von Mitternacht bis 1 Uhr, von 1 bis 2 Uhr usw.</block>
  <block id="e39ecb86ad7d25413ba8ad2976fd3e4a" category="paragraph">In anderen Fällen sollte eine sehr kurze Abfrage optimiert werden. Die beste Option ist ein AWR-Bericht, der auf einem AWR-Snapshot basiert, der beim Start der Abfrage erstellt wurde, und ein zweiter AWR-Snapshot, der beim Ende der Abfrage erstellt wurde. Der Datenbankserver sollte ansonsten ruhig sein, um die Hintergrundaktivität zu minimieren, die die Aktivität der analysierten Abfrage verdunkeln würde.</block>
  <block id="2fe7152c186ec1b5b0451e13ec8b968b" category="admonition">Wenn AWR-Berichte nicht verfügbar sind, sind Oracle Statspack-Berichte eine gute Alternative. Sie enthalten die meisten der gleichen I/O-Statistiken wie ein AWR-Bericht.</block>
  <block id="8b8c73ac7e509a897688356d1d283d75" category="section-title">Oracle AWR und Fehlerbehebung</block>
  <block id="42a92c79ee82f4343e7d65f2f1cf2dd2" category="paragraph">Ein AWR-Bericht ist auch das wichtigste Werkzeug zur Analyse eines Leistungsproblems.</block>
  <block id="ad5387afbab7c506c781c8b12b0a4b24" category="paragraph">Wie bei Benchmarking muss auch bei der Performance-Fehlerbehebung ein bestimmter Workload genau gemessen werden. Wenn möglich, geben Sie AWR-Daten ein, wenn Sie dem NetApp Support Center ein Performance-Problem melden oder wenn Sie mit einem NetApp oder einem Partner Account Team an einer neuen Lösung arbeiten.</block>
  <block id="6471146c0e33e2a8ec8a5c6635c837bc" category="paragraph">Beachten Sie bei der Bereitstellung von AWR-Daten die folgenden Anforderungen:</block>
  <block id="5c3ba265890805919f93c246b3f8d2d6" category="list-text">Führen Sie die aus<block ref="5053a9940effa118d38161a53f3b80f8" prefix=" " category="inline-code"></block> Befehl zum Generieren des Berichts. Die Ausgabe kann entweder Text oder HTML sein.</block>
  <block id="25db65f9a36910f3e488a8b7407130de" category="list-text">Wenn Oracle Real Application Clusters (RACs) verwendet werden, erstellen Sie AWR-Berichte für jede Instanz im Cluster.</block>
  <block id="96d55341f8208c60720e68b3155f3aa1" category="list-text">Ziel der Zeitpunkt, zu dem das Problem aufgetreten ist. Die maximal zulässige verstrichene Zeit eines AWR-Berichts beträgt in der Regel eine Stunde. Wenn ein Problem mehrere Stunden andauert oder einen mehrstündigen Vorgang wie einen Batch-Job umfasst, stellen Sie mehrere einstündige AWR-Berichte bereit, die den gesamten zu analysierenden Zeitraum abdecken.</block>
  <block id="4533b15b837909c68a0171d2638494d2" category="list-text">Wenn möglich, stellen Sie das AWR-Snapshot-Intervall auf 15 Minuten ein. Diese Einstellung ermöglicht eine detailliertere Analyse. Dies erfordert auch zusätzliche Ausführungen von<block ref="5053a9940effa118d38161a53f3b80f8" prefix=" " category="inline-code"></block> Um einen Bericht für jedes 15-Minuten-Intervall bereitzustellen.</block>
  <block id="97eb6d04cc93934c1be4ab95054cd0b0" category="list-text">Wenn es sich bei dem Problem um eine sehr kurze laufende Abfrage handelt, geben Sie einen AWR-Bericht an, der auf einem AWR-Snapshot basiert, der beim Start des Vorgangs erstellt wurde, und einen zweiten AWR-Snapshot, der nach Beendigung des Vorgangs erstellt wurde. Der Datenbankserver sollte ansonsten ruhig sein, um die Hintergrundaktivität zu minimieren, die die Aktivität des analysierten Vorgangs verdunkeln würde.</block>
  <block id="5186f459ce7d75e587a083bcb7c5927a" category="list-text">Wenn ein Leistungsproblem zu bestimmten Zeiten gemeldet wird, aber nicht zu anderen, liefern Sie zusätzliche AWR-Daten, die eine gute Leistung zum Vergleich zeigen.</block>
  <block id="108a72dad388aa310cc52ee3bac7d1cc" category="section-title">Kalibrieren_io</block>
  <block id="f1c07dfccef6a6c47a5ccef938e10e3d" category="paragraph">Der<block ref="108a72dad388aa310cc52ee3bac7d1cc" prefix=" " category="inline-code"></block> Der Befehl sollte niemals zum Testen, Vergleichen oder Vergleichen von Storage-Systemen verwendet werden. Wie in der Oracle-Dokumentation beschrieben, werden mit diesem Verfahren die I/O-Funktionen des Speichers kalibriert.</block>
  <block id="872892cce56a2665ad7ceecc44b38166" category="paragraph">Kalibrierung ist nicht dasselbe wie Benchmarking. Mit diesem Befehl können Sie I/O-Vorgänge ausgeben, um Datenbankvorgänge zu kalibrieren und ihre Effizienz zu verbessern, indem Sie die I/O-Ausgabe für den Host optimieren. Da der Typ der I/O, die vom ausgeführt wird<block ref="108a72dad388aa310cc52ee3bac7d1cc" prefix=" " category="inline-code"></block> Der Betrieb entspricht nicht der tatsächlichen I/O von Datenbankbenutzern, die Ergebnisse sind nicht vorhersehbar und häufig nicht einmal reproduzierbar.</block>
  <block id="e66807fad19acd41db50eedff918a4dd" category="section-title">SLOB2</block>
  <block id="0a6192ff5412539c31db2c7a98f408b3" category="inline-link-macro"><block ref="0a6192ff5412539c31db2c7a98f408b3" category="inline-link-rx"></block></block>
  <block id="d1f6fb23f52090ffd944dc54549735b8" category="paragraph">SLOB2, der Silly Little Oracle Benchmark, ist zum bevorzugten Tool für die Bewertung der Datenbank-Performance geworden. Es wurde von Kevin Closson entwickelt und ist verfügbar unter <block ref="3ccce0719b1965cd915da75778e4e706" category="inline-link-macro-rx"></block>. Die Installation und Konfiguration dauert nur wenige Minuten und mithilfe einer echten Oracle-Datenbank lassen sich I/O-Muster auf einem benutzerdefinierbaren Tablespace generieren. Es ist eine der wenigen verfügbaren Testoptionen, die die Auslastung eines All-Flash-Arrays mit I/O-Vorgängen ermöglichen Er eignet sich auch zur Generierung von deutlich niedrigeren I/O-Werten, um Storage-Workloads zu simulieren, die zwar niedrige IOPS, aber latenzempfindlich sind.</block>
  <block id="baffd8ac40ca7b966340e7b257b4c7b4" category="section-title">Wechselbank</block>
  <block id="905a59d6f73f4e2aea1dc232a3bcd195" category="paragraph">SwingBench kann zum Testen der Datenbank-Performance nützlich sein, aber es ist extrem schwierig, SwingBench auf eine Art und Weise zu verwenden, die den Storage belastet. Bei NetApp gab es noch keine Tests von Swingbench, die genug I/O ergaben, um auf jedem AFF Array eine erhebliche Belastung zu sein. In begrenzten Fällen kann der Order Entry Test (OET) verwendet werden, um die Storage-Systeme unter Latenzsicht zu bewerten. Dies kann in Situationen nützlich sein, in denen eine Datenbank eine bekannte Latenzabhängigkeit für bestimmte Abfragen hat. Achten Sie unbedingt darauf, dass Host und Netzwerk ordnungsgemäß konfiguriert sind, um die Latenzpotenziale eines All-Flash-Arrays auszuschöpfen.</block>
  <block id="e3cf940afa524b20b15c43b20405be77" category="section-title">HammerDB</block>
  <block id="5c9408da71d2ff7554b9fd11add795cf" category="paragraph">HammerDB ist ein Datenbank-Test-Tool, das unter anderem TPC-C- und TPC-H-Benchmarks simuliert. Es kann eine Menge Zeit dauern, bis ein ausreichend großer Datensatz für die ordnungsgemäße Ausführung eines Tests erstellt wurde. Er kann aber ein effektives Tool zur Performance-Evaluierung für OLTP- und Data Warehouse-Applikationen sein.</block>
  <block id="7e1ad070b7fff621d9d64a71cec87684" category="section-title">Orion</block>
  <block id="56fedcbec2842fb62a743c5f84311597" category="paragraph">Das Oracle Orion Tool wurde häufig mit Oracle 9 verwendet, wurde jedoch nicht gewartet, um die Kompatibilität mit Änderungen in verschiedenen Host-Betriebssystemen zu gewährleisten. Er wird aufgrund der Inkompatibilitäten mit der Betriebssystem- und Storage-Konfiguration selten mit Oracle 10 oder Oracle 11 verwendet.</block>
  <block id="38622bf572fa7bec2dc7f3915ebd345f" category="paragraph">Oracle hat das Tool neu geschrieben und es wird standardmäßig mit Oracle 12c installiert. Obwohl dieses Produkt verbessert wurde und viele der gleichen Aufrufe verwendet, die eine echte Oracle-Datenbank verwendet, verwendet es nicht genau den gleichen Codepfad oder das gleiche I/O-Verhalten, das von Oracle verwendet wird. Beispielsweise werden die meisten Oracle I/OS synchron ausgeführt, was bedeutet, dass die Datenbank angehalten wird, bis der I/O-Vorgang abgeschlossen ist, während der I/O-Vorgang im Vordergrund abgeschlossen ist. Eine einfache Überflutung eines Storage-Systems mit zufälligen I/OS ist keine Reproduktion von realen Oracle I/O und bietet keine direkte Methode, Storage Arrays zu vergleichen oder die Auswirkungen von Konfigurationsänderungen zu messen.</block>
  <block id="bb68dfada5ca20e256bbd3ffbbfab9e6" category="paragraph">Dennoch gibt es einige Anwendungsfälle für Orion, wie z. B. die generelle Messung der maximal möglichen Performance einer bestimmten Host-Netzwerk-Storage-Konfiguration oder die Abmessung des Zustands eines Storage-Systems. Mit sorgfältigen Tests können nutzbare Orion Tests entwickelt werden, um Storage-Arrays zu vergleichen oder die Auswirkungen einer Konfigurationsänderung zu bewerten, sofern zu den Parametern IOPS, Durchsatz und Latenz gehören und versucht werden, einen realistischen Workload originalgetreu zu replizieren.</block>
  <block id="6b64090d226d30776368e89b08c7c71b" category="summary">WAFL-Ausrichtung für Oracle Datenbanken</block>
  <block id="03a35ba2b6191f8cfbd1cca378eefc91" category="doc">Überprüfung der WAFL Ausrichtung von Oracle Datenbanken</block>
  <block id="219900dcd21cee2f958254a708ad36f6" category="paragraph">Eine korrekte WAFL-Ausrichtung ist für eine gute Performance von entscheidender Bedeutung. Obwohl ONTAP Blöcke in 4-KB-Einheiten managt, bedeutet dies nicht, dass ONTAP alle Vorgänge in 4-KB-Einheiten ausführt. ONTAP unterstützt zwar Blockoperationen unterschiedlicher Größen, die zugrunde liegende Buchhaltung wird jedoch von WAFL in 4-KB-Einheiten gemanagt.</block>
  <block id="5d3a5c290e14d74acf49b20d15120886" category="paragraph">Der Begriff „Alignment“ bezieht sich darauf, wie Oracle I/O diesen 4-KB-Einheiten entspricht. Für eine optimale Performance ist ein Oracle 8-KB-Block auf zwei physischen 4-KB-WAFL-Blöcken eines Laufwerks erforderlich. Wenn ein Block durch 2 KB verrechnet wird, befindet sich dieser Block auf der Hälfte eines 4-KB-Blocks, einem separaten vollständigen 4-KB-Block und dann der Hälfte eines dritten 4-KB-Blocks. Diese Anordnung führt zu Leistungseinbußen.</block>
  <block id="0073ce99d632b066d5ac09a4ad0cf7fc" category="paragraph">Bei NAS-File-Systemen ist die Ausrichtung nicht relevant. Oracle Datendateien werden am Anfang der Datei auf Basis der Größe des Oracle Blocks ausgerichtet. Daher sind Blockgrößen von 8 KB, 16 KB und 32 KB immer ausgerichtet. Alle Blockoperationen werden vom Anfang der Datei in Einheiten von 4 Kilobyte versetzt.</block>
  <block id="d7f7edd61ab216efc34578584a7d0e7c" category="paragraph">LUNs enthalten dagegen in der Regel zu Beginn eine Art Treiber-Header oder Filesystem-Metadaten, wodurch ein Offset erzeugt wird. Die Ausrichtung ist in modernen Betriebssystemen selten ein Problem, da diese Betriebssysteme für physische Laufwerke ausgelegt sind, die möglicherweise einen nativen 4-KB-Sektor verwenden, was außerdem die Ausrichtung der I/O an 4-KB-Grenzen erfordert, um eine optimale Performance zu erzielen.</block>
  <block id="38b3d1cd67903560954ad297a92e3de1" category="paragraph">Es gibt jedoch einige Ausnahmen. Eine Datenbank wurde möglicherweise von einem älteren Betriebssystem migriert, das nicht für 4 KB I/O optimiert wurde, oder ein Benutzerfehler während der Partitionserstellung hat möglicherweise zu einem Offset geführt, der sich nicht in Einheiten von 4 KB befindet.</block>
  <block id="612eeadd26ceeace7b043d37f8e24b1f" category="paragraph">Die folgenden Beispiele sind Linux-spezifisch, aber das Verfahren kann für jedes Betriebssystem angepasst werden.</block>
  <block id="1d5b772bea21e5e949413e09eedf17de" category="section-title">Ausgerichtet</block>
  <block id="594b0fc1af44897ff4b26f2e6b866685" category="paragraph">Das folgende Beispiel zeigt eine Ausrichtungsüberprüfung einer einzelnen LUN mit einer einzelnen Partition.</block>
  <block id="dc2c18402c86861702fc1d94a6495ac3" category="paragraph">Erstellen Sie zunächst die Partition, die alle auf dem Laufwerk verfügbaren Partitionen verwendet.</block>
  <block id="adff8cc6319e96d2685fd082a5aa043c" category="paragraph">Die Ausrichtung kann mathematisch mit folgendem Befehl überprüft werden:</block>
  <block id="4bc7aba328a710bb7d0935cf830634aa" category="paragraph">Die Ausgabe zeigt an, dass die Einheiten 512 Byte betragen, und der Beginn der Partition ist 32 Einheiten. Dies sind insgesamt 32 x 512 = 16,834 Byte, was ein ganzes Vielfaches von 4-KB-WAFL-Blöcken ist. Diese Partition ist korrekt ausgerichtet.</block>
  <block id="dd343941469b5360af52eb1b94fe5de6" category="paragraph">Führen Sie die folgenden Schritte durch, um die korrekte Ausrichtung zu überprüfen:</block>
  <block id="7d84e72acd05d0c93d99e414f5b092b5" category="list-text">Identifizieren Sie die UUID (Universally Unique Identifier) der LUN.</block>
  <block id="92af2a93f00a6297c490ba0be5fd7a8a" category="list-text">Geben Sie die Node-Shell auf dem ONTAP-Controller ein.</block>
  <block id="5274579d5eff6fe6d1e8ecbf54e84993" category="list-text">Starten Sie statistische Sammlungen auf der im ersten Schritt identifizierten Ziel-UUID.</block>
  <block id="52df0a2ea976123bf02330300b2392e3" category="list-text">Führen Sie einige I/O-Vorgänge aus Es ist wichtig, die zu verwenden<block ref="3c908d68ba53922b92c5595b72fa011c" prefix=" " category="inline-code"></block> Argument, um sicherzustellen, dass I/O synchron und nicht gepuffert ist.</block>
  <block id="fac8a5b490ea29eca93ec213d79e6ff2" category="admonition">Seien Sie sehr vorsichtig mit diesem Befehl. Umkehren der<block ref="39c8942e1038872a822c0dc75eedbde3" prefix=" " category="inline-code"></block> Und<block ref="8bf8854bebe108183caeb845c7676ae4" prefix=" " category="inline-code"></block> Argumente zerstören Daten.</block>
  <block id="4f7dce34a21e3de727f3e3ad05e6e7d8" category="list-text">Stoppen Sie die Statistiken, und zeigen Sie das Alignment-Histogramm an. Alle I/O-Vorgänge sollten sich im befinden<block ref="c6bd178b372b0ddd17fff48819badc6a" prefix=" " category="inline-code"></block> Bucket-Modul zur Angabe von I/O, die an einer 4-KB-Blockgrenze ausgerichtet ist</block>
  <block id="5df81374c01d1b7c20fe8cb3883117eb" category="section-title">Falsch Ausgerichtet</block>
  <block id="5ff266dadf965b3c304513f516a13c12" category="paragraph">Im folgenden Beispiel wird eine falsch ausgerichtete I/O angezeigt:</block>
  <block id="25be03b87e6a5d8170ce85b16ea506ab" category="list-text">Erstellen Sie eine Partition, die nicht an einer 4-KB-Grenze ausgerichtet ist. Dies ist kein Standardverhalten auf modernen Betriebssystemen.</block>
  <block id="4adda0e871000c2b13a4faef6ce4e21b" category="paragraph">Die Fehlausrichtung ist klar. Die I/O fällt meist in das* <block ref="66c8d76cad709856ccec680cab8ab35a" prefix="*" category="inline-code"></block> Bucket, der dem erwarteten Offset entspricht. Bei der Erstellung der Partition wurde sie 512 Byte weiter in das Gerät verschoben als der optimierte Standardwert, was bedeutet, dass das Histogramm durch 512 Byte versetzt wird.</block>
  <block id="b53cb977228ac351c16dfacc3c427a99" category="paragraph">Darüber hinaus der<block ref="20e637b3ddd562732de038fba736c7ee" prefix=" " category="inline-code"></block> Die Statistik ist ein Wert ungleich Null, was bedeutet, dass I/O-Vorgänge ausgeführt wurden, die keinen gesamten 4-KB-Block aufgefüllt haben.</block>
  <block id="d3d92f87c5235ad11c8bc69e85fb2fd0" category="section-title">Wiederherstellungsprotokollierung</block>
  <block id="162b54d51a30629d78e153f8da201780" category="paragraph">Die hier erläuterten Verfahren gelten für Datendateien. Oracle Redo- und Archivprotokolle weisen unterschiedliche I/O-Muster auf. Beispielsweise ist die Wiederherstellungsprotokollierung ein kreisförmiges Überschreiben einer einzelnen Datei. Wenn die standardmäßige 512-Byte-Blockgröße verwendet wird, sehen die Schreibstatistiken in etwa wie folgt aus:</block>
  <block id="d36dbaa617441d3308f5a9b50cb6f5ca" category="paragraph">Die I/O-Vorgänge werden auf alle Histogramm-Buckets verteilt, dies stellt jedoch keine Performance-Sorge dar. Extrem hohe Redo-Protokollierungsraten können jedoch von der Verwendung einer 4-KB-Blockgröße profitieren. In diesem Fall ist es wünschenswert, dass die LUNs für die Wiederherstellungsprotokollierung ordnungsgemäß ausgerichtet sind. Dies ist jedoch für eine gute Performance nicht so wichtig wie die Datendateiausrichtung.</block>
  <block id="3f9e1e3abd12ad7445baea4b681b9ceb" category="paragraph">Wenn ein Oracle-Datenbankserver abstürzt, kann es beim Neustart zu Problemen mit veralteten NFS-Sperren kommen. Dieses Problem ist vermeidbar, indem Sie sorgfältig auf die Konfiguration der Namensauflösung auf dem Server achten.</block>
  <block id="288ebefc25180862eb0448b786b1b9fc" category="paragraph">Dieses Problem tritt auf, weil das Erstellen einer Sperre und das Löschen einer Sperre zwei leicht unterschiedliche Methoden der Namensauflösung verwenden. Es sind zwei Prozesse beteiligt: Der Network Lock Manager (NLM) und der NFS-Client. Der NLM verwendet<block ref="e5caaf154dfeed411b9eec65b903a22a" prefix=" " category="inline-code"></block> Um den Hostnamen zu ermitteln, während der<block ref="d49331f31f40041ebcee19fca74fd9d4" prefix=" " category="inline-code"></block> Prozessanwendungen<block ref="330c4316ae6124616389eb1ca9912f7a" prefix=" " category="inline-code"></block>. Diese Hostnamen müssen übereinstimmen, damit das Betriebssystem veraltete Sperren ordnungsgemäß löschen kann. Beispielsweise sucht der Host nach Sperren, die Eigentum von sind<block ref="c7169a6dd344d0f0a38071a1262e4973" prefix=" " category="inline-code"></block>, Aber die Schlösser wurden vom Host als registriert<block ref="0c32c65f5ac66d0bdb4e63bd5fde7f75" prefix=" " category="inline-code"></block>. Wenn<block ref="330c4316ae6124616389eb1ca9912f7a" prefix=" " category="inline-code"></block> Gibt nicht denselben Wert zurück wie<block ref="5c020b5bb8d1ce1ace51c2a2f4c06fc2" prefix=" " category="inline-code"></block>, Dann ist der Sperrvorgang nicht erfolgreich.</block>
  <block id="19b9a1f540971d74c69042125bab2abc" category="paragraph">Mit dem folgenden Beispielskript wird überprüft, ob die Namensauflösung vollständig konsistent ist:</block>
  <block id="ee04b3fa3b28391874763596275e09c9" category="paragraph">Wenn<block ref="b64c6f211add16fd66ac17ef14c2a2b4" prefix=" " category="inline-code"></block> Stimmt nicht überein<block ref="4040592cec1880aa70936989f05e7c31" prefix=" " category="inline-code"></block>, Veraltete Sperren sind wahrscheinlich. Dieses Ergebnis zeigt beispielsweise ein potenzielles Problem auf:</block>
  <block id="2b500e5914ea5a219ade17dea0808f51" category="paragraph">Die Lösung wird normalerweise durch Ändern der Reihenfolge gefunden, in der Hosts in angezeigt werden<block ref="ad942c725cd0cae65ca4c63717ec464c" prefix=" " category="inline-code"></block>. Nehmen wir beispielsweise an, dass die Hosts-Datei diesen Eintrag enthält:</block>
  <block id="22b58ec1cec48707dfc018e6e216e966" category="paragraph">Um dieses Problem zu beheben, ändern Sie die Reihenfolge, in der der vollständig qualifizierte Domänenname und der kurze Hostname angezeigt werden:</block>
  <block id="e357ccd0745db58f22ea5c0947b613ee" category="paragraph"><block ref="330c4316ae6124616389eb1ca9912f7a" prefix="" category="inline-code"></block> Gibt nun den Short zurück<block ref="c7169a6dd344d0f0a38071a1262e4973" prefix=" " category="inline-code"></block> Host-Name, der mit der Ausgabe von übereinstimmt<block ref="4040592cec1880aa70936989f05e7c31" prefix=" " category="inline-code"></block>. Sperren werden somit nach einem Serverabsturz automatisch gelöscht.</block>
  <block id="ea4c55be196897a16d86999f5c16d582" category="doc">Einheitliche</block>
  <block id="9e28dbd7a4f5940eeaf6d3893c5c4b1f" category="section-title">Storage-Präsentation</block>
  <block id="151ae53a4100e0a6e83bde4d78083f81" category="list-text">Vom iSCSI-Initiator gemanagte iSCSI-LUNs auf der VM, nicht der Hypervisor</block>
  <block id="36f9cea3dc0f65698a9fbe15460591c1" category="list-text">*Portabilität.* Wenn eine VM Eigentümer ihrer Dateisysteme ist, wird der Prozess des Verschiebens einer Oracle-Umgebung viel einfacher. Dateisysteme können problemlos zwischen virtualisierten und nicht virtualisierten Gastsystemen verschoben werden.</block>
  <block id="f114856edc6cd8d6a9add5ec3f67656f" category="section-title">Paravirtualisierte Treiber</block>
  <block id="beeb10ceee6c9aabe178fb51edea7d22" category="paragraph">Für eine optimale Leistung ist die Verwendung paravirtualisierter Netzwerktreiber von entscheidender Bedeutung. Wenn ein Datastore verwendet wird, ist ein paravirtualisierter SCSI-Treiber erforderlich. Ein paravirtualisierter Gerätetreiber ermöglicht es einem Gast, sich tiefer in den Hypervisor zu integrieren, im Gegensatz zu einem emulierten Treiber, bei dem der Hypervisor mehr CPU-Zeit damit verbringt, das Verhalten physischer Hardware nachzuahmen.</block>
  <block id="7c5996fc15a24ce96b30160b60ac2b96" category="section-title">RAM überschreiben</block>
  <block id="5bccdd812c4af1cb5b9e36ddce656b60" category="paragraph">Das Überschreiben von RAM bedeutet, mehr virtualisierten RAM auf verschiedenen Hosts zu konfigurieren, als auf der physischen Hardware vorhanden ist. Dies kann zu unerwarteten Leistungsproblemen führen. Bei der Virtualisierung einer Datenbank dürfen die zugrunde liegenden Blöcke des Oracle SGA nicht durch den Hypervisor in den Storage getauscht werden. Dies führt zu äußerst instabilen Performance-Ergebnissen.</block>
  <block id="be493f1fc0b02ae45602c7c13f7b5dc7" category="summary">TR-4792 unterstützt die Verwendung von NetApp HCI 615C für 3D-Grafik-Workloads in einer VMware Horizon-Umgebung mit NVIDIA-GPUs (Graphics Processing Units) und Virtualisierungssoftware.</block>
  <block id="ff1d278a485c443dc5d8fc9f10f1a702" category="doc">NetApp HCI for Virtual Desktop Infrastructure mit VMware Horizon 7 – stärken Sie Ihre Power-User mit 3D-Grafiken</block>
  <block id="e3162938e0d7b1bbc74111cc5b5871c5" category="paragraph">TR-4792 enthält Anweisungen zur Verwendung des NetApp H615C Computing-Nodes für 3D-Grafik-Workloads in einer VMware Horizon-Umgebung mit NVIDIA-GPUs (Graphics Processing Units) und Virtualisierungssoftware. Er enthält außerdem die Ergebnisse der vorläufigen Tests von SPECviewperf 13 für den H615C.</block>
  <block id="1de37c09602b46db5de57a946efe2768" category="paragraph"><block ref="1de37c09602b46db5de57a946efe2768" category="inline-link-macro-rx"></block></block>
  <block id="93b94e62ad8cb02a1d1a7b0944a5445d" category="summary">Dieses Dokument beschreibt die Produktsicherheit für ONTAP Tools für VMware vSphere.</block>
  <block id="57a033c33a8be6110be46b2914cc4989" category="doc">Verwendung von VVols mit ONTAP</block>
  <block id="69556d50c944e0a07ade0bea62f078a3" category="paragraph">Für die Verwendung von VVols mit ONTAP muss die VASA Provider Software als Bestandteil der virtuellen ONTAP Tools für die VMware vSphere Appliance integriert sein.</block>
  <block id="5e484430124a3d64541b75bbf3c8f529" category="paragraph">Zu den ONTAP-Tools gehören außerdem vCenter-UI-Erweiterungen, REST-API-Server, Storage Replication Adapter für VMware Site Recovery Manager, Monitoring- und Host-Konfigurationstools sowie eine Reihe von Berichten, mit denen Sie Ihre VMware-Umgebung besser managen können.</block>
  <block id="21247c4392dc3c243f693dbf5b762553" category="section-title">Produkte und Dokumentation</block>
  <block id="bf03408d75f93f6e28c07c3c2300b98a" category="section-title">ONTAP nutzt die VASA Provider Architektur unter Verwendung von iSCSI oder FCP</block>
  <block id="94ef809dc16e728151bda996f8b4f8aa" category="inline-image-macro">ONTAP Tools VASA Provider Architektur,240</block>
  <block id="d78de3410d86d345a548abdf67aff375" category="paragraph"><block ref="d78de3410d86d345a548abdf67aff375" category="inline-image-macro-rx" type="image"></block></block>
  <block id="5e5421e75636200787a4449a2cdaa1f1" category="section-title">Produktinstallation</block>
  <block id="3ef3033a2b9a744a1e4e7e93fa01f909" category="paragraph">Bei Neuinstallationen implementieren Sie die virtuelle Appliance in Ihrer vSphere Umgebung. Aktuelle Versionen der ONTAP Tools werden sich automatisch bei Ihrem vCenter registrieren und VASA Provider standardmäßig aktivieren. Zusätzlich zu den ESXi-Host- und vCenter-Server-Informationen benötigen Sie auch die IP-Adresskonfigurationsdetails für die Appliance. Wie bereits erwähnt, muss für VASA Provider die Lizenz ONTAP FlexClone bereits auf allen ONTAP Clustern installiert sein, die Sie für VVols verwenden möchten. Die Appliance verfügt über einen integrierten Watchdog, um die Verfügbarkeit zu gewährleisten. Als Best Practice sollte sie mit VMware High Availability und optional mit Fault Tolerance-Funktionen konfiguriert werden. Weitere Informationen finden Sie in Abschnitt 4.1. Installieren oder verschieben Sie die ONTAP Tools Appliance oder vCenter Server Appliance (VCSA) nicht auf VVols Storage, da dies verhindern kann, dass die Appliances neu gestartet werden.</block>
  <block id="6a5c862f634c8967f7246f42eb9de6e1" category="paragraph">In-Place-Upgrades von ONTAP Tools werden über die ISO-Datei für Upgrades unterstützt, die auf der NetApp Support Website (NSS) zum Download zur Verfügung steht. Befolgen Sie die Anweisungen des Bereitstellungs- und Setup-Handbuchs, um die Appliance zu aktualisieren.</block>
  <block id="3c07c9b4b562a16890d9fee571f0441e" category="inline-link">Leitfaden zur Dimensionierung für ONTAP Tools für VMware vSphere</block>
  <block id="e4e51979718d88ce483f9477be981a76" category="paragraph">Informationen zur Dimensionierung Ihrer virtuellen Appliance und zum Verständnis der Konfigurationsbeschränkungen finden Sie in diesem Knowledge Base-Artikel:<block ref="a75738ed95b0811ed0edd1b8ef0d9568" category="inline-link-rx"></block></block>
  <block id="72333dadacbc964761fb14f718f6d411" category="section-title">Produktdokumentation</block>
  <block id="e0644c46fdc2ced2c094f7db15e15a8a" category="paragraph">Die folgende Dokumentation ist verfügbar, um Sie bei der Implementierung von ONTAP Tools zu unterstützen.</block>
  <block id="09e308ed60d47b80b6cd16eb233f1bb9" category="inline-link">Für die vollständige Dokumentation Repository&amp;amp;#44; besuchen Sie diesen Link zu docs.netapp.com</block>
  <block id="7f21741a70426baea3ee41488da86af4" category="paragraph"><block ref="56b49132b20e341764c8aa067751e8a5" category="inline-link-rx"></block></block>
  <block id="be11c74c1dd7f307bb80183a90dc2067" category="section-title">Los geht's</block>
  <block id="7d0ee6fed10d3d4e5c9ee496729ab519" category="inline-link">Versionshinweise</block>
  <block id="2645deadffb8af180b8a35da3d034ad8" category="list-text"><block ref="2645deadffb8af180b8a35da3d034ad8" category="inline-link-rx"></block></block>
  <block id="b1555cd6358e57b76c72e343dbe31846" category="inline-link">Erfahren Sie mehr über ONTAP Tools für VMware vSphere</block>
  <block id="a0aaeda90fe2ef02fba791bc7c35645c" category="list-text"><block ref="a0aaeda90fe2ef02fba791bc7c35645c" category="inline-link-rx"></block></block>
  <block id="e695b432b77d6bddfcb785c76f5e5442" category="inline-link">ONTAP-Tools Schnellstartanleitung</block>
  <block id="324b0410f22fb210bbf88e1ee7e97428" category="list-text"><block ref="324b0410f22fb210bbf88e1ee7e97428" category="inline-link-rx"></block></block>
  <block id="3d7fe3e43f9f24fffe5ed0d546d12f13" category="inline-link">Implementierung von ONTAP Tools</block>
  <block id="354336d78d1f0a156dcd5221d341dfd0" category="list-text"><block ref="354336d78d1f0a156dcd5221d341dfd0" category="inline-link-rx"></block></block>
  <block id="a2fa58344be7fbd9a2be0320a8df0f77" category="inline-link">Upgrade von ONTAP-Tools</block>
  <block id="33a77e1014f1325d6ba19639a6c95d48" category="list-text"><block ref="33a77e1014f1325d6ba19639a6c95d48" category="inline-link-rx"></block></block>
  <block id="21e2a89d708329706ec6ed3fc989a1f6" category="section-title">Verwenden Sie ONTAP-Tools</block>
  <block id="91a9cfdbaaa44e0a9ad72e92533f763f" category="inline-link">Bereitstellung herkömmlicher Datastores</block>
  <block id="f27fe8f858d8a91950e214753b95722c" category="list-text"><block ref="f27fe8f858d8a91950e214753b95722c" category="inline-link-rx"></block></block>
  <block id="fc7d7e475ac8c3868d7426bb41df9b09" category="inline-link">Bereitstellung von VVols Datastores</block>
  <block id="0c2552c3930f472b2d120f30d7aa0415" category="list-text"><block ref="0c2552c3930f472b2d120f30d7aa0415" category="inline-link-rx"></block></block>
  <block id="317bbf73ff27c5f981628c8fb83ebf22" category="inline-link">Konfigurieren Sie die rollenbasierte Zugriffssteuerung</block>
  <block id="644ae78061acbb2254be4de3ea454b06" category="list-text"><block ref="644ae78061acbb2254be4de3ea454b06" category="inline-link-rx"></block></block>
  <block id="48bc2d028cb2a507ef974230ca3ba793" category="inline-link">Konfigurieren Sie die Remote-Diagnose</block>
  <block id="fc0330812838aba14025bd70d41b943d" category="list-text"><block ref="fc0330812838aba14025bd70d41b943d" category="inline-link-rx"></block></block>
  <block id="641e9457539249e880725760f91d5ee2" category="inline-link">Konfigurieren Sie Hochverfügbarkeit</block>
  <block id="5027a18a60c68e05df84cd699af74497" category="list-text"><block ref="5027a18a60c68e05df84cd699af74497" category="inline-link-rx"></block></block>
  <block id="04edeb5424c826c14a69edb777edf395" category="section-title">Sicherung und Management von Datenspeichern</block>
  <block id="e17534281670c5ea4e8d55420a63c98f" category="inline-link">Sicherung herkömmlicher Datastores</block>
  <block id="c8ac931d575d89c007212a35c8dde74c" category="list-text"><block ref="8f91d2113ff32de18906bad1cd446b0d" category="inline-link-rx"></block> Mit SRM</block>
  <block id="bff05cbbc19cad2bd10b9fe25dc34a1b" category="inline-link">Sicherung von auf VVols basierenden Virtual Machines</block>
  <block id="710b15d782f0c2c0411066b86644a12d" category="list-text"><block ref="7741c74d7508dc983fa4af0dff0bdda1" category="inline-link-rx"></block> Mit SRM</block>
  <block id="5267febc97a2cc385cc5c73995dc64df" category="inline-link">Überwachen Sie herkömmliche Datenspeicher und Virtual Machines</block>
  <block id="bac3b262bb348a54f9986c2b5dbf6024" category="list-text"><block ref="bac3b262bb348a54f9986c2b5dbf6024" category="inline-link-rx"></block></block>
  <block id="3123ece3c3b36f605758cb0c9a28f904" category="inline-link">Überwachen Sie VVols Datastores und Virtual Machines</block>
  <block id="75c07b623699e1947d011983a7823584" category="list-text"><block ref="75c07b623699e1947d011983a7823584" category="inline-link-rx"></block></block>
  <block id="73751ced5959ea3089346b01d42dde76" category="paragraph">Zusätzlich zur Produktdokumentation gibt es möglicherweise Artikel aus der Support Knowledgebase, die hilfreich sein könnten.</block>
  <block id="dd5138e8a4207a5b2fde43b411a4b5e1" category="section-title">VASA Provider Dashboard</block>
  <block id="6da01a643ea28efe37fa2d6fd06a634b" category="paragraph">Vasa Provider umfasst ein Dashboard mit Performance- und Kapazitätsinformationen für einzelne VVols VMs. Diese Informationen stammen direkt von ONTAP für die vVol Dateien und LUNs, einschließlich Latenz, IOPS, Durchsatz und Uptime für die Top 5 VMs sowie Latenz und IOPS für die Top 5 Datastores. Bei Verwendung von ONTAP 9.7 oder höher ist sie standardmäßig aktiviert. Es kann bis zu 30 Minuten dauern, bis die ersten Daten abgerufen und im Dashboard angezeigt werden.</block>
  <block id="9b78e1fecb2bcc40733f2180691c7507" category="section-title">ONTAP Tools VVols Dashboard</block>
  <block id="4adf016da96258b45a1cf762298729b5" category="inline-image-macro">ONTAP Tools VVols Dashboard,400</block>
  <block id="73c1e14ddf3f32984f869ac3f122b2ca" category="paragraph"><block ref="73c1e14ddf3f32984f869ac3f122b2ca" category="inline-image-macro-rx" type="image"></block></block>
  <block id="cd47dd01745339787e4c7300389401f2" category="section-title">Best Practices In Sich Vereint</block>
  <block id="9ae938622297e9d4b14dd0006a8f0bf4" category="paragraph">*Grenzen*</block>
  <block id="b4f52a4fd0b07c9d904b95fc98a1cd45" category="inline-link">Konfigurationsmaxima</block>
  <block id="bc8259e306c73b2969e7ac6297000e64" category="paragraph">ONTAP unterstützt im Allgemeinen VVols-Limits, die durch VMware definiert sind (siehe veröffentlicht<block ref="a6efdd9b1a19df6105024b4869e0582b" category="inline-link-rx"></block>). In der folgenden Tabelle sind bestimmte ONTAP-Limits für Größe und Anzahl von VVols zusammengefasst. Prüfen Sie immer das<block ref="bb9c67b5a4c15a85b5e6aa6c9afd0285" category="inline-link-rx"></block> Für aktualisierte Grenzwerte für Anzahl und Größe der LUNs und Dateien.</block>
  <block id="9b7ad553354519b31ec860eef39e5f6b" category="paragraph">*ONTAP VVols Grenzen*</block>
  <block id="130b78bfd6c9b050a912fec77e285e3f" category="cell">Kapazität/Funktion</block>
  <block id="6231ad61105cd983d3f0042762d3233d" category="cell">SAN (SCSI oder NVMe-of)</block>
  <block id="75d4e7871261a858356a58d682ab71de" category="cell">Maximale VVols-Größe</block>
  <block id="9cfd7100738cee4daf7acd5e01d31e14" category="cell">62 tib*</block>
  <block id="9a728251ad1e90577975ae9395374772" category="cell">Maximale Anzahl VVols pro FlexVol Volume</block>
  <block id="021bbc7ee20b71134d53e20206bd6feb" category="cell">1024</block>
  <block id="46ba990c143630f55072434a6ea958c0" category="cell">2 Milliarden</block>
  <block id="cdbcf58620f0b4b0bbf3291386a99407" category="cell">Maximale Anzahl VVols pro ONTAP Node</block>
  <block id="342264031889898ec1a553b4dd58d98c" category="cell">Bis zu 12,288**</block>
  <block id="659e5faa0bdb08327ba0719230e95be2" category="cell">50 Milliarden</block>
  <block id="43b49f27d93c6f55f54a68e8983d5479" category="cell">Maximale Anzahl VVols pro ONTAP-Paar</block>
  <block id="3941694d19c2d1fc406f055ab62cb9eb" category="cell">Bis zu 24,576**</block>
  <block id="d6b3e65f296fd23975dd6f1a915c4b22" category="cell">Maximale Anzahl von VVols pro ONTAP Cluster</block>
  <block id="2702ff8627a3990fc940d6f53463925e" category="cell">Bis zu 98,304**</block>
  <block id="4f0c25d6c688cf50460d11fecaa97fa8" category="cell">Keine spezifische Cluster-Beschränkung</block>
  <block id="855ecc47bb0d239e1fdf6fee84e24c75" category="cell">Maximale QoS-Objekte (gemeinsam genutzte Richtliniengruppe und individuelle VVols Service-Level)</block>
  <block id="26890bd9dcb27c05f1ab5bcc859501b8" category="cell">12,000 bis ONTAP 9.3; 40,000 mit ONTAP 9.4 und höher</block>
  <block id="0e38f856cb713ce9eea79fa1c6b7dc32" category="list-text">Größenbeschränkung auf Basis von ASA Systemen oder AFF und FAS Systemen mit ONTAP 9.12.1P2 und höher</block>
  <block id="d27c43bb3cdcbfa3852bb91192ffb849" category="list-text">Die Anzahl der SAN-VVols (NVMe-Namespaces oder LUNs) variiert je nach Plattform. Prüfen Sie immer das<block ref="bb9c67b5a4c15a85b5e6aa6c9afd0285" category="inline-link-rx"></block> Für aktualisierte Grenzwerte für Anzahl und Größe der LUNs und Dateien.</block>
  <block id="72e0ee397942f7233122887584f94b8b" category="paragraph">Die Verwendung von ONTAP VVols mit vSphere ist einfach und folgt den veröffentlichten vSphere-Methoden (siehe Arbeiten mit virtuellen Volumes unter vSphere-Speicher in der VMware-Dokumentation für Ihre Version von ESXi). Nachfolgend finden Sie einige weitere Vorgehensweisen, die Sie in Verbindung mit ONTAP in Betracht ziehen sollten.</block>
  <block id="83583580d98b69cb8317f9b285f3c8df" category="cell">*Verwenden Sie ONTAP-Tools für VMware vSphere UI-Erweiterungen oder REST-APIs zur Bereitstellung von VVols-Datastores* *und Protokollendpunkten.*</block>
  <block id="9507d4f97aceb8d05fc962dd53f082af" category="cell">VVols Datastores können über die allgemeine vSphere Schnittstelle erstellt werden, aber mithilfe von ONTAP Tools werden automatisch bei Bedarf Protokollendpunkte erstellt und FlexVol Volumes anhand von ONTAP Best Practices und unter Einhaltung der definierten Storage-Funktionsprofile erstellt. Klicken Sie einfach mit der rechten Maustaste auf den Host/Cluster/Datacenter und wählen Sie dann „_ONTAP Tools_“ und „_Provision Datastore_“ aus. Wählen Sie dann im Assistenten einfach die gewünschten VVols Optionen aus.</block>
  <block id="e8d5f9dc0bff953a9e59755813c7e8bf" category="cell">*Speichern Sie die ONTAP Tools Appliance oder vCenter Server Appliance (VCSA) niemals auf einem VVols Datastore, den sie verwalten.*</block>
  <block id="c6ac0465bff03857ee851d98f6bd782b" category="cell">Dies kann zu einer „Hühnchen- und Eiersituation“ führen, wenn Sie die Appliances neu starten müssen, da sie nicht in der Lage sind, während sie neu starten ihre eigenen VVols abzuheben. Sie können sie auf einem VVols Datastore speichern, der von verschiedenen ONTAP Tools und einer vCenter Implementierung gemanagt wird.</block>
  <block id="ab8d352870fb5334773c7177d3d5dcd1" category="cell">*Vermeiden Sie VVols-Vorgänge über verschiedene ONTAP-Versionen hinweg.*</block>
  <block id="8b2bcef781b7e1f6d64739147d50aae7" category="cell">Unterstützte Storage-Funktionen wie QoS, Personality und mehr haben sich in verschiedenen Versionen des VASA Providers verändert, einige sind von der ONTAP Version abhängig. Die Verwendung verschiedener Versionen in einem ONTAP-Cluster oder das Verschieben von VVols zwischen Clustern mit unterschiedlichen Versionen können zu unerwartetem Verhalten oder Compliance-Alarmen führen.</block>
  <block id="b2da0eca57c759eff034e6fe26f153e6" category="cell">*Zonen Sie Ihre Fibre Channel Fabric vor der Verwendung von NVMe/FC oder FCP für VVols.*</block>
  <block id="cd7bf0e1e61341c1aa3fa23401af8a27" category="inline-image-macro">Zoning mit einem Initiator durchgeht vier Nodes,400</block>
  <block id="51591d05b7e084ba8e5ae7f63172177e" category="inline-link">_TR-4080 Best Practices for Modern SAN ONTAP 9_</block>
  <block id="c1723e6a0e1d3dd28232b33e0cb6e61d" category="inline-link">_TR-4684 Implementierung und Konfiguration moderner SANs mit NVMe-of_</block>
  <block id="9a36c8f0464a7ffce114044f1859c7ff" category="cell">* Planen Sie Ihre Unterstützung FlexVols nach Ihren Bedürfnissen.*</block>
  <block id="03d2e894fa9a05efefa4e8d6b2008a19" category="cell">Es ist durchaus wünschenswert, mehrere Backup-Volumes zum VVols-Datastore hinzuzufügen, um den Workload über das ONTAP-Cluster zu verteilen, verschiedene Richtlinienoptionen zu unterstützen oder die Anzahl der zulässigen LUNs oder Dateien zu erhöhen. Wenn jedoch eine maximale Storage-Effizienz erforderlich ist, platzieren Sie alle Ihre Backup Volumes auf einem einzigen Aggregat. Wenn eine maximale Klon-Performance erforderlich ist, ziehen Sie die Verwendung eines einzelnen FlexVol Volumes in Erwägung und halten Ihre Vorlagen- oder Content Library im selben Volume. Der VASA Provider verlagert viele VVols Storage-Vorgänge auf ONTAP, einschließlich Migration, Klonen und Snapshots. Wenn dies in einem einzelnen FlexVol Volume geschieht, werden platzsparende Klone von Dateien verwendet und stehen so gut wie sofort zur Verfügung. Wenn dies über FlexVol Volumes hinweg durchgeführt wird, sind die Kopien schnell verfügbar und verwenden Inline-Deduplizierung und -Komprimierung. Allerdings kann eine maximale Storage-Effizienz erst dann wiederhergestellt werden, wenn Hintergrundjobs auf Volumes mithilfe von Deduplizierung und Komprimierung im Hintergrund ausgeführt werden. Je nach Quelle und Ziel kann die Effizienz beeinträchtigt werden.</block>
  <block id="80b15469535d95a1690f308427545c7e" category="cell">*Speicherfähigkeitsprofile (SCPs) einfach halten.*</block>
  <block id="33e2905cb722c14575a9bff81ba78b39" category="cell">Vermeiden Sie die Angabe von Funktionen, die nicht erforderlich sind, indem Sie sie auf beliebig festlegen. Dadurch werden Probleme beim Auswählen oder Erstellen von FlexVol-Volumes minimiert. Wenn bei VASA Provider 7.1 und älteren Versionen beispielsweise die Komprimierung mit der SCP-Standardeinstellung „Nein“ beibehalten wird, wird versucht, die Komprimierung selbst auf einem AFF-System zu deaktivieren.</block>
  <block id="5a6d5fdddf6fbd82f1952b532b03c9ef" category="cell">*Verwenden Sie die Standard-SCPs als Beispielvorlagen, um Ihre eigenen zu erstellen.*</block>
  <block id="ee3cbda1c3c4a14ae024478c7ada6d11" category="cell">*Befolgen Sie alle Best Practices für Protokolle.*</block>
  <block id="6a2c1ac1da51eca0c1a65afffab8dfab" category="inline-image-macro">Netzwerkkonfiguration mit VVols über NFS v3.500</block>
  <block id="918febfed3d070cf1250909e5cdcf31d" category="paragraph">Dieses Dokument behandelt die ONTAP Funktionen für VMware vSphere Virtual Volumes (VVols), einschließlich der neuesten Produktinformationen und Anwendungsfälle sowie Best Practices und andere Informationen, um die Implementierung zu optimieren und Fehler zu reduzieren.</block>
  <block id="d30f04665be7a6d9663004b1b5e61a9a" category="paragraph">Andere Dokumente wie Leitfäden und Kompatibilitätslisten werden durch Best Practices ergänzt. Sie werden basierend auf Labortests und umfassenden praktischen Erfahrungen der NetApp Ingenieure und Kunden entwickelt. Es handelt sich hierbei unter Umständen nicht nur um geeignete oder unterstützte Praktiken, sondern im Allgemeinen um die einfachsten Lösungen, die die Anforderungen der meisten Kunden erfüllen.</block>
  <block id="b010d2e253827dbb44b7ba4e1332e24e" category="admonition">Dieses Dokument wurde mit neuen VVols-Funktionen aus vSphere 8.0 Update 1 aktualisiert, die von der Version 9.12 der ONTAP Tools unterstützt werden.</block>
  <block id="cc713c330b9c4e00d3ed7233bc54a889" category="section-title">Virtual Volumes (VVols) – Übersicht</block>
  <block id="64f0863a7fb5f52a7ac6a911e1ad6862" category="paragraph">NetApp begann 2012 die Zusammenarbeit mit VMware zur Unterstützung von vSphere APIs for Storage Awareness (VASA) für vSphere 5. Dieser frühe VASA Provider ermöglichte die Definition von Storage-Funktionen in einem Profil, das zur Filterung von Datastores bei der Bereitstellung und zur Überprüfung der Einhaltung der Richtlinie anschließend verwendet werden konnte. Im Laufe der Zeit wurden neue Funktionen hinzugefügt, um eine stärkere Automatisierung der Bereitstellung zu ermöglichen. Zudem wurden Virtual Volumes oder VVols hinzugefügt, bei denen individuelle Storage-Objekte für Dateien von Virtual Machines und Virtual Disks verwendet werden. Zu diesen Objekten gehören LUNs, Dateien und jetzt auch vSphere 8. NVMe namespaces.NetApp arbeitete eng mit VMware als Referenzpartner für VVols zusammen, die im Jahr 2015 für vSphere 6 veröffentlicht wurden, und erneut als Design-Partner für VVols, die NVMe over Fabrics in vSphere 8 verwenden. NetApp erweitert VVols kontinuierlich, um die Vorteile der neuesten Funktionen von ONTAP zu nutzen.</block>
  <block id="cbf0aa9daeb855e4eace1a3bdc474ce2" category="paragraph">Es gibt mehrere Komponenten, die zu beachten sind:</block>
  <block id="e2f6b83160ea0712fbc59dd84410c4b2" category="cell">*VASA Provider*</block>
  <block id="919601826e1d58bccaaab76d787871d2" category="cell">Dies ist die Softwarekomponente, die die Kommunikation zwischen VMware vSphere und dem Speichersystem übernimmt. Bei ONTAP wird VASA Provider in einer Appliance ausgeführt, bekannt als ONTAP Tools für VMware vSphere (kurz ONTAP Tools). Die ONTAP Tools enthalten außerdem ein vCenter Plug-in, einen Storage Replication Adapter (SRA) für VMware Site Recovery Manager und REST-API-Server zum Erstellen Ihrer eigenen Automatisierung. Sobald ONTAP Tools bei vCenter konfiguriert und registriert sind, besteht kaum noch Bedarf für eine direkte Interaktion mit dem ONTAP System, da sich Ihre Storage-Anforderungen nahezu vollständig über die vCenter UI oder ÜBER REST-API-Automatisierung managen lassen.</block>
  <block id="05ad8543ff165c6b8cd0cc5e976d3245" category="cell">*Protokollendpunkt (PE)*</block>
  <block id="3b2cd9018385f55c53c9a8b756df8bb5" category="cell">Der Protokollendpunkt ist ein Proxy für I/O zwischen den ESXi Hosts und dem VVols Datastore. ONTAP VASA Provider erstellt diese automatisch, entweder eine Protokollendpunkt-LUN (4 MB Größe) pro FlexVol Volume des VVols Datastores oder ein NFS-Bereitstellungspunkt pro NFS-Schnittstelle (LIF) auf dem Storage-Node, der ein FlexVol Volume im Datastore hostet. Der ESXi-Host mountet diese Protokollendpunkte direkt statt einzelner vVol-LUNs und Dateien mit virtuellen Laufwerken. Die Protokollendpunkte müssen nicht verwaltet werden, da sie vom VASA Provider zusammen mit den erforderlichen Schnittstellengruppen oder Exportrichtlinien automatisch erstellt, gemountet, unmountet und gelöscht werden.</block>
  <block id="d3f051e8316cfd36651af1e65be24a75" category="cell">*Virtual Protocol Endpoint (VPE)*</block>
  <block id="71afca0aa1a505ea2ac3ce5a1a681549" category="paragraph">Neu in vSphere 8 ist bei Verwendung von NVMe over Fabrics (NVMe-of) mit VVols, das Konzept eines Protokollendpunkts in ONTAP nicht mehr relevant. Stattdessen wird ein virtueller PE automatisch vom ESXi-Host für jede ANA-Gruppe instanziiert, sobald die erste VM eingeschaltet ist. ONTAP erstellt automatisch ANA-Gruppen für jedes vom Datenspeicher verwendete FlexVol Volume.</block>
  <block id="8202498953d2ebcf9968c37a7e47f4f6" category="paragraph">Ein weiterer Vorteil bei der Nutzung von NVMe-of für VVols besteht darin, dass vom VASA Provider keine Bind-Anfragen erforderlich sind. Stattdessen verarbeitet der ESXi-Host die vVol-Bindungsfunktion intern basierend auf dem VPE. Dies verringert die Möglichkeit, dass ein vVol BIND-Ansturm den Service beeinträchtigt.</block>
  <block id="70532c0374111d897ef1176b34b6396c" category="inline-link">NVMe und Virtual Volumes</block>
  <block id="50ec55042a7f4e8c2c66219ac096cfb3" category="inline-link">VMware.com</block>
  <block id="113c9d4e642d1ba4cc469267fb7fd0de" category="paragraph">Weitere Informationen finden Sie unter<block ref="f8d477cd77073d731aafe4f52026608a" category="inline-link-rx"></block> Ein<block ref="1f1565cca975c4a703345d21e7f273b8" category="inline-link-rx"></block></block>
  <block id="61def0b14af00df4eb90bcd79d858a8b" category="cell">*Datastore des virtuellen Volumes*</block>
  <block id="b945cda41d7a84e7152e09343a17cbe6" category="cell">Der Virtual Volume Datastore ist eine logische Datastore-Darstellung eines VVols-Containers, der von einem VASA Provider erstellt und gemanagt wird. Der Container ist ein Pool an Storage-Kapazität, der aus den vom VASA Provider gemanagten Storage-Systemen bereitgestellt wird. ONTAP Tools unterstützen die Zuweisung mehrerer FlexVol Volumes (auch als Backup Volumes bezeichnet) einem einzelnen VVols-Datastore. Diese VVols Datastores können über mehrere Nodes in einem ONTAP Cluster verteilt werden. Dabei werden Flash- und Hybrid-Systeme mit unterschiedlichen Funktionen kombiniert. Der Administrator kann neue FlexVol Volumes mithilfe des Bereitstellungsassistenten oder DER REST-API erstellen oder vorab erstellte FlexVol Volumes für die Storage-Sicherung auswählen, sofern diese verfügbar sind.</block>
  <block id="c25d2cc7c6540f6ac98af67455eecc39" category="cell">*Virtual Volumes (VVols)*</block>
  <block id="9664cdf7951789389813facac649fec1" category="cell">VVols sind die Dateien und Festplatten der Virtual Machines, die tatsächlich im VVols Datastore gespeichert sind. Der Begriff vVol (Singular) bezieht sich auf eine einzelne spezifische Datei, LUN oder Namespace. ONTAP erstellt NVMe-Namespaces, LUNs oder Dateien, je nachdem, welches Protokoll der Datastore verwendet. Es gibt mehrere unterschiedliche Typen von VVols. Die gängigsten sind Konfiguration (Metadatendateien), Daten (virtuelle Festplatte oder VMDK) und Swap (das beim Einschalten der VM erstellt wird). VVols, die durch die Verschlüsselung von VMware VM geschützt sind, haben einen anderen Typ. Die VMware VM-Verschlüsselung sollte nicht mit der ONTAP-Volume- oder Aggregatverschlüsselung verwechselt werden.</block>
  <block id="937251054fa0c1a7b946f8928cc857b9" category="section-title">Richtlinienbasiertes Management</block>
  <block id="34d2b7d8b570ede7a638d070656b7b81" category="paragraph">VMware vSphere APIs for Storage Awareness (VASA) erleichtern es VM-Administratoren, alle erforderlichen Storage-Funktionen zu nutzen, um VMs bereitzustellen, ohne mit ihrem Storage-Team interagieren zu müssen. Vor VASA konnten VM-Administratoren VM-Storage-Richtlinien definieren, mussten dann aber gemeinsam mit ihren Storage-Administratoren geeignete Datastores ermitteln – oft anhand der Dokumentation oder von Namenskonventionen. Mit VASA können vCenter Administratoren mit den entsprechenden Berechtigungen eine Reihe von Storage-Funktionen definieren, mit denen vCenter Benutzer dann VMs bereitstellen können. Durch die Zuordnung zwischen VM-Storage-Richtlinie und Datastore-Storage-Funktionsprofil kann in vCenter eine Liste kompatibler Datastores zur Auswahl angezeigt werden. Außerdem können andere Technologien wie Aria (ehemals vRealize) Automation oder Tanzu Kubernetes Grid aktiviert werden, um automatisch Storage aus einer zugewiesenen Richtlinie auszuwählen. Dieser Ansatz wird als richtlinienbasiertes Storage-Management bezeichnet. Während Storage-Funktionsprofile und -Richtlinien auch bei herkömmlichen Datastores verwendet werden können, konzentrieren wir uns hier auf VVols Datastores.</block>
  <block id="631bdc7de6608fd57c3fa0b397c3c26f" category="paragraph">Es gibt zwei Elemente:</block>
  <block id="a156a7f38a3727ce705b5466eb11e0bf" category="cell">*Storage Capability Profile (SCP)*</block>
  <block id="43a258e66ca875fa9c5d3e35e06ba2b7" category="cell">Ein Storage-Funktionsprofil (Storage Capability Profile, SCP) ist eine Form von Storage-Vorlage. Damit können vCenter Administratoren festlegen, welche Storage-Funktionen benötigt werden, ohne dass ein Verständnis für das Management dieser Funktionen in ONTAP erforderlich ist. Mit Hilfe einer Vorlage können Administratoren problemlos Storage-Services auf konsistente und vorhersehbare Weise bereitstellen. Zu den in einem SCP beschriebenen Funktionen zählen Performance, Protokolle, Storage-Effizienz und weitere Funktionen. Die spezifischen Funktionen sind je nach Version unterschiedlich. Sie werden mit dem Menü ONTAP Tools für VMware vSphere in der vCenter UI erstellt. SIE können AUCH REST-APIs zum Erstellen von SCPs verwenden. Sie können manuell durch Auswahl einzelner Funktionen erstellt oder automatisch aus vorhandenen (herkömmlichen) Datenspeichern generiert werden.</block>
  <block id="8f50a0757d99fe5dd0df8d19478d4921" category="cell">*VM-Speicherrichtlinie*</block>
  <block id="f99c5cf9298b040025bd3dfe966cb86e" category="cell">VM Storage-Richtlinien werden in vCenter unter Richtlinien und Profile erstellt. Erstellen Sie für VVols mithilfe von Regeln des NetApp VVols Storage-Typ-Providers ein Regelwerk. ONTAP Tools bietet einen vereinfachten Ansatz, da Sie ein SCP einfach auswählen können, anstatt Sie zur Angabe einzelner Regeln zu zwingen.</block>
  <block id="5dda8b04866334dc92ee08001b15701a" category="paragraph">Wie oben erwähnt, kann die Verwendung von Richtlinien zur Vereinfachung der Bereitstellung von Volumes beitragen. Wählen Sie einfach eine entsprechende Richtlinie aus. VASA Provider zeigt VVols-Datastores an, die diese Richtlinie unterstützen, und platziert das vVol in einem individuellen, konformen FlexVol Volume (Abbildung 1).</block>
  <block id="2ce0d6ec59cca7e0b6e0bdc389301e63" category="section-title">Bereitstellung der VM mithilfe der Storage-Richtlinie</block>
  <block id="e0795f8b847058e171dec5a519757ece" category="image-alt">Implementierung einer Virtual Machine mithilfe der Storage-Richtlinie</block>
  <block id="f0d0f0a6174a5a9dde27782042e22ccf" category="paragraph">Sobald eine VM bereitgestellt ist, prüft der VASA Provider weiterhin die Compliance und alarmiert den VM-Administrator mit einem Alarm in vCenter, wenn das Backup-Volume nicht mehr mit der Richtlinie konform ist (Abbildung 2).</block>
  <block id="941431bc1cecb0ec5e89e78e9a5a96fc" category="section-title">Einhaltung von VM-Storage-Richtlinien</block>
  <block id="bfd6dec22a6a1724ff1b67f1f289cdfc" category="image-alt">Einhaltung der Virtual Machine Storage-Richtlinien</block>
  <block id="9fe1fd556b933358881222d5c3da127c" category="paragraph">Zum Zeitpunkt der Veröffentlichung sind Hyperscaler-Umgebungen nur auf herkömmliche NFS v3-Datastores beschränkt. Daher sind VVols nur mit lokalen ONTAP Systemen oder Cloud-vernetzten Systemen verfügbar, die die gesamten Funktionen von On-Premises-Systemen bereitstellen, z. B. von NetApp Partnern und Service-Providern auf der ganzen Welt.</block>
  <block id="274bc582ca884fc158ea9ac9f79f9067" category="inline-link">ONTAP Produktdokumentation</block>
  <block id="5ca60fe92dec435059ba5acefa4ce2c9" category="paragraph">_Weitere Informationen zu ONTAP finden Sie unter<block ref="8a9b15dc18a16e0b1acfa438e27f6aa6" category="inline-link-rx"></block>_</block>
  <block id="3dffa984dadf9ff0006ee9cebd3b04b9" category="inline-link">TR-4597</block>
  <block id="d3fb76da5fbbcd6fa32e40910b5b9a47" category="section-title">Vorteile der Verwendung von VVols mit ONTAP</block>
  <block id="d0f004282063136e91d76d1d43d7c95f" category="paragraph">Als VMware 2015 die VVols-Unterstützung mit VASA 2.0 einführte, bezeichnete das Unternehmen das System als „ein Integrations- und Management-Framework zur Bereitstellung eines neuen Betriebsmodells für externen Storage (SAN/NAS)“. Dieses Betriebsmodell bietet zusammen mit ONTAP Storage mehrere Vorteile.</block>
  <block id="6a74f7c0a9021e009a9c030e54084978" category="paragraph">Wie in Abschnitt 1.2 beschrieben, ermöglicht richtlinienbasiertes Management die Bereitstellung und das Management von VMs anhand von vordefinierten Richtlinien. Dies bietet verschiedene Vorteile FÜR IT-Abläufe:</block>
  <block id="e0eb51b6d319e14c2fbfc1f628021a26" category="list-text">*Beschleunigung.* durch ONTAP Tools muss der vCenter Administrator keine Tickets mehr für die Storage-Bereitstellung beim Storage Team öffnen. ONTAP-Tools RBAC-Rollen in vCenter und im ONTAP System ermöglichen jedoch unabhängigen Teams (z. B. Storage-Teams) oder unabhängigen Aktivitäten desselben Teams, indem bei Bedarf der Zugriff auf bestimmte Funktionen eingeschränkt wird.</block>
  <block id="3f4dbd6687bd0d55bf69fe5fdd0137c4" category="list-text">*Intelligentere Bereitstellung.* die Funktionen des Storage-Systems können über die VASA APIs zugänglich gemacht werden. So können Workflows für die Bereitstellung von erweiterten Funktionen profitieren, ohne dass der VM-Administrator ein Verständnis für das Management des Storage-Systems benötigt.</block>
  <block id="a13fcc9220d042cae454e9ac073636ca" category="list-text">*Schnellere Bereitstellung.* verschiedene Storage-Funktionen können in einem einzelnen Datastore unterstützt und anhand der VM-Richtlinie automatisch für eine VM ausgewählt werden.</block>
  <block id="eb4c052226108afebf26e670208b9a55" category="list-text">*Vermeiden von Fehlern.* Storage- und VM-Richtlinien werden vorab entwickelt und bei Bedarf angewendet, ohne dass bei jeder Bereitstellung einer VM Storage angepasst werden muss. Wenn sich die Storage-Funktionen von den festgelegten Richtlinien abdriften, werden Compliance-Alarme ausgelöst. Wie bereits erwähnt, ist die Erstbereitstellung durch SCPs vorhersehbar und wiederholbar, wobei die korrekte Platzierung durch die Verwendung von VM-Speicherrichtlinien auf den SCPs gewährleistet ist.</block>
  <block id="f9878e32fdf258284c007eaae38773cd" category="section-title">Granulares VM-Management auf dem modernen SAN</block>
  <block id="7e673a7c12cc256f29bcd7b2028d9d5a" category="paragraph">SAN-Storage-Systeme mit Fibre Channel und iSCSI wurden als erste von VMware für ESX unterstützt, allerdings fehlten ihnen die Managementmöglichkeiten individueller VM-Dateien und Festplatten aus dem Storage-System. Stattdessen werden LUNs bereitgestellt und VMFS managt die einzelnen Dateien. Dadurch wird es für das Storage-System schwierig, die Storage-Performance, das Klonen und den Schutz einzelner VMs direkt zu managen. VVols bieten Storage-Granularität, die Kunden, die NFS-Storage bereits nutzen, mit den robusten, hochperformanten SAN-Funktionen von ONTAP.</block>
  <block id="25a46370807e0eab0dfe3853abfb9b6e" category="paragraph">Mit vSphere 8 und ONTAP Tools für VMware vSphere 9.12 und höher sind nun dieselben granularen Steuerelemente, die von VVols für ältere SCSI-basierte Protokolle verwendet werden, in dem modernen Fibre-Channel-SAN unter Verwendung von NVMe over Fabrics verfügbar, um noch höhere Performance im großen Maßstab zu ermöglichen. Mit vSphere 8.0 Update 1 ist es jetzt möglich, eine umfassende End-to-End-NVMe-Lösung mit VVols zu implementieren, ohne dass eine I/O-Verschiebung im Hypervisor-Storage-Stack erforderlich ist.</block>
  <block id="f61f57f748f5f0b52c7f11f1f1bd43f0" category="section-title">Bessere Auslagerungsmöglichkeiten</block>
  <block id="911fa02c34dda38cd69be06e524ed9d0" category="inline-link">Effizienz-Garantie</block>
  <block id="3170031d77f241bc872afcb490c0a806" category="paragraph">VAAI bietet zwar eine Vielzahl an Operationen, die auf Storage verlagert werden, doch bestehen einige Lücken, die vom VASA Provider behoben werden. SAN VAAI kann keine von VMware gemanagten Snapshots in das Storage-System auslagern. NFS VAAI kann über VM gemanagte Snapshots auslagern, aber es gibt Einschränkungen, bei denen eine VM mit nativen Storage-Snapshots platziert wird. Da VVols individuelle LUNs, Namespaces oder Dateien für Virtual-Machine-Festplatten verwenden, kann ONTAP die Dateien oder LUNs schnell und effizient klonen, um VM-granulare Snapshots zu erstellen, die keine Delta-Dateien mehr benötigen. NFS VAAI unterstützt zudem nicht das verlagern von Klonvorgängen bei Migrationen mit heißem (eingeschaltetem) Storage vMotion. Die VM muss ausgeschaltet sein, um bei Verwendung von VAAI mit herkömmlichen NFS-Datastores das verlagern der Migration zu ermöglichen. Der VASA Provider in ONTAP ermöglicht nahezu sofortige, Storage-effiziente Klone für heiße und kalte Migrationen. Zudem unterstützt er nahezu sofortige Kopien für Volume-übergreifende Migrationen von VVols. Aufgrund dieser enormen Vorteile hinsichtlich der Storage-Effizienz können Sie die VVols Workloads unter dem optimal nutzen<block ref="9d1af7a9d4f98887e53c1a9bedbce044" category="inline-link-rx"></block> Programm. Auch wenn Volume-übergreifende Klone mit VAAI nicht Ihren Anforderungen entsprechen, werden Sie wahrscheinlich aufgrund der Verbesserungen bei den Kopien mit VVols eine geschäftliche Herausforderung bewältigen können.</block>
  <block id="a8567b4dc683947384ef2e1dd0fc6ac1" category="section-title">Häufige Anwendungsfälle für VVols</block>
  <block id="48e535b80538b088fd868fde9372715c" category="paragraph">Neben diesen Vorteilen sehen wir auch folgende häufige Anwendungsfälle für vVol Storage:</block>
  <block id="441f01ec38e91fad1b235a0e7422a178" category="list-text">*Bedarfsgesteuerte Bereitstellung von VMs*</block>
  <block id="bd652a918164d8fa5176b5f9b92c2e61" category="list-text">Private Cloud oder Service-Provider-IaaS.</block>
  <block id="fddccd9595370687015a5ab02bcff60f" category="list-text">Automatisierung und Orchestrierung über die Aria (ehemals vRealize) Suite, OpenStack usw.</block>
  <block id="4fbc1141a40f5259e3ab1545de52b1ea" category="list-text">*First Class Disks (FCDs)*</block>
  <block id="e83f2e4e8676ee8faac4d6af599fd7e7" category="list-text">Persistente VMware Tanzu Kubernetes Grid [TKG] Volumes.</block>
  <block id="b2b81490f2245d3d566e0ccc6a29cfc2" category="list-text">Bereitstellung von Amazon EBS-ähnlichen Services über unabhängiges VMDK Lifecycle Management</block>
  <block id="2511e4e8cbb72027ae20f24008d4b939" category="list-text">*On-Demand Bereitstellung temporärer VMs*</block>
  <block id="eeb1044cc319a5d53503e45dbf762291" category="list-text">Labore für Test und Entwicklung</block>
  <block id="858d314810762abccaa7baf230e3dc18" category="list-text">Schulungsumgebungen</block>
  <block id="ab7bbc294bf0f0354980a3014e8f4219" category="section-title">Gemeinsame Vorteile mit VVols</block>
  <block id="ccd6ea8c57e93a5401d07aab9c60f371" category="paragraph">Wenn VVols so eingesetzt werden, wie in den oben genannten Anwendungsfällen, bieten sie folgende spezifische Verbesserungen:</block>
  <block id="13491b3af50183642ea0035b2d1f95f9" category="list-text">Klone werden schnell innerhalb eines einzelnen Volumes oder über mehrere Volumes in einem ONTAP Cluster hinweg erstellt – ein Vorteil im Vergleich zu herkömmlichen VAAI-fähigen Klonen. Außerdem sind sie Storage-effizient. Klone innerhalb eines Volumes nutzen ONTAP-Datei-Klone, die wie FlexClone Volumes sind und speichern nur Änderungen aus der Quell-vVol-Datei/LUN/Namespace. Dadurch werden langfristige VMs für Produktions- oder andere Applikationszwecke schnell erstellt, benötigen nur minimalen Speicherplatz und profitieren vom Schutz auf VM-Ebene (durch das NetApp SnapCenter Plug-in für VMware vSphere, von VMware gemanagte Snapshots oder VADP-Backup) und Performance-Management (mit ONTAP QoS).</block>
  <block id="1c21847cdc5616af138ba0c03aad7adb" category="list-text">VVols stellen die ideale Storage-Technologie dar, wenn ein TKG mit vSphere CSI verwendet wird und separate Storage-Klassen und Kapazitäten bereitstellt, die vom vCenter Administrator gemanagt werden.</block>
  <block id="d5f068caef73978a5483badd927a7e98" category="list-text">Amazon EBS-ähnliche Services können über FCDs bereitgestellt werden, da eine FCD-VMDK, wie der Name schon andeutet, eine erstklassige Antwort in vSphere ist und einen Lebenszyklus hat, der unabhängig von den VMs gemanagt werden kann, an die es angeschlossen werden kann.</block>
  <block id="7e7040600922b2a78715f856613a3c08" category="doc">Sicherung von VVols</block>
  <block id="de92c927f248723e4668aec459709995" category="section-title">VASA Provider High Availability</block>
  <block id="f4d4cb8868f07cad16bd7de7e36c5201" category="paragraph">NetApp VASA Provider wird als Teil der virtuellen Appliance zusammen mit dem vCenter Plug-in und REST API-Server (ehemals Virtual Storage Console [VSC]) und Storage Replication Adapter ausgeführt. Wenn der VASA Provider nicht verfügbar ist, werden VMs mit VVols weiterhin ausgeführt. Es können jedoch keine neuen VVols-Datastores erstellt werden. VVols können nicht über vSphere erstellt oder gebunden werden. Das bedeutet, dass VMs mit VVols nicht eingeschaltet werden können, da vCenter die Erstellung des Swap-vVol nicht anfordern kann. Außerdem können ausgeführte VMs vMotion nicht für die Migration zu einem anderen Host verwenden, da die VVols nicht an den neuen Host gebunden werden können.</block>
  <block id="3958bf41787d7fcc6a9f8dd9348c947f" category="paragraph">VASA Provider 7.1 und höher unterstützen neue Funktionen, damit die Services bei Bedarf verfügbar sind. Sie umfasst neue Watchdog-Prozesse zur Überwachung von VASA Provider und integrierten Datenbankdiensten. Wenn ein Fehler erkannt wird, werden die Protokolldateien aktualisiert und die Dienste dann automatisch neu gestartet.</block>
  <block id="1acc458c695b0d3dfbf588309801fda2" category="paragraph">Der weitere Schutz muss vom vSphere-Administrator mithilfe derselben Verfügbarkeitsfunktionen konfiguriert werden, die auch zum Schutz anderer geschäftskritischer VMs vor Fehlern in Software, Host-Hardware und Netzwerk verwendet werden. Es ist keine zusätzliche Konfiguration für die virtuelle Appliance erforderlich, um diese Funktionen nutzen zu können. Konfigurieren Sie sie einfach mit dem Standard-vSphere-Ansatz. Sie wurden getestet und werden von NetApp unterstützt.</block>
  <block id="ff43afff832597ff45e5abeab2826ccf" category="inline-link">Dokumentation zu ONTAP Tools für VMware vSphere (Konfiguration von Hochverfügbarkeit für ONTAP Tools)</block>
  <block id="d516ca4c9c18446a82d9fc9ee85bd955" category="paragraph">VSphere High Availability lässt sich leicht konfigurieren, um eine VM auf einem anderen Host im Host-Cluster bei einem Ausfall neu zu starten. VSphere Fault Tolerance bietet eine höhere Verfügbarkeit, indem eine sekundäre VM erstellt wird, die kontinuierlich repliziert wird und an jedem beliebigen Punkt übernommen werden kann. Weitere Informationen zu diesen Funktionen finden Sie im<block ref="b6d78cbf1d5afb52929c529b32a350ad" category="inline-link-rx"></block>, Sowie VMware vSphere Dokumentation (suchen Sie nach vSphere Verfügbarkeit unter ESXi und vCenter Server).</block>
  <block id="97ba24d475ff6fdc7743099992cc301f" category="paragraph">ONTAP Tools VASA Provider sichert die VVols Konfiguration automatisch in Echtzeit auf gemanagten ONTAP Systemen, auf denen die VVols Informationen innerhalb der FlexVol Volume-Metadaten gespeichert sind. Sollte die ONTAP Tools Appliance aus irgendeinem Grund nicht mehr verfügbar sein, können Sie schnell und einfach eine neue Appliance implementieren und die Konfiguration importieren. Weitere Informationen zu den Schritten zur Wiederherstellung von VASA Provider finden Sie in diesem KB-Artikel:</block>
  <block id="6799b6ec743f9159edf3b43790210a11" category="inline-link">So führen Sie eine VASA Provider Disaster Recovery - Resolution Guide durch</block>
  <block id="9462790a7557a7eb2fe9daad4b875817" category="paragraph"><block ref="9462790a7557a7eb2fe9daad4b875817" category="inline-link-rx"></block></block>
  <block id="029a7740ecd792264454f9dbe194e980" category="section-title">VVols Replizierung</block>
  <block id="c7b26217fd0c9041b6f779a0f669de5c" category="paragraph">Viele ONTAP Kunden replizieren ihre herkömmlichen Datastores auf sekundäre Storage-Systeme mithilfe von NetApp SnapMirror. Bei einem Ausfall stellen sie dann mithilfe des Sekundärsystems individuelle VMs oder einen kompletten Standort wieder her. In den meisten Fällen verwenden Kunden hierfür ein Software Tool, z. B. ein Backup Software-Produkt wie das NetApp SnapCenter Plug-in für VMware vSphere oder eine Disaster Recovery-Lösung wie Site Recovery Manager von VMware (zusammen mit dem Storage Replication Adapter in ONTAP Tools).</block>
  <block id="bc6303e4538280b1b01d45a5f7d20039" category="paragraph">Diese Anforderung an ein Software-Tool ist für das Management der VVols Replizierung noch wichtiger. Einige Aspekte können durch native Funktionen gemanagt werden (beispielsweise werden durch VMware gemanagte Snapshots von VVols auf ONTAP verlagert, bei denen schnelle, effiziente Datei- oder LUN-Klone verwendet werden), doch ist allgemeine Orchestrierung für das Management der Replizierung und Recovery erforderlich. Metadaten zu VVols werden sowohl durch ONTAP als auch durch den VASA Provider geschützt, für die Nutzung an einem sekundären Standort ist jedoch eine zusätzliche Verarbeitung erforderlich.</block>
  <block id="c6a26835eb364bad22b16cb127b74135" category="paragraph">Die ONTAP Tools 9.7.1 unterstützen in Verbindung mit der VMware Site Recovery Manager (SRM) Version 8.3 zusätzlich die Orchestrierung von Disaster Recovery und Migrations-Workflows mithilfe der NetApp SnapMirror Technologie.</block>
  <block id="2803a799fd66056eff404a17ba640c2b" category="paragraph">In der ersten Version der SRM-Unterstützung mit ONTAP Tools 9.7.1 war es erforderlich, FlexVols vorab zu erstellen und die SnapMirror Sicherung zu aktivieren, bevor sie als Backup-Volumes für einen VVols-Datastore verwendet werden konnten. Ab ONTAP Tools 9.10 wird dieser Prozess nicht mehr benötigt. Sie können jetzt vorhandene Backup Volumes um SnapMirror Schutz erweitern und Ihre VM-Storage-Richtlinien aktualisieren, um von richtlinienbasiertem Management mit Disaster Recovery, Migrationorchestrierung und Automatisierung, integriert in SRM, zu profitieren.</block>
  <block id="8142fcf496d02a58ce02a17558db863e" category="paragraph">Derzeit ist VMware SRM die einzige von NetApp unterstützte Lösung für Disaster Recovery und Migrationsautomatisierung für VVols. ONTAP Tools überprüfen die Existenz eines SRM 8.3 oder eines höheren Servers, der bei vCenter registriert ist, bevor Sie die VVols Replizierung aktivieren können. Es ist zwar möglich, die REST-APIs der ONTAP Tools zur Erstellung eigener Services zu nutzen.</block>
  <block id="7d8e8eced8d8b1a50f20640ffb5d363a" category="section-title">VVols Replizierung mit SRM</block>
  <block id="c95aede1de7a41511c9d962deba49054" category="section-title">MetroCluster-Unterstützung</block>
  <block id="3f15bb720adb42bd521a2ce1cb3a2974" category="paragraph">ONTAP Tools können zwar keine MetroCluster-Umschaltung auslösen, doch es unterstützt NetApp MetroCluster Systeme für VVols, die Volumes in einer einheitlichen vSphere Metro Storage Cluster (vMSC) Konfiguration sichern. Die Umschaltung eines MetroCluster-Systems erfolgt auf normale Weise.</block>
  <block id="cde16c5ee89c209c821e120caf65bb81" category="paragraph">NetApp SnapMirror Business Continuity (SM-BC) kann zwar auch als Basis für eine vMSC Konfiguration verwendet werden, wird jedoch derzeit nicht mit VVols unterstützt.</block>
  <block id="c859f5cd725cb2d1b163e4b066bede9a" category="paragraph">In diesen Leitfäden finden Sie weitere Informationen über NetApp MetroCluster:</block>
  <block id="e5e0fb3ae5564674a6548ade7c601578" category="inline-link">_TR-4689 MetroCluster IP Lösungsarchitektur und Design_</block>
  <block id="e64de4e49a37e62ff7f8c37e859bbeb9" category="paragraph"><block ref="e64de4e49a37e62ff7f8c37e859bbeb9" category="inline-link-rx"></block></block>
  <block id="0229efec9dc9b9a7fdf144b025157176" category="inline-link">TR-4705 NetApp MetroCluster Lösungsarchitektur und Design_</block>
  <block id="6aee4b78ffdfaf193918978472d660a7" category="paragraph"><block ref="6aee4b78ffdfaf193918978472d660a7" category="inline-link-rx"></block></block>
  <block id="876341bac548f230eb390348e8b075ad" category="inline-link">_VMware KB 2031038 VMware vSphere Unterstützung mit NetApp MetroCluster_</block>
  <block id="5e68486fffeebad2ed145408dc250367" category="paragraph"><block ref="5e68486fffeebad2ed145408dc250367" category="inline-link-rx"></block></block>
  <block id="50c323f7050912164118ba47eab75888" category="section-title">VVols Backup-Übersicht</block>
  <block id="6751e706567f4773c4f3138b4de51a0f" category="paragraph">Für die Sicherung von VMs gibt es verschiedene Ansätze, beispielsweise die Verwendung von Backup-Agenten in Gastbetrieben, das Anhängen von VM-Datendateien an einen Backup-Proxy oder die Verwendung definierter APIs wie VMware VADP. VVols können über dieselben Mechanismen geschützt werden, und viele NetApp Partner unterstützen VM-Backups, einschließlich VVols.</block>
  <block id="416d27c872769e934ac8a49ec98ccb53" category="paragraph">Wie bereits erwähnt, werden von VMware vCenter gemanagte Snapshots in platzsparende und schnelle ONTAP Datei-/LUN-Klone ausgelagert. Diese können für schnelle, manuelle Backups verwendet werden, sind aber von vCenter auf maximal 32 Snapshots beschränkt. Sie können vCenter verwenden, um Snapshots zu erstellen und bei Bedarf zurückzusetzen.</block>
  <block id="8f3d5ae5bd7d8922841975ccd84b8526" category="paragraph">Ab dem SnapCenter Plug-in für VMware vSphere (SCV) 4.6 wird in Verbindung mit den ONTAP Tools 9.10 und höher die Unterstützung für absturzkonsistentes Backup und Recovery von VVols-basierten VMs unterstützt. Dabei werden ONTAP FlexVol Volume Snapshots mit Unterstützung für SnapMirror und SnapVault-Replizierung verwendet. Pro Volume werden bis zu 1023 Snapshots unterstützt. SCV kann mithilfe von SnapMirror mit einer Mirror-Vault-Richtlinie auch mehr Snapshots mit längerer Aufbewahrung auf sekundären Laufwerken speichern.</block>
  <block id="1429a9379faa8e82f8b198e93eebdf61" category="paragraph">Die Unterstützung für vSphere 8.0 wurde mit SCV 4.7 eingeführt, wobei eine isolierte lokale Plug-in-Architektur verwendet wurde. Die Unterstützung für vSphere 8.0U1 wurde zu SCV 4.8 hinzugefügt, wodurch die neue Remote-Plug-in-Architektur vollständig umgestellt wurde.</block>
  <block id="de1d77d00dae616276b6ceb6296a18f2" category="section-title">VVols Backup mit SnapCenter Plug-in für VMware vSphere</block>
  <block id="02f41838319aab980999b60967d7fd53" category="paragraph">Mit NetApp SnapCenter können Sie nun auf Tags und/oder Ordnern basierende Ressourcengruppen für VVols erstellen und so automatisch die Vorteile der auf ONTAP FlexVol basierenden Snapshots für VVols basierte VMs nutzen. So können Sie Backup- und Recovery-Services definieren, die VMs automatisch bei der dynamischen Bereitstellung in Ihrer Umgebung sichern.</block>
  <block id="4549a2943666c4815098331d30a872da" category="paragraph">Das SnapCenter Plug-in für VMware vSphere wird als Standalone-Appliance implementiert, die als vCenter-Erweiterung registriert und über die vCenter UI oder ÜBER REST-APIs zur Automatisierung von Backup- und Recovery-Services gemanagt wird.</block>
  <block id="26e2418d177df1dddb008e455ce28752" category="section-title">Architektur von SnapCenter</block>
  <block id="c32197281aa44f57ae568a9a42c7b3b6" category="paragraph">Da zum Zeitpunkt dieses Schreibens die anderen SnapCenter-Plug-ins VVols noch nicht unterstützen, konzentrieren wir uns in diesem Dokument auf das eigenständige Implementierungsmodell.</block>
  <block id="6750052f2c0fb06d80a8efd7cb871190" category="paragraph">Da SnapCenter ONTAP FlexVol Snapshots verwendet, wird kein Overhead auf vSphere platziert. Es gibt auch keine Performance-Einbußen, wie man bei herkömmlichen VMs mit von vCenter gemanagten Snapshots sehen könnte. Da die SCV-Funktionalität über REST-APIs zugänglich ist, wird die Erstellung automatisierter Workflows mit Tools wie VMware Aria Automation, Ansible, Terraform und nahezu jedem anderen Automatisierungs-Tool, das standardmäßige REST-APIs verwenden kann, erleichtert.</block>
  <block id="0c889d77d71512ceb7241bd0f6d6ca23" category="inline-link">Übersicht ÜBER REST-APIs</block>
  <block id="35eab4195b135718da016e20979f9031" category="paragraph">Informationen zu SnapCenter-REST-APIs finden Sie unter<block ref="33852d6f529e96c0b816da3945bcb1e5" category="inline-link-rx"></block></block>
  <block id="4ee2d383554981f4ea8eff4f35db6832" category="inline-link">SnapCenter Plug-in für VMware vSphere REST-APIs</block>
  <block id="63ae61e98eec102c39264a819a3c522e" category="paragraph">Informationen zum SnapCenter Plug-in für VMware vSphere REST-APIs finden Sie unter<block ref="40a6c7462de67e21480b7a31e406f8f9" category="inline-link-rx"></block></block>
  <block id="bdf03d5c2a7086b6c916dc96d06a0a6c" category="paragraph">Die folgenden Best Practices unterstützen Sie dabei, die Vorteile Ihrer SnapCenter Implementierung optimal zu nutzen.</block>
  <block id="60af6a4208b610da54a63a7e3658d5a5" category="list-text">SCV unterstützt sowohl vCenter Server RBAC als auch ONTAP RBAC und umfasst vordefinierte vCenter Rollen, die automatisch für Sie erstellt werden, wenn das Plug-in registriert ist. Sie finden weitere Informationen zu den unterstützten Typen von RBAC<block ref="a8ef9d6a500bd8288101245a3828ddb1" category="inline-link-rx"></block></block>
  <block id="0ed0835259b5b8c4b0f0910e7bfe2b17" category="list-text">Verwenden Sie die vCenter-Benutzeroberfläche, um den Zugriff auf das Konto mit den geringsten Berechtigungen mithilfe der beschriebenen vordefinierten Rollen zuzuweisen<block ref="17b3487f3493b9c198e03f92348bfca0" category="inline-link-rx"></block>.</block>
  <block id="d53326766687d6bc7fa2e28ee177e809" category="list-text">Wenn Sie SCV mit SnapCenter-Server verwenden, müssen Sie die Rolle _SnapCenterAdmin_ zuweisen.</block>
  <block id="61ba6cb0b6021c205d41ed7f6fc1eb71" category="list-text">ONTAP RBAC bezieht sich auf das Benutzerkonto, das zum Hinzufügen und Managen der vom SCV verwendeten Speichersysteme verwendet wird. Die rollenbasierte Zugriffssteuerung von ONTAP gilt nicht für VVols-basierte Backups. Erfahren Sie mehr über ONTAP RBAC und SCV<block ref="e4e865178d3962f87ac5cef3d6d68942" category="inline-link-rx"></block>.</block>
  <block id="1c89ba258526186ea0ec98325b61371b" category="list-text">Replizieren Sie Backup-Datensätze auf ein zweites System und verwenden Sie SnapMirror für vollständige Replikate der Quell-Volumes. Wie bereits erwähnt, können Sie auch Mirror-Vault Richtlinien für die längerfristige Aufbewahrung von Backup-Daten unabhängig von den Quell-Volume Snapshot Aufbewahrungseinstellungen verwenden. Beide Mechanismen werden durch VVols unterstützt.</block>
  <block id="9792eeb3b91469abf77746b005d82d77" category="list-text">Da SCV außerdem ONTAP-Tools für VMware vSphere für VVols Funktionen erfordert, prüfen Sie immer das NetApp Interoperabilitäts-Matrix-Tool (IMT), ob die jeweilige Version kompatibel ist</block>
  <block id="71c989bd68059e2ddfa4200c60b736c7" category="list-text">Wenn Sie eine VVols-Replizierung mit VMware SRM verwenden, sollten Sie Ihre Richtlinien-RPO und Backup-Zeitplan beachten</block>
  <block id="604eb779cbb9af378f7bda71633b5b31" category="list-text">Backup-Richtlinien auf Aufbewahrungseinstellungen erstellen, die die in Ihrem Unternehmen definierten Recovery Point Objectives (RPOs) erfüllen</block>
  <block id="067ff91a08d3ff453b59d81993b96182" category="list-text">Konfigurieren Sie Benachrichtigungseinstellungen für Ihre Ressourcengruppen, um über den Status der Backups informiert zu werden (siehe Abbildung 10 unten).</block>
  <block id="c3add00693172bbbb4dd864bf0ad9116" category="section-title">Benachrichtigungsoptionen für Ressourcengruppen</block>
  <block id="6eae9510711f8df8f55a15e1761875d9" category="section-title">Erste Schritte mit SCV mit diesen Dokumenten</block>
  <block id="235a656535515ab3518937c09836e60d" category="inline-link">Erfahren Sie mehr über das SnapCenter Plug-in für VMware vSphere</block>
  <block id="79eb02d2a96cc634df87bec5bd511bbe" category="paragraph"><block ref="79eb02d2a96cc634df87bec5bd511bbe" category="inline-link-rx"></block></block>
  <block id="04e7bb0411c1c8f56ca4fc8000c62ad8" category="inline-link">Implementieren Sie das SnapCenter Plug-in für VMware vSphere</block>
  <block id="d4ba0aa084ae1145248006481c881d29" category="paragraph"><block ref="d4ba0aa084ae1145248006481c881d29" category="inline-link-rx"></block></block>
  <block id="231cf4c70d866b616c21baddaeed0696" category="doc">Fehlerbehebung</block>
  <block id="9d2169c1c2855b78a7aeaa4f42d27dc7" category="section-title">NetApp Support Website</block>
  <block id="ed9018b2e8611a3d2c03bb6a205b9715" category="inline-link">ONTAP Tools für VMware vSphere</block>
  <block id="890eb741924c425abbb11e4618560181" category="paragraph">Neben einer Vielzahl von Knowledgebase Artikeln zu NetApp Virtualisierungsprodukten bietet die NetApp Support-Website auch eine praktische Landing Page für das<block ref="6e90053136fd326e5d581844ca21966d" category="inline-link-rx"></block> Produkt. Dieses Portal bietet Links zu Artikeln, Downloads, technischen Berichten und Diskussionen zu VMware Lösungen in der NetApp Community. Sie ist verfügbar unter:</block>
  <block id="fff460b039580a1bd42725a397b4c8be" category="inline-link">NetApp Support Site_</block>
  <block id="897515b5a8a03f8d5cbed44fd47ef9f3" category="paragraph"><block ref="897515b5a8a03f8d5cbed44fd47ef9f3" category="inline-link-rx"></block></block>
  <block id="4020fd07f5ee2361dd23956e6a334ffd" category="paragraph">Weitere Dokumentation zur Lösung finden Sie hier:</block>
  <block id="baf17f6d2396be5fd3676baf863a331f" category="section-title">Fehlerbehebung Für Produkte</block>
  <block id="9d6d7fa1a5253698556ac2b0cda2207d" category="paragraph">Die verschiedenen Komponenten von ONTAP Tools wie vCenter Plug-in, VASA Provider und Storage Replication Adapter sind im NetApp Dokumenten-Repository zusammengefasst. Jedes hat jedoch einen separaten Unterabschnitt der Wissensdatenbank und kann spezifische Fehlerbehebungsverfahren haben. Diese betreffen die häufigsten Probleme, die mit dem VASA Provider auftreten können.</block>
  <block id="ae2a93a809929192ecc7b468f234af84" category="section-title">Probleme BEI DER VASA Provider-UI</block>
  <block id="92a2b5cb9c6906035c2864fa225e1940" category="inline-link">Artikel</block>
  <block id="69aecee18a55993c779487758eb051cc" category="paragraph">Gelegentlich stößt der vCenter vSphere Web Client auf Probleme mit den Serenity-Komponenten, wodurch die Menüelemente VASA Provider for ONTAP nicht angezeigt werden. Weitere Informationen finden Sie unter Beheben von Problemen bei der Registrierung von VASA Provider im Implementierungsleitfaden oder in dieser Knowledgebase<block ref="c8979f1604396ce5570d07774ba255f0" category="inline-link-rx"></block>.</block>
  <block id="03cc611b90575ea37b55959adaa1c754" category="section-title">VVols Datastore-Bereitstellung schlägt fehl</block>
  <block id="281b890be0e8280eea1aebb82de90961" category="paragraph">Gelegentlich treten bei der Erstellung des VVols-Datastores bei vCenter-Services möglicherweise eine Zeitlang aus. Um sie zu korrigieren, starten Sie den vmware-sps-Service neu und mounten Sie den VVols-Datastore über die vCenter-Menüs (Storage &gt; New Datastore) neu. Dies wird durch die fehlgeschlagenen VVols Datastore-Bereitstellung mit vCenter Server 6.5 im Administrationshandbuch abgedeckt.</block>
  <block id="3152add163cd8aaf116a259d85ebf8fb" category="section-title">Das Aktualisieren von Unified Appliance schlägt fehl, um ISO zu mounten</block>
  <block id="ffaa56a3f5a1157f2eb3955773bb41da" category="paragraph">Aufgrund eines Fehlers in vCenter kann das zur Aktualisierung der Unified Appliance von einem Release auf das nächste verwendete ISO möglicherweise nicht mounten. Wenn das ISO mit der Appliance in vCenter verbunden werden kann, befolgen Sie den Prozess in dieser Knowledgebase<block ref="9839383ed04f777c0132b57ad0cb14ac" category="inline-link-rx"></block> Zu beseitigen.</block>
  <block id="cacba1b456347ce8f32c7f5ed0d81e64" category="doc">Implementierung von VVols Storage</block>
  <block id="2a5f3dc8d46246216b293c9e41979269" category="paragraph">Die ersten beiden Schritte sind für eine bestehende vSphere-Umgebung, die ONTAP für herkömmliche Datastores verwendet, möglicherweise nicht erforderlich. Möglicherweise verwenden Sie bereits ONTAP Tools für das Management, die Automatisierung und die Berichterstellung mit Ihrem VMFS- oder herkömmlichen NFS-basierten Storage. Diese Schritte werden im folgenden Abschnitt näher erläutert.</block>
  <block id="841db3655338a0c3dc36cf2761bfdafd" category="list-text">Storage Virtual Machine (SVM) und deren Protokollkonfiguration erstellen. Sie wählen zwischen NVMe/FC, NFSv3, NFSv4.1, iSCSI, FCP oder eine Kombination dieser Optionen. Sie können entweder die Assistenten von ONTAP System Manager oder die Cluster Shell-Befehlszeile verwenden.</block>
  <block id="10818a8ad4f650a48d78d728278d4cb3" category="list-text">Mindestens eine LIF pro Node für jede Switch-/Fabric-Verbindung. Als Best Practice sollten Sie mindestens zwei pro Node für FCP-, iSCSI- oder NVMe-basierte Protokolle erstellen.</block>
  <block id="612705dd95888cdc6345d01c93c84c39" category="list-text">Derzeit können Volumes erstellt werden, doch es ist einfacher, sie vom Assistenten „_Provisioning Datastore_“ erstellen zu lassen. Die einzige Ausnahme von dieser Regel ist, wenn Sie eine VVols Replizierung mit VMware Site Recovery Manager verwenden möchten. Die Einrichtung solcher FlexVol Volumes mit bestehenden SnapMirror Beziehungen ist einfacher. Beachten Sie, dass QoS auf keinen Volumes für VVols aktiviert wird, da diese über SPBM und ONTAP Tools gemanagt werden sollen.</block>
  <block id="16421df834af873175559b678c2b3cd9" category="list-text">Implementieren Sie ONTAP Tools für VMware vSphere mit der von der NetApp Support-Website heruntergeladenen OVA.</block>
  <block id="8bdb69b5934ff84bbaafd84585965a90" category="list-text">Konfigurieren Sie ONTAP Tools für Ihre Umgebung.</block>
  <block id="dd07accb290c9c1e3900692630a70767" category="list-text">Fügen Sie den ONTAP-Cluster zu den ONTAP-Tools unter _Storage Systems_ hinzu</block>
  <block id="869ce78032d85111f09ceb58bdb59a36" category="list-text">Wenn sich Ihre ONTAP-Daten-LIFs in unterschiedlichen Subnetzen von Ihren VMkernel-Adaptern befinden, müssen Sie die VMkernel-Adapter-Subnetze zur Liste der ausgewählten Subnetze im Einstellungsmenü der ONTAP-Tools hinzufügen. Standardmäßig sichern ONTAP Tools Ihren Storage-Datenverkehr nur durch lokalen Subnetzzugriff.</block>
  <block id="5a6d084090f063797fc6b863d2817d98" category="list-text">Die ONTAP Tools enthalten mehrere vordefinierte Richtlinien, die verwendet werden können oder sehen <block ref="434866eb159632a70c4db034544336ef" category="inline-xref-macro-rx"></block> Anleitung zum Erstellen von SCPs.</block>
  <block id="ef51190e4c839596248fc4173ed12a3c" category="list-text">Verwenden Sie das Menü „_ONTAP Tools_“ in vCenter, um den Assistenten „_Provisioning Datastore_“ zu starten.</block>
  <block id="66edaf6f622057aeadab950977c1f3e8" category="list-text">Geben Sie einen aussagekräftigen Namen ein, und wählen Sie das gewünschte Protokoll aus. Sie können auch eine Beschreibung des Datastore angeben.</block>
  <block id="d22d6de0c8f11375907a1c10d8d0f251" category="list-text">Wählen Sie einen oder mehrere SCPs aus, die vom VVols-Datastore unterstützt werden sollen. Dadurch werden alle ONTAP-Systeme herausgefiltert, die nicht mit dem Profil übereinstimmen. Wählen Sie in der Ergebnisliste den gewünschten Cluster und die gewünschte SVM aus.</block>
  <block id="6a016a18a0c20e3b805cd9b8e713fb3e" category="list-text">Verwenden Sie den Assistenten, um neue FlexVol-Volumes für jeden der angegebenen SCPs zu erstellen, oder verwenden Sie vorhandene Volumes, indem Sie das entsprechende Optionsfeld auswählen.</block>
  <block id="2527effe721902a77755b8d628fdad93" category="list-text">Erstellen Sie VM-Richtlinien für jedes SCP, das im Datastore verwendet wird, über das Menü „_Policies and Profiles_“ in der vCenter UI.</block>
  <block id="642dadaf5095b4c11730dca0ffcb0d27" category="list-text">Wählen Sie den Storage-Regelsatz „NetApp.Clustered.Data.ONTAP.VP.vvol“ aus. Der Storage-Regelsatz „NetApp.Clustered.Data.ONTAP.VP.VASA10“ gilt für die SPBM-Unterstützung bei Datastores ohne VVols</block>
  <block id="c9565e0957e6e02cf12aef618bd30c3f" category="list-text">Beim Erstellen einer VM-Speicherrichtlinie geben Sie das Storage Capability Profile nach Namen an. Sie können in diesem Schritt auch die SnapMirror Richtlinienabstimmung über die Registerkarte „Replication“ und über die Registerkarte „Tags“ konfigurieren. Beachten Sie, dass Tags bereits erstellt werden müssen, um ausgewählt werden zu können.</block>
  <block id="af70f7acdeed8e2a702372f9bc85d960" category="list-text">Erstellen Sie Ihre VMs, indem Sie unter Select Storage die VM Storage Policy und kompatiblen Datenspeicher auswählen.</block>
  <block id="3db9849d9f30bfc6e05c4e0b36d28848" category="section-title">Migration von VMs von herkömmlichen Datastores auf VVols</block>
  <block id="7be42b7840cc093ce6d05a247df8e63d" category="paragraph">Die Migration von VMs von herkömmlichen Datastores in einen VVols Datastore ist nicht komplizierter als das Verschieben von VMs zwischen herkömmlichen Datastores. Wählen Sie einfach die VM(s) aus, dann Migrate aus der Liste der Aktionen und dann einen Migrationstyp von _change Storage only_ aus. Migrationsvorgänge werden für SAN VMFS zu VVols Migrationen mit vSphere 6.0 und höher verlagert, jedoch nicht von NAS VMDKs zu VVols.</block>
  <block id="58f2a764a13ec89d3c5ead5343e75637" category="section-title">Verwalten von VMs mithilfe von Richtlinien</block>
  <block id="4b746813f1085683be4f71c8affea233" category="paragraph">Um die Storage-Bereitstellung mit richtlinienbasiertem Management zu automatisieren, müssen wir</block>
  <block id="10b68fb9cbee818a044f91644423640c" category="list-text">Definieren Sie mit Storage Capability Profiles (SCPs) die Funktionen des Speichers (ONTAP-Knoten und FlexVol-Volume).</block>
  <block id="2e415dc7a6f21b4fd03f963858c9f680" category="list-text">Erstellen Sie VM-Storage-Richtlinien, die den definierten SCPs zugeordnet sind.</block>
  <block id="67c819108c4ef294ac28a51197046dfc" category="paragraph">NetApp hat die Funktionen und die Zuordnung ab VASA Provider 7.2 vereinfacht, wobei die Verbesserungen in späteren Versionen fortgeführt werden. Dieser Abschnitt konzentriert sich auf diesen neuen Ansatz. Frühere Versionen unterstützten eine größere Anzahl von Funktionen und erlaubten die individuelle Zuordnung zu Storage-Richtlinien, allerdings wird dieser Ansatz nicht mehr unterstützt.</block>
  <block id="0bd86c4955ab0bdce52b49644ce9396d" category="section-title">Funktionen des Storage-Funktionsprofils nach ONTAP-Tools</block>
  <block id="4a6af8a6e216dcc53a63f04143c13222" category="cell">*SCP-Fähigkeit*</block>
  <block id="048424cc97a54b673c837ee7d4c19de0" category="cell">*Fähigkeitswerte*</block>
  <block id="098e77f7c5d9e0c2ad5454817edf0767" category="cell">*Release-Unterstützung*</block>
  <block id="28f44037af103f0c930309365629f8ef" category="cell">*Hinweise*</block>
  <block id="d031377688b064b729a0cc60fb7fbbff" category="cell">*Komprimierung*</block>
  <block id="cc6aee5037bdc5b051433a266ec7d4a3" category="cell">Ja, Nein, Alle</block>
  <block id="7778bddb1c205e9f74d08bd30be0aca3" category="cell">Obligatorisch für AFF ab 7.2.</block>
  <block id="221baf8ad30299306e725e4fb395bf34" category="cell">*Deduplizierung*</block>
  <block id="6868f879256c802b106616a006671848" category="cell">M andatory für AFF in 7.2 und später.</block>
  <block id="504897d4a6b59afadffd3cf9e5d3ea85" category="cell">*Verschlüsselung*</block>
  <block id="95fddd53c531d6efe15e3ac7ace1d9eb" category="cell">7.2 und höher</block>
  <block id="b3870ec121aeafa2e7ca8cb3894c0e3a" category="cell">Wählt/erstellt ein verschlüsseltes FlexVol-Volume. ONTAP-Lizenz erforderlich.</block>
  <block id="e2dcc78cf70dac34550234c95443ac37" category="cell">*Maximale IOPS*</block>
  <block id="b78a981cc40fc4e66208bf5ee6d1a1eb" category="cell">&lt;number&gt;</block>
  <block id="130b32bc15d3fbe6d2e80ecda94135cc" category="cell">7.1 und später, aber Unterschiede</block>
  <block id="c0f84018aff4ff4a1cf4012a8b82e509" category="cell">Aufgeführt unter QoS Policy Group für 7.2 und höher. Siehe <block ref="8fdd8868d80d04eecf1b4189862f536c" category="inline-xref-macro-rx"></block> Finden Sie weitere Informationen.</block>
  <block id="993c56850c1da5de36e798b0c5a72513" category="cell">* Persönlichkeit*</block>
  <block id="8485fa08bf8c556b7b2275349d11eb05" category="cell">A FF, FAS</block>
  <block id="c0acd03e8ba6b951b8c6dff3e95b9560" category="cell">FAS beinhaltet auch andere nicht-All Flash FAS Systeme wie beispielsweise ONTAP Select. AFF umfasst ASA.</block>
  <block id="dce365633ffb688b7532470dfaf4e118" category="cell">*Protokoll*</block>
  <block id="7cff0b96b4a6467ea880f49dd365a81f" category="cell">NFS, NFS 4.1, iSCSI, FCP, NVMe/FC, Alle</block>
  <block id="be18de5e88c84ba55eeb5bc42cca4c0d" category="cell">7.1 und früher, 9.10 und höher</block>
  <block id="cfca0bc0c0b481555ba52be1f4e21da6" category="cell">7.2-9.8 ist effektiv "jede". Ebenfalls ab 9.10, wo NFS 4.1 und NVMe/FC in die ursprüngliche Liste aufgenommen wurden.</block>
  <block id="3dd301ae5682df79e8687ead5b6a3ddf" category="cell">*Speicherplatzreserve (Thin Provisioning)*</block>
  <block id="03868a080fbf4cdbc006da45e13cfad7" category="cell">Dünn, Dick, (Beliebig)</block>
  <block id="0a74dc1caf6ce891d5cbd3878cae2221" category="cell">Alle, aber Unterschiede</block>
  <block id="4b370bcc260aa96e343bbb24d3af2cc5" category="cell">7.1 und früher als Thin Provisioning bezeichnet, wodurch auch beliebige Werte zulässig sind. Genannt Space Reserve in 7.2. Alle Releases sind standardmäßig Thin.</block>
  <block id="40e2e283d064264dd51846580adb367d" category="cell">*Tiering-Richtlinie*</block>
  <block id="06b815f81a7e7aceb1489e3888fb9534" category="cell">Beliebig, Keine, Snapshot, Automatisch</block>
  <block id="00917abc35f0d57db6562a886997c4e8" category="cell">Verwendet für FabricPool - erfordert AFF oder ASA mit ONTAP 9.4 oder höher. Nur Snapshot ist empfohlen, wenn eine lokale S3 Lösung wie NetApp StorageGRID nicht verwendet wird.</block>
  <block id="ffb0d092d584c15937dfacd5be3c684a" category="section-title">Erstellen Von Storage-Funktionsprofilen</block>
  <block id="ad0b6bb809400347fc4806f18385015c" category="paragraph">NetApp VASA Provider verfügt über mehrere vordefinierte SCPs. Neue SCPs können manuell über die vCenter UI oder über die Automatisierung mit REST-APIs erstellt werden. Durch das Angeben von Funktionen in einem neuen Profil, das Klonen eines vorhandenen Profils oder das automatische Generieren von Profilen aus bestehenden herkömmlichen Datastores. Dies erfolgt über die Menüs unter ONTAP Tools. Verwenden Sie _Storage Capability Profiles_, um ein Profil zu erstellen oder zu klonen, und _Storage Mapping_, um ein Profil automatisch zu generieren.</block>
  <block id="62351556db3b0f6f13da226ca3e2df24" category="section-title">Storage-Funktionen für ONTAP Tools 9.10 und höher</block>
  <block id="14b49e2a6b56b1177a77be03a29dfc83" category="inline-image-macro">„Storage-Funktionen für ONTAP Tools 9.10 und höher“.300</block>
  <block id="2a53cbcd07a15d9f13f0cc630c7cc44d" category="paragraph"><block ref="2a53cbcd07a15d9f13f0cc630c7cc44d" category="inline-image-macro-rx" type="image"></block></block>
  <block id="12bb128f53dfaac0a251798e4b4e6954" category="paragraph"><block ref="12bb128f53dfaac0a251798e4b4e6954" category="inline-image-macro-rx" type="image"></block></block>
  <block id="4e6d46028a015a7f3860fc8fdf214b3c" category="paragraph"><block ref="4e6d46028a015a7f3860fc8fdf214b3c" category="inline-image-macro-rx" type="image"></block></block>
  <block id="64ca5ade59a3a7351134cf0db3a8e06a" category="paragraph"><block ref="64ca5ade59a3a7351134cf0db3a8e06a" category="inline-image-macro-rx" type="image"></block></block>
  <block id="2c8f597441425c22b4ae09f872f8bcc4" category="paragraph"><block ref="2c8f597441425c22b4ae09f872f8bcc4" category="inline-image-macro-rx" type="image"></block></block>
  <block id="fd59d4e26c472ef1e14cadb688283604" category="paragraph"><block ref="fd59d4e26c472ef1e14cadb688283604" category="inline-image-macro-rx" type="image"></block></block>
  <block id="049b6d7b78dd8ffebadb5a0be2261637" category="paragraph">*Erstellen von VVols Datastores*
Nachdem die erforderlichen SCPs erstellt wurden, können sie auch zur Erstellung des VVols-Datastores (und optional auch FlexVol Volumes für den Datastore) verwendet werden. Klicken Sie mit der rechten Maustaste auf den Host, das Cluster oder das Datacenter, auf dem Sie den VVols-Datastore erstellen möchten, und wählen Sie dann _ONTAP Tools_ &gt; _Provisioning Datastore_ aus. Wählen Sie einen oder mehrere SCPs aus, die vom Datastore unterstützt werden sollen, und wählen Sie dann aus vorhandenen FlexVol Volumes aus bzw. stellen Sie neue FlexVol Volumes für den Datastore bereit. Geben Sie schließlich das Standard-SCP für den Datastore an, das für VMs verwendet wird, für die kein durch die Richtlinie angegebenes SCP angegeben ist, sowie für Swap-VVols (diese erfordern keinen hochperformanten Storage).</block>
  <block id="a4332a81dbc3a9888be608dea640cd3d" category="section-title">Erstellen von VM-Storage-Richtlinien</block>
  <block id="b4f6f65777757af7630fae901718e51e" category="paragraph">Frühere Versionen sind ähnlich, aber wie in erwähnt <block ref="68072d20e775e4558feadbec7ba8f768" category="inline-xref-macro-rx"></block>, Ihre Optionen variieren.</block>
  <block id="52896487a6472798e1d6cbc9a95ec51e" category="section-title">Erstellen der Richtlinie für den VM-Storage mit ONTAP Tools VASA Provider 9.10</block>
  <block id="c0c5e719ad5439106e5a00588402f206" category="inline-image-macro">„Erstellung der VM Storage-Richtlinien mit ONTAP Tools VASA Provider 9.10„.300</block>
  <block id="378e30f2a60dc28c4afc9ae4f11a39e1" category="paragraph"><block ref="378e30f2a60dc28c4afc9ae4f11a39e1" category="inline-image-macro-rx" type="image"></block></block>
  <block id="95ac22478e141fde0d2878c71530a13a" category="section-title">Performance Management mit ONTAP Tools 9.10 und höher</block>
  <block id="c153012a9aa13e8ed8ce3f3e471a70b4" category="list-text">ONTAP Tools 9.10 verwendet einen eigenen Algorithmus für optimierte Platzierung, um ein neues vVol im besten FlexVol Volume in einem VVols Datastore zu platzieren. Die Platzierung basiert auf dem angegebenen SCP und übereinstimmenden FlexVol-Volumes. Dadurch wird sichergestellt, dass der Datastore und der zugrunde liegende Storage die angegebenen Performance-Anforderungen erfüllen können.</block>
  <block id="e65bde48b0a84fcc60d957de90c3a128" category="list-text">Wenn sich Funktionen für die Performance wie Min. Und Max. Ändern, muss die spezifische Konfiguration entsprechend verändert werden.</block>
  <block id="c43f1c590aab42b849ee038e863a567d" category="list-text">*Min und Max IOPS* können in einem SCP angegeben und in einer VM Policy verwendet werden.</block>
  <block id="404e0fefd61512a9f144586382a62641" category="list-text">Wenn Sie die IOPS im SCP ändern, wird die QoS auf den VVols erst geändert, wenn die VM-Richtlinie bearbeitet und dann auf die VMs angewendet wird, die sie verwenden (siehe <block ref="2358041c73953300d39d98366deee80d" category="inline-xref-macro-rx"></block>). Oder erstellen Sie ein neues SCP mit den gewünschten IOPS und ändern Sie die Richtlinie, um es zu verwenden (und erneut auf VMs anzuwenden). Im Allgemeinen wird empfohlen, einfach separate SCPs und VM-Storage-Richtlinien für verschiedene Service-Ebenen zu definieren und einfach die VM-Storage-Richtlinie für die VM zu ändern.</block>
  <block id="0e05d638b7a9c7a5c32e6c22679eeb82" category="list-text">AFF- und FAS-Persönlichkeiten haben unterschiedliche IOPS-Einstellungen. Sowohl Min. Als auch Max. Sind auf AFF verfügbar. Nicht-All Flash FAS Systeme können jedoch nur die IOPS-Maximaleinstellungen verwenden.</block>
  <block id="97659fa3c3a5aaf088f6020a38faf086" category="list-text">In einigen Fällen muss ein vVol nach einer Richtlinienänderung (entweder manuell oder automatisch durch VASA Provider und ONTAP) migriert werden:</block>
  <block id="81c975fc79a19102fa04ad9c49f7538b" category="list-text">Einige Änderungen erfordern keine Migration (wie beispielsweise eine Änderung der maximalen IOPS, die sofort auf die VM angewendet werden kann, wie oben beschrieben).</block>
  <block id="a426b56f86a12c200f3484efb990e81f" category="list-text">Wenn die Richtlinienänderung nicht vom aktuellen FlexVol Volume unterstützt werden kann, in dem das vVol gespeichert ist (beispielsweise unterstützt die Plattform die angeforderte Verschlüsselungs- oder Tiering-Richtlinie nicht), müssen Sie die VM manuell in vCenter migrieren.</block>
  <block id="7de504873f20af5c8ce6dadd2f79b29a" category="list-text">ONTAP-Tools erstellen individuelle QoS-Richtlinien ohne gemeinsame Nutzung mit derzeit unterstützten Versionen von ONTAP. Daher erhält jede einzelne VMDK eine eigene IOPS-Zuweisung.</block>
  <block id="63a33ddbd2c3ccd2daac06020248049f" category="section-title">Erneutes Anwenden der VM-Speicherrichtlinie</block>
  <block id="d0f5986c3378364e5cb0eda0e98fd0fd" category="inline-image-macro">„VM-Speicherrichtlinie neu anwendet.300</block>
  <block id="2a8c2026166e178134c7e19c2f4a2c15" category="paragraph"><block ref="2a8c2026166e178134c7e19c2f4a2c15" category="inline-image-macro-rx" type="image"></block></block>
  <block id="50b6e1c7f2e81842d993d8ce88acc979" category="summary">ONTAP unterstützt alle maßgeblichen Storage-Protokolle für die Virtualisierung, beispielsweise iSCSI, Fibre Channel (FC), Fibre Channel over Ethernet (FCoE) oder Non-Volatile Memory Express over Fibre Channel (NVMe/FC) für SAN-Umgebungen sowie NFS (v3 und v4.1) und SMB oder S3 für Gastverbindungen. Die Kunden können die für ihre Umgebung am besten geeigneten Protokolle auswählen und sie nach Bedarf in einem einzigen System kombinieren.</block>
  <block id="ee8cc65cae83009c275cd4748f4c58ae" category="doc">Virtualisierungstools für ONTAP</block>
  <block id="0894f8dfb7dd501a78dc2416b7b90c2a" category="paragraph">NetApp bietet verschiedene Standalone-Softwaretools, die gemeinsam mit ONTAP und vSphere für das Management Ihrer virtualisierten Umgebung verwendet werden können.</block>
  <block id="236e1a1bf6abf86968baf738af47b78c" category="paragraph">Die folgenden Tools sind ohne Aufpreis in der ONTAP Lizenz enthalten. In Abbildung 1 sehen Sie eine Darstellung, wie diese Tools in Ihrer vSphere Umgebung zusammenarbeiten.</block>
  <block id="0b266371a133b176a2ee883ccf72b46c" category="list-text">*VCenter UI Extensions.* die UI-Erweiterungen der ONTAP-Tools vereinfachen die Arbeit von Betriebsteams und vCenter Administratoren durch die Verwendung benutzerfreundlicher, kontextabhängiger Menüs zum Managen von Hosts und Storage, Informations-Portlets und nativen Alarmfunktionen direkt in der vCenter-Benutzeroberfläche für optimierte Workflows.</block>
  <block id="e08c666b5127e745f8aacbacfce37dcf" category="list-text">*VASA Provider für ONTAP.* der VASA Provider für ONTAP unterstützt das VMware vStorage APIs for Storage Awareness (VASA) Framework. Er wird im Rahmen von ONTAP Tools für VMware vSphere als eine einzelne virtuelle Appliance zur einfachen Implementierung bereitgestellt. VASA Provider verbindet vCenter Server mit ONTAP und erleichtert so die Bereitstellung und das Monitoring von VM-Storage. Es aktiviert die Unterstützung und das Management von Storage-Funktionsprofilen für VMware Virtual Volumes (VVols) und die VVols Performance für einzelne VMs sowie Alarme für die Monitoring-Kapazität und -Compliance mit den Profilen.</block>
  <block id="1c7bb8764fc089dda8481678ad1ea0b5" category="list-text">*Storage Replication Adapter.* SRA wird zusammen mit VMware Site Recovery Manager (SRM) zum Management der Datenreplizierung zwischen Produktions- und Disaster-Recovery-Standorten sowie zum unterbrechungsfreien Testen der DR-Replikate verwendet. Diese Software hilft bei der Automatisierung der Erkennungs-, Recovery- und Sicherungsaufgaben. Sie enthält sowohl eine SRA Server-Appliance als auch SRA Adapter für den Windows SRM Server und eine SRM Appliance.</block>
  <block id="a1c13ec555198266776a3a7b904810dd" category="paragraph">In der folgenden Abbildung sind die ONTAP Tools für vSphere dargestellt.</block>
  <block id="dd55e255500b9ec15305dc69dfa0e15b" category="section-title">NFS-Plug-in für VMware VAAI</block>
  <block id="4fbfd4c228f22ff3f2841908e853d895" category="paragraph">Das NetApp NFS Plug-in für VMware VAAI ist ein Plug-in für ESXi Hosts, mit dem diese VAAI Funktionen mit NFS-Datastores unter ONTAP verwenden können. Es unterstützt den Copy-Offload für Klonvorgänge, die Speicherplatzreservierung für Thick Virtual Disk Files und Snapshot Offload. Die Verlagerung von Kopiervorgängen in den Storage erfolgt nicht unbedingt schneller, sorgt aber dafür, dass die Anforderungen an die Netzwerkbandbreite reduziert werden und Host-Ressourcen wie CPU-Zyklen, Puffer und Warteschlangen verlagert werden. Sie können das Plug-in mithilfe von ONTAP Tools für VMware vSphere auf ESXi Hosts oder, sofern unterstützt, vSphere Lifecycle Manager (vLCM) installieren.</block>
  <block id="00cdd3f66e02711548cf72d579ec0df8" category="summary">Mit SnapCenter können Sie Backup-Richtlinien erstellen, die auf mehrere Jobs angewendet werden können. In diesen Richtlinien können ein Zeitplan, die Aufbewahrung, die Replizierung und andere Funktionen definiert werden. Damit ist es weiterhin möglich, optional VM-konsistente Snapshots auszuwählen und dadurch die Fähigkeit des Hypervisors auszuschöpfen, das I/O vor dem Erstellen eines VMware Snapshots stillzulegen.</block>
  <block id="f1d1eddd608fe98b6fa2bc3436ce81d0" category="doc">Richtlinienbasiertes Storage-Management und VVols</block>
  <block id="2bdce8d6813e7a3d93783119529cf146" category="paragraph">VMware vSphere APIs for Storage Awareness (VASA) erleichtern einem Storage-Administrator die Konfiguration von Datastores mit klar definierten Funktionen. Der VM-Administrator kann sie zudem im Bedarfsfall jederzeit nutzen, um VMs bereitzustellen, ohne dass eine Interaktion stattfinden muss.</block>
  <block id="871db14a81b3510676400538c6f07073" category="paragraph">Eine genauere Betrachtung dieses Ansatzes lohnt sich für Sie, wenn Sie feststellen möchten, wie er Ihre Storage-Virtualisierungsvorgänge optimieren und Ihnen viele banale Arbeiten ersparen kann.</block>
  <block id="ad0bd815b4fb6f01acc94f160af3dca7" category="paragraph">Vor VASA konnten VM-Administratoren VM-Storage-Richtlinien definieren, mussten dann aber gemeinsam mit dem Storage-Administrator geeignete Datastores ermitteln – oft anhand der Dokumentation oder von Namenskonventionen. Mit VASA kann der Storage-Administrator eine Reihe von Storage-Funktionen definieren, darunter Performance, Tiering, Verschlüsselung und Replizierung. Ein Satz von Funktionen für ein Volume oder eine Gruppe von Volumes wird als Storage-Funktionsprofil (Storage Capability Profile, SCP) bezeichnet.</block>
  <block id="44d75bdf4feecabe2de10427870ae623" category="paragraph">Das SCP unterstützt die minimale und/oder maximale QoS für die Daten-VVols einer VM. Minimale QoS wird nur auf AFF Systemen unterstützt. ONTAP Tools für VMware vSphere umfassen ein Dashboard, in dem die granulare VM-Performance und logische Kapazität für VVols auf ONTAP Systemen angezeigt werden.</block>
  <block id="4240fa68f6d9919e6d039c4038175025" category="paragraph">In der folgenden Abbildung sind die ONTAP Tools für das Dashboard von VMware vSphere 9.8 VVols dargestellt.</block>
  <block id="50e0e66922b3b12510a128829d1fdc70" category="paragraph">Nachdem ein Storage-Funktionsprofil definiert wurde, können damit anhand der Storage-Richtlinie, in der die entsprechenden Anforderungen angegeben sind, VMs bereitgestellt werden. Durch die Zuordnung zwischen der VM-Storage-Richtlinie und dem Datastore-Storage-Funktionsprofil kann in vCenter eine Liste kompatibler Datastores zur Auswahl angezeigt werden. Dieser Ansatz wird als richtlinienbasiertes Storage-Management bezeichnet.</block>
  <block id="794b6e2fc393d40a552fc58975920cb0" category="paragraph">VASA stellt die Technologie bereit, mit der der Storage abgefragt und eine Reihe von Storage-Funktionen an vCenter zurückgegeben werden können. VASA Provider stellen die Übersetzung zwischen den Storage-System-APIs und -Konstrukten einerseits und den von vCenter erkannten VMware APIs bereit. NetApp VASA Provider für ONTAP wird als Teil der ONTAP Tools für die VMware vSphere Appliance VM angeboten. Das vCenter Plug-in bietet die Schnittstelle zum Bereitstellen und Managen von vVol Datastores und bietet die Möglichkeit, Storage-Funktionsprofile zu definieren.</block>
  <block id="dad160104969c39365dd2cd4c98dd300" category="inline-link">TR-4400</block>
  <block id="bd490c925219bcb312338a370589bf70" category="list-text">Ein vVol Datastore kann aus mehreren FlexVol Volumes auf mehreren Cluster-Nodes bestehen. Den einfachsten Ansatz stellt ein einzelner Datastore dar, selbst wenn die Volumes unterschiedliche Funktionen haben. SPBM stellt sicher, dass ein kompatibles Volume für die VM verwendet wird. Die Volumes müssen allerdings alle einer einzigen ONTAP SVM angehören und es muss über ein einziges Protokoll auf sie zugegriffen werden. Für jedes Protokoll reicht eine logische Schnittstelle pro Node aus. Es empfiehlt sich nicht, mehrere ONTAP Versionen in einem einzelnen vVol Datastore zu nutzen, da sich die Storage-Funktionen in verschiedenen Versionen unter Umständen unterscheiden.</block>
  <block id="0cc956d6da91fefae9dad9b2c8c9519b" category="list-text">Verwenden Sie die ONTAP Tools für VMware vSphere Plug-in, um vVol Datastores zu erstellen und zu managen. Neben dem Management des Datastores und dessen Profil erstellt es bei Bedarf automatisch einen Protokollendpunkt für den Zugriff auf die VVols. Falls LUNs verwendet werden, werden LUN-Protokollendpunkte (PES) mit LUN-IDs ab 300 zugeordnet. Vergewissern Sie sich, dass die erweiterte Systemeinstellung des ESXi-Hosts aktiviert ist<block ref="ce913b4e625461a33f54f9e4ac38c2d7" prefix=" " category="inline-code"></block> Ermöglicht eine LUN-ID-Nummer, die über 300 liegt (Standard ist 1,024). Wählen Sie diesen Schritt aus: ESXi Host in vCenter, dann Registerkarte „Configure“ und suchen Sie<block ref="ce913b4e625461a33f54f9e4ac38c2d7" prefix=" " category="inline-code"></block> In der Liste der erweiterten Systemeinstellungen.</block>
  <block id="fac85f355a1eb47127ef1b94e7603924" category="list-text">Installieren oder migrieren Sie VASA Provider, vCenter Server (Appliance oder Windows basierte Version) oder ONTAP Tools für VMware vSphere selbst nicht auf einem VVols Datastore, da diese dann voneinander abhängen. Im Falle eines Stromausfalls oder einer anderen Störung im Datacenter könnten Sie sie dann nur begrenzt managen.</block>
  <block id="dd76252ea7a50def2a98f218cb1505b4" category="inline-link">KB-Artikel</block>
  <block id="15ef75dd3d20b8278c16cd01a708e13e" category="list-text">Sichern Sie die VASA Provider VM in regelmäßigen Abständen. Erstellen Sie mindestens stündlich Snapshots des herkömmlichen Datastores, der VASA Provider umfasst. Weitere Informationen zum Sichern und Wiederherstellen von VASA Provider finden Sie in diesem Abschnitt<block ref="9d642870dca1748c69f7aa0b6fb95a89" category="inline-link-rx"></block>.</block>
  <block id="2d2d2e356f9ab117b7d2a58d42cc5988" category="paragraph">In der folgenden Abbildung werden die VVols Komponenten angezeigt.</block>
  <block id="d3ba101543fc97ba1dd9272c87bf65da" category="doc">Virtual Volumes (VVols) und richtlinienbasiertes Storage-Management (SPBM)</block>
  <block id="a42f13867ea459821488608dc4d1b7c4" category="paragraph">NetApp war schon früh als Design-Partner von VMware an der Entwicklung von vSphere Virtual Volumes (VVols) beteiligt und stellte architekturspezifischen Input und frühzeitig Unterstützung für VVols und VMware vSphere APIs for Storage Awareness (VASA) bereit. Durch diesen Ansatz wurde VMFS nicht nur granulares VM-Storage-Management ermöglicht, sondern auch die Automatisierung der Storage-Bereitstellung durch Storage Policy Based Management (SPBM) unterstützt.</block>
  <block id="ce09fc365ce48a9d89c0fdb5222c3909" category="paragraph">SPBM bietet ein Framework, das als Abstraktionsebene zwischen den für Ihre Virtualisierungsumgebung verfügbaren Storage-Services und den über Richtlinien bereitgestellten Storage-Elementen dient. Storage-Architekten können mit diesem Ansatz Storage-Pools mit unterschiedlichen Funktionen entwerfen, die von VM-Administratoren einfach genutzt werden können. Administratoren können die Workload-Anforderungen von Virtual Machines an die bereitgestellten Storage Pools anpassen, wodurch verschiedene Einstellungen pro VM oder Virtual Disk granular kontrolliert werden können.</block>
  <block id="2902acda09b9786460c2dd928a2d8f1a" category="paragraph">Bei VVols ist ONTAP eine der führenden Lösungen in der Storage-Branche, da es Hunderttausende VVols in einem einzigen Cluster unterstützt. Anbieter von Enterprise-Arrays und kleineren Flash-Arrays hingegen unterstützen gerade einmal mehrere Tausend VVols pro Array. Zudem bringt NetApp mit neuen Funktionen bei der Unterstützung von VVols 3.0 die Weiterentwicklung des granularen VM-Managements voran.</block>
  <block id="fda9c21a4aefd4a1f42ad0b195e6ef0a" category="inline-link">TR-4400: VMware vSphere Virtual Volumes with ONTAP</block>
  <block id="61823c0f572643e0ce2e4edcb74cae51" category="paragraph">Mit Snapshots können Sie ohne Auswirkungen auf die Performance schnell Kopien Ihrer VMs oder Datastores erstellen und diese dann zur längerfristigen externen Datensicherung mit SnapMirror an ein sekundäres System senden. Durch diesen Ansatz werden der Storage-Platzbedarf und die Netzwerkbandbreite minimiert, da nur geänderte Informationen gespeichert werden.</block>
  <block id="0aef2da721f0ab14676f8d6ffb9e7e8c" category="inline-link">Empfehlenswert</block>
  <block id="022e604112aa3d1870f0da5e7806f48b" category="paragraph">Mit SnapCenter können Sie Backup-Richtlinien erstellen, die auf mehrere Jobs angewendet werden können. In diesen Richtlinien können ein Zeitplan, die Aufbewahrung, die Replizierung und andere Funktionen definiert werden. Damit ist es weiterhin möglich, optional VM-konsistente Snapshots auszuwählen und dadurch die Fähigkeit des Hypervisors auszuschöpfen, das I/O vor dem Erstellen eines VMware Snapshots stillzulegen. Aufgrund der Performance-Auswirkungen von VMware Snapshots werden diese jedoch im Allgemeinen nicht empfohlen, es sei denn, Sie müssen das Gast-Betriebssystem stilllegen. Verwenden Sie stattdessen Snapshots für die allgemeine Sicherung und Applikationstools wie SnapCenter Plug-ins, um transaktionsorientierte Daten – beispielsweise SQL Server oder Oracle Daten – zu sichern. Diese Snapshots unterscheiden sich von den VMware (Konsistenz-)Snapshots und sind für längerfristigen Schutz geeignet.  VMware Snapshots sind nur<block ref="75f5ad475d959e2ecb55267e1a9a54f1" category="inline-link-rx"></block> Für den kurzfristigen Einsatz aufgrund von Performance und anderen Auswirkungen.</block>
  <block id="26aae91a7d92b32a60b1fbc86c29ee81" category="paragraph">Diese Plug-ins bieten erweiterte Funktionen zur Sicherung von Datenbanken in physischen und virtuellen Umgebungen. Bei vSphere können Sie sie zur Sicherung von SQL Server oder Oracle Datenbanken heranziehen, in denen die Daten in RDM-LUNs, direkt mit dem Gastbetriebssystem verbundenen iSCSI-LUNs oder VMDK-Dateien in VMFS oder NFS-Datastores gespeichert werden. Mit den Plug-ins können unterschiedliche Typen von Datenbank-Backups angegeben, Online- oder Offline-Backups unterstützt und neben Protokolldateien auch Datenbankdateien gesichert werden. Zusätzlich zum Backup und Recovery unterstützen die Plug-ins auch das Klonen von Datenbanken zu Entwicklungs- oder Testzwecken.</block>
  <block id="1556ba0bf6ea4783cda0ff40e96f0265" category="paragraph">Die folgende Abbildung zeigt ein Beispiel für die Implementierung von SnapCenter.</block>
  <block id="6c1bef29e8dc34a00e17b06c221d5efe" category="paragraph">Falls Sie erweiterte Disaster-Recovery-Funktionen nutzen möchten, sollten Sie in Betracht ziehen, NetApp SRA für ONTAP mit VMware Site Recovery Manager zu kombinieren. Dadurch wird die Replizierung von Datastores an einen DR-Standort unterstützt. Darüber hinaus werden unterbrechungsfreie Tests in der DR-Umgebung ermöglicht, indem die replizierten Datastores geklont werden. Das Recovery nach einem Ausfall und die erneute Sicherung der Produktion nach Behebung des Ausfalls wurden durch die in SRA integrierte Automatisierung ebenfalls vereinfacht.</block>
  <block id="650062685b0df8284f1b87f62fa3f9f3" category="inline-link">TR-4128</block>
  <block id="92515b597b8c0784d6000b89ee47c5fd" category="doc">Unified Storage</block>
  <block id="d6154a0901f9ca6bb5d8fb15c113581e" category="paragraph"><block ref="d6154a0901f9ca6bb5d8fb15c113581e" category="inline-image-macro-rx" type="image"></block></block>
  <block id="e9e0f5f2c513c7138ea70b4c929d6fef" category="inline-link">Storage-Virtualisierung</block>
  <block id="400cc516a4a40acd605aef2cfd4ba4c0" category="doc">VMware Storage Distributed Resource Scheduler</block>
  <block id="6e210af5c49ef2b81c06b57f989ed5c6" category="paragraph">VMware Storage Distributed Resource Scheduler (SDRS) ist eine Funktion von vSphere, die VMs auf Storage basierend auf der aktuellen I/O-Latenz und der Speicherplatznutzung platziert.</block>
  <block id="e7649bfbacc438bf8e20ba3564db4bdd" category="paragraph">Danach werden die VM oder VMDKs unterbrechungsfrei zwischen den Datastores in einem Datastore-Cluster (auch Pod genannt) verschoben und es wird der beste Datastore ausgewählt, in dem die VM oder die VMDKs im Datastore-Cluster platziert werden sollen. Ein Datastore-Cluster ist eine Sammlung ähnlicher Datastores, die aus Sicht des vSphere Administrators in einer einzigen Verbrauchseinheit aggregiert werden.</block>
  <block id="b028cce2007c32457c41f20d2954c0c2" category="paragraph">Weitere ONTAP Best Practices für SDRS:</block>
  <block id="d11eff5be5178b7ba95b06270611ffc8" category="list-text">Alle Datastores im Cluster sollten denselben Storage-Typ (beispielsweise SAS, SATA oder SSD) verwenden. Zudem sollte es sich bei allen entweder um VMFS oder NFS-Datastores handeln und sie sollten dieselben Replizierungs- und Sicherungseinstellungen aufweisen.</block>
  <block id="76db7ce67615bbc16da685df79ed0aa9" category="list-text">Sie sollten SDRS eventuell im Standardmodus (manuell) verwenden. Mit diesem Ansatz können Sie die Empfehlungen prüfen und entscheiden, ob Sie sie anwenden oder nicht. Beachten Sie diese Auswirkungen von VMDK Migrationen:</block>
  <block id="97f4ed95cb52bb29629002a25cb5039f" category="list-text">Wenn VMDKs VON SDRS zwischen Datastores verschoben werden, gehen sämtliche Speicherersparnisse durch ONTAP Klone oder Deduplizierung verloren. Sie können die Deduplizierung erneut ausführen, um diese Einsparungen zurückzugewinnen.</block>
  <block id="38f492286c3edf236c76e993ce0f0bf2" category="list-text">Nachdem SDRS die VMDKs verschoben hat, empfiehlt NetApp, die Snapshots im Quell-Datastore neu zu erstellen, da der Speicherplatz andernfalls von der verschobenen VM gesperrt wird.</block>
  <block id="09a99cbc4fb85d10c7af970e79032a05" category="list-text">Die Verschiebung von VMDKs zwischen Datastores im selben Aggregat bietet nur wenige Vorteile. Zudem sind andere Workloads, die das Aggregat möglicherweise teilen, FÜR SDRS nicht sichtbar.</block>
  <block id="09b616c445ab7efa8240062532e526e4" category="doc">VMware vSphere mit ONTAP –</block>
  <block id="48e29772cebbf1133d022577e278d5c1" category="admonition">Diese Dokumentation ersetzt zuvor veröffentlichte technische Berichte _TR-4597: VMware vSphere for ONTAP_</block>
  <block id="bfa7e7fc7c6bf4cc4e4c5c74b09763eb" category="paragraph">Andere Dokumente wie Leitfäden und Kompatibilitätslisten werden durch Best Practices ergänzt. Sie werden basierend auf Labortests und umfassenden praktischen Erfahrungen der NetApp Ingenieure und Kunden entwickelt. Es handelt sich hierbei unter Umständen nicht nur um die einzigen unterstützten Praktiken, die in jeder Umgebung funktionieren. Im Allgemeinen sind sie aber die einfachsten Lösungen, die die Anforderungen der meisten Kunden erfüllen.</block>
  <block id="9ed4a475dd3705157a9fed8811f63ce2" category="inline-link">NetApp Interoperabilitäts-Matrix-Tool</block>
  <block id="e08175c6fa64cba68719dea85ebd9d34" category="inline-link">VMware Compatibility Guide</block>
  <block id="c7e9d53e7f1a53451fa25cb6340fbf6f" category="paragraph">Der Schwerpunkt dieses Dokuments liegt auf den Funktionen der neuesten Versionen von ONTAP (9.x), die unter vSphere 7.0 oder höher ausgeführt werden. Siehe<block ref="37aa2814221d042697a27bc81e148f64" category="inline-link-rx"></block> Und<block ref="e1593898142109fc8cd8025356d3f312" category="inline-link-rx"></block> Finden Sie Details zu bestimmten Versionen.</block>
  <block id="74587fa4ad90f8713c0446529a3670e0" category="section-title">Warum ONTAP für vSphere?</block>
  <block id="a110ecc6d4ce7014019c560e5fedf572" category="paragraph">Zehntausende Kunden haben sich für ONTAP als Storage-Lösung für vSphere entschieden. Dafür gibt es viele Gründe, beispielsweise ein Unified-Storage-System, das sowohl SAN- als auch NAS-Protokolle unterstützt, robuste Datensicherungsfunktionen mittels platzsparender Snapshots und eine Fülle von Tools, die Sie beim Management von Applikationsdaten unterstützen. Wenn Sie ein Storage-System getrennt vom Hypervisor verwenden, können Sie viele Funktionen verlagern und Ihre Investitionen in vSphere Host-Systeme optimal nutzen. Hierdurch wird sichergestellt, dass Ihre Host-Ressourcen schwerpunktmäßig für Applikations-Workloads verwendet werden. Darüber hinaus werden zufällige Auswirkungen auf die Performance von Applikationen aufgrund des Storage-Betriebs vermieden.</block>
  <block id="0f50acddbdbb9aa9712a471f2276e655" category="paragraph">Die Kombination von ONTAP und vSphere ermöglicht Kosteneinsparungen für Host-Hardware und VMware Software. Schützen Sie Ihre Daten außerdem zu geringeren Kosten mit konstant hoher Performance. Da virtualisierte Workloads mobil sind, können Sie mit Storage vMotion verschiedene Ansätze nutzen, um VMs auf VMFS-, NFS- oder VVols-Datastores zu verschieben. Und das alles auf ein und demselben Storage-System.</block>
  <block id="90977175b761542615a11f2b96cc4cbd" category="paragraph">Im Folgenden sind wichtige Faktoren aufgeführt, die heutzutage von Kunden wertvoll sind:</block>
  <block id="0ce37df1cd9b66a2abde8244f17fc051" category="list-text">*Auf virtuellen Volumes und richtlinienbasiertem Storage-Management.* NetApp war bereits früh als Designpartner von VMware an der Entwicklung von vSphere Virtual Volumes (VVols) beteiligt und stellte architekturspezifischen Input und frühzeitig Unterstützung für VVols und VMware vSphere APIs for Storage Awareness (VASA) bereit. Durch diesen Ansatz wurde nicht nur das granulare VM-Storage-Management in VMFS integriert, sondern auch die Automatisierung der Storage-Bereitstellung durch richtlinienbasiertes Management unterstützt. Storage-Architekten können mit diesem Ansatz Storage-Pools mit unterschiedlichen Funktionen entwerfen, die von VM-Administratoren einfach genutzt werden können. ONTAP ist einer der führenden Anbieter von vVol Storage-Lösungen in der Storage-Branche, da es Hunderttausende VVols in einem einzigen Cluster unterstützt. Anbieter von Enterprise-Arrays und kleineren Flash-Arrays hingegen unterstützen gerade einmal mehrere Tausend VVols pro Array. Zudem bringt NetApp mit neuen Funktionen bei der Unterstützung von VVols 3.0 die Weiterentwicklung des granularen VM-Managements voran.</block>
  <block id="9eeea06a6a7f860f801343c04af5d7d2" category="list-text">*Storage-Effizienz.* Obwohl NetApp als erster Anbieter Deduplizierung für Produktions-Workloads bereitgestellt hat, war diese Innovation weder die erste noch die letzte in diesem Bereich. Es begann mit Snapshots, einem platzsparenden Datensicherungsmechanismus ohne Auswirkungen auf die Performance, sowie mit FlexClone Technologie, bei der sofort Lese-/Schreibkopien von VMs für die Produktion und die Nutzung von Backups erstellt werden können. Danach stellte NetApp Inline-Funktionen bereit, darunter Deduplizierung, Komprimierung und Zero-Block-Deduplizierung, mit denen sich der Storage kostspieliger SSDs maximal ausschöpfen lässt. Zuletzt wurde ONTAP um die Möglichkeit erweitert, kleinere I/O-Vorgänge und Dateien durch Data-Compaction in einen Festplattenblock zu packen. Dank der Kombination dieser Funktionen verzeichnen Kunden Einsparungen im Verhältnis von bis zu 5:1 für VSI und von bis zu 30:1 für VDI.</block>
  <block id="bb73602f3344c326a7b63c94db24362a" category="list-text">*Hybrid Cloud.* ob in der Private Cloud vor Ort, in einer Public-Cloud-Infrastruktur oder in einer Hybrid Cloud, die das Beste der beiden Lösungen vereint – ONTAP Lösungen helfen Ihnen, Ihre Data Fabric zur Optimierung und zum Optimieren des Datenmanagements aufzubauen. Den Anfang machen hochperformante All-Flash-Systeme, die dann für die Datensicherung und das Cloud-Computing mit Festplatten- oder Cloud-Storage-Systemen gekoppelt werden. Zur Kostenoptimierung und Vermeidung einer Anbieterbindung stehen hierbei Azure, AWS, IBM oder Google Clouds zur Auswahl. Bei Bedarf kann die erweiterte Unterstützung für OpenStack und Containertechnologien genutzt werden. NetApp bietet darüber hinaus Cloud-basiertes Tiering und Archivierungstools (SnapMirror Cloud, Cloud Backup Service und Cloud Sync) sowie Storage-Systeme (FabricPool) für ONTAP, um die Betriebskosten zu senken und die große Reichweite der Cloud auszuschöpfen.</block>
  <block id="7a61ec7965eebc0b5486cb78f096c770" category="list-text">* Und mehr.* Nutzen Sie die extreme Performance von NetApp AFF A-Series Arrays, um Ihre virtualisierte Infrastruktur zu beschleunigen und gleichzeitig die Kosten im Griff zu haben. Mit horizontal skalierbaren ONTAP Clustern profitieren Sie bei der Wartung, bei Upgrades und selbst beim kompletten Ersatz Ihres Storage-Systems von einem durchgängig unterbrechungsfreien Betrieb. Daten im Ruhezustand werden mit NetApp Verschlüsselungsfunktionen ohne zusätzliche Kosten geschützt. Durch fein abgestimmte Quality-of-Service- Funktionen stellen Sie sicher, dass die Performance den geschäftlichen Service-Levels entspricht. Sie alle sind Bestandteil des umfangreichen Funktionsbereichs, das in ONTAP, der branchenführenden Software für das Enterprise-Datenmanagement, enthalten ist.</block>
  <block id="6104c8ef7be4222a76596ab6a22bb422" category="doc">Empfohlene ESXi Host-Einstellungen und andere ONTAP Einstellungen</block>
  <block id="678582eb0cbbe0e063e618d230b63223" category="cell">*Hosteinstellung*</block>
  <block id="dfe49b73bd7e747701b49f9ed21ea2d6" category="cell">*Von NetApp empfohlener Wert*</block>
  <block id="edceae57d92287f4f6200f94d3259cd0" category="cell">*Neustart Erforderlich*</block>
  <block id="dc3a6955a23583876d020db0f6b80d4d" category="cell">*ESXi Advanced Configuration*</block>
  <block id="73663d0d00956f0632e4ae75d811d53f" category="cell">VMFS3.HardwareAcceleratLocking</block>
  <block id="59b1aceaef0203fdc32e11df232b16e6" category="cell">Standard beibehalten (1)</block>
  <block id="bafd7322c6e97d25b6299b5d6fe8920b" category="cell">Nein</block>
  <block id="1b50110aedae4d8d2043a4eb9beb20af" category="cell">VMFS3.EnableBlockDelete</block>
  <block id="5d1b81f1885499e392018ebd7124ad37" category="inline-link-macro">VMware KB 2007427</block>
  <block id="316084648bbafe451240702bd19c6ee9" category="cell">VMFS3.EnableVMFS6Entmappen</block>
  <block id="befd90cd01403237f6fdf296822efbee" category="inline-link-macro">VMware vSphere APIs – Array-Integration (VAAI)</block>
  <block id="e385d2442208cc1e23ca841b1f217380" category="cell">Standard beibehalten (1)
Weitere Informationen finden Sie unter <block ref="d124e203790e5f214a195f69cd068c6d" category="inline-link-macro-rx"></block></block>
  <block id="571cc1d48c6d1ff3cf7e3a54cb035753" category="cell">*NFS-Einstellungen*</block>
  <block id="c3b49a515dd0046685e06092642aa139" category="cell">NET.TcpipHeapSize</block>
  <block id="875f6b98e20edfecb56f9013d00987f6" category="cell">VSphere 6.0 oder höher: Auf 32 einstellen.
Alle anderen NFS-Konfigurationen: Auf 30 einstellen</block>
  <block id="93cba07454f06a4a960172bbd6e2a435" category="cell">Ja.</block>
  <block id="504e96cc79e7efe30a8271cea152d9d1" category="cell">NET.TcpipHeapMax</block>
  <block id="5f1e7b6431061565550e292d05c73588" category="cell">MaxVolumes: NFS</block>
  <block id="3058676a1cf088aa4a73312ac36f2a58" category="cell">NFS41.MaxVolumes</block>
  <block id="62c07f94c64184ee5d530e4c0917093f" category="cell">VSphere 6.0 oder höher: Auf 256 einstellen.</block>
  <block id="e18600af98c17fae10fdba0dc89979ed" category="cell">NFS.MaxQueueDepth^1^</block>
  <block id="be7707fa525d0124489c9fe3072f78de" category="cell">VSphere 6.0 oder höher: Auf 128 einstellen</block>
  <block id="18e1fb6924459470760771b20ab0c379" category="cell">NFS.HeartbeatMaxFailures</block>
  <block id="138989cbe5dd3a4997662e0f65c17878" category="cell">Alle NFS-Konfigurationen: Auf 10 einstellen</block>
  <block id="1aa26eb281359ba3b9a9a4b83a09ed46" category="cell">HeartbeatFrequency NFS.HeartbeatFrequency</block>
  <block id="7ea6e8664542e3beb1aea10425efb4db" category="cell">Alle NFS-Konfigurationen: Auf 12 einstellen</block>
  <block id="eb4182715f8cde98563dc59ea36461d3" category="cell">HeartbeatTimeout NFS.HeartbeatTimeout</block>
  <block id="45aa5481fa55802a29a96aefd2f5dab1" category="cell">Alle NFS-Konfigurationen: Auf 5 einstellen.</block>
  <block id="54a08b15243d9078d3b5a47f8242da6f" category="cell">SunRPC.MaxConnPerIP</block>
  <block id="9a9ccbecb3f315797ff8e709d85a0e18" category="cell">VSphere 7.0 oder höher: Auf 128 einstellen.</block>
  <block id="c0b3fd5512c2093847f753f398290f30" category="cell">*FC/FCoE-Einstellungen*</block>
  <block id="2e556ad678160413b32dcca55c7d1f5f" category="cell">Pfadauswahl-Richtlinie</block>
  <block id="fb62337a3ef32f4701b639339a4ceb33" category="cell">Wenn FC-Pfade mit ALUA verwendet werden: Auf RR (Round Robin) einstellen. Alle anderen Konfigurationen: Auf FIXED einstellen.
Wenn Sie diesen Wert auf RR einstellen, ist für alle aktiven/optimierten Pfade ein besserer Lastausgleich möglich.
Der Wert FIXED wird für ältere Konfigurationen ohne ALUA verwendet und verhindert Proxy-I/O-Vorgänge Er trägt also dazu bei, dass I/O-Vorgänge bei einem HA-Paar in einer Umgebung, in der Data ONTAP im 7-Mode ausgeführt wird, nicht auf den anderen Node verlagert werden</block>
  <block id="117704664d6cada78ac3107380f63d23" category="cell">Disk.QFullSampleSize</block>
  <block id="0e82e1f81de2159b0e9bee0b7be00dc9" category="cell">Alle Konfigurationen: Auf 32 einstellen.
Durch die Festlegung dieses Wertes werden I/O-Fehler verhindert.</block>
  <block id="f66a3c183f0e736240b77f8b755c880e" category="cell">Disk.QFullThreshold</block>
  <block id="bdd8c1d08eabb5e1923ed8f0c4b10ba4" category="cell">Alle Konfigurationen: Auf 8 einstellen.
Durch die Festlegung dieses Wertes werden I/O-Fehler verhindert.</block>
  <block id="19c08de7404551288e1274186e039ef4" category="cell">Emulex FC-HBA-Zeitüberschreitungen</block>
  <block id="b4dbfc2d52627e923cbb563a31ff756d" category="cell">Standardwert verwenden.</block>
  <block id="d46cddda4da7ec11f17472d11df95b68" category="cell">QLogic FC-HBA-Zeitüberschreitungen</block>
  <block id="19b5aaf3e2cb86809996ca63a2a416b0" category="cell">*ISCSI-Einstellungen*</block>
  <block id="2aa077454308383f1e82025427cd7362" category="cell">Alle iSCSI-Pfade: Auf RR (Round Robin) einstellen.
Wenn Sie diesen Wert auf RR einstellen, ist für alle aktiven/optimierten Pfade ein besserer Lastausgleich möglich.</block>
  <block id="0340eb1bcbfc6cd3c62b8a878d8d643b" category="cell">Alle Konfigurationen: Auf 32 einstellen.
Durch die Festlegung dieses Wertes werden I/O-Fehler verhindert</block>
  <block id="891c10448731ad57490b9a932dabfff8" category="inline-link-macro">VMware KB 86331</block>
  <block id="2c572e3c397fe100b075a09359c9cc23" category="admonition">1 - die erweiterte NFS-Konfigurationsoption MaxQueueDepth funktioniert möglicherweise nicht wie vorgesehen bei der Verwendung von VMware vSphere ESXi 7.0.1 und VMware vSphere ESXi 7.0. Bitte referenzieren <block ref="861055760f3cd527823fd34a49459992" category="inline-link-macro-rx"></block> Finden Sie weitere Informationen.</block>
  <block id="fb55ee99c04280ac638757869929785d" category="paragraph">ONTAP-Tools legen beim Erstellen von ONTAP FlexVol Volumes und LUNs bestimmte Standardeinstellungen fest:</block>
  <block id="3f7502a5f109d7a73706aba1c0741a4b" category="cell">*ONTAP-Tool*</block>
  <block id="485577e97e8efcd504fc8e8b13e613f1" category="cell">*Standardeinstellung*</block>
  <block id="5938fe448c8661febcef3f4e779e3adb" category="cell">Snapshot-Reserve (-percent-Snapshot-space)</block>
  <block id="cfcd208495d565ef66e7dff9f98764da" category="cell">0</block>
  <block id="7218aec62575561a6de1cae8d5658390" category="cell">Fraktionale Reserve (-fractional-Reserve)</block>
  <block id="3a7e09ff0fa38663ffe6d91324ea75d9" category="cell">Aktualisierung der Zugriffszeit (-atime-Update)</block>
  <block id="f8320b26d30ab433c5a54546d21f414c" category="cell">Falsch</block>
  <block id="e93200d5b0d16915bf04236790544b2f" category="cell">Minimales Vorauslesen (-min-readahead)</block>
  <block id="c877d89702fdd14e51b02028a20c7d07" category="cell">Geplante Snapshots</block>
  <block id="fd18465573ec21a6218982055981f6b1" category="cell">Storage-Effizienz</block>
  <block id="00d23a76e43b46dae9ec7aa9dcbebb32" category="cell">Aktiviert</block>
  <block id="f65fd63b63c9e9f25bf6bc141e83de34" category="cell">Volume-Garantie</block>
  <block id="c2410aebdf425dabcc65e546e506b03e" category="cell">Keine (Thin Provisioning)</block>
  <block id="f97c03bf3e9580446d70d479f5aad1e0" category="cell">Automatische Volumengröße</block>
  <block id="d89f4f8b1d7c18847b88b46142f61535" category="cell">Vergrößern_verkleinern</block>
  <block id="ace72ec54e17777d27eb8bdb9d57e782" category="cell">LUN-Speicherplatzreservierung</block>
  <block id="b9f5c797ebbf55adccdd8539a65a0241" category="cell">Deaktiviert</block>
  <block id="514aa7792a71ab4ab677eb2385131aae" category="cell">Zuweisung von LUN-Speicherplatz</block>
  <block id="dbe1e98b00264bd496095ed5c95f9350" category="section-title">Multipath-Einstellungen für die Performance</block>
  <block id="de9f44b08eec22d6b405acff2c925d56" category="paragraph">Obwohl NetApp derzeit nicht durch verfügbare ONTAP-Tools konfiguriert ist, empfiehlt es folgende Konfigurationsoptionen:</block>
  <block id="840731ae1cb9e4b23e19f645ab018b6f" category="inline-link">2069356</block>
  <block id="5073fc9da327263a7db04fd449991e6a" category="list-text">In hochperformanten Umgebungen oder bei Tests der Performance mit einem einzelnen LUN-Datastore sollte die Einstellung der Lastverteilung für die Round-Robin (VMW_PSP_RR) Path Selection Policy (PSP) von der standardmäßigen IOPS-Einstellung 1000 auf einen Wert 1 geändert werden. Siehe VMware KB<block ref="f3f4762788a6845119bdb5b9c9179bda" category="inline-link-rx"></block> Finden Sie weitere Informationen.</block>
  <block id="1ea3eb55b985cf75e100607ee8bd935d" category="inline-link">Pfadauswahl-Plug-ins und -Richtlinien</block>
  <block id="01d954fb2edbf283d121f0651b04de83" category="section-title">Zusätzliche Dokumentation</block>
  <block id="40db5019ed68654f4eb2cf21ec5021db" category="inline-link">Verwenden Sie VMware vSphere 7.x mit ONTAP</block>
  <block id="30d85838b9488ee908d075559c8978cd" category="inline-link">Verwenden Sie VMware vSphere 8.x mit ONTAP</block>
  <block id="000c9205416c1ea85f6841a7ca379c17" category="inline-link">Für NVMe-of finden Sie weitere Details unter NVMe-of Host Configuration for ESXi 7.x with ONTAP</block>
  <block id="5a27e6dbe6279e323aec51d205b2b455" category="inline-link">Für NVMe-of finden Sie weitere Details unter NVMe-of Host Configuration for ESXi 8.x with ONTAP</block>
  <block id="842acd8b4932f6c9329df8e0fefe8b9b" category="paragraph">Für FCP und iSCSI mit vSphere 7 finden Sie weitere Details unter<block ref="f3eae508de4d3c1748a9c65337197b21" category="inline-link-rx"></block>
Für FCP und iSCSI mit vSphere 8 finden Sie weitere Details unter<block ref="3f8db75ee0177b85df9cdf31ddb51abe" category="inline-link-rx"></block>
Für NVMe-of mit vSphere 7 finden Sie weitere Informationen unter<block ref="6c040a6e611e78bb9d0ad92946e613ea" category="inline-link-rx"></block>
Für NVMe-of mit vSphere 8 finden Sie weitere Informationen unter<block ref="030788e4c7bbe3137759b534ea62d7d9" category="inline-link-rx"></block></block>
  <block id="99a66df20e19a8e18fae37a7f714750a" category="doc">Klonen von VMs und Datastores</block>
  <block id="fe12a277656562082ad50b980c227a84" category="paragraph">In vSphere können Sie VMs, virtuelle Festplatten, vVol oder Datastores klonen. Nach dem Klonen kann das betreffende Objekt weiter angepasst werden. Dies geschieht häufig durch einen automatisierten Prozess. VSphere unterstützt sowohl vollständige Klone als auch Linked Clones, bei denen Änderungen separat vom ursprünglichen Objekt verfolgt werden.</block>
  <block id="883a97a36ab0c59ec785b03161a1cc32" category="paragraph">Linked Clones eignen sich sehr gut, um Speicherplatz zu sparen, aber sie erhöhen die Menge der I/O-Vorgänge, die vSphere für die VM verarbeitet. Dies wirkt sich auf die Performance der betreffenden VM und vielleicht auch des gesamten Hosts aus. Aus diesem Grund nutzen NetApp Kunden häufig Klone, die auf Storage-Systemen basieren, um das Beste aus beiden Welten zu erhalten: Effiziente Storage-Nutzung und höhere Performance.</block>
  <block id="17e1a153b5d05c5a642c93ba9ce3c6de" category="paragraph">In der folgenden Abbildung ist das Klonen von ONTAP dargestellt.</block>
  <block id="d957242cd67db8ffccec78d93c14449d" category="list-text">VVols, die den NetApp vSphere APIs for Storage Awareness (VASA) Provider verwenden.  ONTAP Klone unterstützen von vCenter gemanagte vVol Snapshots, die platzsparend sind und bei der Erstellung und Löschung eine minimale I/O-Auswirkung haben.  VMs können auch mit vCenter geklont werden. Sie werden dann auch zu ONTAP verlagert, sei es innerhalb eines einzelnen Datastores/Volumes oder zwischen Datastores/Volumes.</block>
  <block id="059e5a6b7f963e9876ba0129dc1b667d" category="list-text">VSphere Klone und Migration mit vSphere APIs – Array Integration (VAAI). VM-Klonvorgänge können in SAN- und NAS-Umgebungen zu ONTAP verlagert werden (NetApp stellt ein ESXi Plug-in zur Aktivierung von VAAI für NFS bereit).  VSphere verlagert den Betrieb nur auf „kalte“ (ausgeschalteten) VMs in einem NAS-Datastore, während Vorgänge auf heißen VMs (Klonen und Storage vMotion) ebenfalls für SAN verlagert werden. ONTAP nutzt je nach Quelle, Ziel und installierten Produktlizenzen den effizientesten Ansatz. Diese Funktion wird auch von VMware Horizon View unterstützt.</block>
  <block id="a603c207c7b5801942ff108502676c12" category="list-text">SRA (wird mit VMware Site Recovery Manager verwendet). Hier werden Klone zum unterbrechungsfreien Testen der Recovery des DR-Replikats herangezogen.</block>
  <block id="bb300d356d258c084dbe5da2ca367e08" category="list-text">Backup und Recovery mit NetApp Tools wie SnapCenter. Mit VM-Klonen werden Backup-Vorgänge sichergestellt. Darüber hinaus können VM-Backups gemountet werden, so dass einzelne Dateien kopiert / zurückgesichert werden können.</block>
  <block id="524b5b8bc03312baeafbea86df667c4b" category="paragraph">Verlagerte ONTAP Klone können durch VMware, NetApp und Drittanbietertools aufgerufen werden. Zu ONTAP verlagerte Klone haben mehrere Vorteile. Sie sind in den meisten Fällen platzsparend, da sie nur für Änderungen am Objekt Storage benötigen. Es entstehen keine zusätzlichen Performance-Einbußen, wenn sie gelesen und geschrieben werden, und in einigen Fällen wird die Performance durch die Freigabe von Blöcken in High-Speed-Caches erhöht. Zudem verlagern sie CPU-Zyklen und Netzwerk-I/O-Vorgänge vom ESXi Server. Die Verlagerung von Kopien in einen herkömmlichen Datastore, bei dem ein FlexVol Volume verwendet wird, kann mit einer Lizenzierung von FlexClone schnell und effizient sein. Kopien zwischen FlexVol Volumes sind jedoch unter Umständen langsamer. Wenn Sie VM-Vorlagen als Klonquelle bereithalten, sollten Sie sie in Betracht ziehen, sie im Datastore-Volume zu platzieren (Ordner oder Inhaltsbibliotheken zur Organisation dieser Klone einsetzen), um schnelle, platzsparende Klone zu erstellen.</block>
  <block id="f3968368501d882b3969da59138db6e2" category="paragraph">Zum Klonen eines Datastores können Sie ein Volume oder eine LUN auch direkt in ONTAP klonen. Mithilfe der FlexClone Technologie kann bei NFS-Datastores ein gesamtes Volume geklont und der Klon anschließend aus ONTAP exportiert und von ESXi als weiterer Datastore gemountet werden. Bei VMFS Datastores kann in ONTAP eine LUN innerhalb eines Volumes oder das gesamte Volume (einschließlich einer oder mehrerer darin enthaltener LUNs) geklont werden. Eine LUN, die ein VMFS enthält, muss einer ESXi Initiatorgruppe zugeordnet und dann von ESXi neu signiert werden, damit sie gemountet und als regulärer Datastore verwendet werden kann. Ein geklontes VMFS kann für einige temporäre Anwendungsfällte ohne erneute Signatur gemountet werden. Nachdem ein Datastore geklont wurde, können die darin enthaltenen VMs registriert, neu konfiguriert und angepasst werden, als wären sie einzeln geklonte VMs.</block>
  <block id="2d506b6088a383ccf08479b0f80c0ff4" category="paragraph">In einigen Fällen kann das Klonen durch zusätzliche lizenzierte Funktionen wie SnapRestore für Backups oder FlexClone optimiert werden. Diese Lizenzen sind oft in Lizenz-Bundles ohne zusätzliche Kosten enthalten. Für vVol Klonvorgänge und zur Unterstützung gemanagter Snapshots eines vVol (die vom Hypervisor zu ONTAP verlagert werden) ist eine FlexClone Lizenz erforderlich. Durch eine FlexClone Lizenz können auch bestimmte VAAI basierte Klone optimiert werden, wenn sie in einem Datastore/Volume verwendet werden. Dabei werden sofortige platzsparende Kopien anstelle von Blockkopien erstellt.  Sie wird zudem von SRA beim Testen der Recovery eines DR-Replikats sowie von SnapCenter für Klonvorgänge und zum Durchsuchen von Backup-Kopien zum Wiederherstellen einzelner Dateien genutzt.</block>
  <block id="5dd57c2a315d3af44c2eb40eefc47111" category="doc">Netzwerkkonfiguration</block>
  <block id="66507f357c74ee12194270cfe053b98d" category="paragraph">Folgende Punkte sind dabei zu berücksichtigen:</block>
  <block id="7c4904daaa282388097ad83bf382e1a5" category="list-text">Jumbo Frames können genutzt werden, sofern dies gewünscht ist und von Ihrem Netzwerk unterstützt wird, insbesondere bei Verwendung von iSCSI. Vergewissern Sie sich bei ihrem Einsatz, dass sie auf allen Netzwerkgeräten, VLANs etc. Im Pfad zwischen Storage und dem ESXi Host gleich konfiguriert sind. Anderenfalls kann es zu Performance- oder Verbindungsproblemen kommen. Auf dem virtuellen ESXi Switch, dem VMkernel Port, sowie den physischen Ports oder den Interface Groups muss für jeden ONTAP Node auch jeweils dieselbe MTU festgelegt sein.</block>
  <block id="8b6e7f43d1d697e8f99164f0aa2ab1b6" category="inline-link">TR-4182</block>
  <block id="730a8287f758961d602b1b050bbe2751" category="list-text">Wenn ESXi und ONTAP Storage-Arrays mit Ethernet-Storage-Netzwerken verbunden werden, empfiehlt NetApp, die Ethernet-Ports, mit denen diese Systeme verbunden werden, mit der Cisco PortFast Funktion oder als Rapid Spanning Tree Protocol (RSTP)-Edge-Ports zu konfigurieren. NetApp empfiehlt die Aktivierung der Spanning Tree PortFast Trunk-Funktion in Umgebungen mit Verwendung der Cisco PortFast Funktion und 802.1Q VLAN-Trunking entweder für den ESXi Server oder für die ONTAP Storage-Arrays.</block>
  <block id="ac4976538e1fc8726f77afc202561afc" category="list-text">Für die Link-Aggregation empfiehlt NetApp die folgenden Best Practices:</block>
  <block id="19d137845c7f6687c99b386dfad0aa97" category="list-text">Deaktivieren Sie LACP für mit ESXi verbundene Switch Ports, es sei denn, Sie verwenden dvSwitches ab 5.1 mit konfiguriertem LACP.</block>
  <block id="30254a2bb3e4b3b958b06d08c724e7db" category="list-text">Erstellen Sie mit LACP Link-Aggregate für ONTAP Storage-Systeme mit dynamischen Multimode-Schnittstellengruppen mit IP-Hash.</block>
  <block id="d608421cceb8e4c584a2a32d9127879e" category="list-text">Verwenden Sie eine IP-Hash-Teaming-Richtlinie für ESXi.</block>
  <block id="2432030fc6183a1a6c52cf5e93f3e9ae" category="paragraph">Die folgende Tabelle enthält eine Zusammenfassung der Netzwerkkonfigurationselemente sowie Angaben dazu, wo die Einstellungen angewendet werden.</block>
  <block id="7d74f3b92b19da5e606d737d339a9679" category="cell">Element</block>
  <block id="1aa66ef3a8e34a7a538960a80d2e1e31" category="cell">ESXi</block>
  <block id="bbc155fb2b111bf61c4f5ff892915e6b" category="cell">Switch</block>
  <block id="6c3a6944a808a7c0bbb6788dbec54a9f" category="cell">Knoten</block>
  <block id="0b7e67b6cdcf0432b624d53588d520fb" category="cell">SVM</block>
  <block id="75ba8d70e3692ba200f0e0df37b4d2ae" category="cell">IP-Adresse</block>
  <block id="ee6c0f25c2881cc69947a2ef23be5b8c" category="cell">VMkernel</block>
  <block id="e11f298a5bd5344d2d975b5157c27b69" category="cell">Nein**</block>
  <block id="d6f0876f76c273b8962b749c36153157" category="cell">Link-Aggregation</block>
  <block id="fabd320b3b2249377e53e93099e27657" category="cell">Virtueller Switch</block>
  <block id="b1b40427c8eb8d0a083ddb16e250325b" category="cell">Nein*</block>
  <block id="9881f82f0dd89588831f9d1682bd5492" category="cell">VLAN</block>
  <block id="5fc0d231160fa9650ea5c877f146bbe6" category="cell">VMkernel und VM-Portgruppen</block>
  <block id="39a3e05f380e583ae2887c79c7443f11" category="cell">Flusskontrolle</block>
  <block id="220071ab44b426f80ef21f1c552c363c" category="cell">NIC</block>
  <block id="6f2c08412f489e52a17a30217f50b3c1" category="cell">Spanning Tree</block>
  <block id="4b1c0df98ba3f3d1681c3c3dbdc97746" category="cell">MTU (für Jumbo Frames)</block>
  <block id="7d6feaf896df4a937157d4ee5b91f4e1" category="cell">Virtueller Switch und VMkernel Port (9000)</block>
  <block id="0110e50bd8629573701e73a4ba8b056f" category="cell">Ja (auf Maximalwert eingestellt)</block>
  <block id="061ff5255adf00e4ae68469f43a7bf55" category="cell">Ja (9000)</block>
  <block id="bcc0e5a3e08cd34e80692195d056b06d" category="cell">Failover-Gruppen</block>
  <block id="a4d1a66a448e327c12d3e287098a31d5" category="cell">Ja (erstellen)</block>
  <block id="465d5674eb6808b997037470f4dace73" category="cell">Ja (auswählen)</block>
  <block id="ba78d695573c8d4f1205452c675fa539" category="paragraph">*SVM-LIFs werden mit Ports, Schnittstellengruppen oder VLAN-Schnittstellen verbunden, die über VLAN-, MTU- und andere Einstellungen verfügen. Diese Einstellungen werden jedoch nicht auf SVM-Ebene gemanagt.</block>
  <block id="50ecdab93bf5a2f1bfb4cd8f81b9bd3d" category="paragraph">**Diese Geräte haben eigene IP-Adressen für das Management, aber diese Adressen werden nicht im Zusammenhang mit ESXi Storage Networking verwendet.</block>
  <block id="efd0bde36323238fec688b9cdf0c75a3" category="section-title">SAN (FC, FCoE, NVMe/FC, iSCSI), RDM</block>
  <block id="29cdd488736a6111699f7348320bbed7" category="paragraph">Mit vSphere gibt es drei Methoden, blockbasierten Speicher zu nutzen:</block>
  <block id="8982d243303f9d1a8c2470b7d55278e6" category="list-text">Mit VMFS Datastores</block>
  <block id="9a0098c8c1be287e8d1da1aaf3bd2e59" category="list-text">Mit Raw Device Mapping (RDM)</block>
  <block id="93e209176746ac6f25c618631a02643b" category="list-text">Auf diese LUN wird von einem Software-Initiator aus einem VM-Gastbetriebssystem zugegriffen und gesteuert</block>
  <block id="c25dc676ca39a05ee6067ac9a50dbce0" category="paragraph">VMFS ist ein hochperformantes geclustertes Filesystem, das Datastores bereitstellt, bei denen es sich um Shared-Storage-Pools handelt. VMFS Datastores können mit LUNs konfiguriert werden, auf die über FC, iSCSI, FCoE oder NVMe Namespaces zugegriffen wird, auf die das NVMe/FC-Protokoll zugegriffen wird. Bei VMFS können alle ESX Server in einem Cluster gleichzeitig auf herkömmliche LUNs zugreifen. Die maximale LUN-Größe beträgt bei ONTAP im Allgemeinen 16 TB; daher wird ein VMFS 5 Datastore mit einer maximalen Größe von 64 TB (siehe erste Tabelle in diesem Abschnitt) aus vier 16-TB-LUNs erstellt (alle SAN-Array-Systeme unterstützen die maximale VMFS-LUN-Größe von 64 TB). Da die ONTAP LUN-Architektur keine kleinen individuellen „Queue Depths“ aufweist, sind VMFS Datastores in ONTAP relativ problemlos in einem höheren Maße skalierbar gegenüber herkömmlichen Array-Architekturen.</block>
  <block id="43f67f108e50dab0978a343603cbfec9" category="paragraph">VSphere umfasst integrierte Unterstützung für mehrere Pfade zu Storage-Geräten. Dieses Verfahren wird als natives Multipathing (NMP) bezeichnet. NMP kann den Storage-Typ für unterstützte Storage-Systeme erkennen und den NMP-Stack automatisch so konfigurieren, dass die Funktionen des verwendeten Storage-Systems unterstützt werden.</block>
  <block id="b284ee628d89c3e804c4def2a90f0520" category="paragraph">ESXi 6 unterstützt bis zu 256 LUNs und insgesamt bis zu 1,024 Pfade zu LUNs. Alle über diese Grenzen hinausgehenden LUNs oder Pfade werden von ESXi nicht erkannt. Ausgehend von dieser maximalen Anzahl an LUNs lässt das Pfadlimit vier Pfade pro LUN zu. In einem größeren ONTAP Cluster ist es möglich, dass das Pfadlimit vor dem LUN-Limit erreicht wird. Zur Beseitigung dieser Beschränkung unterstützt ONTAP ab Version 8.3 die selektive LUN-Zuordnung (Selective LUN Map, SLM).</block>
  <block id="8466b1e5abb4751e931c5feb1af4e469" category="inline-link">TR-4080</block>
  <block id="4157f0c23fc104508f492dc846f187f3" category="list-text">SLM ist standardmäßig aktiviert. Sofern Sie keine Portsätze verwenden, ist keine weitere Konfiguration erforderlich.</block>
  <block id="597f1433ad810b3b86c7ae0f8f95afa8" category="list-text">Für LUNs, die vor Data ONTAP 8.3 erstellt wurden, wenden Sie SLM manuell an, indem Sie die ausführen<block ref="886fd385b633a0746a8a08f8f00c67cd" prefix=" " category="inline-code"></block> Befehl, um die LUN-Nodes für die Berichterstellung zu entfernen und den LUN-Zugriff auf den LUN-Eigentümer-Node und seinen HA-Partner zu beschränken.</block>
  <block id="43919b4d4d6d446ed498e26bff62db84" category="paragraph">Blockprotokolle (iSCSI, FC und FCoE) greifen mithilfe von LUN-IDs und Seriennummern sowie mit eindeutigen Namen auf LUNs zu. FC und FCoE verwenden weltweite Namen (WWNNs und WWPNs) und iSCSI verwendet qualifizierte iSCSI-Namen (IQNs). Der Pfad zu LUNs innerhalb des Storage hat für die Blockprotokolle keine Bedeutung und wird nirgendwo im Protokoll angegeben. Daher muss ein Volume, das nur LUNs enthält, nicht intern gemountet werden. Zudem ist für Volumes, die in Datastores verwendete LUNs enthalten, kein Verbindungspfad erforderlich. Das NVMe-Subsystem in ONTAP funktioniert ähnlich.</block>
  <block id="ed0e76ed86e852bd7317a09d856ec4e8" category="paragraph">Weitere Best Practices, die berücksichtigt werden sollten:</block>
  <block id="93586ad57931e0f16fff61cdba9d77df" category="list-text">Vergewissern Sie sich, dass für jede SVM auf jedem Node im ONTAP Cluster eine logische Schnittstelle (LIF) erstellt wird, um maximale Verfügbarkeit und Mobilität zu gewährleisten. Als Best Practice empfiehlt sich für ONTAP SANs die Verwendung von zwei physischen Ports und LIFs pro Node, einer für jede Fabric. Mit ALUA werden Pfade geparst und aktive optimierte (direkte) Pfade im Gegensatz zu aktiven nicht optimierten Pfaden identifiziert. ALUA wird für FC, FCoE und iSCSI verwendet.</block>
  <block id="b3cebb7381bddbabb1e950a46d264190" category="list-text">Nutzen Sie für iSCSI-Netzwerke mehrere VMkernel Netzwerkschnittstellen für verschiedene Subnetze mit NIC-Teaming, wenn mehrere virtuelle Switches vorhanden sind. Darüber hinaus können Sie mehrere physische NICs nutzen, die mit mehreren physischen Switches verbunden sind, um Hochverfügbarkeit und einen höheren Durchsatz bereitzustellen. Die folgende Abbildung zeigt ein Beispiel für Multipath-Konnektivität. Konfigurieren Sie in ONTAP entweder eine Single-Mode-Schnittstellengruppe für Failover mit zwei oder mehr Links, die mit zwei oder mehreren Switches verbunden sind, oder nutzen Sie LACP oder eine andere Link-Aggregationstechnologie mit Multimode-Schnittstellengruppen, um Hochverfügbarkeit und die Vorteile der Link-Aggregation bereitzustellen.</block>
  <block id="2968b9f3c141bf23bc73d0e6e15a174a" category="list-text">Wenn das Challenge-Handshake Authentication Protocol (CHAP) in ESXi für die Zielauthentifizierung verwendet wird, muss es auch in ONTAP über die CLI konfiguriert werden <block ref="199c9b970eacf1c35d84a383b7ceb210" prefix="(" category="inline-code"></block>) Oder mit System Manager (bearbeiten Sie die Initiatorsicherheit unter „Storage“ &gt; „SVMs“ &gt; „SVM-Einstellungen“ &gt; „Protocols“ &gt; „iSCSI“).</block>
  <block id="d31f2e075df4ca75ca0874fa08f74b12" category="list-text">Verwenden Sie ONTAP Tools für VMware vSphere, um LUNs und Initiatorgruppen zu erstellen und zu managen. Das Plug-in bestimmt automatisch die WWPNs von Servern und erstellt entsprechende Initiatorgruppen. Darüber hinaus konfiguriert er LUNs gemäß Best Practices und ordnet sie den richtigen Initiatorgruppen zu.</block>
  <block id="de78559e8aed2cd62fbf852f73dc58c2" category="inline-link">Kompatibilitätsmodus für physischen und virtuellen Modus</block>
  <block id="905221150fdd103b5241870a0a1f2c42" category="list-text">Setzen Sie RDMs mit Bedacht ein, da ihr Management schwieriger sein kann. Zudem verwenden sie auch Pfade, die wie bereits beschrieben beschränkt sind. ONTAP LUNs unterstützen beide<block ref="27e1f8e5a88cfbb2836b083145b004c9" category="inline-link-rx"></block> RDMs:</block>
  <block id="636da39a2033c6179a718983c5ce1222" category="inline-link">ONTAP NVMe/FC-Host-Konfigurationsleitfaden</block>
  <block id="81ef6507a13da5635951bc5b533deb59" category="inline-link">TR-4684</block>
  <block id="15483b0826bb9a4b11d1c89af2029c5c" category="list-text">Weitere Informationen zur Verwendung von NVMe/FC mit vSphere 7.0 finden Sie im hier<block ref="6b4ed262d14e47ef641cd57b65c32c38" category="inline-link-rx"></block> Und<block ref="7a29b2f31035a451d5b068728d2b79a7" category="inline-link-rx"></block>Die folgende Abbildung zeigt die Multipath-Konnektivität von einem vSphere Host zu einer ONTAP LUN.</block>
  <block id="f2a693e1f056b96566c4551fcab418f2" category="paragraph">Bei vSphere können Kunden mithilfe von NFS-Arrays der Enterprise-Klasse gleichzeitigen Zugriff auf Datastores auf allen Nodes in einem ESXi Cluster ermöglichen. Wie im Abschnitt zu Datastores erwähnt, gibt es bei der Verwendung von NFS mit vSphere einige Vorteile im Hinblick auf Benutzerfreundlichkeit, Storage-Effizienz und Sichtbarkeit.</block>
  <block id="743fa5a7bc7d4ba215b096e34779d298" category="paragraph">Für die Verwendung von ONTAP NFS mit vSphere werden folgende Best Practices empfohlen:</block>
  <block id="c05061b6d3055b83136fa96cb38f0f9a" category="list-text">Verwenden einer einzelnen logischen Schnittstelle (LIF) für jede SVM auf jedem Node im ONTAP-Cluster Die bisherigen Empfehlungen eines LIF pro Datenspeicher sind nicht mehr erforderlich. Der direkte Zugriff (LIF und Datastore auf demselben Node) ist zwar am besten, aber indirekte Zugriffe müssen sich keine Sorgen machen, da die Performance-Auswirkungen im Allgemeinen minimal sind (Mikrosekunden).</block>
  <block id="7b997b3be9c993d49799dd9d71d8c3f6" category="list-text">VMware unterstützt NFSv3 seit VMware Infrastructure 3. VSphere 6.0 bietet zusätzlich Unterstützung für NFSv4.1 und ermöglicht damit einige erweiterte Funktionen wie Kerberos Sicherheit. In NFSv3 wird „Client-side locking“ verwendet, in NFSv4.1 „Server-side locking“. Ein ONTAP Volume kann zwar mit beiden Protokollen exportiert werden, doch ESXi kann nur durch ein Protokoll gemountet werden. Bei diesem Einzelprotokoll-Mounting ist jedoch nicht ausgeschlossen, dass ESXi Hosts denselben Datastore auch durch eine andere Version mounten. Denken Sie daran, die beim Mounten verwendete Protokollversion anzugeben, damit alle Hosts dieselbe Version und somit auch denselben Sperrungsstil anwenden. Verwenden Sie auf verschiedenen Hosts nicht unterschiedliche NFS-Versionen. Falls möglich, prüfen Sie mithilfe von Hostprofilen die Compliance.</block>
  <block id="ae4c5baa79e370d8f601cc7082f8123e" category="list-text">Da keine automatische Datastore-Konvertierung zwischen NFSv3 und NFSv4.1 stattfindet, erstellen Sie einen neuen Datastore für NFSv4.1 und migrieren Sie die VMs mithilfe von Storage vMotion zum neuen Datastore.</block>
  <block id="8065bff4940ae8d7161f926b0df47ac4" category="inline-link">NetApp Interoperabilitäts-Matrix-Tool</block>
  <block id="24aaa4a688e881fa13d31656a85d40a9" category="list-text">Zur Steuerung des Zugriffs durch vSphere Hosts kommen NFS-Exportrichtlinien zur Anwendung. Sie können eine Richtlinie für mehrere Volumes (Datastores) nutzen. Bei NFSv3 verwendet ESXi den Sicherheitsstil „sys“ (UNIX). Zur Ausführung von VMs ist dabei die Root-Mount-Option erforderlich. In ONTAP wird diese Option als Superuser bezeichnet. Wenn die Option Superuser verwendet wird, ist es nicht erforderlich, die anonyme Benutzer-ID anzugeben. Beachten Sie, dass Exportrichtlinien mit unterschiedlichen Werten für gelten<block ref="ea72d12ecaa06afdb0c8a3b15e485e95" prefix=" " category="inline-code"></block> Und<block ref="df809a941c1287d18c08bb5927b639dc" prefix=" " category="inline-code"></block> Die ONTAP-Tools können zu Problemen bei der SVM-Erkennung führen. Hier sehen Sie eine Beispielrichtlinie:</block>
  <block id="69abbd5c7cd9c1b3828b9aa5a894ff47" category="list-text">Client Match Spec: 192.168.42.21</block>
  <block id="bd6cc42f9cd65cca72cbd56d2f4bf43d" category="list-text">RO-Zugriffsregel: Sys</block>
  <block id="581c470a5099548fc9865a4bab8761f8" category="list-text">RW Access Rule: Sys</block>
  <block id="0bbc71e6a4ea49af70dd616d3807e524" category="list-text">Anonyme UID</block>
  <block id="59d5d8c898df18115bd4ced36c6bc0de" category="list-text">Superuser: Sys</block>
  <block id="6670a317619282ec228a9ef492af0a76" category="list-text">NFS-Datastore-Volumes werden aus dem Root-Volume der SVM heraus verbunden. Daher muss ESXi zum Navigieren und Mounten von Datastore Volumes auch Zugriff auf das Root-Volume haben. Die Exportrichtlinie für das Root-Volume und für alle anderen Volumes, in denen die Verbindung des Datastore Volumes geschachtelt ist, muss eine oder mehrere Regeln für die ESXi Server einschließen, die ihnen schreibgeschützten Zugriff gewähren. Hier sehen Sie eine Beispielrichtlinie für das Root-Volume, bei der auch das VAAI Plug-in genutzt wird:</block>
  <block id="8c93fb3ff528165613797962f5a0ed9c" category="list-text">Access Protocol: nfs (schließt nfsv3 und NFSv4 ein)</block>
  <block id="80e6a4f964e826c69c1c62bd5ea67590" category="list-text">RW Access Rule: Never (höchste Sicherheit für Root-Volume)</block>
  <block id="f96625174b4a06d13267f9b0a5a96d59" category="list-text">Superuser: Sys (auch für Root-Volume mit VAAI erforderlich)</block>
  <block id="4c571520a8128d1692fc709ffbfb2b1e" category="list-text">Verwenden Sie ONTAP Tools für VMware vSphere (die wichtigste Best Practice):</block>
  <block id="b8d278c6184cb65727ca5ae729db92d7" category="list-text">Mit ONTAP Tools für VMware vSphere können Sie Datastores bereitstellen, da es das Management von Richtlinien für den Export automatisch vereinfacht.</block>
  <block id="23aa7b9aeed3fba2a1ec43e630313af7" category="list-text">Wählen Sie beim Erstellen von Datastores für VMware Cluster mithilfe des Plug-ins das Cluster anstelle eines einzelnen ESX Servers aus. Bei dieser Auswahl mountet der Datastore automatisch auf alle Hosts im Cluster.</block>
  <block id="e4e62770daaf9887c7868fbc8e248a4b" category="list-text">Wenden Sie mithilfe der Plug- in-Mount-Funktion vorhandene Datastores auf neue Server an.</block>
  <block id="928562eb576ed6ae31cb1a149d3e751a" category="list-text">Wenn Sie die ONTAP Tools nicht für VMware vSphere verwenden, verwenden Sie eine Exportrichtlinie für alle Server oder für jeden Server-Cluster, wo eine zusätzliche Zugriffs-Kontrolle erforderlich ist.</block>
  <block id="7bccab0fa4c0864835c645c9fde7ebf6" category="list-text">Obwohl ONTAP eine flexible Namespace-Struktur für Volumes bietet, in der Volumes mithilfe von Verbindungen in einer Baumstruktur angeordnet werden können, ist dieser Ansatz für vSphere nicht praktikabel. Für jede VM im Root-Verzeichnis des Datastores wird unabhängig von der Namespace-Hierarchie des Storage ein Verzeichnis erstellt. Daher besteht die Best Practice darin, den Verbindungspfad für Volumes für vSphere im Root-Volume der SVM zu erstellen. Dies entspricht auch der Art und Weise, wie ONTAP Tools für VMware vSphere Datastores bereitstellt. Ohne geschachtelte Verbindungspfade besteht bei Volumes zudem nur eine Abhängigkeit zum Root-Volume. Wenn ein Volume dann offline geschaltet oder sogar absichtlich zerstört wird, wirkt sich dies also nicht auf den Pfad zu den anderen Volumes aus.</block>
  <block id="ea6fc1288f1d3b21238d29ff28cb867a" category="list-text">Eine Blockgröße von 4 KB ist für NTFS-Partitionen auf NFS-Datenspeichern gut. In der folgenden Abbildung ist die Konnektivität eines vSphere Hosts zu einem ONTAP NFS-Datastore dargestellt.</block>
  <block id="16c864cb4f3b00c59e1124932e49f342" category="paragraph"><block ref="16c864cb4f3b00c59e1124932e49f342" category="inline-image-macro-rx" type="image"></block></block>
  <block id="e54433644e830ed1503561b778e9ded4" category="paragraph">In der folgenden Tabelle sind NFS-Versionen und unterstützte Funktionen aufgeführt.</block>
  <block id="9fb9dac7513c7ef8452aade8a52ad40d" category="cell">Funktionen von vSphere</block>
  <block id="5b12ee0369243579651edca43ff65c85" category="cell">NFSv3</block>
  <block id="de841868c2911da76b8ae2da9fa3166d" category="cell">NFSv4.1</block>
  <block id="64e3cc476958b05086721cd6070004cf" category="cell">VMotion und Storage vMotion</block>
  <block id="05807e454c19f244770adae059b3c330" category="cell">Hochverfügbarkeit</block>
  <block id="45a75e44a713fdf94dbf1dbaa0749188" category="cell">Fehlertoleranz</block>
  <block id="f1e3446c69e4d87279ff7863482f9dcb" category="cell">DRS</block>
  <block id="6ca09f98e8925b2cb78bbf24eccc0186" category="cell">Hostprofile</block>
  <block id="ec8420797e3b089812499cce4047ded8" category="cell">Storage DRS</block>
  <block id="7c19bf1cbe62a87a42857ef0b4fe22ae" category="cell">Storage-I/O-Steuerung</block>
  <block id="56d43ad1a280ada5e16fd2960ae44b32" category="cell">SRM</block>
  <block id="d595c8f30866b71ea121ced5cde66b1d" category="cell">Virtual Volumes</block>
  <block id="6ad5114acf73f68a85c72f11646920cb" category="cell">Hardwarebeschleunigung (VAAI)</block>
  <block id="ad37ded9046268d8e5ea7cb253bb5dda" category="cell">Kerberos Authentifizierung</block>
  <block id="45701e6ef9bc09dca07f8f4abe661dc6" category="cell">Ja (Erweiterung mit vSphere 6.5 und höher zur Unterstützung von AES, krb5i)</block>
  <block id="76969a36155d2f41fc6cc3bd3faff244" category="cell">Multipathing-Unterstützung</block>
  <block id="be0ec3d9251292dd856ea0924ecc6873" category="doc">Servicequalität (QoS)</block>
  <block id="534ecd446aac2f9c809eef9064f44aab" category="paragraph">Die maximale QoS-Durchsatzbegrenzung für ein Objekt kann in Megabit pro Sekunde und/oder IOPS festgelegt werden. Wenn beide verwendet werden, wird das erste erreichte Limit von ONTAP durchgesetzt. Ein Workload kann mehrere Objekte umfassen. Auf einen oder mehrere Workloads kann eine QoS-Richtlinie angewendet werden. Wird eine Richtlinie auf mehrere Workloads angewendet, teilen diese das in der Richtlinie zulässige Gesamtlimit. Geschachtelte Objekte werden nicht unterstützt (so können beispielsweise nicht jede Datei in einem Volume eine eigene Richtlinie aufweisen). QoS-Mindestwerte können nur als IOPS angegeben werden.</block>
  <block id="040f184b126879657b253b6d258661d9" category="paragraph">Derzeit sind folgende Tools für das Management von ONTAP QoS-Richtlinien und deren Anwendung auf Objekte verfügbar:</block>
  <block id="de134183bea5df515a6756a539ce6f18" category="list-text">CLI VON ONTAP</block>
  <block id="6ad5e0a20f49ad6a370454bb3afc9a9e" category="list-text">ONTAP System Manager</block>
  <block id="8e23e05fc0865e950d6a3581e0dd8ce9" category="list-text">OnCommand Workflow-Automatisierung</block>
  <block id="5ecb6b24c169667ff1a176768115659e" category="list-text">Active IQ Unified Manager</block>
  <block id="8cc4e713850f67a131e9dbf8b997f61e" category="list-text">NetApp PowerShell Toolkit für ONTAP</block>
  <block id="fcdb48d0d22627e4910a8352c80bd87a" category="list-text">ONTAP-Tools für VMware vSphere VASA Provider</block>
  <block id="e6defd03e0f49585b6d47614713e97f7" category="paragraph">Beachten Sie folgende Vorgaben, wenn Sie eine QoS-Richtlinie auf eine VMDK in NFS anwenden:</block>
  <block id="2175e0e29bc4b213fa8b9c7afd591b6b" category="list-text">Wenden Sie keine Richtlinien auf andere VM-Dateien wie virtuelle Swap-Dateien an <block ref="aa33c3c1850a3f99cec7426e2c411d08" prefix="(" category="inline-code"></block>).</block>
  <block id="8230e9b5c1e9862fedb2c69f7cc5d3d1" category="list-text">Wenn Sie Dateipfade mithilfe des vSphere Webclients ermitteln („Datastore“ &gt; „Files“), denken Sie daran, dass dieser die Informationen der zusammenfasst<block ref="f149d7424e10a266ec998ca5b32b81bb" prefix=" " category="inline-code"></block> Und<block ref="4a78be158c13cd6b2dc2100a80bb7070" prefix=" " category="inline-code"></block> Und zeigt einfach eine Datei mit dem Namen des an<block ref="4a78be158c13cd6b2dc2100a80bb7070" prefix=" " category="inline-code"></block> Aber die Größe der<block ref="f149d7424e10a266ec998ca5b32b81bb" prefix=" " category="inline-code"></block>. Zusatz<block ref="4fc4e556dbb9dcede037ccd80fabd784" prefix=" " category="inline-code"></block> In den Dateinamen, um den richtigen Pfad zu erhalten.</block>
  <block id="1d4614aca13104c98d5d8cc2d415b941" category="paragraph">Wenn Sie eine QoS-Richtlinie einschließlich VMFS und RDM einer LUN zuweisen möchten, können Sie die ONTAP SVM (angezeigt als „vServer“), den LUN-Pfad und die Seriennummer auf der ONTAP Tools für VMware vSphere Startseite aus dem Menü „Storage Systems“ abrufen. Wählen Sie das Storage-System (SVM) und anschließend „Related Objects“ &gt; „SAN“ aus.  Verwenden Sie diesen Ansatz, wenn Sie die QoS mit einem der ONTAP Tools angeben.</block>
  <block id="77bb2ba950959bd3f1c55da523ace0be" category="section-title">ONTAP QoS und VMware SIOC</block>
  <block id="5ad234cb2cde4266195252a23ca7d84e" category="cell">Eigenschaft</block>
  <block id="b8b8d1ba8fa345f03438a90f29af5784" category="cell">ONTAP-QoS</block>
  <block id="c2a4cb962db2a4a6782b69864f0e411c" category="cell">VMware SIOC</block>
  <block id="aa628d5a66888444926b4dc042458200" category="cell">Wenn aktiv</block>
  <block id="688b10fcfff927dee65634e3b3636064" category="cell">Richtlinie ist immer aktiv</block>
  <block id="a981c4aa67c29ff2d36fc614d8b28808" category="cell">Aktiv, wenn ein Konflikt besteht (Datastore-Latenz über Schwellenwert)</block>
  <block id="f46e64a82c96db56f3d6657d4fb53f00" category="cell">Einheiten</block>
  <block id="878b4223bb85b95d8caf0429d9406cd5" category="cell">IOPS, MB/Sek.</block>
  <block id="3e189a34f422a141311dad231f9ad5b5" category="cell">IOPS, Freigaben</block>
  <block id="7367a0ec46339213625cbc77d56a9572" category="cell">Umfang von vCenter oder Applikation</block>
  <block id="1faff19290bb64ce8ff6bc1220496881" category="cell">Mehrere vCenter Umgebungen, andere Hypervisoren und Applikationen</block>
  <block id="1e664a3cc0ab109fcabcc81b488cebb1" category="cell">Einzelner vCenter Server</block>
  <block id="ebccc9f5c939841d86e5382988dfcc47" category="cell">QoS auf VM festlegen?</block>
  <block id="cddd0980ce99890d2ea90288f6b03117" category="cell">VMDK nur auf NFS</block>
  <block id="928629d8a2a889f5274cad9940a06da0" category="cell">VMDK auf NFS oder VMFS</block>
  <block id="305e85b992089534091855224f60cf75" category="cell">QoS auf LUN festlegen (RDM)?</block>
  <block id="e798014243cf5682e7fb3aaf4ee7cecf" category="cell">QoS auf LUN festlegen (VMFS)?</block>
  <block id="9cd021ea9d3c88de723fe9a2e50a2380" category="cell">QoS auf Volume festlegen (NFS-Datastore)?</block>
  <block id="c352b53c32380efcf4436926da6d0414" category="cell">QoS auf SVM festlegen (Mandant)?</block>
  <block id="5dad340ee8f9e8fa9b65bf86a9fd5341" category="cell">Richtlinienbasierter Ansatz?</block>
  <block id="b31c76176f26196c6379a9bcce212d99" category="cell">Ja – kann von allen Workloads in der Richtlinie geteilt oder vollständig auf jeden Workload in der Richtlinie angewendet werden.</block>
  <block id="1ff33557bad923d68973872cacf2e43a" category="cell">Ja, mit vSphere 6.5 und höher.</block>
  <block id="d5f1465492190ee9d30a09b36f64f4b7" category="cell">Lizenz erforderlich</block>
  <block id="4cec76d82991abd453484c23f7e3788a" category="cell">In ONTAP enthalten</block>
  <block id="6bb36803f4670824ff9ebdf665654829" category="cell">Enterprise Plus</block>
  <block id="d4d4b5b65bebf236590d70c702a6ba6b" category="paragraph">VMware Storage Distributed Resource Scheduler (SDRS) ist eine Funktion von vSphere, die VMs auf Storage basierend auf der aktuellen I/O-Latenz und der Speicherplatznutzung platziert. Danach werden die VM oder VMDKs unterbrechungsfrei zwischen den Datastores in einem Datastore-Cluster (auch Pod genannt) verschoben und es wird der beste Datastore ausgewählt, in dem die VM oder die VMDKs im Datastore-Cluster platziert werden sollen. Ein Datastore-Cluster ist eine Sammlung ähnlicher Datastores, die aus Sicht des vSphere Administrators in einer einzigen Verbrauchseinheit aggregiert werden.</block>
  <block id="01e838e1c124042f75adb374a81f72a6" category="paragraph">VMware vSphere APIs for Storage Awareness (VASA) erleichtern einem Storage-Administrator die Konfiguration von Datastores mit klar definierten Funktionen. Der VM-Administrator kann sie zudem im Bedarfsfall jederzeit nutzen, um VMs bereitzustellen, ohne dass eine Interaktion stattfinden muss. Eine genauere Betrachtung dieses Ansatzes lohnt sich für Sie, wenn Sie feststellen möchten, wie er Ihre Storage-Virtualisierungsvorgänge optimieren und Ihnen viele banale Arbeiten ersparen kann.</block>
  <block id="0ce8b0e3e0d7dc7f39c62fb74a28b3bf" category="section-title">Cloud-Migration und -Backup</block>
  <block id="4fe11b4a91ff38ec9933009a15dd5b0b" category="paragraph">Eine weitere Stärke von ONTAP ist die umfassende Unterstützung für die Hybrid Cloud, bei der Systeme in Ihrer Private Cloud vor Ort mit Public-Cloud-Funktionen vereint werden. Im Folgenden sind einige NetApp Cloud-Lösungen aufgeführt, die gemeinsam mit vSphere verwendet werden können:</block>
  <block id="99c6c4d349151c1f4a6694e424aef741" category="inline-link">Speichern Sie mehr Snapshots Ihrer VMs</block>
  <block id="01fa297dbdab5aceb83a45a98161efe0" category="list-text">*FabricPool.* FabricPool bietet schnelles und einfaches Tiering für ONTAP Daten. Selten genutzte, „kalte“ Blöcke können zu einem Objektspeicher in Public Clouds oder zu einem privaten StorageGRID Objektspeicher migriert werden und beim erneuten Zugriff auf die ONTAP-Daten automatisch wieder abgerufen werden. Alternativ können Sie die Objekt-Tier als dritte Schutzebene für Daten verwenden, die bereits von SnapVault gemanagt werden. Dieser Ansatz kann Ihnen ermöglichen<block ref="60d027dbb3c3f076cbb70c6b246eb433" category="inline-link-rx"></block> Auf primären und/oder sekundären ONTAP-Storage-Systemen.</block>
  <block id="1a7072854c9dd945a98f4314b3f77408" category="list-text">*ONTAP Select.* mit softwaredefiniertem NetApp Storage erweitern Sie Ihre Private Cloud über das Internet auf Remote-Einrichtungen und Niederlassungen, in denen Sie ONTAP Select zur Unterstützung von Block- und Fileservices sowie denselben vSphere Datenmanagementfunktionen nutzen können, die Sie in Ihrem Unternehmens-Datacenter haben.</block>
  <block id="79aec220098057ef8c0e46281a45d6c2" category="paragraph">Ziehen Sie bei dem Entwurf Ihrer VM-basierten Applikationen zukünftige Cloud-Mobilität in Erwägung. Anstatt beispielsweise Applikations- und Datendateien gemeinsam zu platzieren, verwenden Sie einen separaten LUN- oder NFS-Export für die Daten. Damit können Sie VM und Daten getrennt zu Cloud-Services migrieren.</block>
  <block id="af63597853dc0983258c8f86a12aae0b" category="section-title">Verschlüsselung für vSphere Daten</block>
  <block id="63a07ab9352c9297cea3894d6cd41c3f" category="paragraph">Heute besteht eine wachsende Nachfrage, Daten im Ruhezustand durch Verschlüsselung zu sichern. Obwohl der Schwerpunkt anfänglich auf Informationen im Finanz- und Gesundheitswesen lag, gibt es ein zunehmendes Interesse an der Sicherung sämtlicher Informationen – seien sie in Dateien, Datenbanken oder in anderen Datentypen gesichert.</block>
  <block id="937a352e0f824ff55e4cd50c8e3b051e" category="paragraph">Für die Sicherung der Daten virtualisierter Applikationen unter VMware vSphere gibt es verschiedene Ansätze. Einer besteht darin, die Daten mit Software innerhalb der VM auf der Ebene des Gastbetriebssystems zu sichern. Alternativ dazu unterstützen neuere Hypervisoren wie vSphere 6.5 jetzt auch Verschlüsselung auf VM-Ebene. Die NetApp Softwareverschlüsselung ist jedoch eine einfache und bietet folgende Vorteile:</block>
  <block id="6eda79cc710edc32583cf567b00e7672" category="list-text">*Keine Auswirkung auf die virtuelle Server-CPU.* in einigen virtuellen Server-Umgebungen ist jeder verfügbare CPU-Zyklus für ihre Anwendungen erforderlich, aber Tests haben ergeben, dass bei Verschlüsselung auf Hypervisor-Ebene bis zu 5x CPU-Ressourcen benötigt werden. Selbst wenn die Verschlüsselungssoftware zur Verlagerung von Verschlüsselungs-Workloads den AES-NI Befehlssatz von Intel unterstützt (wie es bei der NetApp-Softwareverschlüsselung der Fall ist), ist dieser Ansatz aufgrund der Notwendigkeit neuer CPUs, die nicht mit älteren Servern kompatibel sind, unter Umständen nicht realisierbar.</block>
  <block id="6abee98aea26f85e6f0fe8d4db70f998" category="list-text">*Onboard Key Manager inbegriffen.* die NetApp Software-Verschlüsselung umfasst einen Onboard-Schlüsselmanager ohne zusätzliche Kosten und erleichtert den Einstieg ohne hochverfügbare Verschlüsselungsmanagement-Server, deren Erwerb und Nutzung ein hohes Maß an Komplexität mit sich bringt.</block>
  <block id="c216121021de8554d33503ef6616b256" category="list-text">*Keine Auswirkungen auf die Storage-Effizienz.* Storage-Effizienztechniken wie Deduplizierung und Komprimierung werden heute weit verbreitet und sind für eine kostengünstige Nutzung von Flash-Speicher von zentraler Bedeutung. Verschlüsselte Daten können in der Regel jedoch nicht dedupliziert oder komprimiert werden. Die Hardware- und Storage-Verschlüsselung von NetApp arbeitet auf niedrigerer Ebene und ermöglicht im Gegensatz zu anderen Ansätzen die vollständige Nutzung der branchenführenden NetApp Storage-Effizienzfunktionen.</block>
  <block id="5f9aa8333f218d1c21e67c8608a48672" category="list-text">*Einfache granulare Datastore-Verschlüsselung.* mit NetApp Volume Encryption erhält jedes Volume einen eigenen AES 256-Bit-Schlüssel. Wenn Sie diesen ändern müssen, müssen Sie dazu nur einen einzigen Befehl ausführen. Dieser Ansatz eignet sich ideal, wenn Sie mehrere Mandanten haben oder für unterschiedliche Abteilungen oder Apps eine unabhängige Verschlüsselung nachweisen müssen. Diese Verschlüsselung wird auf Datastore-Ebene gemanagt, was viel einfacher ist als das Management einzelner VMs.</block>
  <block id="de2b3baa1cd4980c36e8bbf7c827ae0c" category="paragraph">Die ersten Schritte mit Softwareverschlüsselung sind ganz einfach. Nach der Installation der Lizenz konfigurieren Sie einfach das Onboard-Verschlüsselungsmanagement, indem Sie eine Passphrase angeben und dann entweder ein neues Volume erstellen oder ein Storage-seitiges Volume verschieben, um die Verschlüsselung zu aktivieren. NetApp arbeitet daran, künftige Versionen seiner VMware Tools um zusätzliche integrierte Unterstützung von Verschlüsselungsfunktionen zu erweitern.</block>
  <block id="c95fa5b56a01e5c1a80c540c1ab1a750" category="paragraph">Active IQ Unified Manager bietet einen Überblick über die VMs in Ihrer virtuellen Infrastruktur und ermöglicht die Überwachung und Fehlerbehebung von Storage- und Performance-Problemen in Ihrer virtuellen Umgebung.</block>
  <block id="be75250c2f30870dc1e359b4ade2a767" category="paragraph">Eine typische Implementierung einer virtuellen Infrastruktur auf ONTAP setzt auf verschiedene Komponenten, die auf Computing-, Netzwerk- und Storage-Ebenen verteilt sind. Alle Performance-Einbußen bei einer VM-Applikation können aufgrund einer Kombination aus Latenzen auftreten, die bei den verschiedenen Komponenten auf den jeweiligen Ebenen auftreten.</block>
  <block id="d1734f6f2e65a1488896f74695db5254" category="paragraph">Der folgende Screenshot zeigt die Ansicht der virtuellen Active IQ Unified Manager Machines.</block>
  <block id="2118336fa447fcc0f213cb8c40c516af" category="paragraph">Unified Manager stellt das zugrunde liegende Untersystem einer virtuellen Umgebung in einer topologischen Übersicht vor, um zu ermitteln, ob beim Computing-Node, Netzwerk oder Storage ein Latenzproblem aufgetreten ist. Die Ansicht zeigt außerdem das spezifische Objekt, das aufgrund der Performance-Verzögerung Korrekturmaßnahmen ergreifen und das zugrunde liegende Problem lösen kann.</block>
  <block id="d8a78b994fcd4e04901203a2e7bc6414" category="paragraph">Der folgende Screenshot zeigt die erweiterte AIQUM-Topologie.</block>
  <block id="f02207fb26160d7e47cf5ea35d07df03" category="doc">Datenspeicher und Protokolle</block>
  <block id="a7c815fafe60ae637d38216fa6300395" category="list-text">FCP</block>
  <block id="14eae6ee278098bfad4f8a10fd6c1195" category="list-text">FCoE</block>
  <block id="26bcb9f1cb6f391f4b2c18e676bb458d" category="list-text">NVMe/FC</block>
  <block id="8b7c42123642e2e4ea7dd426aeb2de58" category="list-text">NVMe/TCP</block>
  <block id="e4e1c13bb0b14f6cb7608cbecea948ef" category="list-text">ISCSI</block>
  <block id="f7d773fd2896401c143676940bc001fc" category="list-text">NFS v3</block>
  <block id="0b99b245267ef194ca3b9f6e694b0983" category="list-text">NFS 4.1</block>
  <block id="cfa070ed8af17c3125cfd1530fbd387a" category="paragraph">FCP, FCoE, NVMe/FC, NVMe/TCP und iSCSI sind Blockprotokolle. VMware Datastores werden über das vSphere Virtual Machine File System (VMFS) gespeichert, um VMs innerhalb von ONTAP LUNs oder NVMe Namespaces zu speichern, die in einem ONTAP FlexVol Volume enthalten sind. Beachten Sie, dass VMware ab vSphere 7.0 keine Software FCoE mehr in Produktionsumgebungen unterstützt. NFS ist ein File-Protokoll. Hierbei werden die Datastores nicht zusätzlich mit VMFS formatiert. VMs laufen direkt auf dem ONTAP Volume. SMB (CIFS), iSCSI, NVMe/TCP oder NFS kann direkt aus einem Gastbetriebssystem für ONTAP genutzt werden.</block>
  <block id="2b19cb432f68f1ba3dc7b0f175a14778" category="inline-link">Maximalwerte für die VMware Konfiguration</block>
  <block id="f8ca1b19553d4c965e40e9eeb1eb5aca" category="paragraph">In der folgenden Tabelle sind die Funktionen herkömmlicher Datastores dargestellt ONTAP, die von vSphere unterstützt werden. Diese Informationen gelten nicht für VVols Datastores, sie gelten jedoch im Allgemeinen für vSphere 6.x bzw. neuere Versionen, bei denen unterstützte ONTAP Versionen verwendet werden. Sie können sich auch beraten<block ref="61b579b455015dfa2bbf16bf62f07f93" category="inline-link-rx"></block> Bestätigen Sie für bestimmte vSphere Versionen bestimmte Limits.</block>
  <block id="9da73946f1bc3be4a41898d06c9d2e8b" category="cell">Funktion/Feature</block>
  <block id="c982f804d02e2d7e937f125d2cac1024" category="cell">FC/FCoE</block>
  <block id="6151632541dcd80356c6c76b34d69d0c" category="cell">NVMe-of</block>
  <block id="520d0db389f362bf79ef56ca0af3dcab" category="cell">Formatieren</block>
  <block id="99a3142584ca8ad0a3afadeaa6f2ef69" category="cell">VMFS oder Raw Device Mapping (RDM)</block>
  <block id="26aae26f61844ac20ab7b04ba6f5c515" category="cell">VMFS oder RDM</block>
  <block id="18263a30df8afc60a733e59c60676ac0" category="cell">VMFS</block>
  <block id="382b0f5185773fa0f67a8ed8056c7759" category="cell">K. A.</block>
  <block id="8dc6d0c66219ddb809322c0d248e55e6" category="cell">Maximale Anzahl an Datastores oder LUNs</block>
  <block id="aa23f9914ce1939ea7a4d479edfdc915" category="cell">1024 LUNs pro Host</block>
  <block id="1af3b27207afc48a363629a315d76b59" category="cell">1024 LUNs pro Server</block>
  <block id="0705fd627c25b5fba17cd9a022ed7d72" category="cell">256 Namespeces pro Server</block>
  <block id="2774ddb6f193789c5d9a480b9e53a38d" category="cell">256 Halterungen
Standard-NFS. MaxVolumes ist 8. Erhöhen Sie mit den ONTAP Tools für VMware vSphere auf 256.</block>
  <block id="51c0dc217976e27de06e262947947a62" category="cell">Maximale Datastore-Größe</block>
  <block id="856c997f909830b8249756c07dc9452b" category="cell">64 TB</block>
  <block id="8e5d3a4b085389c59a1a7af7c4067a9b" category="cell">100 TB FlexVol Volume oder mehr mit FlexGroup Volume</block>
  <block id="b4fb1d41fedaec79072964cbb4d16e3d" category="cell">Maximale Datastore-Dateigröße</block>
  <block id="d10549beb2f7bd1e91e5ef8965edd84a" category="cell">62 TB</block>
  <block id="18301e675ce7d51c23071d7b135edbdc" category="cell">62 TB mit ONTAP 9.12.1P2 und höher</block>
  <block id="9aa56a79640231adaec83bc53e59c929" category="cell">Optimale „Queue depth“ pro LUN oder Filesystem</block>
  <block id="c7dfde106afbb4e1d55096403af252e0" category="cell">64-256</block>
  <block id="ca6e77250a239af7b4aed9a1ac058825" category="cell">Autonegotiation Ist Eingeschaltet</block>
  <block id="edc5121cb9b42a76b718b8ece9d67437" category="paragraph">In der folgenden Tabelle sind die unterstützten Funktionen in Bezug auf VMware Storage aufgeführt.</block>
  <block id="6c1a84b789b54fd42511ce582be94d21" category="cell">VMotion</block>
  <block id="e95b58d02782c970bb339c6cc587ff39" category="cell">Storage vMotion</block>
  <block id="0486e0e3dc3e86859f43b6d8e32b8071" category="cell">VMware HA</block>
  <block id="8be6bd7f20474cbefdd496cb9fd495ca" category="cell">Storage Distributed Resource Scheduler (SDRS)</block>
  <block id="4244919a64848265426dfd82a14f8248" category="cell">VMware vStorage APIs for Data Protection (VADP)-fähige Backup-Software</block>
  <block id="28c2f8ab9cdbf296aa4261bc3c58ba0d" category="cell">Microsoft Cluster Service (MSCS) oder Failover Clustering in einer VM</block>
  <block id="01a9a3b8d0fcae3055dc91935e8ec6c6" category="cell">Ja*</block>
  <block id="aa7f77e663b832d5b0e544c5511e680c" category="cell">Nicht unterstützt</block>
  <block id="aa18abedec09bd4f85e612c307e2eaa6" category="cell">Fehlertoleranz</block>
  <block id="bfd52522de4ac6efd6f680e0e754eeb5" category="cell">Site Recovery Manager</block>
  <block id="d6407966cf588d8d78d4f7e65f1ea6fd" category="cell">Nur V3**</block>
  <block id="17d495427086fa21259b9df40b27e00a" category="cell">VMs (virtuelle Festplatten) mit Thin Provisioning</block>
  <block id="4ee7af004efef335d6a14c60ee758f5e" category="cell">Ja.
Diese Einstellung ist der Standard für alle VMs im NFS, wenn nicht VAAI verwendet wird.</block>
  <block id="13bc264ff420559de4156ec40980a4b9" category="cell">Natives VMware Multipathing</block>
  <block id="50ed43e1a7a22335069a344bc706b600" category="cell">Ja, Verwendung des neuen High Performance Plug-ins (HPP)</block>
  <block id="a06819d91a165d0491c5816c54b41234" category="paragraph">In der folgenden Tabelle werden die unterstützten ONTAP Storage-Managementfunktionen aufgeführt.</block>
  <block id="f8d61013f1a3bbf1342f4ced3279c7cf" category="cell">Datendeduplizierung</block>
  <block id="72167540968147afed9deb59f0e9fe00" category="cell">Einsparungen im Array</block>
  <block id="9a5473a6abaea16b736f096913a749be" category="cell">Einsparungen im Datastore</block>
  <block id="6cd880eabbec3488232c6cba3fbcf998" category="cell">Thin Provisioning</block>
  <block id="5caeb11c4ed379b808d7f7cdca7cfbf0" category="cell">Datenspeicher oder RDM</block>
  <block id="8a5227cc82d14862956938caffc1acf2" category="cell">Datenspeicher</block>
  <block id="b71ba22dcd42deceac87150206f7bd12" category="cell">Datenspeichergröße ändern</block>
  <block id="c6b792bfd7aac8067d36337e67d11545" category="cell">Erweitern Sie nur</block>
  <block id="29e23534878bb63341e0b689426b7fb0" category="cell">Vergrößerung, Autogrow und Verkleinerung</block>
  <block id="76021405cc62a4b8c542e7f65e3d24ed" category="cell">SnapCenter Plug-ins für Windows, Linux Applikationen (in Gast-BS)</block>
  <block id="c0b3c629432c82a9e8500242abb35eaf" category="cell">Monitoring und Host-Konfiguration mit ONTAP Tools für VMware vSphere</block>
  <block id="5a915a8215e9a66bcff0f034e8adff18" category="cell">Bereitstellung mit ONTAP Tools für VMware vSphere</block>
  <block id="6495c7cda42bf1b80bc1c2af6166ef58" category="paragraph">In der folgenden Tabelle sind die unterstützten Backup-Funktionen aufgeführt.</block>
  <block id="ad336d1fccea3e918d07055edac855a3" category="cell">ONTAP Snapshots</block>
  <block id="bf22a1b5bc2b72a75825094826a942de" category="cell">Durch replizierte Backups unterstütztes SRM</block>
  <block id="60d3f7d9f265eb34e826da352cb545be" category="cell">Volume SnapMirror</block>
  <block id="fabf31a57c142c986c5d88cb089b3d7b" category="cell">VDMK Image-Zugriff</block>
  <block id="40b655e79545b3922b8095be28d59428" category="cell">VADP fähige Backup-Software</block>
  <block id="8215e73d220182794dfbd75ad494c727" category="cell">VADP fähige Backup-Software, vSphere Client und vSphere Web Client Datastore-Browser</block>
  <block id="d7605580ecad0fcb4528789d74cdcad5" category="cell">VDMK-Zugriff auf Dateiebene</block>
  <block id="906ece32da286d6c4b323bc0d6443b7c" category="cell">VADP fähige Backup-Software, nur Windows</block>
  <block id="d17e2fa63a62622f88bf170612132383" category="cell">VADP fähige Backup-Software und Applikationen von Drittanbietern</block>
  <block id="4e3d259d82a536e12c5c9c948b62e26a" category="cell">NDMP-Granularität</block>
  <block id="90d45e02e50c81603c36a4e4ad3649d9" category="cell">Datastore oder VM</block>
  <block id="8339c514e73f5cced3b83d504d60441b" category="inline-link">Einrichtung für Windows Server Failover Clustering</block>
  <block id="45f7e6f7f6f0f4093754d7d48810e17f" category="paragraph">*NetApp empfiehlt für Microsoft Cluster die Verwendung von in-Guest iSCSI anstelle von Multiwriter-fähigen VMDKs in einem VMFS Datastore. Dieser Ansatz wird von Microsoft und VMware vollständig unterstützt. Er bietet mit ONTAP ein hohes Maß an Flexibilität (SnapMirror auf ONTAP Systeme vor Ort oder in der Cloud), lässt sich leicht konfigurieren und automatisieren und kann mit SnapCenter gesichert werden. In vSphere 7 wurde eine neue Clustered VMDK-Option hinzugefügt. Dies unterscheidet sich von VMDKs mit mehreren Schreibenden, die einen Datenspeicher benötigen, der über das FC-Protokoll bereitgestellt wird, für das die Unterstützung für geclusterte VMDK aktiviert ist. Weitere Einschränkungen sind möglich. VMware's ansehen<block ref="b158594c147457b5b6417413cb30f800" category="inline-link-rx"></block> Dokumentation für Konfigurationsrichtlinien.</block>
  <block id="e2bd40041a6472b580305ce9017b0b2e" category="paragraph">**Datastores mit NVMe-of und NFS v4.1 erfordern die vSphere Replizierung. Array-basierte Replizierung wird von SRM nicht unterstützt.</block>
  <block id="54b3e11ce37eaaa9884f4086e72dd2be" category="section-title">Auswahl eines Storage-Protokolls</block>
  <block id="0212b4e8bc965dcd8a7ff771dd96bf21" category="paragraph">Die folgenden Faktoren könnten bei Überlegungen zur Auswahl eines Protokolls hilfreich sein:</block>
  <block id="0425ddb6dbbe7f13280e19e1f925b0a8" category="list-text">*Gegenwärtige Kundenumgebung.* Obwohl IT-Teams normalerweise erfahren sind, um Ethernet IP-Infrastrukturen zu managen, sind nicht alle erfahren im Management einer FC SAN Fabric. Die Nutzung eines nicht auf Storage-Traffic ausgelegten dedizierten IP-Netzwerks ist jedoch unter Umständen keine gute Lösung. Berücksichtigen Sie Ihre vorhandene Netzwerkinfrastruktur, alle geplanten Optimierungen sowie die Fähigkeiten und die Verfügbarkeit von Mitarbeitern, die diese managen.</block>
  <block id="774f1348caf3e145710073eb634c4fa1" category="list-text">*Einfache Einrichtung.* über die Erstkonfiguration der FC-Fabric hinaus (zusätzliche Switches und Kabel, Zoning und die Verifizierung der Interoperabilität von HBA und Firmware) müssen Blockprotokolle auch LUNs erstellen und zuordnen sowie vom Gastbetriebssystem Erkennung und Formatierung vornehmen. Nach der Erstellung und dem Export der NFS-Volumes werden sie vom ESXi Host gemountet und sind dann betriebsbereit. Für NFS sind keine besonderen Hardwarequalifizierungen oder Firmware für das Management erforderlich.</block>
  <block id="bf5169ce5bb544313eaf17435f756da2" category="list-text">*Einfaches Management.* bei SAN-Protokollen sind bei Bedarf mehrere Schritte erforderlich, darunter das Vergrößern einer LUN, das erneute Erkennen der neuen Größe und das Anwachsen des Dateisystems). Obwohl eine LUN vergrößert werden kann, ist eine Reduzierung der Größe einer LUN nicht möglich. Auch das Recovery von ungenutztem Speicherplatz kann weiteren Aufwand bedeuten. NFS ermöglicht eine problemlose Größenanpassung, die durch das Storage-System automatisiert werden kann. SAN bietet über TRIM/UNMAP-Befehle des Gast-Betriebssystems eine Speicherplatzrückgewinnung, sodass Speicherplatz aus gelöschten Dateien an das Array zurückgegeben werden kann. Diese Art der Rückgewinnung von ungenutztem Speicherplatz ist bei NFS-Datenspeichern schwieriger.</block>
  <block id="530c759326761c6ac026b313985b255f" category="list-text">*Storage-Speicherplatztransparenz.* die Storage-Auslastung ist in NFS-Umgebungen in der Regel einfacher zu erkennen, da Thin Provisioning unmittelbare Einsparungen ermöglicht. In ähnlicher Form sind Einsparungen durch Deduplizierung und Klonen unmittelbar für andere VMs im selben Datastore oder für Storage-System-Volumes verfügbar. Die VM-Dichte ist typischerweise ebenfalls größer als in einem NFS-Datastore. Hierdurch können höhere Einsparungen bei der Deduplizierung sowie eine Senkung der Managementkosten erzielt werden, da weniger Datastores gemanagt werden müssen.</block>
  <block id="16ee928b12bc598bf52b0f312879ec95" category="section-title">Datenspeicher-Layout</block>
  <block id="b8c499c7b537f5bd4e51b0a6d6a1e9d0" category="list-text">Der Einsatz von vSphere mit ONTAP-NFS-Datastores sorgt für eine hochperformante, einfach zu managende Implementierung mit VM/Datastore-Verhältnissen, die mit blockbasierten Storage-Protokollen nicht erreicht werden können. Diese Architektur kann zu einer Verzehnfachung der Datastore-Dichte und einer damit korrelierenden Verringerung der Datastore-Anzahl führen. Obwohl ein größerer Datastore die Storage-Effizienz begünstigen und betriebliche Vorteile bieten ONTAP kann, sollten Sie mindestens vier Datastores (FlexVol Volumes) verwenden. Durch die Verteilung der Datastores auf die Controller kann so die bestmögliche Ausnutzung der Hardware gewährleistet werden. Mit diesem Ansatz können Sie auch Datastores mit unterschiedlichen Recovery-Richtlinien erstellen. Einige können je nach den geschäftlichen Anforderungen häufiger gesichert oder repliziert werden als andere. Da FlexGroup Volumes eine Skalierung pro Design durchführen, sind für mehrere Datastores nicht erforderlich.</block>
  <block id="e15ed0ec7af4278259b7618024b3e7ce" category="list-text">NetApp empfiehlt die Verwendung von FlexVol Volumes für die meisten NFS-Datastores. Ab ONTAP 9.8 werden FlexGroup Volumes auch für die Nutzung als Datastores unterstützt und für bestimmte Anwendungsfälle im Allgemeinen empfohlen. Andere ONTAP Storage-Container wie qtrees werden im Allgemeinen nicht empfohlen, da diese derzeit weder durch ONTAP Tools für VMware vSphere noch durch das NetApp SnapCenter Plug-in für VMware vSphere unterstützt werden. Indessen könnte die Implementierung von Datastores als mehrere qtrees in einem einzelnen Volume in hoch automatisierten Umgebungen nützlich sein, die von Kontingenten auf Datastore-Ebene oder VM-Dateiklonen profitieren können.</block>
  <block id="dc4d78be836807b32c7ef7f42028b1ef" category="list-text">Eine gute Größe für einen FlexVol Volume-Datastore liegt bei etwa 4 TB bis 8 TB. Diese Größe bildet einen guten Ausgleichspunkt im Hinblick auf Performance, einfaches Management und Datensicherung. Beginnen Sie mit einem kleinen Datastore (beispielsweise 4 TB) und vergrößern Sie diesen nach Bedarf (bis auf maximal 100 TB). Kleinere Datenspeicher lassen sich nach einem Backup oder nach einem Ausfall schneller wiederherstellen und können schnell im Cluster verschoben werden. Die automatische Größenanpassung von ONTAP kann sinnvoll sein, um das Volume bei wechselnder Speicherplatzbelegung automatisch zu vergrößern oder zu verkleinern. Der ONTAP Tools für die Bereitstellung von VMware vSphere Datastores verwendet Autosize standardmäßig für neue Datastores. Eine weitere Anpassung der Vergrößerungs- und Verkleinerungsschwellenwerte sowie der maximalen und minimalen Größe kann mit System Manager oder über die Befehlszeile erfolgen.</block>
  <block id="964a81e46088ae1cc6306072464a9d73" category="list-text">Alternativ können VMFS Datastores mit LUNs konfiguriert werden, auf die über FC, iSCSI oder FCoE zugegriffen wird. Bei VMFS können alle ESX Server in einem Cluster gleichzeitig auf herkömmliche LUNs zugreifen. VMFS Datastores können eine Größe von bis zu 64 TB haben und bestehen aus bis zu 32 2TB LUNs (VMFS 3) oder einer einzelnen 64-TB-LUN (VMFS 5). Die maximale LUN-Größe von ONTAP beträgt auf den meisten Systemen 16 TB und 128 TB auf All-SAN-Array-Systemen. Daher kann auf den meisten ONTAP Systemen ein VMFS 5 Datastore mit maximaler Größe aus vier 16-TB-LUNs erstellt werden. Für Workloads mit hohem I/O-Aufkommen und mehreren LUNs (bei High-End FAS oder AFF Systemen) können Performance-Vorteile zum Tragen kommen, allerdings werden diese durch das komplexere Management beim Erstellen, Managen und Sichern der Datastore-LUNs und ein erhöhtes Verfügbarkeitsrisiko ausgeglichen. NetApp empfiehlt im Allgemeinen, eine einzelne, große LUN für jeden Datastore zu verwenden. Und nur im Ausnahmefall, wenn größere Datastores mit über 16 TB gebraucht werden, mit Extends zu arbeiten. Analog zu dem NFS Ansatz, verteilen ONTAP Sie ebenfalls die Datastores über die Controller, um die bestmögliche Performance zu erzielen.</block>
  <block id="c74cab014e402128efd8102b941c0b0c" category="list-text">Ältere Gastbetriebssysteme (OS) mussten an das Storage-System angeglichen werden (Alignment), um die bestmögliche Performance und Storage-Effizienz zu erzielen. Bei modernen Betriebssystemen mit Anbieterunterstützung von Microsoft und Linux Distributoren wie Red hat sind jedoch keine Anpassungen mehr erforderlich, um die Filesystem-Partition mit den Blöcken des zugrunde liegenden Storage-Systems in einer virtuellen Umgebung zu alignen. Wenn Sie ein altes Betriebssystem verwenden, für das unter Umständen ein Alignment erforderlich ist, suchen Sie in der NetApp Support Knowledgebase nach Artikeln, in denen das Thema VM Alignment behandelt wird, oder fordern Sie bei einem NetApp Ansprechpartner für den Vertrieb oder für Partner ein Exemplar des technischen Berichts TR-3747 an.</block>
  <block id="8c761039ef4dd69451490b849ace1f82" category="list-text">Vermeiden Sie die Verwendung von Defragmentierungsprogrammen innerhalb des Gast-Betriebssystems, da dies keinen Performance-Vorteil bietet und die Speichereffizienz und Snapshot-Speicherplatznutzung beeinträchtigt. Zudem sollten Sie die Suchindizierung im Gastbetriebssystem für virtuelle Desktops deaktivieren.</block>
  <block id="14abb814748566a24367c4459a54840b" category="list-text">ONTAP ist eines der branchenweit führenden Unternehmen mit innovativen Storage-Effizienzfunktionen, mit denen Sie Ihren nutzbaren Festplattenspeicherplatz maximal ausschöpfen können. AFF Systeme sind durch Inline-Deduplizierung und -Komprimierung sogar noch effizienter. Die Daten werden über alle Volumes hinweg in einem Aggregat dedupliziert. Daher müssen zur Maximierung der Einsparungen keine ähnlichen Betriebssysteme und ähnlichen Applikationen in einem einzelnen Datastore mehr gruppieren.</block>
  <block id="446548f2b1301893641e29657fc7fe53" category="inline-link-macro">Oracle-Datenbanken auf ONTAP</block>
  <block id="6cda576d2fd323c1a728b7ac67d6034f" category="list-text">Festplatten der ersten Klasse (oder verbesserte virtuelle Festplatten) ermöglichen über vCenter gemanagte Festplatten unabhängig von einer VM mit vSphere 6.5 und höher. Sie werden zwar primär durch API gemanagt, sind aber auch mit VVols nützlich, insbesondere bei dem Management mit OpenStack oder Kubernetes-Tools. Sie werden von ONTAP unterstützt sowie ONTAP Tools für VMware vSphere.</block>
  <block id="71318121439b39fdf872bb0bd57494f2" category="section-title">Datastore und VM-Migration</block>
  <block id="e8824ce2062c6d1fc536163f7b8940bd" category="paragraph">Wenn Sie VMs aus einem bestehenden Datastore in einem anderen Storage-System zu ONTAP migrieren, sollten Sie die folgenden Praktiken berücksichtigen:</block>
  <block id="73b729fbc964c7d78dd90b64aaaeb785" category="list-text">Verwenden Sie Storage vMotion, um den Großteil Ihrer Virtual Machines in ONTAP zu verschieben. Dieser Ansatz ermöglicht nicht nur einen unterbrechungsfreien Betrieb der VMs, sondern auch die Nutzung von ONTAP Storage-Effizienzfunktionen wie Inline-Deduplizierung und -Komprimierung zur Verarbeitung der Daten während der Migration. Es empfiehlt sich unter Umständen, mithilfe von vCenter Funktionen mehrere VMs aus der Bestandsliste auszuwählen und die Migration dann zu einem geeigneten Zeitpunkt zu planen (dazu klicken Sie mit gedrückter Strg-Taste auf „Actions“).</block>
  <block id="e512c890754c686d8deccac97d84a290" category="list-text">Sie können eine Migration auf geeignete Ziel-Datastores zwar genau planen, doch es ist oft einfacher, große Datenmengen zu migrieren und diese anschließend nach Bedarf zu organisieren. Vielleicht möchten Sie diesen Ansatz nutzen, um Ihre Migration in verschiedene Datastores zu steuern, wenn Sie spezielle Datensicherungsanforderungen, z. B. unterschiedliche Snapshot Zeitpläne, haben.</block>
  <block id="616b2180a0659911fed5aa758dba722c" category="list-text">Die meisten VMs und deren Storage können im Betrieb (eingeschalteter Zustand) migriert werden. Attached Storage (nicht im Datastore) – beispielsweise in Form von ISOs, LUNs oder NFS-Volumes – aus einem anderen Storage-System muss jedoch unter Umständen im ausgeschalteten Zustand migriert werden.</block>
  <block id="9fb2cc2ced88ce872ef16c37d8284360" category="paragraph">Das Plug-in hilft Ihnen auch bei der Nutzung anderer ONTAP Tools in vSphere Umgebungen. Damit können Sie das NFS-Plug-in für VMware VAAI installieren, das einen Copy-Offload zu ONTAP für VM-Klonvorgänge, eine Speicherplatzreservierung für Thick Virtual Disk Files und ONTAP Snapshot Offload ermöglicht.</block>
  <block id="9baea886f208a41896164d4ade5b3fde" category="paragraph">Im Allgemeinen empfiehlt NetApp zur Bereitstellung herkömmlicher und VVols Datastores die Verwendung der ONTAP Tools für die Schnittstelle VMware vSphere in vCenter, um die Einhaltung von Best Practices sicherzustellen.</block>
  <block id="6161d374e9022f89382fd63b4be15aa1" category="section-title">Allgemeines Networking</block>
  <block id="c7949782ad094d3ffc126bee722642a3" category="list-text">Verwenden Sie Switches, die die Link-Aggregation von Ports in zwei separaten Switch-Chassis durch einen Ansatz mit einer Multi-Chassis-Link-Aggregationsgruppe wie Virtual PortChannel (vPC) von Cisco unterstützen.</block>
  <block id="9dc1904e0c3ace141704b477cd9b345d" category="inline-link">Netzwerkmanagement</block>
  <block id="68ecdb0d7a96e68318a53d12e9fab5b5" category="list-text">Erstellen Sie mit LACP Link-Aggregate für ONTAP Storage-Systeme mit dynamischen Multimode-Schnittstellengruppen mit Port- oder IP-Hash. Siehe<block ref="b87480ad0ff34784c4be1445a72a3aaf" category="inline-link-rx"></block> Für weitere Hinweise.</block>
  <block id="3b407fe7c9654197902b1dd26af6dc81" category="list-text">Verwenden Sie eine IP-Hash-Teaming-Richtlinie für ESXi bei Verwendung von statischer Link-Aggregation (z. B. EtherChannel) und Standard-vSwitches oder LACP-basierter Link-Aggregation mit vSphere Distributed Switches. Wenn die Link-Aggregation nicht verwendet wird, verwenden Sie stattdessen „Weiterleiten basierend auf der ursprünglichen virtuellen Port-ID“.</block>
  <block id="ee1ef6cf0f3971b44eb2abcac958fb8d" category="section-title">FlexGroup Volumes</block>
  <block id="dd9bd4a686bdeb1c7f11dc3cd0cda75f" category="paragraph">Betrachten wir das folgende Szenario:</block>
  <block id="6b277580cb59b13790b36344e827e22f" category="list-text">Sie haben eine neue FlexGroup mit 8 Komponenten erstellt</block>
  <block id="32efd8b10cc3b73c344f4c7e6c4b741e" category="list-text">Das Cache-Zeitlimit für die neue FlexGroup ist auf 160 Minuten festgelegt</block>
  <block id="879f959734466d50681eae9b95e7e591" category="paragraph">In diesem Szenario sind die ersten 8 Klone vollständig vollständige Kopien anstatt lokale Dateiklone. Für jedes weitere Klonen dieser VM vor Ablauf der 160-Sekunden-Zeitüberschreitung wird die Datei-Klon-Engine innerhalb jeder Komponente nach dem Round-Robin-Verfahren verwendet, um nahezu sofortige Kopien zu erstellen, die gleichmäßig über die einzelnen Volumes verteilt sind.</block>
  <block id="0f98175ace2dd0d56e47961b22b2544b" category="paragraph">In Umgebungen, in denen Unternehmen nicht alle Vorteile des FlexGroup Cache ausschöpfen können, aber trotzdem schnelles standortübergreifendes Klonen benötigen, ist die Verwendung von VVols eine erwägen. Das Volume-übergreifende Klonen mit VVols erfolgt viel schneller als bei herkömmlichen Datastores und ist nicht auf einen Cache angewiesen.</block>
  <block id="189aae773e13462525f07e99fcd9e90f" category="inline-link">VAAI: Wie funktioniert Caching mit FlexGroup Volumes?</block>
  <block id="718103d957da7fb9e8210a49a85aedab" category="paragraph">Weitere Informationen zur Verwendung von FlexGroups mit VAAI finden Sie in diesem KB-Artikel:<block ref="23db52497ce446299e31ecc0b7572649" category="inline-link-rx"></block></block>
  <block id="ce4df831657d2621e80bbef9271c33cd" category="summary">Seit dem Übergang von der älteren virtuellen Appliance bieten ONTAP Tools zahlreiche neue Funktionen, höhere Limits und neue VVols Unterstützung.</block>
  <block id="45b1565c472fff4046d0f7ddbe1342f2" category="doc">Neue Funktionen mit SRM und ONTAP Tools</block>
  <block id="116d20fc387d73c8e4a86e7998913947" category="section-title">Aktuelle Versionen von vSphere und Site Recovery Manager</block>
  <block id="57359b8d2b77ce83f2e854fdd323377f" category="paragraph">Mit der Veröffentlichung von SRM 8.7 und höher sowie mit den Versionen 9.12 und höher von ONTAP Tools sind Sie nun in der Lage, VMs zu schützen, die auf VMware vSphere 8 Update 1 ausgeführt werden.</block>
  <block id="96ca6fcc2ffdab3c06d0875be8d4fe29" category="paragraph">NetApp verbindet seit nahezu zwei Jahrzehnten eine enge Partnerschaft mit VMware und ist bestrebt, die neuesten Versionen so schnell wie möglich zu unterstützen. Aktuelle Kombinationen der Software sind im NetApp Interoperabilitäts-Matrix-Tool (IMT) verfügbar.</block>
  <block id="4ada3eb5b908caaedb757052562dfcf5" category="inline-link-macro"><block ref="4ada3eb5b908caaedb757052562dfcf5" category="inline-link-rx"></block></block>
  <block id="0a0a2a299629bed81a283fcd3b7833e9" category="paragraph">Die NetApp IMT finden Sie unter <block ref="83e08b5e992bc41d59feec38bb24f26d" category="inline-link-macro-rx"></block>.</block>
  <block id="9d6e1631f68d52fd0f82222bf276cdc4" category="section-title">Unterstützung für VVols (und darum, warum Storage Policy Based Management (SPBM) wichtig ist, selbst mit SRM)</block>
  <block id="b0b7bc3c901f340c708b7560d1f2b7e1" category="paragraph">Ab Version 8.3 unterstützt SRM jetzt das auf Storage Policy Based Management (SPBM) der Replizierung mit VVols und Array-basierte Replizierung für Datastores mit iSCSI, FCP und NFS v3. Dazu wurde der SRM-Server aktualisiert und um einen neuen SRM VVols Provider-Service erweitert, der mit dem SMS-Service des vCenter Servers für VASA-bezogene Aufgaben kommuniziert.</block>
  <block id="4567f5576b4577c63b126cac3f6aa5fa" category="paragraph">Ein Vorteil dieser Architektur besteht darin, dass SRA nicht mehr benötigt wird, da alles mit VASA behandelt wird.</block>
  <block id="e04031e1cecdb6732f91d44b204dbcf3" category="paragraph">SPBM ist ein leistungsstarkes Tool in der vSphere Toolbox und bietet vereinfachte, vorhersehbare und konsistente Storage-Services für die Nutzung durch Automatisierungs-Frameworks in Private- und Hybrid-Cloud-Umgebungen. Im Grunde können Sie mit SPBM Serviceklassen definieren, die die Anforderungen Ihres vielfältigen Kundenstamms erfüllen. Mit SRM können Sie Ihren Kunden jetzt Replizierungsfunktionen für kritische Workloads bereitstellen, die eine robuste Disaster-Recovery-Orchestrierung und -Automatisierung erfordern.</block>
  <block id="1b6204fa76bd422416757c4e2d6187bd" category="paragraph">Beispiel für eine VVols-Architektur mit FCP oder iSCSI:</block>
  <block id="6f1ad7cac2af0e3b92a3936efc393a0e" category="paragraph"><block ref="6f1ad7cac2af0e3b92a3936efc393a0e" category="inline-image-macro-rx" type="image"></block></block>
  <block id="9abbd7aed7e9791d9c52e8e17f34a756" category="section-title">Unterstützung von Appliance-basierten SRM Servern</block>
  <block id="925c477f139cce0692e13946a7f75eac" category="paragraph">Photon OS-basierte SRM-Server werden jetzt zusätzlich zu älteren Windows-basierten Plattformen unterstützt.</block>
  <block id="62c2c4203aebe081ddbc6cdd489f8ad7" category="paragraph">Sie können nun SRA Adapter unabhängig vom bevorzugten SRM-Servertyp installieren.</block>
  <block id="7cb1700928c008eb817bc79250cc9002" category="section-title">IPv6 wird unterstützt</block>
  <block id="ea9aacd1495949730de8eeab11e66459" category="paragraph">IPv6 wird jetzt mit folgenden Einschränkungen unterstützt:</block>
  <block id="d6d7f3d43de975c07655ebba1936867a" category="list-text">VCenter 6.7 oder höher</block>
  <block id="eb26ee1a20fe3dbde98204ec8b3ab838" category="list-text">Nicht unterstützt mit SRM 8.2 (8.1, 8.3 und 8. 4 werden unterstützt)</block>
  <block id="918d32b75f0ab883abba3a8dc0c3ae8d" category="inline-link">Interoperabilitäts-Matrix-Tool</block>
  <block id="02e7e71042609d3158020ab7befe45c6" category="list-text">Prüfen Sie die<block ref="218f4ad7153f69cdc5467065434cd2f0" category="inline-link-rx"></block> Für die neuesten qualifizierten Versionen.</block>
  <block id="e887aed8f9c3dbecd37e8896d96b6637" category="section-title">Verbesserte Performance</block>
  <block id="2a71574e89a69c8218fbac3f604b5728" category="paragraph">Die betriebliche Performance ist eine wichtige Anforderung für die Ausführung von SRM-Aufgaben. Um die Anforderungen moderner RTOs und RPOs zu erfüllen, wurden für die SRA mit ONTAP Tools drei neue Verbesserungen hinzugefügt.</block>
  <block id="bd403c539a0c414afacd7477bf3d311f" category="list-text">*Unterstützung für gleichzeitige Reprotect-Vorgänge.* Diese Funktion wurde erstmals in SRA 9.7 eingeführt und ermöglicht die gleichzeitige Ausführung von Reprotect auf zwei oder mehr Recovery-Plänen. Dadurch verringert sich der Zeitaufwand für die erneute Sicherung von Datastores nach einem Failover oder einer Migration und bleibt innerhalb der RTO- und RPO-Parameter.</block>
  <block id="f63b947f655a5aee2cf69d4f30d7f96b" category="list-text">*ONTAP Tools 9.8 fügt einen neuen nur für NAS optimierten Modus hinzu.* Wenn Sie SVM-Scoped-Konten und Verbindungen zu ONTAP-Clustern mit nur NFS-basierten Datastores verwenden, können Sie nur für NAS optimierten Modus für Spitzen-Performance in unterstützten Umgebungen aktivieren.</block>
  <block id="c82379d922d9cdc55fa21affe9f9b8bf" category="list-text">*Die ONTAP Tools 9.12 bieten jetzt Unterstützung für die Funktion zur schnellen Resync von ONTAP.* Dies ermöglicht eine schnelle Resynkronisierung der Spiegel unter dem Aspekt, dass nach dem Prozess die Storage-Einsparungen neu berechnet werden müssen. Diese Funktion wird standardmäßig nicht verwendet, kann aber in großen Umgebungen aktiviert werden, in denen die herkömmliche Resynchronisierung zu lange dauert oder ein Timing-Out stattfindet.</block>
  <block id="4a4842d7448ef71eb69b013e560b18bb" category="section-title">Besser skalieren</block>
  <block id="26738e8d40ae8035805f13bdbf185c30" category="paragraph">Die ONTAP-Tools SRA unterstützt jetzt bei der Verwendung mit SRM 8.3 und höher bis zu 500 Schutzgruppen (PGS).</block>
  <block id="96cb4d41ca97bd9d3d446984d3170a57" category="paragraph">Eine lang erwartete und viel erwartete neue Funktion ist SnapMirror Synchronous (SM-S) mit ONTAP 9.5 und höher, die eine granulare Band RPO Datenreplizierungslösung für Ihre geschäftskritischen Anwendungen liefert. SM-S erfordert ONTAP-Tools 9.8 oder höher.</block>
  <block id="b9832b10a87d0c3793ed3b413ee1d839" category="section-title">REST-API-Unterstützung</block>
  <block id="0065ce57adc813187a8e9c640ba6223a" category="paragraph">Die SRA Serverkonfiguration kann jetzt durch REST-APIs gemanagt werden. Eine Swagger UI wurde hinzugefügt, um Sie beim Erstellen Ihrer Automatisierungs-Workflows zu unterstützen. Sie finden sie auf Ihrer ONTAP-Tools-Appliance unter<block ref="579ce3ee0a7c1521f3dce6a507ea64a0" prefix=" " category="inline-code"></block>.</block>
  <block id="5bdb12cbb14efa98a61e7c5e3b4de108" category="admonition">Diese Dokumentation ersetzt den zuvor veröffentlichten technischen Bericht _TR-4900: VMware Site Recovery Manager mit ONTAP_</block>
  <block id="e3445c3aa798f5ad98665e33d34dd952" category="paragraph">Andere Dokumente wie Leitfäden und Kompatibilitäts-Tools werden durch Best Practices ergänzt. Sie werden basierend auf Labortests und umfassenden praktischen Erfahrungen der NetApp Ingenieure und Kunden entwickelt. In einigen Fällen passen empfohlene Best Practices möglicherweise nicht zu Ihrer Umgebung. Sie sind jedoch im Allgemeinen die einfachsten Lösungen, die die Anforderungen der meisten Kunden erfüllen.</block>
  <block id="10b8f80c23f8a44dfa6b10ae56d7dbe9" category="paragraph">Der Schwerpunkt dieses Dokuments liegt auf den Funktionen der neuesten Versionen von ONTAP 9, die in Verbindung mit ONTAP-Tools für VMware vSphere 9.12 (einschließlich NetApp Storage Replication Adapter [SRA] und VASA Provider [VP]) sowie VMware Site Recovery Manager 8.7 verwendet werden.</block>
  <block id="c40f8c2af56356193284f6b46865d954" category="section-title">Vorteile von ONTAP mit SRM</block>
  <block id="2242b80f4627736a48c4c9a36b7428f6" category="paragraph">Wenn Sie SnapMirror für die Array-basierte Replizierung nutzen, profitieren Sie von einer der bewährten und ausgereiftesten Technologien von ONTAP. Mit SnapMirror profitieren Sie von sicheren und hocheffizienten Datentransfers, wobei nur geänderte Datenblöcke kopiert werden, nicht die gesamten VMs oder Datastores. Selbst diese Blöcke profitieren von Platzeinsparungen wie Deduplizierung, Komprimierung und Data-Compaction. Moderne ONTAP Systeme verwenden jetzt versionsunabhängiges SnapMirror für die flexible Auswahl von Quell- und Ziel-Clustern. SnapMirror hat sich tatsächlich zu einem der leistungsstärksten Tools für Disaster Recovery entwickelt.</block>
  <block id="36b04390d6103d8075862e4b825a485e" category="paragraph">Ganz gleich, ob Sie herkömmliche NFS-, iSCSI- oder Fibre Channel-Attached Datastores verwenden (jetzt mit Unterstützung für VVols Datastores) – SRM bietet Ihnen einen robusten Erstanbieter, der die besten ONTAP Funktionen für Disaster Recovery oder Planung der Datacenter-Migration und -Orchestrierung nutzt.</block>
  <block id="5d111e8a6885460b43ecf1f7abb6377e" category="section-title">Wie SRM ONTAP 9 nutzt</block>
  <block id="16d4ec242c9f020f3a1fade311c776ed" category="paragraph">SRM nutzt die erweiterten Datenmanagement-Technologien von ONTAP Systemen. Die Integration mit ONTAP Tools für VMware vSphere, einer virtuellen Appliance mit drei Hauptkomponenten:</block>
  <block id="a106bb15d6dc2b11535ed4fefc81fbf4" category="list-text">Vasa Provider für ONTAP unterstützt das VMware vStorage APIs for Storage Awareness (VASA) Framework. VASA Provider verbindet vCenter Server mit ONTAP und erleichtert so die Bereitstellung und das Monitoring von VM-Storage. Es unterstützt die Unterstützung von VMware Virtual Volumes (VVols) und das Management von Storage-Funktionsprofilen (einschließlich VVols Replizierungsfunktionen) und der individuellen VM VVols Performance. Außerdem gibt es Alarme zur Überwachung der Kapazität und der Konformität mit den Profilen. In Verbindung mit SRM ermöglicht der VASA Provider for ONTAP Unterstützung für VVols basierte Virtual Machines, ohne dass ein SRA Adapter auf dem SRM Server installiert werden muss.</block>
  <block id="f2007951062862f5f0d593acbb23bcb4" category="list-text">SRA wird zusammen mit SRM eingesetzt, um die Replizierung von VM-Daten zwischen Produktions- und Disaster-Recovery-Standorten bei herkömmlichen VMFS- und NFS-Datenspeichern sowie zum unterbrechungsfreien Testen von DR-Replikaten zu managen. Diese Software hilft bei der Automatisierung der Erkennungs-, Recovery- und Sicherungsaufgaben. Sie enthält sowohl eine SRA Server-Appliance als auch SRA Adapter für den Windows SRM Server und die SRM Appliance.</block>
  <block id="8ccaa60ee78ecc93c4870b3b2ad15284" category="paragraph">Nachdem Sie die SRA Adapter auf dem SRM-Server zum Schutz von Datastores außerhalb von VVols sowie zur aktivierten VVols-Replizierung in den VASA Provider-Einstellungen installiert und konfiguriert haben, können Sie mit der Aufgabe beginnen, Ihre vSphere Umgebung für die Disaster Recovery zu konfigurieren.</block>
  <block id="0a1e25166326359cd5f50d0ab83e6a37" category="paragraph">SRA und VASA Provider bieten eine Befehlszeilenschnittstelle für den SRM Server zum Managen der ONTAP FlexVols, die Ihre VMware Virtual Machines (VMs) enthalten, sowie zur SnapMirror Replizierung, die sie sichern.</block>
  <block id="38db855935949f86c5db7bfe7d49873e" category="paragraph">Ab SRM 8.3 wurde ein neuer SRM VVols Provider-Kontrollpfad in den SRM Server eingeführt, der die IT in die Lage versetzt, mit dem vCenter Server und darüber hinaus ohne SRA mit dem VASA Provider zu kommunizieren. Auf diese Weise konnte der SRM Server eine wesentlich umfassendere Kontrolle über das ONTAP Cluster nutzen als bisher möglich, da VASA eine vollständige API für eine nahtlose Integration bietet.</block>
  <block id="1c27364cc3705aff403c6ef131c347ff" category="paragraph">SRM kann Ihren DR-Plan mithilfe der proprietären NetApp FlexClone Technologie unterbrechungsfrei testen, um nahezu sofortige Klone Ihrer geschützten Datenspeicher an Ihrem DR-Standort zu erstellen. SRM erstellt eine Sandbox-Umgebung für sichere Tests, damit sowohl Ihre Organisation als auch Ihre Kunden bei einem echten Ausfall geschützt sind. So können Ihre Unternehmen sicher sein, dass bei einem Ausfall ein Failover ausgeführt werden kann.</block>
  <block id="04a8f82007ed4bec45053912a49b6f85" category="paragraph">Bei einem echten Ausfall oder sogar einer geplanten Migration können Sie mit SRM alle Last-Minute-Änderungen am Datensatz über ein letztes SnapMirror Update senden (sofern Sie dies tun). Dann wird die Spiegelung unterbrochen und der Datenspeicher wird Ihren DR-Hosts gemountet. An diesem Punkt können Ihre VMs automatisch in beliebiger Reihenfolge gemäß Ihrer vorab geplanten Strategie hochgefahren werden.</block>
  <block id="b2560426a08ae6ceb167dad2bfb5e8ae" category="section-title">SRM mit ONTAP und anderen Anwendungsfällen: Hybrid Cloud und Migration</block>
  <block id="ed4c730bd8636c27d3eaa876c63b3d45" category="inline-link">NetApp Private Storage in Equinix</block>
  <block id="b422e05bc5c6f52208b66ff51d718b9d" category="paragraph">Durch Integration Ihrer SRM-Implementierung mit erweiterten Datenmanagement-Funktionen von ONTAP lassen sich im Vergleich zu lokalen Storage-Optionen deutlich bessere Skalierungs- und Performance-Möglichkeiten erzielen. Darüber hinaus bringt sie jedoch noch mehr die Flexibilität der Hybrid Cloud. Mit der Hybrid Cloud können Sie Geld sparen, indem Sie ungenutzte Datenblöcke des High-Performance-Arrays mittels FabricPool in den bevorzugten Hyperscaler verschieben, was ein lokaler S3-Speicher wie NetApp StorageGRID sein könnte. Außerdem können Edge-basierte Systeme mit softwaredefiniertem ONTAP Select oder Cloud-basierter DR mithilfe von Cloud Volumes ONTAP (CVO) oder verwendet werden<block ref="37a666d3a37f1c4a8c4c8dba379664a4" category="inline-link-rx"></block> Um einen vollständig integrierten Storage-, Networking- und Computing-Service-Stack in der Cloud zu erstellen, führt Amazon Web Services (AWS), Microsoft Azure und Google Cloud Platform (GCP) zum Vorteil.</block>
  <block id="87a7b114355b836acefe833c497611bb" category="paragraph">Anschließend könnten Sie dank FlexClone ein Test-Failover innerhalb des Datacenters eines Cloud-Service-Providers durchführen, bei einem Storage-Platzbedarf von nahezu null. Der Schutz Ihres Unternehmens ist jetzt günstiger als je zuvor.</block>
  <block id="12d0cbcf61df7df30bf9b020f6fbb051" category="paragraph">Mit SRM können auch geplante Migrationen durchgeführt werden, indem VMs mit SnapMirror effizient von einem Datacenter in ein anderes oder sogar innerhalb desselben Datacenters übertragen werden, unabhängig davon, ob es sich um Ihr eigenes Datacenter oder über eine beliebige Anzahl an Service Providern von NetApp Partnern handelt.</block>
  <block id="89fafdb11a7e8cd8abebb1d4df053dad" category="doc">Replizierungstopologien</block>
  <block id="be01acd2f018d38f9446f06b99ee4a2a" category="paragraph">In ONTAP 9 sind die physischen Komponenten eines Clusters für Cluster-Administratoren sichtbar, sind aber für die Applikationen und Hosts, die das Cluster nutzen, nicht direkt sichtbar. Die physischen Komponenten stellen einen Pool mit gemeinsam genutzten Ressourcen bereit, anhand dessen die logischen Clusterressourcen erstellt werden. Applikationen und Hosts greifen ausschließlich über SVMs auf Daten zu, die Volumes und LIFs enthalten.</block>
  <block id="7758fc02779ee5917b0a09ee22b52221" category="paragraph">Jede NetApp SVM wird im VMware vCenter Site Recovery Manager als Array behandelt. SRM unterstützt bestimmte Array-to-Array (oder SVM-zu-SVM) Replizierungslayouts.</block>
  <block id="58c0b55a67f4331cc49b073a7183e06c" category="paragraph">Eine einzelne VM kann aus den folgenden Gründen keine Daten besitzen – Virtual Machine Disk (VMDK) oder RDM – auf mehr als einem SRM Array:</block>
  <block id="880aeda0b58c0b30120b6319ae5f3beb" category="list-text">SRM sieht nur die SVM, nicht einen individuellen physischen Controller.</block>
  <block id="6f55124e085dbeb640e4cb8ad2c97905" category="list-text">Eine SVM kann LUNs und Volumes steuern, die mehrere Nodes in einem Cluster umfassen.</block>
  <block id="39d9903201320a83d45d3b36d512f051" category="cell">Best Practices In Sich</block>
  <block id="d86019fe4ba86b184ac71cf24a22f0c6" category="cell">Bedenken Sie bei der Ermittlung von Supportmöglichkeiten diese Regel: Um eine VM mithilfe von SRM und der NetApp SRA zu schützen, müssen alle Bestandteile der VM nur auf einer SVM vorhanden sein. Diese Regel gilt sowohl für den geschützten Standort als auch für den Recovery-Standort.</block>
  <block id="2e4472d35f80aa8b7eca52ae3dd43dc6" category="section-title">Unterstützte SnapMirror Layouts</block>
  <block id="d070ad9d8397892aa7f317cbb9046661" category="paragraph">Die folgenden Abbildungen zeigen die Szenarien des SnapMirror Beziehungs-Layouts, die von SRM und SRA unterstützt werden. Jede VM in den replizierten Volumes besitzt die Daten auf nur einem SRM Array (SVM) an jedem Standort.</block>
  <block id="86cf5bd2bdb1b1d67f8f907f41099509" category="section-title">Unterstützte Array Manager-Layouts</block>
  <block id="aacc8f18d659f75715d91a983e872233" category="paragraph">Wenn Sie in SRM Array-basierte Replizierung (ABR) verwenden, werden Schutzgruppen auf ein einzelnes Array-Paar isoliert, wie im folgenden Screenshot dargestellt. In diesem Szenario<block ref="129864522ed0e4ac80f306ecfa130ef7" prefix=" " category="inline-code"></block> Und<block ref="ec87aa7f4c8e70a139fd988f87b6549f" prefix=" " category="inline-code"></block> Werden mit Peering durchgeführt<block ref="387d04b65848414d4697f145a3782f90" prefix=" " category="inline-code"></block> Und<block ref="58bdbfcb72c2dd3d883a7ca754443a4c" prefix=" " category="inline-code"></block> Am Recovery-Standort. Sie können jedoch nur eines der beiden Array-Paare auswählen, wenn Sie eine Schutzgruppe erstellen.</block>
  <block id="1ceb46f3e1c6c7995de4ec5f05601227" category="section-title">Nicht unterstützte Layouts</block>
  <block id="611b9b35ac984f20e6594baec9181dbf" category="paragraph">Nicht unterstützte Konfigurationen beinhalten Daten (VMDK oder RDM) auf mehreren SVMs, die sich im Besitz einer individuellen VM befinden. In den folgenden Abbildungen sind<block ref="df777c4d2477e55571925353ce589f7e" prefix=" " category="inline-code"></block> Kann aus dem Grund nicht für den Schutz mit SRM konfiguriert werden<block ref="df777c4d2477e55571925353ce589f7e" prefix=" " category="inline-code"></block> Verfügt über Daten auf zwei SVMs.</block>
  <block id="69851ac8c2fa81b32799efc8522798d5" category="paragraph">Jegliche Replizierungsbeziehungen, bei denen ein einzelnes NetApp Volume von einer Quell-SVM auf mehrere Ziele in derselben SVM oder in verschiedenen SVMs repliziert wird, werden als SnapMirror Fan-out bezeichnet. Fan-out wird mit SRM nicht unterstützt. In der folgenden Abbildung ist das Beispiel dargestellt.<block ref="df777c4d2477e55571925353ce589f7e" prefix=" " category="inline-code"></block> Kann nicht für den Schutz in SRM konfiguriert werden, da es mit SnapMirror an zwei verschiedenen Standorten repliziert wird.</block>
  <block id="3f88384b0d5cdcdc25895c644126c1b0" category="section-title">SnapMirror Kaskadierung</block>
  <block id="7ebde774ad43c45667d5544308a6d688" category="paragraph">SRM unterstützt keine Kaskadierung von SnapMirror Beziehungen, bei denen ein Quell-Volume auf einem Ziel-Volume repliziert wird und das Ziel-Volume ebenfalls mit SnapMirror auf einem anderen Ziel-Volume repliziert wird. In dem in der folgenden Abbildung gezeigten Szenario kann SRM nicht für das Failover zwischen mehreren Standorten verwendet werden.</block>
  <block id="8bd15e818d377a0a5cf6ebc429555f79" category="section-title">SnapMirror und SnapVault</block>
  <block id="d440734d1f5d9c6e59aa97f7cbbcd525" category="paragraph">Die NetApp SnapVault Software ermöglicht festplattenbasierte Backups von Unternehmensdaten zwischen NetApp Storage-Systemen. SnapVault und SnapMirror können in derselben Umgebung nebeneinander bestehen. SRM unterstützt jedoch nur das Failover der SnapMirror Beziehungen.</block>
  <block id="7878dfbf4b6ad3bc60d095cf89d79202" category="admonition">Die NetApp SRA unterstützt das<block ref="e7d3e049f94ce3ff8a47f209f012173d" prefix=" " category="inline-code"></block> Richtlinientyp.</block>
  <block id="883e69a285ae2d5fd23c80cd29285804" category="paragraph">SnapVault wurde für ONTAP 8.2 von Grund auf neu aufgebaut. Obwohl frühere Benutzer von Data ONTAP 7-Mode Ähnlichkeiten finden sollten, wurden in dieser Version von SnapVault wesentliche Verbesserungen vorgenommen. Eine wichtige Verbesserung ist die Möglichkeit zur Wahrung der Storage-Effizienz von Primärdaten während der SnapVault Transfers.</block>
  <block id="9bc21373e0e299579a7ac86dcd4f513d" category="paragraph">Eine wichtige Architekturänderung ist, dass SnapVault in ONTAP 9 wie bei 7-Mode SnapVault auf Volume-Ebene repliziert, nicht auf qtree-Ebene. Bei diesem Setup muss die Quelle einer SnapVault Beziehung ein Volume sein, und das Volume muss auf sein eigenes Volume auf dem sekundären SnapVault System repliziert werden.</block>
  <block id="d447048519c85a2fb3bea0790cd109fb" category="paragraph">In einer Umgebung, in der SnapVault verwendet wird, werden auf dem primären Storage-System speziell benannte Snapshots erstellt. Je nach implementierter Konfiguration können die benannten Snapshots auf dem Primärsystem nach einem SnapVault-Zeitplan oder durch eine Anwendung wie NetApp Active IQ Unified Manager erstellt werden. Die benannten Snapshots, die auf dem Primärsystem erstellt werden, werden dann auf das SnapMirror Ziel repliziert und von dort auf das SnapVault Ziel archiviert.</block>
  <block id="5dd37b7cd36f7d38ba2e6a63c603be4c" category="paragraph">Ein Quell-Volume kann in einer Kaskadenkonfiguration erstellt werden, bei der ein Volume auf ein SnapMirror Ziel am DR-Standort repliziert wird und von dort aus auf ein SnapVault Ziel verlagert wird. Ein Quell-Volume kann auch in einer Fan-out-Beziehung erstellt werden, wobei ein Ziel ein SnapMirror Ziel ist und das andere Ziel eine SnapVault Ziel ist. SRA rekonfiguriert jedoch nicht automatisch die SnapVault-Beziehung neu, um das SnapMirror Ziel-Volume als Quelle für den Vault zu verwenden, wenn das SRM Failover oder eine Umkehrung der Replizierung stattfindet.</block>
  <block id="5bc66b9c84b3f7cc0b375e729376b68d" category="inline-link">TR-4015 SnapMirror Configuration Best Practice Guide für ONTAP 9.</block>
  <block id="0d510e96259254d285c6aaeb8458f45d" category="paragraph">Aktuelle Informationen zu SnapMirror und SnapVault für ONTAP 9 finden Sie unter<block ref="32e7c991f675bf983c547a2a174c2caf" category="inline-link-rx"></block></block>
  <block id="75f8a2d99eea9f336120aec69d8c1010" category="cell">Wenn in derselben Umgebung SnapVault und SRM eingesetzt werden, empfiehlt NetApp, eine Kaskadenkonfiguration von SnapMirror auf SnapVault zu verwenden, bei der SnapVault Backups normalerweise über das SnapMirror Ziel am DR-Standort ausgeführt werden. Bei einem Notfall kann der primäre Standort durch diese Konfiguration nicht mehr zugänglich sein. Indem das SnapVault Ziel am Recovery-Standort gehalten wird, können SnapVault Backups nach dem Failover neu konfiguriert werden, sodass SnapVault Backups weiterhin am Recovery-Standort ausgeführt werden können.</block>
  <block id="e082a67e3ddd7f4f92096536440cbb34" category="paragraph">In einer VMware Umgebung verfügt jeder Datenspeicher über eine universelle eindeutige Kennung (Universal Unique Identifier, UUID) und jede VM über eine eindeutige Managed Object ID (MOID). Diese IDs werden während Failover oder Failback durch SRM nicht gepflegt. Da Datastore-UIDs und VM-MOIDs beim Failover durch SRM nicht gepflegt werden, müssen nach dem SRM Failover alle Applikationen, die von diesen IDs abhängen, neu konfiguriert werden. Eine Beispielapplikation ist NetApp Active IQ Unified Manager, wo die SnapVault Replizierung mit der vSphere Umgebung koordiniert wird.</block>
  <block id="657a41cce55646ab17754ac1427970af" category="paragraph">Die folgende Abbildung zeigt die Kaskadenkonfiguration von SnapMirror auf SnapVault. Wenn sich das SnapVault Ziel am DR-Standort oder an einem tertiären Standort befindet, der nicht von einem Ausfall am primären Standort betroffen ist, kann die Umgebung neu konfiguriert werden, sodass Backups nach dem Failover fortgesetzt werden können.</block>
  <block id="e068178460f103498576eac8ddb7fb82" category="paragraph">In der folgenden Abbildung wird die Konfiguration dargestellt, nachdem SRM die SnapMirror Replizierung zurück auf den primären Standort umgekehrt hat. Die Umgebung wurde außerdem neu konfiguriert, sodass SnapVault Backups von der jetzt SnapMirror Quelle durchgeführt werden. Bei dieser Einrichtung handelt es sich um eine Fan-out-Konfiguration für SnapMirror SnapVault.</block>
  <block id="b4607a4ee7e1830ab6ba0007df206956" category="paragraph">Nachdem SRM ein Failback und eine zweite Umkehrung der SnapMirror Beziehungen durchführt, sind die Produktionsdaten am primären Standort zurück. Die Daten werden jetzt auf dieselbe Weise gesichert wie vor dem Failover zum DR-Standort – über SnapMirror und SnapVault Backups.</block>
  <block id="63fd1f18d5d53fa97fd05a7fd96090b4" category="section-title">Verwendung von Qtrees in Site Recovery Manager-Umgebungen</block>
  <block id="2d88570ceb2de0d8699a068b0140454b" category="paragraph">Qtrees sind spezielle Verzeichnisse, die die Anwendung von Filesystem-Kontingenten für NAS ermöglichen. ONTAP 9 ermöglicht die Erstellung von qtrees und qtrees in Volumes, die mit SnapMirror repliziert werden. SnapMirror ermöglicht jedoch nicht die Replizierung einzelner qtrees oder Qtree-Level-Replikationen. Alle SnapMirror Replikation befindet sich nur auf Volume-Ebene. Aus diesem Grund empfiehlt NetApp die Verwendung von qtrees mit SRM nicht.</block>
  <block id="345cb3f040f9f7f3f1a4261ed260aa79" category="section-title">Gemischte FC- und iSCSI-Umgebungen</block>
  <block id="a60b166c2b69b4fdfbb733dc2193668a" category="paragraph">Mit den unterstützten SAN-Protokollen (FC, FCoE und iSCSI) bietet ONTAP 9 LUN-Services an, d. h. die Möglichkeit, LUNs zu erstellen und angebundenen Hosts zuzuweisen. Da das Cluster aus mehreren Controllern besteht, gibt es mehrere logische Pfade, die von Multipath I/O zu einer beliebigen einzelnen LUN gemanagt werden. Auf den Hosts wird mithilfe des Asymmetric Logical Unit Access (ALUA) der optimale Pfad zu einer LUN ausgewählt und für den Datentransfer aktiviert. Wenn sich der optimierte Pfad zu einer LUN ändert (z. B. weil das zugehörige Volume verschoben wird), erkennt ONTAP 9 diese Änderung automatisch und passt sich unterbrechungsfrei an. Wenn der optimierte Pfad nicht mehr verfügbar ist, kann ONTAP ohne Unterbrechungen zu einem anderen verfügbaren Pfad wechseln.</block>
  <block id="b9921ac732e72656bc2836f83bb09522" category="paragraph">VMware SRM und NetApp SRA unterstützen die Nutzung des FC-Protokolls an einem Standort und das iSCSI-Protokoll am anderen Standort. Eine Kombination aus FC-Attached Datastores und iSCSI-Attached Datastores wird jedoch auf demselben ESXi Host oder auf verschiedenen Hosts im selben Cluster nicht unterstützt. Diese Konfiguration wird mit SRM nicht unterstützt, da SRM während des SRM Failover oder des Test-Failovers alle FC- und iSCSI-Initiatoren in den ESXi-Hosts in der Anforderung enthält.</block>
  <block id="fccd44a96243e459128798803dca70aa" category="cell">SRM und SRA unterstützen gemischte FC- und iSCSI-Protokolle zwischen den geschützten und den Recovery-Standorten. Allerdings sollte jeder Standort nur mit einem Protokoll, entweder FC oder iSCSI, konfiguriert werden, nicht mit beiden Protokollen am selben Standort. Wenn FC- und iSCSI-Protokolle am selben Standort konfiguriert werden müssen, empfiehlt NetApp, dass einige Hosts iSCSI verwenden und andere Hosts FC verwenden. NetApp empfiehlt in diesem Fall außerdem die SRM-Ressourcenzuordnung, damit die VMs für das Failover in eine Gruppe von Hosts oder die andere konfiguriert werden.</block>
  <block id="56687c1a324f03f5d1429500efea710e" category="doc">Fehlerbehebung bei SRM bei Nutzung der VVols-Replizierung</block>
  <block id="14acf40cbe69d8682b4c844ac7fbf7f6" category="paragraph">Der Workflow in SRM unterscheidet sich deutlich, wenn VVols Replizierung mit dem verwendet wird, was mit SRA und herkömmlichen Datastores verwendet wird. Zum Beispiel gibt es kein Konzept für Array-Manager. So,<block ref="0ec64480a9620a1430c0ae1b6603ea01" prefix=" " category="inline-code"></block> Und<block ref="ba6c59072f543cb828e47d5bac560371" prefix=" " category="inline-code"></block> Befehle werden nie gesehen.</block>
  <block id="ef74d715544e3b067a6bec3218ac5044" category="paragraph">Bei der Fehlerbehebung sind die neuen Workflows zu verstehen, die im Folgenden aufgeführt sind:</block>
  <block id="47f0942d341ee10381b2eff35089d75d" category="list-text">QueryReplicationPeer: Ermittelt die Replikationsvereinbarungen zwischen zwei Fehlerdomänen.</block>
  <block id="136a299aae29998a147852cb0cba5b4a" category="list-text">QueryFaultDomain: Ermittelt die Fehlerdomäne-Hierarchie.</block>
  <block id="c071098c5df923c31f6245e4db2f3861" category="list-text">QueryReplicationGroup: Ermittelt die in den Quell- oder Zieldomänen vorhandenen Replikationsgruppen.</block>
  <block id="792cfe9ad3a3dc8528080a262e92989a" category="list-text">SyncReplicationGroup: Synchronisiert die Daten zwischen Quelle und Ziel.</block>
  <block id="39afb1e1e3b180a6230e07bb8571667e" category="list-text">QueryPointInTimeReplica: Ermittelt die Point-in-Time-Replikate auf einem Ziel.</block>
  <block id="9fb25fe2ff084dc9ccfb99eaccba7212" category="list-text">TestFailoverReplicationGroupStart: Startet Test Failover.</block>
  <block id="bd4e1dacaca4c37a53b5286b6ae0239d" category="list-text">TestFailoverReplicationGroupStop: Beendet das Test-Failover.</block>
  <block id="887f376a43cf9de1d6e14a8d69e588f6" category="list-text">PromoteReplicationGroup: Fördert eine Gruppe, die sich derzeit in der Produktion befindet.</block>
  <block id="17ab16c5c0786ff381e6a085318bad9b" category="list-text">PreparreFailoverReplicationGroup: Bereitet sich auf eine Notfallwiederherstellung vor.</block>
  <block id="665dd64ebcc4016bda94e0e8020d8c8e" category="list-text">Failover ReplicationGroup: Durchführung einer Disaster Recovery</block>
  <block id="6936cf00e30b7cd4421225113a023c3e" category="list-text">ReverseReplicateGroup: Initiiert Reverse-Replikation.</block>
  <block id="61ac8950a043e6998b458d4f8171154c" category="list-text">QueryMatchingContainer: Sucht Container (zusammen mit Hosts oder Replikationsgruppen), die eine Bereitstellungsanfrage mit einer bestimmten Richtlinie erfüllen können.</block>
  <block id="05c9efdc056d2af4640bc441ab61b53a" category="list-text">QueryResourceMetadaten: Ermittelt die Metadaten aller Ressourcen des VASA Providers, kann die Ressourcenauslastung als Antwort auf die queryMatchingContainer-Funktion zurückgegeben werden.</block>
  <block id="01fa56986f885c588b1be8c206623de8" category="paragraph">Der häufigste Fehler bei der Konfiguration der VVols-Replizierung ist das Erkennen der SnapMirror Beziehungen. Dies geschieht, weil die Volumes und SnapMirror Beziehungen außerhalb der ONTAP Tools-Ansicht erstellt werden. Daher empfiehlt es sich, immer sicherzustellen, dass die SnapMirror Beziehung vollständig initialisiert ist und dass Sie an beiden Standorten eine erneute Bestandsaufnahme in ONTAP Tools ausführen, bevor Sie versuchen, einen replizierten VVols Datastore zu erstellen.</block>
  <block id="3e4627f6667f830baceaaf35890b575a" category="summary">Sehen Sie sich die folgenden Dokumente und/oder Websites an, um mehr über die in diesem Dokument beschriebenen Informationen zu erfahren.</block>
  <block id="092bf78f9c97ba171b8232ddff585392" category="doc">Weitere Informationen</block>
  <block id="7d5b957cd473f6eaf5ad335a9c63c4ff" category="paragraph">Sehen Sie sich die folgenden Dokumente und/oder Websites an, um mehr über die in diesem Dokument beschriebenen Informationen zu erfahren:</block>
  <block id="cbe2f6eb2a73f4a2c871a06264f10dc7" category="inline-link"><block ref="cbe2f6eb2a73f4a2c871a06264f10dc7" category="inline-link-rx"></block></block>
  <block id="c88037eb28ae3cceb0e32c2cd3c322d7" category="inline-link"><block ref="c88037eb28ae3cceb0e32c2cd3c322d7" category="inline-link-rx"></block></block>
  <block id="9aa6e8f8fac5c131e5924ee033660d29" category="inline-link"><block ref="9aa6e8f8fac5c131e5924ee033660d29" category="inline-link-rx"></block></block>
  <block id="c9dd6bad4c7d561872f2e2d498a990bf" category="inline-link">Interoperabilitäts-Matrix-Tool (IMT)</block>
  <block id="1f82b18dcba5f32a085bc502cdd0c6b7" category="summary">Mit ONTAP sorgt das Konzept der Storage Virtual Machine (SVM) für eine strenge Segmentierung in sicheren mandantenfähigen Umgebungen.</block>
  <block id="7520e12a140ccf3de279fe2fd48889e4" category="doc">Best Practices für die Implementierung</block>
  <block id="1b4574c516231b685bb37b013b789b06" category="section-title">SVM-Layout und Segmentierung für SMT</block>
  <block id="446e04546ec0567eeeeef8e07368ab40" category="paragraph">Mit ONTAP sorgt das Konzept der Storage Virtual Machine (SVM) für eine strenge Segmentierung in sicheren mandantenfähigen Umgebungen. SVM-Benutzer auf einer SVM können nicht auf Ressourcen einer anderen SVM zugreifen bzw. diese managen. Auf diese Weise können Sie die ONTAP Technologie nutzen, indem Sie separate SVMs für verschiedene Geschäftseinheiten erstellen, die ihre eigenen SRM Workflows im selben Cluster managen, um eine größere Storage-Effizienz zu erzielen.</block>
  <block id="f98c1d223f34b2fdaf90eab1a3bc6a25" category="paragraph">Erwägen Sie die Verwaltung von ONTAP mit SVM-Scoped-Konten und SVM-Management-LIFs, um nicht nur die Sicherheitskontrolle zu verbessern, sondern auch die Performance zu verbessern. Die Performance ist bei der Nutzung von Verbindungen mit SVM-Umfang höher, da der SRA nicht erforderlich ist, alle Ressourcen eines gesamten Clusters – einschließlich physischer Ressourcen – zu verarbeiten. Stattdessen müssen sie nur die logischen Ressourcen verstehen, die zu der jeweiligen SVM abstrahiert sind.</block>
  <block id="849d4d5e8a5d5b2747d8375d2ec03b29" category="paragraph">Nur bei der Verwendung von NAS-Protokollen (kein SAN-Zugriff) können Sie sogar den neuen NAS-optimierten Modus nutzen, indem Sie den folgenden Parameter einstellen (beachten Sie, dass der Name so ist, da SRA und VASA dieselben Backend-Services in der Appliance nutzen):</block>
  <block id="05f551a655f40911851a53ba0c721997" category="list-text">Melden Sie sich am Bedienfeld unter an<block ref="501a3478972ee5e3d6f109bdc0514bdc" prefix=" " category="inline-code"></block> Und klicken Sie auf webbasierte CLI-Schnittstelle.</block>
  <block id="b0cc185c73ca964d1429a601551c68f5" category="list-text">Führen Sie den Befehl aus<block ref="4f35e9b3adeccfaf0287d002b134221c" prefix=" " category="inline-code"></block>.</block>
  <block id="5f7e35f90689bef91afe9f01257c61ac" category="list-text">Führen Sie den Befehl aus<block ref="aa610e3ab4e5162f432950793f30a387" prefix=" " category="inline-code"></block>.</block>
  <block id="ba2a1e4d95dc56709260e82cca20a789" category="list-text">Führen Sie den Befehl aus<block ref="6d83e9fce5e947159c3c34b9f499e80e" prefix=" " category="inline-code"></block>.</block>
  <block id="2d14ca751c8ce5a724606e4f75f02c50" category="section-title">Implementieren von ONTAP-Tools und Überlegungen für VVols</block>
  <block id="fa3a193f1892cf0f45f4a95a35aa3c4b" category="paragraph">Falls Sie SRM mit VVols verwenden möchten, müssen Sie den Storage mit Anmeldedaten für den Cluster-Umfang und einer Cluster-Management-LIF managen. Der Grund dafür ist, dass VASA Provider die zugrunde liegende physische Architektur verstehen muss, um die für VM Storage-Richtlinien erforderlichen Richtlinien erfüllen zu können. Wenn Sie beispielsweise eine Richtlinie haben, die All-Flash-Storage erfordert, muss der VASA Provider in der Lage sein, zu sehen, welche All-Flash-Systeme sind.</block>
  <block id="31d279d1cc3bc5a4281567ba697f678d" category="paragraph">Eine weitere Best Practice bei der Implementierung besteht darin, Ihre ONTAP Tools Appliance niemals auf einem VVols Datastore zu speichern, den sie managen. Dies kann dazu führen, dass Sie den VASA Provider nicht einschalten können, da Sie die Swap-vVol für die Appliance nicht erstellen können, da die Appliance offline ist.</block>
  <block id="97399fb37deca0d463aa5c7dc8d064a6" category="section-title">Best Practices für das Management von ONTAP 9 Systemen</block>
  <block id="18f8b81b7fb7ea92f333e127583df17c" category="paragraph">Wie bereits erwähnt, können Sie ONTAP Cluster mit Anmeldedaten im Cluster oder SVM-Umfang und Management-LIFs managen. Um die optimale Performance zu erzielen, sollten Sie immer dann die Verwendung von VVols in Betracht ziehen, wenn Sie über den SVM-Umfang verfügen. Dabei sollten Sie sich jedoch einigen Anforderungen bewusst sein und dass einige Funktionen verloren gehen.</block>
  <block id="fec05e53ca7d96aaf3e4c1a1b2502bea" category="list-text">Das Standard-vsadmin SVM-Konto verfügt nicht über die erforderliche Zugriffsebene, um ONTAP-Tools-Aufgaben durchzuführen. Daher müssen Sie ein neues SVM-Konto erstellen.</block>
  <block id="d508cbe8ee8a3aa0f975b96403baf33c" category="list-text">Wenn Sie ONTAP 9.8 oder höher verwenden, empfiehlt NetApp die Erstellung eines RBAC-Kontos mit den geringsten Berechtigungen über das Benutzermenü von ONTAP System Manager sowie die JSON-Datei, die auf der ONTAP Tools-Appliance unter verfügbar ist<block ref="137f8d4bb89dcc34c3bfe48f2a61c95e" prefix=" " category="inline-code"></block>. Verwenden Sie Ihr Administratorpasswort, um die JSON-Datei herunterzuladen. Diese Option kann für SVM oder Konten mit Cluster-Umfang verwendet werden.</block>
  <block id="a7a83b5e4f375c0585588b9c7ff92219" category="inline-link">NetApp Support Site Tool</block>
  <block id="fdfe9ba9104df6a32fff0f859291ea3c" category="paragraph">Wenn Sie ONTAP 9.6 oder eine frühere Version verwenden, sollten Sie das RUC-Tool (RBAC User Creator) verwenden, das im verfügbar ist<block ref="bdbf96c18cf8ac76d01fba7057b81b87" category="inline-link-rx"></block>.</block>
  <block id="ea7a3d08d55dc3e8ecf89d2a5d5e302d" category="list-text">Da die vCenter UI Plug-in, VASA Provider und SRA Server vollständig integrierte Services sind, müssen Sie den SRA-Adapter in SRM um Storage ebenso ergänzen wie in der vCenter UI für ONTAP-Tools. Andernfalls erkennt der SRA-Server möglicherweise nicht die Anfragen, die von SRM über den SRA-Adapter gesendet werden.</block>
  <block id="b473de4177eeaa79a66448798a446db3" category="list-text">Die NFS-Pfadprüfung wird bei der Verwendung der SVM-Scoped-Anmeldedaten nicht durchgeführt. Der Grund dafür ist, dass der physische Standort logisch von der SVM abstrahiert ist. Dies stellt jedoch keine Sorge mehr dar, da bei der Verwendung von indirekten Pfaden nicht mehr deutliche Performance-Einbußen bei modernen ONTAP Systemen auftreten.</block>
  <block id="6e62430b92a7c5a0b8512436b163a10d" category="list-text">Es werden möglicherweise keine Aggregat-Platzeinsparungen aufgrund von Storage-Effizienz gemeldet.</block>
  <block id="93d86aa51e0909549d1b1210f887aef3" category="list-text">Wenn unterstützt, können Spiegelungen zur Lastverteilung nicht aktualisiert werden.</block>
  <block id="647d380b52e259fe7ed2e2582bc54b27" category="list-text">Die EMS-Protokollierung wird möglicherweise nicht auf ONTAP Systemen durchgeführt, die mit den Anmeldedaten im Umfang des SVM-Service gemanagt werden.</block>
  <block id="c5cd657df037c277c37216b73efb0b08" category="summary">Wenn möglich, verwenden Sie immer ONTAP Tools, um Datenspeicher und Volumes bereitzustellen. Damit stellen wir sicher, dass Volumes, Verbindungspfade, LUNs, Initiatorgruppen, Exportrichtlinien Und andere Einstellungen sind kompatibel konfiguriert.</block>
  <block id="d3b26e9021e83573a3d2a9050400a33a" category="doc">Best Practices für betriebliche Prozesse</block>
  <block id="4d15db58f26b374b36c283df6b5a5fc1" category="paragraph">SRM unterstützt iSCSI, Fibre Channel und NFS Version 3 mit ONTAP 9 bei Nutzung der Array-basierten Replizierung über SRA. SRM unterstützt nicht die Array-basierte Replizierung für NFS Version 4.1 mit herkömmlichen oder VVols-Datastores.</block>
  <block id="61f54ceeb0338787a2ee2d801cbc62cd" category="paragraph">Zur Bestätigung der Konnektivität überprüfen Sie immer, ob Sie einen neuen Testdatenspeicher am DR-Standort vom Ziel-ONTAP-Cluster aus mounten und wieder mounten können. Testen Sie jedes Protokoll, das Sie für die Datastore-Konnektivität verwenden möchten. Eine Best Practice besteht darin, mit ONTAP-Tools Ihren Testdatenspeicher zu erstellen, da dies die gesamte Datastore-Automatisierung gemäß den Anweisungen von SRM erfolgt.</block>
  <block id="0ac3e8abbf45a28af68d22b0f59a973e" category="paragraph">SAN-Protokolle sollten für jeden Standort homogen sein. Sie können NFS und SAN mixen, aber die SAN-Protokolle sollten nicht innerhalb eines Standorts gemischt werden. Beispielsweise können Sie FCP in Seite A und iSCSI in Standort B verwenden Sie sollten FCP und iSCSI nicht an Standort A verwenden Der Grund hierfür: Der SRA erstellt nicht gemischte Initiatorgruppen am Recovery-Standort, und SRM filtert nicht die Initiatorliste, die den SRA gegeben wurde.</block>
  <block id="63b6c8d5e17ef7185a34c4ddd86f9d19" category="inline-link-macro">Automatisches Vergrößern oder Verkleinern von Volumes</block>
  <block id="a1874a28c8389c5c687d10167e80dc1f" category="paragraph">Die automatische Volume-Größe sollte auf festgelegt werden<block ref="4d200fce73a8e1cc965cfc2c43343824" prefix=" " category="inline-code"></block> Für Volumes mit SAN-Datastores und<block ref="d89f4f8b1d7c18847b88b46142f61535" prefix=" " category="inline-code"></block> Für NFS-Datastores. Weitere Informationen zu <block ref="ebd664f1c7cf128a3758282775d35e72" category="inline-link-macro-rx"></block>.</block>
  <block id="e401bd7c98141814eaf5528ddb318da6" category="section-title">Storage Policy Based Management (SPBM) und VVols</block>
  <block id="be41c3baf700984021c43e2e6b661b0a" category="paragraph">Der folgende Screenshot zeigt ein Beispiel zu SnapMirror Zeitplänen, die im Assistenten zur Erstellung von VM-Storage-Richtlinien angezeigt werden.</block>
  <block id="f6292d9eb6ea467a3c3560a65b7f63b5" category="paragraph">Der ONTAP VASA Provider unterstützt den Failover auf unterschiedlichen Storage. So kann beispielsweise ein Failover des Systems von ONTAP Select an einem Edge-Standort auf ein AFF System im Core-Datacenter durchgeführt werden. Unabhängig von Ähnlichkeit zum Storage müssen Sie für VM Storage-Richtlinien immer Storage-Richtlinien-Zuordnungen und Reverse-Mappings konfigurieren, damit die Services am Recovery-Standort die Erwartungen und Anforderungen erfüllen. Der folgende Screenshot zeigt ein Beispiel für eine Richtlinienzuordnung.</block>
  <block id="1eb90dc8f7adfd00c74af3b815e2ac15" category="section-title">Erstellung replizierter Volumes für VVols-Datastores</block>
  <block id="550d51e15336d4a33bcbb0f26a6810f7" category="paragraph">Bei VVols und SRM ist Vorsicht geboten. Niemals geschützte und ungesicherte VMs in demselben VVols Datastore zusammen Der Grund dafür: Wenn Sie SRM für das Failover an Ihrem DR-Standort verwenden, werden nur die VMs, die Teil der Sicherungsgruppe sind, in die DR online geschaltet. Wenn Sie die Sicherung rückgängig machen (das SnapMirror aus der DR wieder in die Produktionsumgebung verschieben), können die VMs, die nicht Failover waren, überschrieben werden und wertvolle Daten enthalten.</block>
  <block id="6d717f70d89096b731dfc63d7a1379e1" category="section-title">Allgemeines zu Array-Paaren</block>
  <block id="2219228418c053eb593ce0a1f318da47" category="paragraph">Wenn Sie Array-Paare in SRM konfigurieren, sollten Sie sie immer in SRM auf die gleiche Weise hinzufügen, wie Sie sie den ONTAP Tools hinzugefügt haben. Das bedeutet, dass sie denselben Benutzernamen, dasselbe Passwort und dieselbe Management-LIF verwenden müssen. Diese Anforderung stellt sicher, dass SRA ordnungsgemäß mit dem Array kommuniziert. Der folgende Screenshot veranschaulicht, wie ein Cluster in ONTAP-Tools angezeigt wird und wie es zu einem Array Manager hinzugefügt werden kann.</block>
  <block id="3b34135c8c4d2b14ceb7ebf595f00193" category="section-title">Allgemeines zu Replikationsgruppen</block>
  <block id="6be59afd3c8bb91501f55a7e77ebe794" category="paragraph">Replikationsgruppen enthalten logische Sammlungen von virtuellen Maschinen, die zusammen wiederhergestellt werden. Mit den ONTAP Tools VASA Provider werden automatisch Replikationsgruppen für Sie erstellt. Da die ONTAP SnapMirror Replizierung auf Volume-Ebene stattfindet, befinden sich alle VMs in einem Volume in derselben Replizierungsgruppe.</block>
  <block id="c727c1020d3128dd7495a6e3e6f23736" category="paragraph">Eine letzte Überlegung für Replikationsgruppen besteht darin, dass jede von Natur aus eine logische Konsistenzgruppe ist (nicht zu verwechseln mit SRM-Konsistenzgruppen). Das liegt daran, dass alle VMs im Volume mithilfe desselben Snapshots zusammen übertragen werden. Wenn Sie also VMs haben, die stets konsistent sein müssen, sollten Sie sie in der gleichen FlexVol speichern.</block>
  <block id="9f8a350a2113ea4be6b4e5e5112514a3" category="section-title">Allgemeines zu Schutzgruppen</block>
  <block id="df77671ab2743af7f1d1457c200eb12c" category="paragraph">Sicherungsgruppen definieren VMs und Datastores in Gruppen, die am geschützten Standort zusammen wiederhergestellt werden. Am geschützten Standort befinden sich die VMs, die in einer Schutzgruppe konfiguriert sind, im normalen Steady-State-Betrieb. Es ist wichtig zu beachten, dass eine Schutzgruppe nicht mehrere Array-Manager umfassen kann, obwohl SRM möglicherweise mehrere Array-Manager für eine Schutzgruppe anzeigt. Aus diesem Grund sollten Sie VM-Dateien nicht über Datastores auf unterschiedlichen SVMs verteilen.</block>
  <block id="b148467d645c3613c1c7a2607764c515" category="section-title">Recovery-Pläne sprechen</block>
  <block id="fd28cd9834ad3aa213c4fec2881064b5" category="paragraph">Recovery-Pläne legen fest, welche Schutzgruppen im gleichen Prozess wiederhergestellt werden. Mehrere Sicherungsgruppen können im selben Recovery-Plan konfiguriert werden. Um darüber hinaus mehr Optionen für die Ausführung von Recovery-Plänen zu aktivieren, kann eine einzige Sicherungsgruppe in mehreren Recovery-Plänen enthalten sein.</block>
  <block id="77aa84394124509e5e4006dcfd18789f" category="paragraph">Durch Recovery-Pläne können SRM-Administratoren Recovery-Workflows definieren, indem VMs einer Prioritätsgruppe von 1 (hoch) bis 5 (niedrig) zugewiesen werden, wobei 3 (mittel) standardmäßig verwendet wird. Innerhalb einer Prioritätsgruppe können VMs für Abhängigkeiten konfiguriert werden.</block>
  <block id="030d749269f8866de516a551b0286001" category="paragraph">NetApp empfiehlt besonders, mit Ihren Applikationsteams zusammenarbeiten zu müssen, um die Reihenfolge der für ein Failover-Szenario erforderlichen Operationen zu ermitteln und die Recovery-Pläne entsprechend zu erstellen.</block>
  <block id="cebb1167d3e78885137c837f0abf8026" category="section-title">Testen Sie den Failover</block>
  <block id="6ae8acfc86f5df3426d525fb95767506" category="paragraph">NetApp empfiehlt zudem, die Funktion der in Gast-Applikationen gelegentlich zu bestätigen, insbesondere nach der Neukonfiguration von VM-Storage.</block>
  <block id="bd47371bcd8b3d08d6b4b15481b08d93" category="paragraph">Wenn ein Test-Recovery-Vorgang ausgeführt wird, wird auf dem ESXi Host für die VMs ein privates Test-Bubble-Netzwerk erstellt. Dieses Netzwerk wird jedoch nicht automatisch mit physischen Netzwerkadaptern verbunden und bietet daher keine Verbindung zwischen den ESXi Hosts. Um die Kommunikation zwischen VMs zu ermöglichen, die während des DR-Tests auf verschiedenen ESXi Hosts ausgeführt werden, wird ein physisches privates Netzwerk zwischen den ESXi Hosts am DR-Standort erstellt. Um zu überprüfen, ob das Testnetzwerk privat ist, kann das Testblasennetzwerk physisch oder mittels VLANs oder VLAN-Tagging getrennt werden. Dieses Netzwerk muss von dem Produktionsnetzwerk getrennt werden, da die VMs wiederhergestellt werden und nicht mit IP-Adressen im Produktionsnetzwerk platziert werden können, die mit den tatsächlichen Produktionssystemen kollidieren können. Nach dem Erstellen eines Recovery-Plans in SRM kann das erstellte Testnetzwerk als privates Netzwerk ausgewählt werden, um die VMs mit während des Tests zu verbinden.</block>
  <block id="2be58fc410ddf2ba9c350135a617944a" category="paragraph">Nachdem der Test validiert und nicht mehr erforderlich ist, führen Sie eine Bereinigung durch. Bei der Durchführung der Bereinigung werden die geschützten VMs in ihren Ausgangszustand zurückversetzt und der Recovery-Plan wird auf den Status „bereit“ zurückgesetzt.</block>
  <block id="6f07b53963ee80903f0f13654de2cc3a" category="section-title">Überlegungen zum Failover</block>
  <block id="9d33d94019b4a595a1ea232926a4da1b" category="paragraph">Wenn es um Failover an einem Standort zusätzlich zur in diesem Leitfaden beschriebenen Reihenfolge geht, müssen noch einige weitere Aspekte berücksichtigt werden.</block>
  <block id="74abdd5cb502cd65bcf1d7ca484ab0c2" category="paragraph">Ein Problem, mit dem Sie möglicherweise zu kämpfen haben, ist die Netzwerkunterschiede zwischen den Standorten. In einigen Umgebungen können am primären Standort und am DR-Standort dieselben Netzwerk-IP-Adressen verwendet werden. Diese Fähigkeit wird als Stretched Virtual LAN (VLAN) oder Stretched Network Setup bezeichnet. Andere Umgebungen müssen möglicherweise unterschiedliche Netzwerk-IP-Adressen (z. B. in unterschiedlichen VLANs) am primären Standort relativ zum DR-Standort verwenden.</block>
  <block id="9b8890fe5b1ab29543118b12a7bd0a07" category="inline-link-macro">NSX-T-Optionen mit SRM</block>
  <block id="25154023650451d6cc971fc54bff8898" category="paragraph">VMware bietet verschiedene Möglichkeiten zur Lösung dieses Problems. Netzwerkvirtualisierungstechnologien wie VMware NSX-T Data Center abstrahieren den gesamten Netzwerk-Stack von Ebene 2 bis 7 von der Betriebsumgebung und ermöglichen so portablere Lösungen. Weitere Informationen zu <block ref="3e6a2bd8e1acf69d423b7944c6543271" category="inline-link-macro-rx"></block>.</block>
  <block id="b56336aa967325217297d8725b64eb0b" category="inline-link-macro">VMware Dokumentation</block>
  <block id="d32598436410006707b2ad1d79395f02" category="paragraph">Um SRM so zu konfigurieren, dass verschiedene Netzwerkeinstellungen auf mehrere VMs angewendet werden können, ohne die Eigenschaften der einzelnen im Recovery-Plan bearbeiten zu müssen, stellt VMware ein Tool namens dr-ip-Customizer bereit. Informationen zur Verwendung dieses Dienstprogramms finden Sie unter <block ref="837c7339284b78d4fd5550c5d0fb1a68" category="inline-link-macro-rx"></block>.</block>
  <block id="3c8c527afa8ce5009c8f707f7fd4fabf" category="section-title">Schützen</block>
  <block id="3b496038cb575fc404f8b0783e570f7a" category="paragraph">Nach einem Recovery wird der Recovery-Standort zum neuen Produktionsstandort. Da der Recovery-Vorgang die SnapMirror Replizierung ausbrach, ist der neue Produktionsstandort nicht vor zukünftigen Ausfällen geschützt. Als Best Practice wird empfohlen, den neuen Produktionsstandort unmittelbar nach dem Recovery auf einen anderen Standort zu schützen. Wenn der ursprüngliche Produktionsstandort betriebsbereit ist, kann der VMware Administrator den ursprünglichen Produktionsstandort als neuen Recovery-Standort zum Schutz des neuen Produktionsstandorts verwenden und damit die Richtung des Schutzes umkehren. Repschutz ist nur bei nicht-katastrophalen Ausfällen verfügbar. Daher müssen die ursprünglichen vCenter Server, ESXi Server, SRM Server und entsprechenden Datenbanken irgendwann wiederhergestellt werden können. Falls diese nicht verfügbar sind, müssen eine neue Schutzgruppe und ein neuer Recovery-Plan erstellt werden.</block>
  <block id="2bf236f8aceaaff2b305848df524c42e" category="paragraph">Ein Failback-Vorgang ist im Grunde ein Failover in eine andere Richtung als zuvor. Als Best Practice überprüfen Sie, ob der ursprüngliche Standort wieder zu akzeptablen Funktionsstufen zurückkehrt, bevor Sie ein Failback durchführen, oder, anders ausgedrückt, ein Failover zum ursprünglichen Standort durchführen. Falls der ursprüngliche Standort weiterhin kompromittiert wird, sollten Sie ein Failback verzögern, bis der Ausfall ausreichend behoben ist.</block>
  <block id="cb03753531bad31391d7156433b98ae6" category="paragraph">Eine weitere Failback Best Practice besteht darin, immer einen Test-Failover auszuführen, nachdem der erneute Schutz abgeschlossen und bevor das endgültige Failback durchgeführt wurde. Dadurch wird sichergestellt, dass die vorhandenen Systeme am ursprünglichen Standort den Betrieb abschließen können.</block>
  <block id="900a3b65ea54b6dcff72ed0d6ac66fdf" category="section-title">Wiederherstellung der Originalseite</block>
  <block id="12d059a2c7519f2e3497b30a71121503" category="paragraph">Wenn eine erneute Sicherung nach dem Failback ausgeführt wird, befindet sich die Umgebung im Wesentlichen in dem Zustand, in dem sie sich zu Beginn befand. Die SnapMirror Replizierung wird erneut vom Produktionsstandort zum Recovery-Standort ausgeführt.</block>
  <block id="39e5fc9eb192f011ab14dae6c2974c4e" category="list-text">*Threat Modeling.* der Zweck der Bedrohungsmodellierung ist es, Sicherheitslücken in einem Feature, einer Komponente oder einem Produkt frühzeitig im Lebenszyklus der Softwareentwicklung zu entdecken. Ein Bedrohungsmodell ist eine strukturierte Darstellung aller Informationen, die die Sicherheit einer Anwendung beeinflussen. Im Wesentlichen ist es ein Blick auf die Anwendung und ihre Umgebung durch die Linsen der Sicherheit.</block>
  <block id="9f16ab3a50bca32d19c4729319035c8d" category="list-text">*Dynamic Application Security Testing (DAST).* Diese Technologie wurde entwickelt, um gefährdete Bedingungen für Anwendungen im laufenden Zustand zu erkennen. DAST testet die freigesetzten HTTP- und HTML-Schnittstellen von Web-enable-Anwendungen.</block>
  <block id="267bed0525e4dab477e4c24ca5a1794e" category="list-text">*Codewährung von Drittanbietern.* im Rahmen der Softwareentwicklung mit Open-Source-Software (OSS) müssen Sie Sicherheitslücken schließen, die mit jedem OSS in Ihr Produkt integriert werden könnten. Dies ist ein fortdauernde Bemühung, da bei einer neuen OSS-Version möglicherweise jederzeit eine neu entdeckte Sicherheitsanfälligkeit gemeldet wird.</block>
  <block id="db7bc87c89c1ee76d313865a32fc0e06" category="list-text">*Schwachstellenscans.* Zweck der Schwachstellenanalyse ist es, häufige und bekannte Sicherheitslücken in NetApp Produkten zu erkennen, bevor diese bei den Kunden freigegeben werden.</block>
  <block id="1a4c104571630526efc77b84e82380fe" category="list-text">*Penetrationstest.* Penetrationstest ist der Prozess, um ein System, eine Web-Anwendung oder ein Netzwerk zu bewerten, um Sicherheitslücken zu finden, die von einem Angreifer ausgenutzt werden könnten. Penetrationstests (Penetrationstests) bei NetApp werden von einer Gruppe genehmigter und vertrauenswürdiger Drittanbieter durchgeführt. Ihr Testumfang umfasst die Einleitung von Angriffen gegen eine Anwendung oder Software ähnlich wie feindliche Eindringlinge oder Hacker mit ausgereiften Methoden oder Tools zur Ausbeutung.</block>
  <block id="7e98fc3ea1ec077cfd1727a58e9c9020" category="section-title">Produktsicherheitsfunktionen</block>
  <block id="dff88c2ef0b49b09246ffd6f9ec55195" category="list-text">*Anmeldebanner* SSH ist standardmäßig deaktiviert und erlaubt nur einmalige Anmeldungen, wenn sie über die VM-Konsole aktiviert sind. Das folgende Anmeldebanner wird angezeigt, nachdem der Benutzer einen Benutzernamen in die Anmeldeaufforderung eingegeben hat:</block>
  <block id="399875025e8e96cc13102a4dd72f2434" category="paragraph">*WARNUNG:* der unerlaubte Zugriff auf dieses System ist verboten und wird gesetzlich verfolgt. Durch den Zugriff auf dieses System erklären Sie sich damit einverstanden, dass Ihre Maßnahmen überwacht werden können, wenn eine unbefugte Nutzung vermutet wird.</block>
  <block id="2d8330ed99c80a842ffbd1362e038e5e" category="paragraph">Nachdem der Benutzer die Anmeldung über den SSH-Kanal abgeschlossen hat, wird der folgende Text angezeigt:</block>
  <block id="677528ad13d460c058ac50e8a62092cf" category="list-text">*Rollenbasierte Zugriffssteuerung (Role Based Access Control, RBAC).* ONTAP Tools verfügen über zwei Arten von RBAC-Steuerungsoptionen:</block>
  <block id="ac38773d017495a97d5b88f242578cb8" category="list-text">Native vCenter Server-Berechtigungen</block>
  <block id="c2e2fce3a995f59900d7afbbe58683f5" category="inline-link">Dieser Link</block>
  <block id="31cedac82fef311cb790a12f96897223" category="list-text">Spezifische Berechtigungen für vCenter Plug-in Weitere Informationen finden Sie unter<block ref="e5f6920797dbff91ef59367270a82669" category="inline-link-rx"></block>.</block>
  <block id="5d32469f8a65b884a8f70abf95fe485a" category="list-text">*Verschlüsselte Kommunikationskanäle.* Alle externen Kommunikation erfolgt über HTTPS mit Version 1.2 von TLS.</block>
  <block id="600d4f98483fae8e58054f976d7e0c0e" category="list-text">*Minimal Port Exposure.* nur die benötigten Ports sind an der Firewall geöffnet.</block>
  <block id="0406f8902379502d1eba01f043c232b7" category="paragraph">In der folgenden Tabelle werden die Details zum offenen Anschluss beschrieben.</block>
  <block id="69fc9fe9cbb7709e97a433352aecf77d" category="cell">TCP v4/v6-Port #</block>
  <block id="02674a4ef33e11c879283629996c8ff8" category="cell">Richtung</block>
  <block id="86408593c34af77fdd90df932f8b5261" category="cell">Funktion</block>
  <block id="0c95054981de037de06e544a52eb3613" category="cell">8143</block>
  <block id="a8e6fe5b9e68f30a146cefebaa7edcc3" category="cell">Eingehend</block>
  <block id="d16c4afdc5f340936b747baf91efc843" category="cell">HTTPS-Verbindungen für REST-API</block>
  <block id="5bd529d5b07b647a8863cf71e98d651a" category="cell">8043</block>
  <block id="1f4deeb2f64336d4ff65ea3d2b4ffe2f" category="cell">HTTPS-Verbindungen</block>
  <block id="cb4b69eb9bd10da82c15dca2f86a1385" category="cell">9060</block>
  <block id="5852f8019b572f4cdef5bab783fa799f" category="cell">HTTPS-Verbindungen
Wird für SOAP-über-https-Verbindungen verwendet
Dieser Port muss geöffnet werden, damit ein Client eine Verbindung zum ONTAP Tools API-Server herstellen kann.</block>
  <block id="b6d767d2f8ed5d21a44b0e5886680cb9" category="cell">22</block>
  <block id="f4e339608b243f9b57b78ffaf061b498" category="cell">SSH (standardmäßig deaktiviert)</block>
  <block id="d82d678e9583c1f5f283ec56fbf1abb7" category="cell">9080</block>
  <block id="ef34781e03f49b5db5dd8fa267c4c41b" category="cell">HTTPS-Verbindungen - VP und SRA - nur interne Verbindungen von Loopback</block>
  <block id="a44ba9086b2b83ccf2baf7c678723449" category="cell">9083</block>
  <block id="fe01ac95967cd8cb5f5b5669b685277a" category="cell">HTTPS-Verbindungen – VP und SRA
Wird für SOAP-über-https-Verbindungen verwendet</block>
  <block id="abea47ba24142ed16b7d8fbf2c740e0d" category="cell">1162</block>
  <block id="4a0724da5c6f4e4817663ae822550800" category="cell">VP SNMP-Trap-Pakete</block>
  <block id="5cce8dede893813f879b873962fb669f" category="cell">1527</block>
  <block id="91c15b8eb529bf2100c10383d1010a1e" category="cell">Nur zur internen Nutzung</block>
  <block id="60135f95267c6e0711bf5a2858dfff2f" category="cell">Derby-Datenbank-Port, nur zwischen diesem Computer und sich selbst, externe Verbindungen nicht akzeptiert -- nur interne Verbindungen</block>
  <block id="13f3cf8c531952d72e5847c4183e6910" category="cell">443</block>
  <block id="1ef2b5426b0824e93e33753fa87ddbf5" category="cell">Bidirektional</block>
  <block id="e4b8a8d8761aab7733e317b585d5d606" category="cell">Wird für Verbindungen zu ONTAP-Clustern verwendet</block>
  <block id="bf6184406d1e18fffc86ecf770fbb391" category="inline-link">kb-Artikel</block>
  <block id="fedac49792829de4ff38fcf7a3846faa" category="list-text">*Unterstützung für Zertifizierungsstelle (CA) signierte Zertifikate.* ONTAP Tools für VMware vSphere unterstützt CA signierte Zertifikate. Siehe das<block ref="0903a062b2072644a9b744a7fece4216" category="inline-link-rx"></block> Finden Sie weitere Informationen.</block>
  <block id="9fca19988370da2a459b24505e9d23d5" category="list-text">*Audit Logging.* Supportpakete können heruntergeladen werden und sind äußerst detailliert. Die ONTAP Tools protokollieren alle Benutzer-Login- und -Abmeldeaktivitäten in einer separaten Protokolldatei. VASA API-Aufrufe werden in einem dedizierten VASA Audit Log (Local cxf.log) protokolliert.</block>
  <block id="7d60a2806aedd934b75cce55d6693689" category="list-text">*Passwortrichtlinien.* folgende Kennwortrichtlinien werden befolgt:</block>
  <block id="f82f7513742f08b9d2784923213bdc75" category="list-text">Passwörter werden nicht in Protokolldateien protokolliert.</block>
  <block id="83040c348e071499488a8128db207545" category="list-text">Passwörter werden nicht im Klartext kommuniziert.</block>
  <block id="bc319862df3312f423a50fece7730db9" category="list-text">Während des Installationsvorgangs selbst werden Passwörter konfiguriert.</block>
  <block id="853c2ea85b86882d0ea73b606a9800a4" category="list-text">Der Passwortverlauf ist ein konfigurierbarer Parameter.</block>
  <block id="b3d3877bc96752ae50a409c72597d47a" category="list-text">Das Mindestalter des Kennworts ist auf 24 Stunden festgelegt.</block>
  <block id="fb0bdec50022a37424a405b53da7c9c8" category="list-text">Die Felder für das Kennwort werden automatisch ausgefüllt.</block>
  <block id="3a04f054be8ad983ea82fd5079519810" category="list-text">ONTAP-Tools verschlüsselt alle gespeicherten Anmeldeinformationen mithilfe von SHA256 Hashing.</block>
  <block id="46e61d6e7c848d43ddcede8efd36c3d8" category="doc">SnapCenter Plug-in VMware vSphere</block>
  <block id="ae9e79ac4b60eba67cf1c7508417b42c" category="paragraph">Das NetApp SnapCenter Plug-in für VMware vSphere nutzt folgende sichere Entwicklungsaktivitäten:</block>
  <block id="b25a1098aff7d0c5ee590f1e1a114bb9" category="list-text">*Dynamic Application Security Testing (DAST).* Technologien, die entwickelt wurden, um gefährdete Bedingungen für Anwendungen in ihrem laufenden Zustand zu erkennen. DAST testet die freigesetzten HTTP- und HTML-Schnittstellen von Web-enable-Anwendungen.</block>
  <block id="6cc1fee6b4fc48755d78e93170bbed93" category="list-text">*Codewährung von Drittanbietern.* im Rahmen der Entwicklung von Software und der Verwendung von Open-Source-Software (OSS) ist es wichtig, Sicherheitslücken zu beheben, die mit OSS verbunden sein könnten, die in Ihr Produkt integriert wurden. Dies ist ein kontinuierlicher Aufwand, da bei der Version der OSS-Komponente eine neu entdeckte Sicherheitsanfälligkeit jederzeit gemeldet wird.</block>
  <block id="87bd0fb9b4198f33535e0e1888a809f5" category="list-text">*Penetrationstest.* Penetrationstest ist der Prozess, um ein System, eine Web-Anwendung oder ein Netzwerk zu evaluieren, um Sicherheitslücken zu finden, die von einem Angreifer ausgenutzt werden könnten. Penetrationstests (Penetrationstests) bei NetApp werden von einer Gruppe genehmigter und vertrauenswürdiger Drittanbieter durchgeführt. Ihr Testumfang umfasst die Einleitung von Angriffen gegen eine Anwendung oder Software wie feindliche Eindringlinge oder Hacker mit ausgereiften Exploitationsmethoden oder -Tools.</block>
  <block id="e91db3b3278f7bd95cc704d74c5c9597" category="paragraph">Das NetApp SnapCenter Plug-in für VMware vSphere umfasst die folgenden Sicherheitsfunktionen in jeder Version:</block>
  <block id="4abd7d1d5d223683d3346671efb7e89c" category="list-text">*Eingeschränkter Shell-Zugriff.* SSH ist standardmäßig deaktiviert, und einmalige Anmeldungen sind nur erlaubt, wenn sie über die VM-Konsole aktiviert sind.</block>
  <block id="8c264fa5ca2c588d960d98bd30cbc897" category="list-text">*Zugangswarnung im Anmeldebanner.* das folgende Anmeldebanner wird angezeigt, nachdem der Benutzer einen Benutzernamen in die Anmeldeaufforderung eingegeben hat:</block>
  <block id="0d6633ada3a9803e206b2359d859e892" category="paragraph">Nachdem der Benutzer die Anmeldung über den SSH-Kanal abgeschlossen hat, wird die folgende Ausgabe angezeigt:</block>
  <block id="b883bf79639dfce77e395b8458ee69e4" category="list-text">Native vCenter Server-Berechtigungen.</block>
  <block id="721796b1cb3468f0daf819180184d460" category="inline-link">Rollenbasierte Zugriffssteuerung (Role Based Access Control, RBAC)</block>
  <block id="1d08c46b7567a29be0690a92b9722a58" category="list-text">Spezifische Berechtigungen für VMware vCenter Plug-in Weitere Informationen finden Sie unter<block ref="f351263ad3706bd0a472039400dece78" category="inline-link-rx"></block>.</block>
  <block id="acab4c655c4a7b4db67fc07e22b86222" category="list-text">*Verschlüsselte Kommunikationskanäle.* Alle externen Kommunikation erfolgt über HTTPS mit TLS.</block>
  <block id="47a37e2d5fc8b7c936ad9b2b7a569a08" category="paragraph">Die folgende Tabelle enthält die Details zum offenen Anschluss.</block>
  <block id="10bd16a816f831080065e807daa36a9a" category="cell">TCP v4/v6-Portnummer</block>
  <block id="edf0320adc8658b25ca26be5351b6c4a" category="cell">8144</block>
  <block id="d4a973e303ec37692cc8923e3148eef7" category="cell">8080</block>
  <block id="2c25c3d66f80eeaa8d6e5a144c6f942e" category="cell">HTTPS-Verbindungen für OVA GUI</block>
  <block id="14b3e670eec63cc0cc456fd904bbfbd9" category="cell">SSH (standardmäßig deaktiviert)</block>
  <block id="16fc18d787294ad5171100e33d05d4e2" category="cell">3306</block>
  <block id="4cbf9c234f6b34c242a110cb993252bd" category="cell">MySQL (nur interne Verbindungen; externe Verbindungen standardmäßig deaktiviert)</block>
  <block id="29ab4efbdb7c727c81ee1382593bc5e2" category="cell">Nginx (Datensicherungsservices)</block>
  <block id="89f152cad33314315b4995ed1ca71535" category="inline-link">Erstellen und/oder Importieren eines SSL-Zertifikats in das SnapCenter Plug-in für VMware vSphere (SCV)</block>
  <block id="450adc59e39f23172756ac9369494a2d" category="list-text">*Unterstützung für Zertifizierungsstelle (CA) signierte Zertifikate.* SnapCenter Plug-in für VMware vSphere unterstützt die Funktion von CA signierten Zertifikaten. Siehe<block ref="9f83d87d5d9f604d96288df21d450fac" category="inline-link-rx"></block>.</block>
  <block id="05a1aa8021df4579874c4eeb8788e5c9" category="list-text">*Passwortrichtlinien.* die folgenden Kennwortrichtlinien sind in Kraft:</block>
  <block id="2c87609dbbc3630573c64e271de8207d" category="list-text">Alle Anmeldeinformationen werden mit SHA256 Hashing gespeichert.</block>
  <block id="0f1a6fa0a65f9d5c3c4a16b070104d7a" category="list-text">*Basis Betriebssystem-Image.* das Produkt wird mit Debian Base OS für OVA ausgeliefert, mit eingeschränktem Zugriff und Shell-Zugriff. So wird die Angriffsfläche reduziert. Jedes Betriebssystem der SnapCenter Version wird mit den neuesten Sicherheits-Patches aktualisiert, die für maximale Sicherheit verfügbar sind.</block>
  <block id="83cd5867ba912e517fa1100299fd1cf3" category="paragraph">NetApp entwickelt Softwarefunktionen und Sicherheits-Patches zu den SnapCenter Plug-ins für die VMware vSphere Appliance und gibt sie anschließend dem Kunden als gebündelte Software-Plattform frei. Da diese Appliances bestimmte Linux Unterbetriebssystem-Abhängigkeiten sowie unsere proprietäre Software umfassen, empfiehlt NetApp, am Unterbetriebssystem keine Änderungen vorzunehmen, da dies ein hohes Potenzial hat, die NetApp Appliance zu beeinträchtigen. Dies könnte sich darauf auswirken, inwieweit NetApp die Appliance unterstützt. NetApp empfiehlt, unsere neueste Code-Version für Appliances zu testen und zu implementieren, da sie veröffentlicht werden, um sicherheitsbezogene Probleme zu patchen.</block>
  <block id="1bc6cee680286aae1b12889369b7f18a" category="summary">MySQL auf ONTAP</block>
  <block id="dac8ea4fbfd87a535663c227b91f50e3" category="doc">I/O-Planer</block>
  <block id="215dca219378b291ae287eb076489fdd" category="paragraph">Der Linux-Kernel ermöglicht eine Steuerung auf niedriger Ebene über die Art und Weise, wie I/O-Vorgänge zum Blockieren von Geräten geplant werden.</block>
  <block id="dc36d23ef487690f1698c69d9e49cee2" category="paragraph">Die Standardwerte auf verschiedenen Linux-Distributionen variieren erheblich. MySQL empfiehlt, dass Sie verwenden<block ref="722d122e81cbbe543bd5520bb8678c0e" prefix=" " category="inline-code"></block> Oder A<block ref="30e482a8ab57cfc19075dece53b0a56b" prefix=" " category="inline-code"></block> I/O-Scheduler mit nativem asynchronem I/O (AIO) unter Linux. Im Allgemeinen zeigen NetApp Kunden und interne Tests mit NoOps bessere Ergebnisse.</block>
  <block id="6def78482bd1915cb81f0f5c19ee8fe6" category="paragraph">Die InnoDB Storage Engine von MySQL verwendet das asynchrone I/O-Subsystem (native AIO) unter Linux, um Lese- und Schreibanforderungen für Datendateiseiten durchzuführen. Dieses Verhalten wird vom gesteuert<block ref="2c03186a22d036ce7dad84be5de2f2ce" prefix=" " category="inline-code"></block> Die standardmäßig aktivierte Konfigurationsoption. Bei nativem AIO hat der Typ des I/O-Planers einen größeren Einfluss auf die I/O-Performance. Durchführung von Benchmarks zur Bestimmung des I/O-Planers, der die besten Ergebnisse für Ihren Workload und Ihre Umgebung liefert</block>
  <block id="e7b09beaf26efc7d8bee91786cf794b5" category="paragraph">Anweisungen zur Konfiguration des I/O-Planers finden Sie in der entsprechenden Dokumentation zu Linux und MySQL.</block>
  <block id="6e8775bd755a8835ce86806d669677ea" category="doc">Storage-Konfiguration</block>
  <block id="2f09ac000b4a1808cb795b7bdbb20ecc" category="doc">innodb_Log_file_size</block>
  <block id="b8674b2897b288ca55b74328df4780b3" category="paragraph">Die Auswahl der richtigen Größe für die InnoDB-Protokolldateigröße ist wichtig für die Schreibvorgänge und für eine anständige Wiederherstellungszeit nach einem Serverabsturz.</block>
  <block id="b61bad6f7bf48bf9abda0c86c9beed79" category="paragraph">Da so viele Transaktionen in der Datei angemeldet sind, ist die Größe der Protokolldatei für Schreibvorgänge wichtig. Wenn Datensätze geändert werden, wird die Änderung nicht sofort in den Tablespace zurückgeschrieben. Stattdessen wird die Änderung am Ende der Protokolldatei aufgezeichnet und die Seite als verschmutzt markiert. InnoDB verwendet sein Protokoll, um zufällige I/O in sequenzielle I/O-Vorgänge zu konvertieren</block>
  <block id="e87d83a3c0af4df83efb4784b5182f8f" category="paragraph">Wenn das Protokoll voll ist, wird die fehlerhafte Seite nacheinander in den Tablespace geschrieben, um Speicherplatz in der Protokolldatei freizugeben. Angenommen, ein Server stürzt mitten in einer Transaktion ab, und die Schreibvorgänge werden nur in der Protokolldatei aufgezeichnet. Bevor der Server wieder live gehen kann, muss er eine Wiederherstellungsphase durchlaufen, in der die in der Protokolldatei aufgezeichneten Änderungen wiedergegeben werden. Je mehr Einträge in der Protokolldatei vorhanden sind, desto länger dauert es, bis der Server wiederhergestellt ist.</block>
  <block id="8321756b7fedf72543a1b0917bf92613" category="paragraph">In diesem Beispiel wirkt sich die Größe der Protokolldatei sowohl auf die Wiederherstellungszeit als auch auf die Schreib-Performance aus. Wenn Sie die richtige Zahl für die Größe der Protokolldatei wählen, gleichen Sie die Recovery-Zeit mit der Schreib-Performance ab. Normalerweise ist alles zwischen 128M und 512M ein gutes Preis-Leistungs-Verhältnis.</block>
  <block id="d4ae77cd65c244ceb4277b27553d6931" category="doc">MySQL-Datenbanken auf ONTAP</block>
  <block id="598132a821e8cc696fbbae934047d991" category="paragraph">MySQL und seine Varianten, darunter MariaDB und Percona MySQL, ist die weltweit beliebteste Datenbank.</block>
  <block id="35701e1bcd40c8e1a520fc33d0e14842" category="doc">innodb_Flush_Method</block>
  <block id="138738e11d5fda5c496edaa92e32e158" category="paragraph">Der Parameter innodb_flush_method gibt an, wie InnoDB die Protokoll- und Datendateien öffnet und löscht.</block>
  <block id="c262c8cd82b96f69fbbc058d7a35683b" category="section-title">Optimierungen</block>
  <block id="d1e67ebe6122765789852eb430c42c75" category="paragraph">In der InnoDB-Optimierung wird durch die Einstellung dieses Parameters die Datenbankleistung ggf. optimiert.</block>
  <block id="51ca63a7fb332d4d18e7139ea5787c50" category="paragraph">Die folgenden Optionen sind für das Spülen der Dateien über InnoDB:</block>
  <block id="a9b8d4c72b58a9554274cba7904d47e2" category="list-text"><block ref="e590322920bebc1156ab585bd6597d76" prefix="" category="inline-code"></block>. InnoDB verwendet die<block ref="4d0b2ade287f8f35e993511729bc75a1" prefix=" " category="inline-code"></block> Systemaufruf, um sowohl die Daten- als auch die Protokolldateien zu leeren. Diese Option ist die Standardeinstellung.</block>
  <block id="8198b1fd4bf2fafa4f4e8ffb4a8ac900" category="list-text"><block ref="a48be70a21bca2cf4b3e481efed66ae4" prefix="" category="inline-code"></block>. InnoDB verwendet die<block ref="a48be70a21bca2cf4b3e481efed66ae4" prefix=" " category="inline-code"></block> Option zum Öffnen und Leeren der Protokolldateien und fsync() zum Leeren der Datendateien. InnoDB wird nicht verwendet<block ref="a48be70a21bca2cf4b3e481efed66ae4" prefix=" " category="inline-code"></block> Direkt, weil es Probleme mit ihm auf vielen Sorten von UNIX.</block>
  <block id="f89f9147df5dde99a7944d2028b59e7e" category="list-text"><block ref="6a334fd488ef92b18584207148410cc4" prefix="" category="inline-code"></block>. InnoDB verwendet die<block ref="6a334fd488ef92b18584207148410cc4" prefix=" " category="inline-code"></block> Option (oder<block ref="42d27f470f62deaad644fef8618b59bb" prefix=" " category="inline-code"></block> Unter Solaris), um die Datendateien zu öffnen und zu verwenden<block ref="4d0b2ade287f8f35e993511729bc75a1" prefix=" " category="inline-code"></block> Um sowohl die Daten als auch die Protokolldateien zu löschen. Diese Option ist auf einigen GNU/Linux-Versionen, FreeBSD und Solaris verfügbar.</block>
  <block id="5e90100e134d805a66f8cda2e73f5298" category="list-text"><block ref="d66655d6d13113f23421f282ed1eaeb7" prefix="" category="inline-code"></block>. InnoDB verwendet die<block ref="6a334fd488ef92b18584207148410cc4" prefix=" " category="inline-code"></block> Option beim Spülen von E/A, wird jedoch übersprungen<block ref="4d0b2ade287f8f35e993511729bc75a1" prefix=" " category="inline-code"></block> Systemaufruf danach. Diese Option ist für einige Arten von Dateisystemen ungeeignet (z. B. XFS). Wenn Sie sich nicht sicher sind, ob Ihr Dateisystem einen erfordert<block ref="4d0b2ade287f8f35e993511729bc75a1" prefix=" " category="inline-code"></block> Systemaufruf – zum Beispiel zum Beibehalten aller Dateimetadaten – verwendet den<block ref="6a334fd488ef92b18584207148410cc4" prefix=" " category="inline-code"></block> Wählen Sie stattdessen.</block>
  <block id="c680d437163cc6bab4f9bdb35c3073d0" category="section-title">Beobachtung</block>
  <block id="907dee0040812e8e7d1fd00b870b5063" category="paragraph">In den NetApp-Labortests ist der<block ref="e590322920bebc1156ab585bd6597d76" prefix=" " category="inline-code"></block> Auf NFS und SAN kam die Standardoption zum Einsatz. Im Vergleich dazu war sie ein großartiger Performance-Improvisator<block ref="6a334fd488ef92b18584207148410cc4" prefix=" " category="inline-code"></block>. Bei Verwendung der Spülmethode als<block ref="6a334fd488ef92b18584207148410cc4" prefix=" " category="inline-code"></block> Bei ONTAP konnten wir beobachten, dass der Client viele Single-Byte-Schreibvorgänge am Rand des 4096. Blocks in serieller Form schreibt. Diese Schreibvorgänge haben die Latenz über das Netzwerk erhöht und die Performance beeinträchtigt.</block>
  <block id="2f0b9319cf6d59e07ea4b1ef65a12be1" category="doc">Open_File_Limits</block>
  <block id="11e52b21a9795b7dcf947027b0ec5d01" category="paragraph">Der<block ref="2f0b9319cf6d59e07ea4b1ef65a12be1" prefix=" " category="inline-code"></block> Parameter bestimmt die Anzahl der Dateien, die das Betriebssystem mysqld zum Öffnen zulässt.</block>
  <block id="e0732f22b2900006e8fcb6367fd162f7" category="paragraph">Der Wert dieses Parameters zur Laufzeit ist der vom System zulässige Realwert und kann sich von dem Wert unterscheiden, den Sie beim Serverstart angeben. Der Wert ist 0 auf Systemen, bei denen MySQL die Anzahl der geöffneten Dateien nicht ändern kann. Die effektive<block ref="3d39e9c5aa2f78830dd6993cec18e93e" prefix=" " category="inline-code"></block> Der Wert basiert auf dem Wert, der beim Systemstart (falls vorhanden) angegeben wurde, und den Werten von<block ref="0bc91339c0d774f560a9a38fc9dfac30" prefix=" " category="inline-code"></block> Und<block ref="023a3b50ddf424c9a1d51984190a0c92" prefix=" " category="inline-code"></block> Mit diesen Formeln:</block>
  <block id="6b840ff0a7667f4642a0844269657fc4" category="list-text">10 +<block ref="0bc91339c0d774f560a9a38fc9dfac30" prefix=" " category="inline-code"></block> + <block ref="023a3b50ddf424c9a1d51984190a0c92" prefix="(" category="inline-code"></block> 2 x)</block>
  <block id="d3de183489c20b8efa2947eeff3028d7" category="list-text"><block ref="0bc91339c0d774f560a9a38fc9dfac30" prefix="" category="inline-code"></block> X 5</block>
  <block id="2c08922fbaba089e7c2982df43fd352a" category="list-text">Betriebssystemgrenze, wenn positiv</block>
  <block id="8ca153937fcbdf56497ad779a91f31d1" category="list-text">Wenn die Betriebssystemgrenze unendlich ist:<block ref="3d39e9c5aa2f78830dd6993cec18e93e" prefix=" " category="inline-code"></block> Wert wird beim Start angegeben; 5,000 wenn keine</block>
  <block id="98038bb4026a70f5ee4041522c6eebe4" category="paragraph">Der Server versucht, die Anzahl der Dateideskriptoren mit dem Maximum dieser vier Werte zu ermitteln. Wenn so viele Deskriptoren nicht abgerufen werden können, versucht der Server, so viele zu erhalten, wie das System zulässt.</block>
  <block id="3ee58d04a0dc9b2551698643e0421005" category="doc">innodb_lru_Scan_depth</block>
  <block id="29d0f4f8b383d39e73abd11ac73ca28a" category="paragraph">Der<block ref="3ee58d04a0dc9b2551698643e0421005" prefix=" " category="inline-code"></block> Parameter beeinflusst die Algorithmen und Heuristiken des Flush-Vorgangs für den InnoDB Pufferpool.</block>
  <block id="eeb16232629bd788f0d29c104105bba2" category="paragraph">Dieser Parameter ist in erster Linie an Performance-Experten interessiert, die I/O-intensive Workloads optimieren. Dieser Parameter gibt für jede Pufferpoolinstanz an, wie weit der Seitenauflauf des Seitenreinigers in der LRU-Seitenliste (Least Recently Used) weiter scannen soll, und sucht nach verschmutzten Seiten, die bereinigt werden sollen. Dieser Hintergrundvorgang wird einmal pro Sekunde durchgeführt.</block>
  <block id="39359bf2914e4e1cf84a9dea580fd20a" category="paragraph">Sie können den Wert nach oben oder unten anpassen, um die Anzahl der freien Seiten zu minimieren. Stellen Sie den Wert nicht wesentlich höher ein als erforderlich, da die Scans erhebliche Performancekosten verursachen können. Ziehen Sie außerdem in Betracht, diesen Parameter anzupassen, wenn Sie die Anzahl der Pufferpool-Instanzen ändern, da<block ref="f2c8312d381c16c6c95a727c3f61a4c7" prefix=" " category="inline-code"></block> Definiert den Umfang der Arbeit, die durch den Seitenreinigungsfaden jede Sekunde durchgeführt wird.</block>
  <block id="513a275422fae2456617321a2f3fc0af" category="paragraph">Eine Einstellung, die kleiner als die Standardeinstellung ist, eignet sich für die meisten Workloads. Ziehen Sie in Betracht, diesen Wert nur zu erhöhen, wenn Sie freie I/O-Kapazität bei einem typischen Workload haben. Wenn ein schreibintensiver Workload die I/O-Kapazität aussättigt, verringern Sie den Wert umgekehrt, insbesondere wenn ein großer Puffer-Pool vorhanden ist.</block>
  <block id="62812b938ba1113b81bd7f612ad0e6f8" category="doc">innodb_Buffer_Pool_size</block>
  <block id="d7a8b502d05f5e477158eb80c1fb8dae" category="paragraph">Der InnoDB Pufferpool ist der wichtigste Teil jeder Tuning-Aktivität.</block>
  <block id="5cb6c45d18482180f34ae727b53379fb" category="paragraph">InnoDB setzt stark auf den Pufferpool für das Caching von Indizes und Rudern der Daten, den adaptiven Hash-Index, den Insert-Puffer und viele andere interne Datenstrukturen. Der Puffer-Pool puffert Änderungen an Daten, damit Schreibzugriffe nicht sofort in den Storage übernommen werden müssen, was die Performance verbessert. Der Pufferpool ist integraler Bestandteil von InnoDB und muss entsprechend angepasst werden. Berücksichtigen Sie beim Festlegen der Größe des Puffer-Pools die folgenden Faktoren:</block>
  <block id="03f07291794c3b9cb00d04cb9f30be81" category="list-text">Stellen Sie für eine dedizierte InnoDB-Maschine die Pufferpoolgröße auf 80 % oder mehr verfügbaren RAM ein.</block>
  <block id="7b0119b7fa59f87d23d5c65da456b226" category="list-text">Wenn es sich nicht um einen dedizierten MySQL-Server handelt, stellen Sie die Größe auf 50 % des RAM ein.</block>
  <block id="a6a9695e1464f5554c0006c55facfadf" category="doc">Dateideskriptoren</block>
  <block id="8e222c68dd214db9f8a1b5b572ee6267" category="paragraph">Sie werden verwendet, um neue Verbindungen zu öffnen, Tabellen im Cache zu speichern, temporäre Tabellen zum Beheben komplizierter Abfragen zu erstellen und auf persistente zuzugreifen. Wenn mysqld nicht in der Lage ist, neue Dateien zu öffnen, wenn nötig, kann es nicht mehr richtig funktionieren. Ein häufiges Symptom dieses Problems ist Fehler 24, „zu viele geöffnete Dateien“. Die Anzahl der Dateideskriptoren, die mysqld gleichzeitig öffnen kann, wird durch das definiert<block ref="3d39e9c5aa2f78830dd6993cec18e93e" prefix=" " category="inline-code"></block> In der Konfigurationsdatei festgelegte Option <block ref="86d0d007043d911697753b35e2c18f35" prefix="(" category="inline-code"></block>). Aber<block ref="3d39e9c5aa2f78830dd6993cec18e93e" prefix=" " category="inline-code"></block> Hängt auch von den Grenzen des Betriebssystems ab. Diese Abhängigkeit macht die Einstellung der Variable komplizierter.</block>
  <block id="07561a91f81dfe3c8e4364aac8bdbea4" category="paragraph">MySQL kann seine Einstellung nicht festlegen<block ref="3d39e9c5aa2f78830dd6993cec18e93e" prefix=" " category="inline-code"></block> Option höher als unter angegeben<block ref="dd1c6d8164dfd1cee0afa39b177e843b" prefix=" " category="inline-code"></block>. Daher müssen Sie diese Grenzwerte explizit auf Betriebssystemebene festlegen, damit MySQL Dateien nach Bedarf öffnen kann. Es gibt zwei Möglichkeiten, die Dateibegrenzung in Linux zu überprüfen:</block>
  <block id="84e705ac120a887df441f4161dc29409" category="list-text">Der<block ref="2eabb4efed7dffb32ea1170eab71b798" prefix=" " category="inline-code"></block> Befehl schnell gibt Ihnen eine detaillierte Beschreibung der Parameter, die erlaubt oder gesperrt werden. Die durch die Ausführung dieses Befehls vorgenommenen Änderungen sind nicht dauerhaft und werden nach einem Neustart des Systems gelöscht.</block>
  <block id="a4c404dc92855948bc4c007a86091dbe" category="list-text">Änderungen an<block ref="b803f460349bd101b1de260021ef0e60" prefix=" " category="inline-code"></block> Die Datei ist dauerhaft und wird durch einen Systemneustart nicht beeinträchtigt.</block>
  <block id="cbcab8220d4ffddbdc44f9936125d83d" category="paragraph">Stellen Sie sicher, dass Sie sowohl die harten als auch die weichen Grenzwerte für Benutzer mysql ändern. Die folgenden Auszüge stammen aus der Konfiguration:</block>
  <block id="97dc47f90c600a96fac6642560ffaceb" category="paragraph">Aktualisieren Sie gleichzeitig dieselbe Konfiguration in<block ref="90f16be5b82ca0aa94088c89fe00b3dd" prefix=" " category="inline-code"></block> Um die offenen Dateigrenzen vollständig zu verwenden.</block>
  <block id="395ec860c9530814e94538b7121a8319" category="doc">innodb_Flush_log_at_trx_commit</block>
  <block id="2f2c7742844f13de59d435b9099c78cc" category="paragraph">Wenn eine Datenänderung vorgenommen wird, wird nicht sofort in den Storage geschrieben.</block>
  <block id="0b36b8f791a48c7b4692e7df069dabc8" category="paragraph">Stattdessen werden die Daten in einem Protokollpuffer aufgezeichnet, einem Teil des Speichers, den InnoDB Pufferänderungen zuweist, die in der Protokolldatei aufgezeichnet werden. InnoDB leert den Puffer in die Protokolldatei, wenn eine Transaktion durchgeführt wird, wenn der Puffer voll wird, oder einmal pro Sekunde, je nachdem, welches Ereignis zuerst eintritt. Die Konfigurationsvariable, die diesen Prozess steuert, ist innodb_flush_log_at_trx_commit. Die Wertoptionen umfassen:</block>
  <block id="280a38864b81c14ba3db52801e087ed0" category="list-text">Wenn Sie es einstellen<block ref="cab9a35054c49f0ba619a6e6943f461b" prefix=" " category="inline-code"></block>, InnoDB schreibt die geänderten Daten (im InnoDB-Pufferpool) in die Log-Datei (ib_logfile) und leert die Log-Datei (Write to Storage) jede Sekunde. Es tut jedoch nichts, wenn die Transaktion durchgeführt wird. Bei einem Stromausfall oder Systemabsturz können die nicht gespeicherten Daten nicht wiederhergestellt werden, da sie weder auf die Protokolldatei noch auf die Laufwerke geschrieben werden.</block>
  <block id="5967bb5566cb7cd8bdb0b372ceb21565" category="list-text">Wenn Sie es einstellen<block ref="1ab7a7611b59147e84a38b6f3e7d9d09" prefix=" " category="inline-code"></block>, InnoDB schreibt den Protokollpuffer in das Transaktionsprotokoll und leert für jede Transaktion auf eine dauerhafte Speicherung. Beispielsweise schreibt InnoDB für alle Transaction Commits in das Protokoll und schreibt anschließend in den Speicher. Langsamer Speicher beeinträchtigt die Performance, beispielsweise wird die Anzahl der InnoDB-Transaktionen pro Sekunde reduziert.</block>
  <block id="9e9b3a43cb7a1b13c5ee3f6519d7f60e" category="list-text">Wenn Sie es einstellen<block ref="44d4dfee9b1c95f8768bec989cd3baf5" prefix=" " category="inline-code"></block>, InnoDB schreibt den Protokollpuffer bei jedem Commit in die Protokolldatei, schreibt aber keine Daten in den Speicher. InnoDB leert die Daten einmal pro Sekunde. Selbst bei einem Stromausfall oder Systemabsturz sind die Daten von Option 2 in der Protokolldatei verfügbar und können wiederhergestellt werden.</block>
  <block id="14f3e0328e25aa02b37bd6817a442a58" category="paragraph">Wenn die Leistung das Hauptziel ist, setzen Sie den Wert auf 2. Da InnoDB einmal pro Sekunde auf die Laufwerke schreibt, nicht für jede Transaktion, verbessert sich die Performance erheblich. Bei einem Stromausfall oder Absturz können die Daten aus dem Transaktions-Log wiederhergestellt werden.</block>
  <block id="4a34cbb1957909093a2e216fc1cd0962" category="paragraph">Wenn die Datensicherheit das Hauptziel ist, setzen Sie den Wert auf 1, sodass InnoDB für jeden Transaktionscommit zu den Laufwerken leert. Möglicherweise ist die Performance jedoch beeinträchtigt.</block>
  <block id="53be3047aa6f8a62c640cc8b4d6089c6" category="admonition">*NetApp empfiehlt* Setzen Sie den innodb_flush_log_trx_commit Wert auf 2, um eine bessere Leistung zu erzielen.</block>
  <block id="2199371f8380c97ad11a03fba3b501c9" category="doc">innodb_io_Capacity</block>
  <block id="ac165f23bb7435219b226ed6e76c5fa4" category="paragraph">Im InnoDB Plug-in wurde ein neuer Parameter namens innodb_io_Capacity aus MySQL 5.7 hinzugefügt.</block>
  <block id="b4f38c42ef968074288f239b42e92b7f" category="paragraph">Er steuert die maximale Anzahl von IOPS, die InnoDB durchführt (einschließlich der Spülrate von schmutzigen Seiten sowie der Batch-Größe des Insert Buffer [ibuf]). Der innodb_io_capacity Parameter setzt eine Obergrenze für IOPS durch InnoDB-Hintergrundaufgaben, wie das Leeren von Seiten aus dem Pufferpool und das Zusammenführen von Daten aus dem Änderungspuffer.</block>
  <block id="9679a08c293f29d9546c42207b09d418" category="paragraph">Legen Sie den Parameter innodb_io_Capacity auf die ungefähre Anzahl von E/A-Vorgängen fest, die das System pro Sekunde durchführen kann. Halten Sie die Einstellung idealerweise so niedrig wie möglich, aber nicht so niedrig, dass Hintergrundaktivitäten langsamer werden. Ist die Einstellung zu hoch, werden die Daten aus dem Puffer-Pool entfernt und Puffer für das Caching zu schnell eingefügt, um einen wesentlichen Vorteil zu erzielen.</block>
  <block id="668b6acc98c3469094062ed50b43e43a" category="admonition">*NetApp empfiehlt*, wenn Sie diese Einstellung über NFS verwenden, das Testergebnis von IOPS (SysBench/FiO) analysieren und den Parameter entsprechend einstellen. Verwenden Sie den kleinstmöglichen Wert zum Spülen und Spülen, um mit dem Schritt zu bleiben, es sei denn, Sie sehen mehr geänderte oder schmutzige Seiten als Sie im InnoDB-Puffer-Pool möchten.</block>
  <block id="038b11d42ee14bd77b6370c575c63b16" category="admonition">Verwenden Sie keine Extremwerte wie 20,000 oder mehr, es sei denn, Sie haben bewiesen, dass niedrigere Werte für Ihre Arbeitsbelastung nicht ausreichen.</block>
  <block id="62385f5ceb285bca676cb0561555d719" category="paragraph">Der InnoDB_IO_Capacity Parameter regelt Spülraten und zugehörige I/O.</block>
  <block id="c957ef06af970a7526a73e741b47fef1" category="admonition">Sie können die Leistung ernsthaft beeinträchtigen, indem Sie diesen Parameter oder den innodb_io_Capacity_max-Parameter zu hoch einstellen und I/O-Operationen mit vorzeitigem Spülen verschwenden.</block>
  <block id="b238d5dd75fe5ed855c5b3047e074050" category="paragraph">Es gibt zwei Optionen zur Konfiguration von MySQL mit SAN unter Verwendung des üblichen zwei-Volume-Modells.</block>
  <block id="58d94bb6e53e154e2f4b16efa09f4c0d" category="paragraph">Kleinere Datenbanken können auf einem Standard-LUN-Paar platziert werden, sofern die I/O- und Kapazitätsanforderungen innerhalb der Grenzen eines einzigen LUN-Filesystems liegen. Eine Datenbank, die etwa 2.000 zufällige IOPS benötigt, kann beispielsweise auf einem einzelnen Filesystem auf einer einzelnen LUN gehostet werden. In ähnlicher Weise würde eine Datenbank mit einer Größe von nur 100 GB auf eine einzige LUN passen, ohne ein Management-Problem zu verursachen.</block>
  <block id="c33bcb4f520265bb106616e844397ffb" category="paragraph">Bei größeren Datenbanken sind mehrere LUNs erforderlich. Beispielsweise würde eine Datenbank, die 100.000 IOPS benötigt, höchstwahrscheinlich mindestens acht LUNs benötigen. Eine einzelne LUN würde zu einem Engpass werden, da die Anzahl der SCSI-Kanäle zu Laufwerken nicht ausreicht. Eine 10-TB-Datenbank wäre ähnlich schwierig zu managen auf einer einzigen 10-TB-LUN. Logical Volume Manager sind so konzipiert, die Performance- und Kapazitätsfunktionen mehrerer LUNs miteinander zu verbinden, um Performance und Management zu verbessern.</block>
  <block id="428e88a1415b00f5eb396829b4bd9a2c" category="paragraph">In beiden Fällen sollte ein Paar ONTAP Volumes ausreichend sein. Bei einer einfachen Konfiguration würde die Datendatei-LUN wie die Protokoll-LUN in ein dediziertes Volume platziert werden. Bei einer logischen Volume Manager-Konfiguration befinden sich alle LUNs in der Datendatei-Volume-Gruppe in einem dedizierten Volume, und die LUNs der Log-Volume-Gruppe befinden sich in einem zweiten dedizierten Volume.</block>
  <block id="e8c31916b755be39cb0a66804a1d8f8b" category="paragraph">*NetApp empfiehlt*, zwei Dateisysteme für MySQL-Bereitstellungen auf SAN zu verwenden:</block>
  <block id="fe9d8ca2c46747e16ed31f64638e66e6" category="list-text">Das erste Dateisystem speichert alle MySQL-Daten einschließlich Tablespace, Daten und Index.</block>
  <block id="04d82ed32fa1eeaa6df117c178062441" category="list-text">Das zweite Filesystem speichert alle Protokolle (binäre Protokolle, langsame Protokolle und Transaktionsprotokolle).</block>
  <block id="15bcb76b7e8b441c0d2a6e723f70c20c" category="paragraph">Es gibt mehrere Gründe für die Trennung von Daten auf diese Weise, unter anderem:</block>
  <block id="02774797d31a8f1c6d805860038c32a3" category="list-text">Die I/O-Muster von Datendateien und Protokolldateien unterscheiden sich. Eine Trennung würde mehr Optionen mit QoS-Steuerung ermöglichen.</block>
  <block id="6cab2c793e807e2d0c57f5b0d96505c1" category="list-text">Für eine optimale Nutzung der Snapshot Technologie müssen Datendateien unabhängig wiederhergestellt werden können. Das Komminglieren von Datendateien mit Protokolldateien beeinträchtigt die Wiederherstellung von Datendateien.</block>
  <block id="553ad1f7b6f5cfee021e5cf56e56a289" category="list-text">NetApp SnapMirror bietet zwar eine einfache Disaster Recovery-Funktion mit einem geringen RPO für eine Datenbank, erfordert jedoch unterschiedliche Replizierungspläne für die Dateien und Protokolle.</block>
  <block id="ff5f52a69a35dd41da3e3c4b810e4f94" category="admonition">Mit diesem grundlegenden Layout für zwei Volumes wird die Lösung zukunftssicher, sodass alle ONTAP Funktionen bei Bedarf genutzt werden können.</block>
  <block id="7b755794e9de5716b2d6305bd10bee1b" category="paragraph">*NetApp empfiehlt* das Formatieren Ihres Laufwerks mit dem ext4-Dateisystem aufgrund der folgenden Features:</block>
  <block id="fa92f58d5035dec5e24a787f76504783" category="list-text">Erweiterter Ansatz für Blockverwaltungsfunktionen, die im Journaling-Dateisystem (JFS) verwendet werden, und verzögerte Zuweisungsfunktionen des erweiterten Dateisystems (XFS).</block>
  <block id="a72d73cde02c10703b72dcae6bc7c812" category="list-text">Ext4 erlaubt Dateisysteme mit bis zu 1 Exbibyte (2^60 Bytes) und Dateien mit bis zu 16 Tebibyte (16 * 2^40 Bytes). Im Gegensatz dazu unterstützt das ext3-Dateisystem nur eine maximale Filesystem-Größe von 16 TB und eine maximale Dateigröße von 2 TB.</block>
  <block id="56b4168424fa999bc383d24daf2da4eb" category="list-text">In ext4-Dateisystemen weist die Multiblockzuweisung (mballoc) mehreren Blöcken einer Datei in einem einzigen Vorgang zu, anstatt sie einzeln zuzuweisen, wie in ext3. Diese Konfiguration reduziert den Overhead beim mehrmaligen Aufrufen des Block Allocator und optimiert die Zuweisung von Speicher.</block>
  <block id="632635d22b347f0140fb15d0eb0ed09e" category="list-text">Obwohl XFS der Standard für viele Linux-Distributionen ist, verwaltet es Metadaten anders und ist nicht für einige MySQL-Konfigurationen geeignet.</block>
  <block id="aff8b2ed0a4719f014e5baeb861ea7d4" category="paragraph">*NetApp empfiehlt* die Verwendung von 4k-Blockgrößenoptionen mit dem mkfs-Dienstprogramm, um die bestehende Block-LUN-Größe auszurichten.</block>
  <block id="0a24d0336b7b0b29a9daa97f88499884" category="paragraph"><block ref="018d13c1d8b9abcae080b8abbc7c2d8b" prefix="" category="inline-code"></block></block>
  <block id="86fc8cb3bf2cae621315896a94b35f67" category="paragraph">NetApp LUNs speichern Daten in physischen 4-KB-Blöcken, wodurch acht logische 512-Byte-Blöcke entstehen.</block>
  <block id="5f441ceabc94de2a9cced94d9f53a2d0" category="paragraph">Wenn Sie nicht dieselbe Blockgröße einrichten, werden I/O-Vorgänge nicht korrekt an physischen Blöcken ausgerichtet und können in zwei verschiedene Laufwerke in einer RAID-Gruppe geschrieben werden, was zu Latenz führt.</block>
  <block id="236ac851d05b8822524001920948f23d" category="admonition">Es ist wichtig, dass Sie den I/O so ausrichten, dass reibungslose Lese-/Schreibvorgänge erfolgen. Wenn die I/O jedoch bei einem logischen Block beginnt, der nicht am Anfang eines physischen Blocks liegt, ist die I/O falsch ausgerichtet. I/O-Vorgänge werden nur ausgerichtet, wenn sie an einem logischen Block beginnen – dem ersten logischen Block in einem physischen Block.</block>
  <block id="3225a10b07f1580f10dee4abc3779e6c" category="cell">Parameter</block>
  <block id="c82a6100dace2b41087ba6cf99a5976a" category="cell">Werte</block>
  <block id="b10db909b8fda84e70bf9b308d0938d9" category="cell">256 MIO.</block>
  <block id="c81e728d9d4c2f636f067f89cc14862c" category="cell">2</block>
  <block id="7ccc31934ae8244d8263d3c09bcee186" category="cell">innodb_doublewrite</block>
  <block id="e590322920bebc1156ab585bd6597d76" category="cell">Fsync</block>
  <block id="adb0c1ec32461889a5093441599260eb" category="cell">11 G</block>
  <block id="774412967f19ea61d448977ad9749078" category="cell">8192</block>
  <block id="311887a53ec7390fc383fa2ad813f012" category="cell">innodb_Buffer_Pool_Instances</block>
  <block id="c9f0f895fb98ab9159f51fd0297e236d" category="cell">8</block>
  <block id="1006f82d8df7d2325439b28ea691737c" category="cell">Open_File_Limit</block>
  <block id="3c9b5f1cd6a030bcb7bed9eac80589fb" category="cell">65535</block>
  <block id="c9f5a4d3ede4d084dda0a788ed06783c" category="paragraph">Um die in diesem Abschnitt beschriebenen Parameter einzustellen, müssen Sie sie in der MySQL-Konfigurationsdatei (my.cnf) ändern. Die Best Practices von NetApp wurden in internen Tests durchgeführt.</block>
  <block id="6a554ff1dc93539a626e576bf0d28a02" category="paragraph">Die Containerisierung von MySQL-Datenbanken nimmt immer mehr zu.</block>
  <block id="bcc002a9f2e2ade112438f50a6630498" category="paragraph">Das Low-Level-Container-Management wird fast immer über Docker durchgeführt. Container-Managementplattformen wie OpenShift und Kubernetes vereinfachen das Management großer Container-Umgebungen noch. Die Vorteile der Containerisierung sind niedrigere Kosten, da keine Hypervisor-Lizenz erforderlich ist. Darüber hinaus ermöglichen Container die Ausführung mehrerer Datenbanken isoliert voneinander, während gleichzeitig der gleiche zugrunde liegende Kernel und das gleiche Betriebssystem verwendet werden. Container können in Mikrosekunden bereitgestellt werden.</block>
  <block id="14d9b10c6196045941347a9e85704b50" category="inline-link-macro">Astra Trident-Dokumentation</block>
  <block id="b8c3891a902a70c4f7f774363652f4b4" category="summary">MySQL-Dateistruktur</block>
  <block id="5713a921d815fd7d19875846450f8ea8" category="doc">Dateistruktur</block>
  <block id="4955922857da5499a2f1be602a96b0ff" category="paragraph">InnoDB fungiert als mittlere Schicht zwischen Speicher und MySQL-Server, es speichert die Daten auf den Laufwerken.</block>
  <block id="8a6fab0b8bb36427eda4071adfd7780b" category="inline-image-macro">Fehler: Grafikbild nicht gefunden</block>
  <block id="5d416cf65dd1e2c908bf430f957c89f8" category="paragraph">MySQL I/O wird in zwei Typen unterteilt:</block>
  <block id="e8a320970c929abda9ae744b0b9e8f5f" category="list-text">Zufällige Datei-I/O</block>
  <block id="85430c0aa096ec282f2a14156baa312e" category="list-text">Sequenzielle Datei-I/O</block>
  <block id="b7b9a4c6983139a2673988826094a3cf" category="paragraph">Datendateien werden zufällig gelesen und überschrieben, was zu einem hohen IOPS-Wert führt. Daher wird SSD-Speicher empfohlen.</block>
  <block id="322f79fd77365edecd173807cb429ca2" category="paragraph">Redo-Log-Dateien und binäre Log-Dateien sind Transaktionsprotokolle. Sie werden sequenziell geschrieben, sodass Sie mit dem Schreib-Cache eine gute Performance auf der Festplatte erzielen können. Ein sequenzieller Lesevorgang findet bei der Wiederherstellung statt, jedoch verursacht er selten ein Performance-Problem, da die Größe der Protokolldatei normalerweise kleiner ist als die von Datendateien und sequenzielle Lesevorgänge schneller sind als zufällige Lesevorgänge (die bei Datendateien auftreten).</block>
  <block id="a898d206a27508975fea2e241af2f04e" category="paragraph">Der Double-Write-Puffer ist eine Besonderheit von InnoDB. InnoDB schreibt zunächst gerötete Seiten in den Double-Write-Puffer und schreibt dann die Seiten an die richtigen Positionen in den Datendateien. Dieser Prozess verhindert eine Beschädigung der Seite. Ohne den Double-Write-Puffer kann die Seite beschädigt werden, wenn während des Write-to-Drive-Prozesses ein Stromausfall auftritt. Da das Schreiben in den Doppelschreibpuffer sequenziell ist, wird es für HDDs stark optimiert. Bei der Wiederherstellung werden sequenzielle Lesevorgänge durchgeführt.</block>
  <block id="363512900e45bc17bdd6edfa50582862" category="paragraph">Da ONTAP NVRAM bereits einen Schreibschutz bietet, ist keine doppelte Schreibpufferung erforderlich. MySQL hat einen Parameter,<block ref="02d3c518a4019ea00e34e16491d826b4" prefix=" " category="inline-code"></block>, Um den Double-Write-Puffer zu deaktivieren. Diese Funktion kann die Leistung erheblich verbessern.</block>
  <block id="49a7df723922bddb4df4002b51087a1c" category="paragraph">Der Insert-Puffer ist ebenfalls eine Besonderheit von InnoDB. Wenn sich nicht eindeutige sekundäre Indexblöcke nicht im Speicher befinden, fügt InnoDB Einträge in den Insert-Puffer ein, um zufällige I/O-Vorgänge zu vermeiden. Der Insert-Puffer wird in regelmäßigen Abständen in die sekundären Indexbäume der Datenbank eingebunden. Der Insert-Puffer reduziert die Anzahl der I/O-Vorgänge, indem I/O-Anfragen an denselben Block zusammengeführt werden. Zufällige I/O-Vorgänge können sequenziell sein. Der Einfügepuffer ist auch für HDDs stark optimiert. Sowohl sequenzielle Schreibvorgänge als auch Lesevorgänge erfolgen im normalen Betrieb.</block>
  <block id="6a0190340f27f29279bbfd319800289d" category="paragraph">Rückgängig-Segmente sind wahlfrei E/A-orientiert. Um die Multiversionsparallelität (MVCC) zu gewährleisten, muss InnoDB alte Bilder in den Undo-Segmenten registrieren. Beim Lesen vorheriger Bilder aus den Rückgängigmachungssegmenten sind zufällige Lesevorgänge erforderlich. Wenn Sie eine lange Transaktion mit wiederholbaren Lesevorgängen ausführen (wie z. B. mysqldump – einzelne Transaktion) oder eine lange Abfrage ausführen, können zufällige Lesevorgänge auftreten. Daher ist das Speichern von undo-Segmenten auf SSDs in dieser Instanz besser. Wenn Sie nur kurze Transaktionen oder Abfragen ausführen, stellen die zufälligen Lesevorgänge kein Problem dar.</block>
  <block id="a27e0d4b650aa6db30fdd95b54c94b27" category="paragraph">*NetApp empfiehlt* aufgrund der InnoDB I/O-Eigenschaften das folgende Speicherdesign-Layout.</block>
  <block id="defc1bfda928f419df5e4af79c98343e" category="list-text">Ein Volume zur Speicherung zufälliger und sequenzieller I/O-orientierter MySQL-Dateien</block>
  <block id="7fd750411093ebd1faa0d99dd2608514" category="list-text">Ein weiteres Volume zur Speicherung rein sequenzieller I/O-orientierter Dateien von MySQL</block>
  <block id="0561a3b014eef0e5125fa63e1626cf57" category="paragraph">Dieses Layout hilft Ihnen auch bei der Entwicklung von Datensicherungsrichtlinien und -Strategien.</block>
  <block id="9166d970028a28a614af39e6286371b7" category="paragraph">Sie können diesen Parameter mit deaktivieren<block ref="8cafbd8d9096941261e87f4f9152ab92" prefix=" " category="inline-code"></block> Für Benchmarks oder wenn Sie sich mehr um Top-Performance als um Datenintegrität oder mögliche Ausfälle sorgen. InnoDB verwendet eine Datei-Flush-Technik namens Double-Write. Bevor die Seiten in die Datendateien geschrieben werden, schreibt InnoDB sie in einen zusammenhängenden Bereich, den sogenannten Double-Write-Puffer. Nachdem das Schreiben und der Flush in den Double-Write-Puffer abgeschlossen sind, schreibt InnoDB die Seiten an die richtigen Positionen in der Datendatei. Falls das Betriebssystem oder ein mysqld-Prozess während eines Page Write abstürzt, kann InnoDB später während der Crash-Wiederherstellung eine gute Kopie der Seite aus dem Double-Write-Puffer finden.</block>
  <block id="3dfd4b9a526f8a8cf17021e34638b375" category="admonition">*NetApp empfiehlt* den Double-Write-Puffer zu deaktivieren. ONTAP NVRAM dient dieselbe Funktion. Die doppelte Pufferung beeinträchtigt die Leistung unnötig.</block>
  <block id="0e5e83102103fcdd25af430bf7327034" category="paragraph">Die MySQL-Dokumentation empfiehlt, NFSv4 für NAS-Bereitstellungen zu verwenden.</block>
  <block id="ca0d9fdc80b88e73b548638af421788f" category="section-title">ONTAP NFS-Übertragungsgrößen</block>
  <block id="efcd0ba3fcbee9896a25804677e70b4b" category="paragraph">Standardmäßig beschränkt ONTAP die NFS-I/O-Größe auf 64K. Zufällige IO mit einer MySQL-Datenbank verwendet eine viel kleinere Blockgröße, die weit unter dem 64K Maximum liegt. I/O mit großen Blöcken wird in der Regel parallelisiert, sodass die 64K-Maximalgröße ebenfalls keine Einschränkung darstellt.</block>
  <block id="4655f686de75b23f16d530ed7351c447" category="paragraph">Es gibt einige Workloads, bei denen das 64K-Maximum eine Einschränkung darstellt. Insbesondere Vorgänge mit einem Thread, wie z. B. Backup-Vorgänge mit vollständiger Tabelle, werden schneller und effizienter ausgeführt, wenn die Datenbank weniger, aber größere I/O-Vorgänge ausführen kann. Die optimale I/O-Handhabungsgröße für ONTAP mit Datenbank-Workloads beträgt 256.000. Die unten aufgeführten NFS-Mount-Optionen für spezifische Betriebssysteme wurden entsprechend von 64K auf 256K aktualisiert.</block>
  <block id="3ab93db30f3dded2c8d71859afbb53f6" category="admonition">Verringern Sie niemals die maximal zulässige Übertragungsgröße auf ONTAP unter den Wert rsize/wsize der aktuell gemounteten NFS-Dateisysteme. Dies kann bei einigen Betriebssystemen zu Hängebleiben oder sogar Datenbeschädigungen führen. Wenn beispielsweise NFS-Clients derzeit auf 65536 rsize/wsize gesetzt sind, dann könnte die maximale Übertragungsgröße für ONTAP ohne Auswirkung auf die Clients selbst begrenzt werden, zwischen 65536 und 1048576 angepasst werden. Wenn Sie die maximale Übertragungsgröße unter 65536 verringern, können die Verfügbarkeit oder die Daten beeinträchtigt werden.</block>
  <block id="5ea0bb5d4dab46d7971fef94ccde9369" category="paragraph">*NetApp empfiehlt*</block>
  <block id="09c6aa928c90ba68a4e03deadfeaa644" category="paragraph">Einstellen der folgenden Einstellung für NFSv4 fstab (/etc/fstab):</block>
  <block id="0ae3c9648bc2d8c33fa57415604da370" category="paragraph"><block ref="a63a3dc825ff52908de7c9dfcb29ffa5" prefix="" category="inline-code"></block></block>
  <block id="3e0b2c100b093560f63ad57c6849bc8a" category="admonition">Ein häufiges Problem mit NFSv3 waren die gesperrten InnoDB-Protokolldateien nach einem Stromausfall. Durch die Verwendung von Zeit oder das Wechseln von Protokolldateien wurde dieses Problem behoben. NFSv4 verfügt jedoch über Sperrvorgänge und verfolgt die offenen Dateien und Delegationen.</block>
  <block id="62e4eb800d20b46085ccb975da78bafe" category="sidebar">ONTAP ist die Grundlage für Datenmanagement und Datensicherung für zahlreiche Enterprise-Applikationen und Datenbanktechnologien. Auf den folgenden Seiten finden Sie Anleitungen zu Best Practices und Implementierungsverfahren für ONTAP- und Enterprise-Applikationen und -Infrastrukturen.</block>
  <block id="78898e52fcbdc5337204a384f2456d4f" category="sidebar">Microsoft SQL Server</block>
  <block id="4f60acc41a8bf3fa75e7f74768637787" category="sidebar">Microsoft SQL Server Datensicherung</block>
  <block id="b8529ff730dcf8dd2178d72de271dc4d" category="sidebar">Open-Source-Datenbanken</block>
  <block id="bb9a5a6fadabab849d2e87f0898d7601" category="sidebar">MariaDB und MySQL auf ONTAP</block>
  <block id="5ad2a11eceac3ebd7cbf3b534ebdb1db" category="sidebar">PostgreSQL auf ONTAP</block>
  <block id="c30dd1a85cf86d95b11c1a08f70130ce" category="sidebar">Oracle Datenbank</block>
  <block id="4948dede9f4e7c3dfe48c38f7902f6e5" category="sidebar">Oracle auf ONTAP</block>
  <block id="d71074394b511143c8d1108e74a81abb" category="sidebar">Oracle Datensicherung</block>
  <block id="f4d8825927d11df25770df5e00d6a52e" category="sidebar">Oracle-Migration</block>
  <block id="86083f3ea21c2385b4f013aae3c57858" category="sidebar">SAP HANA</block>
  <block id="677f2e7e550075f7bbbdac91e0918f2f" category="sidebar">SAP Lösungen</block>
  <block id="1b83f2b1c0b7fb3af048c2a59624fe3b" category="sidebar">SAP HANA mit AFF und FC.</block>
  <block id="6ab3c3f05076d213f0b1feff267607c6" category="sidebar">SAP HANA mit AFF und NFS</block>
  <block id="56079badd056a19303cc26e6a4fcc7e0" category="sidebar">VMware</block>
  <block id="f07084a11252c9bca97503ea9a627c89" category="sidebar">Virtual Volumes (VVols) mit ONTAP</block>
  <block id="2d90ac09cbf108bddad31dd341f8614e" category="sidebar">VMware Site Recovery Manager mit ONTAP</block>
  <block id="b0dae11b82e63781abdc8f893c41b3c0" category="sidebar">Enterprise-Applikationen</block>
  <block id="999bc6b18351bbe817d26f559ba408ae" category="sidebar">SAP</block>
  <block id="2d2eef88dbdd0c34bde24e2632d9944f" category="sidebar">SAP HANA und AnyDB</block>
  <block id="399bd1ee587245ecac6f39beaa99886f" category="sidebar">PostgreSQL</block>
  <block id="f4f70727dc34561dfde1a3c529b6205c" category="sidebar">Einstellungen</block>
  <block id="7d219314ccabde6609510141bb175a6c" category="sidebar">Shared Instance oder dedizierte Instanz</block>
  <block id="d5363ae19ecaede49e2ced297df7737a" category="sidebar">Speicherkonfiguration</block>
  <block id="500c09aa1bb9af431f2c18348e69bfd0" category="sidebar">Tempdb-Dateien</block>
  <block id="4c5d94eb4dc4bff93f6a96a08ab7b244" category="sidebar">Datensicherheit</block>
  <block id="cf91513c6fc63cebe4b63a6647238d6d" category="sidebar">RAID</block>
  <block id="83db97189c47f7bb4edd879e8756f6e6" category="sidebar">Kapazitätsgrenzen</block>
  <block id="af81930661b267e8f1e68c4d66e9ee56" category="sidebar">Failover und Switchover</block>
  <block id="dfd1f50170457b961c15bb13ad7e3bed" category="sidebar">Datendatei- und Redo-Blockgrößen</block>
  <block id="49f83bf3a15b8626eb91fde93c6b1788" category="sidebar">Oracle RAC</block>
  <block id="13d80777c8fd43e555c1f5b1b72d7ef0" category="sidebar">Host-Konfiguration</block>
  <block id="7e6bb0931a72759d39514aa924b420bc" category="sidebar">AIX</block>
  <block id="6bb83af717367dd4a47f75801ae7a2b0" category="sidebar">Linux mit ASMlib und AFD</block>
  <block id="619615da0f93507e22a6054ef2aea4f1" category="sidebar">Logische Schnittstellen</block>
  <block id="aca8e861c6f30fe2d42d0310a387312b" category="sidebar">Ethernet-Konfiguration</block>
  <block id="63a5b8bd41978e8124f9ccdfa9063e9e" category="sidebar">FC SAN-Konfiguration</block>
  <block id="c3f378ef942f6bf228a43aaacc628fea" category="sidebar">LVM-Striping</block>
  <block id="254f642527b45bc260048e30704edb39" category="sidebar">Konfiguration</block>
  <block id="f3a78e25ce9f95fc3c61ccc39f32fb4d" category="sidebar">Oracle Direct NFS (dNFS)</block>
  <block id="989cefa74de0e10a575103f446258d99" category="sidebar">NFS-Lease und -Sperren</block>
  <block id="0d695ec265a3b0b5e7e32e33ffe4ed22" category="sidebar">NFS-Caching</block>
  <block id="eb4d10d6eda7c9266cd36c4eb0a223cf" category="sidebar">ASM Reclamation Utility</block>
  <block id="297e2ac2ec0bd88ad598062f4c07ee45" category="sidebar">Tiering-Richtlinien</block>
  <block id="9af6f452cd88c83f2bde555de8f93690" category="sidebar">Senden von Daten an einen Objektspeicher</block>
  <block id="08af6d4e6b562e2d0b418d7198f5a0f7" category="sidebar">Daten werden aus dem Objektspeicher abgerufen</block>
  <block id="02d01636975b3ae0c0ab22368cea8d54" category="sidebar">Tiering-Strategien</block>
  <block id="7fbd108e1fc6910c941094e95a677878" category="sidebar">Ganze Dateien</block>
  <block id="eb43c2427eb06b9f561363863b4a4f0b" category="sidebar">Teildateien</block>
  <block id="1afe200fa26ba361f59b603b22ad6392" category="sidebar">Wählen Sie Dateien aus</block>
  <block id="448d15345bb99c834095da225c529b8a" category="sidebar">Datenverfügbarkeit</block>
  <block id="5869e10a9dea6d88d61bb10040557f35" category="sidebar">Datenintegrität</block>
  <block id="7b68149402c05a52968bda6af0435c8a" category="sidebar">Snapshot basierte Online-Backups</block>
  <block id="49ba0ee205d016d8236db9c1e8c130e5" category="sidebar">Storage Snapshot optimierte Backups</block>
  <block id="2c6d98297c340e390e0783220b038545" category="sidebar">Physische Architektur</block>
  <block id="13a854bdd11f8f57a0b25c00631f1430" category="sidebar">Logische Architektur</block>
  <block id="5e7988aeba0e1d3c15c5c78a0db738d1" category="sidebar">SyncMirror</block>
  <block id="5ba8ee2f32815884475d1f2244ba16a6" category="sidebar">Ausfallszenarien</block>
  <block id="e31593850dc03f8f10d036df5dc8633b" category="sidebar">Migration der Oracle Datenbank</block>
  <block id="5102abd5a9cc202678054b832caf678d" category="sidebar">Verfahren</block>
  <block id="1495d3ed3ebff783c0f480b583372450" category="sidebar">Host-Datenkopie</block>
  <block id="d81ca5686c955ed50927409eb8c14bb0" category="sidebar">Import fremder LUNs</block>
  <block id="2fea59b7e470acac5ec1589d504da14e" category="sidebar">Abschluss</block>
  <block id="e9cc85ef98d982c3c3e53d6b82293126" category="sidebar">Protokollkonvertierung</block>
  <block id="fbbd0e8905df3950eca4d1d8f55881ad" category="sidebar">Zusätzliche Anmerkungen</block>
  <block id="ac0465a970688dc3c326dcdd8fe2805e" category="sidebar">Performance-Optimierung und Benchmarking</block>
  <block id="7cb9c3cd8b9873d39a1fe87c083cd55c" category="sidebar">Veraltete NFS-Sperren</block>
  <block id="c0916addb4f26afa58c5a6df4f463fa5" category="sidebar">Unified Storage</block>
  <block id="c0032c6bab44eeeb6886c7a4aa67cf77" category="sidebar">Virtualisierungs-Tools</block>
  <block id="1c77ac70000b6804e930748d5df7be5e" category="sidebar">Richtlinienbasiertes Management von Virtual Volumes und Storage</block>
  <block id="217601d383b816e6a2548e57b5006f92" category="sidebar">Klonen</block>
  <block id="8f2db90dd4a6fbd95ba8f0bc54fd6b27" category="sidebar">QoS</block>
  <block id="09b4a81ec7b05cbacc7cbfa13e006254" category="sidebar">Richtlinienbasiertes Storage-Management</block>
  <block id="2e8526cbe762cb1b8393d935a3b0bf16" category="sidebar">Empfohlene Einstellungen</block>
  <block id="10a764f91b59ce3c8931d55c2fd54222" category="sidebar">Implementierung von VVols Storage</block>
  <block id="30f6724a02fc69093960468c0a2fbcd4" category="sidebar">Produktsicherheit</block>
  <block id="3390e3575b3e341afa2b7172ff5b33a7" category="sidebar">SnapCenter Plug-in für VMware vSphere</block>
  <block id="62a004b95946bb97541afa471dcca73a" category="sidebar">MySQL</block>
  <block id="7f1a6761f7160e33a32105202e47303b" category="sidebar">Containerisierung</block>
  <block id="bd57f4f74ca087a92a48b23d45753729" category="paragraph">Der ONTAP-Tools VASA Provider managt FCP- und iSCSI-Initiatorgruppen sowie NVMe-Subsysteme in ONTAP, die auf erkannten Initiatoren von gemanagten ESXi-Hosts basieren. Es ist jedoch nicht in Fibre-Channel-Switches integriert, um das Zoning zu managen. Bevor eine Bereitstellung stattfinden kann, muss das Zoning nach Best Practices erfolgen. Nachfolgend ein Beispiel für das Einzel-Initiator-Zoning für vier ONTAP-Systeme:</block>
  <block id="6b14e3f4a37881185acf7a766f8f6272" category="paragraph">Einzel-Initiator-Zoning:</block>
  <block id="17ccf64e30cb518fd7ca87ce752ad738" category="paragraph"><block ref="17ccf64e30cb518fd7ca87ce752ad738" category="inline-image-macro-rx" type="image"></block></block>
  <block id="58b5cd2a94730c2e044c14891f1e0e87" category="paragraph">Weitere Best Practices finden Sie in folgenden Dokumenten:</block>
  <block id="fc9e4d789d968936f8ff5ae5a99847e0" category="paragraph"><block ref="fc9e4d789d968936f8ff5ae5a99847e0" category="inline-link-rx"></block></block>
  <block id="d748a2834a7220eef4ec4775a51632f9" category="paragraph"><block ref="d748a2834a7220eef4ec4775a51632f9" category="inline-link-rx"></block></block>
  <block id="4047245d73abc6e3ae8bf58ec063e52e" category="paragraph">Die im Lieferumfang enthaltenen SCPs sind für die meisten allgemeinen Anwendungen geeignet, aber Ihre Anforderungen können unterschiedlich sein.</block>
  <block id="80dad10df0d1a031764e7c648c46aa23" category="paragraph">*Erwägen Sie die Verwendung von max IOPS zur Steuerung unbekannter VMs oder zum Testen von VMs.*</block>
  <block id="93e4f73afe8b787b33161ec273ca087f" category="paragraph">Erstmals in VASA Provider 7.1 verfügbar, können maximale IOPS verwendet werden, um IOPS bei einem unbekannten Workload auf ein bestimmtes vVol zu beschränken und so Auswirkungen auf andere, kritischere Workloads zu vermeiden. Tabelle 4 enthält weitere Informationen zum Performance-Management.</block>
  <block id="bdbd10905a1446f944699405bca52654" category="paragraph">*Stellen Sie sicher, dass Sie ausreichend Daten-LIFs haben.*
Erstellen Sie mindestens zwei LIFs pro Node und HA-Paar. Je nach Workload werden weitere erforderlich.</block>
  <block id="4e0d7e0587bf99c9b7a64a84fd42f6bd" category="paragraph">Weitere Best Practice-Leitfäden zu dem von Ihnen gewählten Protokoll finden Sie in den Leitfäden von NetApp und VMware. Im Allgemeinen gibt es keine anderen Änderungen als die bereits erwähnten.</block>
  <block id="97f87e9ab1d660463c63beb8745fd9e5" category="paragraph">*Beispiel einer Netzwerkkonfiguration mit VVols über NFS v3*</block>
  <block id="c80b3cc8b5013bd4039769fb0f783e60" category="paragraph"><block ref="c80b3cc8b5013bd4039769fb0f783e60" category="inline-image-macro-rx" type="image"></block></block>
  <block id="dee28dd6d1a9ef7bf399f642fd9d588f" category="section-title">NFS-Übertragungsgrößen</block>
  <block id="d8299d00f914e35b80644e5781e47a77" category="paragraph"><block ref="d8299d00f914e35b80644e5781e47a77" category="inline-image-macro-rx" type="image"></block></block>
  <block id="861f074a1ecd9d1cc088d2fe81903565" category="paragraph"><block ref="861f074a1ecd9d1cc088d2fe81903565" category="inline-image-macro-rx" type="image"></block></block>
  <block id="710e9cd79e3865d47122a799f425b31f" category="paragraph"><block ref="710e9cd79e3865d47122a799f425b31f" category="inline-image-macro-rx" type="image"></block></block>
  <block id="5a656ddcc72bacc3a5900c23edf7a08e" category="paragraph"><block ref="5a656ddcc72bacc3a5900c23edf7a08e" category="inline-image-macro-rx" type="image"></block></block>
  <block id="073ce0a0568adc64d52dd9b1d8c02c54" category="paragraph"><block ref="073ce0a0568adc64d52dd9b1d8c02c54" category="inline-image-macro-rx" type="image"></block></block>
  <block id="3401a9880dfc322da8a56c4d632361f6" category="paragraph"><block ref="3401a9880dfc322da8a56c4d632361f6" category="inline-image-macro-rx" type="image"></block></block>
  <block id="87f823f4f3b4258445d5af029afe5ad9" category="paragraph"><block ref="87f823f4f3b4258445d5af029afe5ad9" category="inline-image-macro-rx" type="image"></block></block>
  <block id="9213fe9d391179276ff6c03552486255" category="paragraph"><block ref="9213fe9d391179276ff6c03552486255" category="inline-image-macro-rx" type="image"></block></block>
  <block id="3105b4c0642b9710179c18edcf8e8718" category="paragraph"><block ref="3105b4c0642b9710179c18edcf8e8718" category="inline-image-macro-rx" type="image"></block></block>
  <block id="1a43000eb9893bff7ab7b1ce1d3fd6a8" category="paragraph"><block ref="1a43000eb9893bff7ab7b1ce1d3fd6a8" category="inline-image-macro-rx" type="image"></block></block>
  <block id="4b7a8562c98b8c652bdfb6aa79788222" category="paragraph"><block ref="4b7a8562c98b8c652bdfb6aa79788222" category="inline-image-macro-rx" type="image"></block></block>
  <block id="38ac6f4e1538397bd4f659237ec03ebf" category="paragraph"><block ref="38ac6f4e1538397bd4f659237ec03ebf" category="inline-image-macro-rx" type="image"></block></block>
  <block id="f01518e5f3b18a9ea241bc63a3475dbd" category="paragraph"><block ref="f01518e5f3b18a9ea241bc63a3475dbd" category="inline-image-macro-rx" type="image"></block></block>
  <block id="acce752526553bfa73c92799accd9cb8" category="summary">Überblick über die Oracle Datensicherung</block>
  <block id="deb8cb7644231cae002f5f37c725b86f" category="paragraph">Eine Migration über größere Entfernungen erfordert in der Regel einen kreativen Ansatz, wie der Protokollversand-Prozess in erläutert <block ref="86efe12ea0d3a769a4b7cfc2b6362f49" category="inline-link-macro-rx"></block>. IP-Netzwerke über große Entfernungen verfügen selten überall in der Nähe von LAN- oder SAN-Geschwindigkeiten über eine entsprechende Bandbreite. In einem Fall unterstützte NetApp die Fernmigration einer 220 TB großen Datenbank mit sehr hohen Archiv- Log-Generierungsraten. Der gewählte Ansatz für die Datenübertragung war der tägliche Versand von Bändern, da diese Methode die maximal mögliche Bandbreite bot.</block>
  <block id="055362c7082175c34483772a43b776cf" category="paragraph">Das Kopieren einer 10-TB-Datenbank dauert beispielsweise in der Regel ungefähr sieben Stunden. Wenn das Unternehmen einen Ausfall von sieben Stunden zulassen muss, ist Dateikopien eine einfache und sichere Möglichkeit für die Migration. Wenn fünf Stunden nicht akzeptabel sind, lässt sich ein einfacher Protokollversand-Prozess wie (siehe) <block ref="eee8a7589b6555fc3411165c58a62ca5" category="inline-link-macro-rx"></block>) Kann mit minimalem Aufwand eingerichtet werden, um die Umstellungszeit auf etwa 15 Minuten zu reduzieren. Während dieser Zeit kann ein Datenbankadministrator den Prozess abschließen. Wenn 15 Minuten nicht akzeptabel sind, kann der endgültige Umstellungsprozess durch Skripting automatisiert werden, um die Umstellungszeit auf wenige Minuten zu verkürzen. Sie können eine Migration jederzeit beschleunigen, doch dies kostet Zeit und Aufwand. Die Umstellungszeitziele sollten darauf basieren, was für das Unternehmen akzeptabel ist.</block>
  <block id="911e63244fc82405263306a64e535c69" category="paragraph">Ein zpool sollte erst nach den Schritten im erstellt werden <block ref="e8673fdb712a2edcdf56742c7eec0045" category="inline-link-macro-rx"></block> Durchgeführt werden. Wenn das Verfahren nicht korrekt durchgeführt wird, kann es durch die I/O-Ausrichtung zu einer ernsthaften Verschlechterung der Performance kommen. Eine optimale Performance auf ONTAP erfordert, dass der I/O an einer 4-KB-Grenze auf einem Laufwerk ausgerichtet ist. Die auf einem zpool erstellten Dateisysteme verwenden eine effektive Blockgröße, die über einen Parameter mit dem Namen gesteuert wird<block ref="81df978a8e3da4bee74b135548f390f3" prefix=" " category="inline-code"></block>, Die durch Ausführen des Befehls angezeigt werden kann<block ref="1615297782a0dff59b20451e2ca97ea7" prefix=" " category="inline-code"></block>.</block>
  <block id="b2ca9da17ac7a0807fb313343c1ba9cb" category="list-text">Die Partition wurde mit einem 33-Sektor-Offset anstelle der Standardeinstellung 32 erstellt. Wiederholen Sie den in beschriebenen Vorgang <block ref="6ba0f94fdd8f9504e3fef5712023fb85" category="inline-link-macro-rx"></block>. Das Histogramm wird wie folgt angezeigt:</block>
  <block id="eb7f9a88026c895932f70c650c364132" category="list-text">Erhöhen Sie die Größe der LUNs</block>
  <block id="0f11c45e6e7377b9d10a93534d0fa2f3" category="list-text">Fügen Sie einer vorhandenen Volume-Gruppe eine LUN hinzu und vergrößern Sie das enthaltene logische Volume</block>
  <block id="e4da750cce448393b2597f8f66086894" category="paragraph">Eine Initiatorgruppe (Initiatorgruppe) ist Teil der ONTAP LUN-Masking-Architektur. Auf eine neu erstellte LUN kann nur dann zugegriffen werden, wenn einem Host der erste Zugriff gewährt wurde. Dazu wird eine Initiatorgruppe erstellt, die entweder die FC-WWNs oder iSCSI-Initiatornamen auflistet, denen Zugriff gewährt werden soll. Zum Zeitpunkt der Erstellung dieses Berichts wurde FLI nur für FC LUNs unterstützt. Die Konvertierung in iSCSI nach der Migration ist jedoch eine einfache Aufgabe, wie in dargestellt <block ref="43085dc2a05784d4912331d1ed1db91d" category="inline-link-macro-rx"></block>.</block>
  <block id="525e71fb331412cd820122c3d51d453c" category="paragraph">Auf diese Weise können DBAs nach dem Löschen von Daten Speicherplatz im Storage-Array zurückgewinnen. ONTAP fängt die Nullen ab und hebt den Speicherplatz von der LUN ab. Die Rückgewinnung erfolgt äußerst schnell, da innerhalb des Storage-Systems keine Daten geschrieben werden.</block>
  <block id="be17af715060572d2df93d7ffe4ce6dd" category="paragraph">ONTAP Storage-Systeme bieten beim Erstellen von Datastores für VMs und virtuelle Festplatten ein hohes Maß an Flexibilität. Obwohl viele ONTAP Best Practices angewendet werden, wenn Datastores für vSphere mit VSC bereitgestellt werden (siehe Abschnitt) <block ref="2e24324ebf41be836715e1e0dd648f4f" category="inline-link-macro-rx"></block>), hier sind einige zusätzliche Richtlinien zu berücksichtigen:</block>
  <block id="141c3dc68be34b8f11c2a120b36a1eb8" category="list-text">In einigen Fällen benötigen Sie eventuell nicht einmal einen Datastore. Um die beste Performance und ein optimales Management zu erzielen, sollten Sie für Applikationen mit hohem I/O-Aufkommen – beispielsweise für Datenbanken und bestimmte Applikationen – keinen Datastore verwenden. Hier sind „inguest“-Ansätze via NFS oder iSCSI in Erwägung zu ziehen, die vom Gastbetriebssystem verwaltet werden oder via Raw Device Mapping (RDM). Eine Anleitung zu bestimmten Applikationen finden Sie in den technischen Berichten von NetApp für die jeweilige Applikation. Beispiel: <block ref="cfa0393f870bc70fd8973ae18376facc" category="inline-link-macro-rx"></block> Ein Abschnitt zur Virtualisierung mit hilfreichen Details.</block>
  <block id="3cd9527ff0ddb17e00c08bdb4cd1a776" category="paragraph">VM-Storage-Richtlinien managen in vSphere optionale Funktionen wie Storage I/O Control oder vSphere Encryption. Sie werden auch zusammen mit VVols verwendet, um spezifische Storage-Funktionen auf die VM anzuwenden. Verwenden Sie den Storage-Typ „NetApp.Clustered.Data.ONTAP.VP.vvol“ und die Regel „ProfileName“, um mithilfe der Richtlinie ein bestimmtes SCP auf VMs anzuwenden. Ein Beispiel hierfür mit den ONTAP Tools VASA Provider finden Sie unter Link:vmware-vvols-ontap.HTML#Best Practices[Beispiel für eine Netzwerkkonfiguration mit VVols über NFS v3]. Regeln für Storage „NetApp.Clustered.Data.ONTAP.VP.VASA10“ sollen mit Datastores ohne VVols verwendet werden.</block>
  <block id="36ad91be49fdde7d3fadeba0fb782510" category="paragraph">Sobald die Storage-Richtlinie erstellt wurde, kann sie verwendet werden, wenn neue VMs bereitgestellt werden, wie in dargestellt <block ref="d202b3fe265f968d81802fcec9f3c381" category="inline-link-macro-rx"></block>. Richtlinien zur Nutzung von Performance-Management-Funktionen mit VASA Provider 7.2 finden Sie in <block ref="8fdd8868d80d04eecf1b4189862f536c" category="inline-xref-macro-rx"></block>.</block>
  <block id="14bdf3569ec0024259b2b02c5a668eae" category="paragraph">Wenn<block ref="7ccc31934ae8244d8263d3c09bcee186" prefix=" " category="inline-code"></block> Ist aktiviert (Standard), speichert InnoDB alle Daten zweimal: Zuerst in den Double-Write-Puffer und dann in die eigentlichen Datendateien.</block>
  <block id="394ad91b4701dbd32675896bed204755" category="sidebar">Hostkonfiguration für PostgreSQL</block>
  <block id="9852487272d9b2aa9026d0fddc0616fc" category="sidebar">Storage-Konfiguration für PostgreSQL</block>
  <block id="bc237072428e0e4c415160b898946d4c" category="sidebar">Datensicherung für PostgreSQL</block>
  <block id="3423582eed75b4bd6632c18238fd78e4" category="sidebar">Datenbankkonfiguration für Microsoft SQL Server</block>
  <block id="6e97cf67ec5979ab5828852639efa208" category="sidebar">Storage-Konfiguration für Microsoft SQL Server</block>
  <block id="5a4517368fb2ab5498fc2108c2ea64c5" category="sidebar">Datenbankkonfiguration für Oracle Database</block>
  <block id="e483c0ecf3a2b1a60f886d4a5ac553ff" category="sidebar">Hostkonfiguration für Oracle Database</block>
  <block id="06b078afa802a679ad4694d13a8a2495" category="sidebar">Netzwerkkonfiguration für Oracle Database</block>
  <block id="f1991b28c5d48cb53798755acf9dfe62" category="sidebar">Storage-Konfiguration für Oracle Database</block>
  <block id="ea9cdfcbfcf1906c15b6779f0f327028" category="sidebar">Datenbankkonfiguration für MySQL</block>
  <block id="308f8f580d0248ac023557cfd4b10d83" category="sidebar">Hostkonfiguration für MySQL</block>
  <block id="830f353b3e6055499fac447b16b0c935" category="sidebar">Storage-Konfiguration für MySQL</block>
  <block id="3106f8f17f2974fe05da16fb0cc50505" category="summary">PostgreSQL-Datenbanken mit SAN auf ONTAP</block>
  <block id="74cacb0c34a37e2859e3b054938a344d" category="doc">PostgreSQL mit SAN-Dateisystemen</block>
  <block id="1758f5c81d60f1a0ac8600fddc99a919" category="paragraph">PostgreSQL-Datenbanken mit SAN werden in der Regel auf xfs-Dateisystemen gehostet, aber andere können verwendet werden, wenn sie vom OS-Anbieter unterstützt werden</block>
  <block id="d73e7b56b7baa813a32c521ca074e7c4" category="paragraph">Während eine einzelne LUN in der Regel bis zu 100.000 IOPS unterstützen kann, erfordern IO-intensive Datenbanken in der Regel die Verwendung von LVM mit Striping.</block>
  <block id="f7cc8c4234952826818ad4d0c2050ac9" category="paragraph">PostgreSQL-Datenbanken können auf NFSv3- oder NFSv4-Dateisystemen gehostet werden. Die beste Option hängt von Faktoren außerhalb der Datenbank ab.</block>
  <block id="1482c42acc1dd31ee8496c71a805e73c" category="paragraph">Beispielsweise könnte das Sperrverhalten von NFSv4 in bestimmten Cluster-Umgebungen vorzuziehen sein. (Siehe <block ref="bc3a7e667477b8b3590fa1451dcb924c" category="inline-link-macro-rx"></block> Für weitere Details)</block>
  <block id="9674525687765646f35916d0e569eda9" category="paragraph">Ansonsten sollte die Datenbankfunktionalität, einschließlich der Performance, nahezu identisch sein. Die einzige Voraussetzung ist die Verwendung des<block ref="d64a84456adc959f56de6af685d0dadd" prefix=" " category="inline-code"></block> Mount-Option. Dies ist erforderlich, um sicherzustellen, dass weiche Timeouts keine nicht behebbaren E/A-Fehler verursachen.</block>
  <block id="0c6430fad6c643c444b5d6d52b18edb4" category="paragraph">Wenn NFSv4 als Protokoll gewählt wird, empfiehlt NetApp die Verwendung von NFSv4.1. Es gibt einige funktionale Verbesserungen am NFSv4.1-Protokoll, die die Ausfallsicherheit gegenüber NFSv4.0 verbessern.</block>
  <block id="248212b04c23627e2719e106224d2eb3" category="paragraph">Verwenden Sie die folgenden Mount-Optionen für allgemeine Datenbank-Workloads:</block>
  <block id="9efbdff43e91279291e95690414bec95" category="paragraph">Sobald die Übertragungsgröße auf ONTAP-Ebene erhöht wurde, werden die folgenden Mount-Optionen verwendet:</block>
  <block id="395201f6a339cc4cf6b345e0c259f383" category="section-title">NFSv3 TCP-Slot-Tabellen</block>
  <block id="1f8260aa2c5a5b35bf771c26828d2096" category="paragraph">Wenn NFSv3 mit Linux verwendet wird, ist es wichtig, die TCP-Slot-Tabellen ordnungsgemäß festzulegen.</block>
  <block id="92d0a1aef2134a70d5c0f0279dc22ec6" category="doc">Snapshots</block>
  <block id="0c47b4eb567b018e3d2e5882b88eaca9" category="paragraph">Speicher-Snapshots sind Point-in-Time-Replikate der Zieldaten. Die Implementierung von ONTAP umfasst die Möglichkeiten, verschiedene Richtlinien festzulegen und bis zu 1024 Snapshots pro Volume zu speichern. Snapshots in ONTAP sind platzsparend. Speicherplatz wird nur dann verbraucht, wenn sich der ursprüngliche Datensatz ändert. Sie sind auch schreibgeschützt. Ein Snapshot kann gelöscht, jedoch nicht geändert werden.</block>
  <block id="7986a4827296ca9cd0b991ded9f8a10d" category="paragraph">In einigen Fällen können Snapshots direkt auf ONTAP geplant werden. In anderen Fällen muss Software wie SnapCenter vor der Erstellung von Snapshots Applikations- oder Betriebssystemvorgänge orchestrieren. Ganz gleich, welcher Ansatz für Ihren Workload am besten geeignet ist: Eine aggressive Snapshot-Strategie bietet Datensicherheit durch häufigen, leicht zugänglichen Zugriff auf Backups aller Komponenten, von Boot-LUNs bis hin zu geschäftskritischen Datenbanken.</block>
  <block id="210c45246ad11e0b53bdb20b64266780" category="paragraph">*Hinweis*: Ein flexibles ONTAP-Volume, oder einfacher gesagt, ein Volume ist nicht gleichbedeutend mit einer LUN. Volumes sind Management-Container für Daten wie Dateien oder LUNs. Eine Datenbank kann beispielsweise auf einen Stripe-Satz mit 8 LUNs platziert werden, wobei alle LUNs in einem einzelnen Volume enthalten sind.</block>
  <block id="ad590deba1cd4423ee565e822c3a91d1" category="paragraph">Weitere Informationen zu Snapshots finden Sie auf <block ref="211d7effcac0cb0488144c6fc8b3cb7c" category="inline-link-macro-rx"></block></block>
  <block id="c4bf0f56feeec9a45e973768fbc4f47e" category="section-title">Manipulationssichere Snapshots</block>
  <block id="d2e783db61d20da61c761d4d473bed14" category="paragraph">Ab ONTAP 9.12.1 sind Snapshots nicht nur schreibgeschützt, sondern können auch vor versehentlichem oder absichtlichem Löschen geschützt werden. Die Funktion wird Tamperproof Snapshots genannt. Über die Snapshot-Richtlinie kann eine Aufbewahrungsfrist festgelegt und durchgesetzt werden. Die resultierenden Snapshots können erst gelöscht werden, wenn sie ihr Ablaufdatum erreicht haben. Es gibt keine administrativen oder Support Center-Überschreibungen.</block>
  <block id="38fef959179f8d3eb79a68a5ce747bdc" category="paragraph">So wird sichergestellt, dass ein Eindringling, ein böswilliger Insider oder sogar ein Ransomware-Angriff die Backups nicht kompromittieren kann, selbst wenn er zum Zugriff auf das ONTAP-System selbst geführt hat. In Verbindung mit häufigen Snapshot-Zeitplänen ist das Ergebnis eine äußerst leistungsstarke Datensicherheit mit einem sehr niedrigen RPO.</block>
  <block id="c64db0c79e66c1c0e5b932c5d2dedfea" category="paragraph">Weitere Informationen zu manipulationssicheren Snapshots finden Sie auf <block ref="134603be635c039f9224bff191c6103c" category="inline-link-macro-rx"></block></block>
  <block id="a1f41e52e9ddbd9f87ecd032c237b1c0" category="section-title">SnapMirror Replizierung</block>
  <block id="1bb837ddfb635c39a6c6e23d847823f9" category="paragraph">Snapshots können auch auf ein Remote-System repliziert werden. Dazu gehören manipulationssichere Snapshots, bei denen die Aufbewahrungsfrist auf das Remote-System angewendet und durchgesetzt wird. Dies führt zu denselben Vorteilen in Bezug auf die Datensicherung wie bei lokalen Snapshots, aber die Daten befinden sich auf einem zweiten Storage-Array. Dadurch wird sichergestellt, dass die Backups durch die Zerstörung des ursprünglichen Arrays nicht beeinträchtigt werden.</block>
  <block id="18d315ae923785b3f9c5234724b0ab42" category="paragraph">Ein zweites System eröffnet auch neue Optionen für die administrative Sicherheit. Beispielsweise trennen einige NetApp Kunden die Authentifizierungsdaten für die primären und sekundären Storage-Systeme. Kein administrativer Benutzer hat Zugriff auf beide Systeme. Das bedeutet, dass ein böswilliger Administrator nicht alle Datenkopien löschen kann.</block>
  <block id="e805272dcce51c35975e830e5ee9ecaf" category="paragraph">Weitere Informationen zu SnapMirror finden Sie unter <block ref="9abdfd5d94773cf0a996248d1d722cac" category="inline-link-macro-rx"></block></block>
  <block id="500569e3cc0764260a592cb176921ca3" category="section-title">Storage Virtual Machines</block>
  <block id="d5174dd4c760ef1a826b3e5bc22215e5" category="paragraph">Ein neu konfiguriertes ONTAP Storage-System ähnelt einem neu bereitgestellten VMware ESX Server, da keiner von ihnen bis zum Erstellen einer Virtual Machine Benutzer unterstützen kann. Mit ONTAP erstellen Sie eine Storage Virtual Machine (SVM), die die grundlegende Storage-Managementeinheit darstellt. Jede SVM verfügt über eigene Storage-Ressourcen, Protokollkonfigurationen, IP-Adressen und FCP-WWNs.  Dies ist die Grundlage der ONTAP Mult-Tenancy.</block>
  <block id="597cb258dd509add7cd999794c60d430" category="paragraph">Beispielsweise können Sie eine SVM für kritische Produktions-Workloads und eine zweite SVM in einem anderen Netzwerksegment für Entwicklungsaktivitäten konfigurieren. Anschließend könnten Sie den Zugriff auf die Produktions-SVM auf bestimmte Administratoren beschränken und Entwicklern eine umfassendere Kontrolle über die Storage-Ressourcen in der Entwicklungs-SVM gewähren. Möglicherweise müssen Sie Ihren Finanz- und HR-Teams auch eine dritte SVM bereitstellen, damit sie besonders wichtige „eyed-only“-Daten speichern können.</block>
  <block id="18519d16720b1e13890c2639657b5bf8" category="paragraph">Weitere Informationen zu SVMs finden Sie unter <block ref="a06a0fd566387240178b1c926e07cb7b" category="inline-link-macro-rx"></block></block>
  <block id="7d0fc71fe28e011ca49b06214469d484" category="section-title">Administrative RBAC</block>
  <block id="4f0dae2ba2fd11c533349d6ade4f81de" category="paragraph">ONTAP bietet eine leistungsstarke rollenbasierte Zugriffssteuerung (RBAC) für administrative Anmeldungen. Einige Administratoren benötigen unter Umständen vollständigen Cluster-Zugriff, während andere unter Umständen nur Zugriff auf bestimmte SVMs benötigen. Erfahrene Helpdesk-Mitarbeiter müssen möglicherweise die Volumegröße erhöhen können. Das Ergebnis ist, dass Sie administrativen Benutzern den Zugriff gewähren können, der für die Ausführung ihrer Aufgaben erforderlich ist, und nicht mehr. Darüber hinaus können Sie diese Anmeldungen mit PKI von verschiedenen Anbietern sichern, den Zugriff auf SSH-Schlüssel beschränken und Sperrungen bei fehlgeschlagenen Anmeldeversuchen erzwingen.</block>
  <block id="6ae13b0198daa7d13d79a8631297e64d" category="paragraph">Weitere Informationen zur administrativen Zugriffssteuerung finden Sie unter <block ref="db5be08b697b5ef26e0e94da1d5f4970" category="inline-link-macro-rx"></block></block>
  <block id="671a94a08dff3cf44fad087e2346c8b0" category="section-title">Multi-Faktor-Authentifizierung</block>
  <block id="945c88a63c229c39ee73ab5592b3ef63" category="paragraph">Weitere Informationen finden Sie unter <block ref="03628d0cd13b6640ef56c399aa1c2070" category="inline-link-macro-rx"></block></block>
  <block id="852741d2a7fb0a8a1a5dd748dbbd6734" category="section-title">API RBAC</block>
  <block id="bbbea5fec67f15424fa77aa563754c49" category="paragraph">Für die Automatisierung sind API-Aufrufe erforderlich, aber nicht alle Tools benötigen vollständigen administrativen Zugriff. Um Automatisierungssysteme zu sichern, ist RBAC auch auf API-Ebene verfügbar. Sie können die Benutzerkonten für die Automatisierung auf die erforderlichen API-Aufrufe beschränken. Die Überwachungssoftware benötigt beispielsweise keinen Änderungszugriff, sondern nur Lesezugriff. Workflows, die Storage bereitstellen, müssen Storage nicht löschen können.</block>
  <block id="7604bad489eaec7e27d112c63d3e2936" category="paragraph">Die Multi-Faktor-Authentifizierung kann noch weiter ausgebaut werden, indem zwei verschiedene Administratoren mit jeweils eigenen Anmeldeinformationen bestimmte Aktivitäten genehmigen müssen. Dazu gehören das Ändern von Anmeldeberechtigungen, das Ausführen von Diagnosebefehlen und das Löschen von Daten.</block>
  <block id="e890d04cd745ac382688bbb307655e81" category="paragraph">Weitere Informationen zur Multi-Admin-Verifizierung (MAV) finden Sie auf <block ref="88f84d4fec0424d978600980bf11baf0" category="inline-link-macro-rx"></block></block>
  <block id="b39c0496b44447232912303246b46aaa" category="paragraph">Wenn sequenzielle I/O-Vorgänge mit hohem I/O-Wert zu erwarten sind, kann die NFS-Übertragungsgröße wie im folgenden Abschnitt beschrieben erhöht werden.</block>
  <block id="959e8f746f4ff03f6225b95cf646e65f" category="admonition">Diese Dokumentation ersetzt den zuvor veröffentlichten technischen Bericht _TR-4590: Best Practice Guide for Microsoft SQL Server with ONTAP_</block>
  <block id="15d2f797f7375b32cb5e92538291f2af" category="list-text"><block ref="15d2f797f7375b32cb5e92538291f2af" category="inline-link-rx"></block></block>
  <block id="36d72286c0ef70108b2f7791b73fdc6d" category="list-text">In den vorherigen Leitfäden wurde das Erstellen von LIF zur Datenlokalität empfohlen. Das heißt, mounten Sie immer einen Datenspeicher mit einer LIF auf dem Node, der physisch Eigentümer des Volume ist. In modernen Versionen von ONTAP 9 ist das nicht mehr erforderlich. Wenn möglich und im Cluster-Umfang Zugangsdaten angegeben, entscheiden sich ONTAP Tools weiterhin für den Lastausgleich über lokale LIFs hinweg für die Daten, allerdings sind dies keine Voraussetzungen für Hochverfügbarkeit oder Performance.</block>
  <block id="aa131b86ebd9f719bab1aba612ae3c3f" category="list-text">SRM führt am besten aus, wenn die Anzahl der Datastores und damit die Schutzgruppen in Ihren Recovery-Plänen minimiert wird. Daher sollten Sie die Optimierung für die VM-Dichte in SRM-geschützten Umgebungen in Betracht ziehen, in denen RTO eine zentrale Bedeutung hat.</block>
  <block id="afab42482b97a8b43c62320c373ded74" category="list-text">Nutzen Sie den Distributed Resource Scheduler (DRS), um die Last auf den geschützten und Recovery ESXi Clustern auszugleichen. Wenn Sie ein Failback planen, werden die zuvor geschützten Cluster beim Ausführen eines Reprotect zu den neuen Recovery-Clustern. DRS hilft dabei, die Platzierung in beide Richtungen auszugleichen.</block>
  <block id="8ed6c5d9143eeb92cf75bb8a08bb7819" category="list-text">Wenn möglich, vermeiden Sie die Verwendung von IP-Anpassung mit SRM, da dies Ihre RTO erhöhen kann.</block>
  <block id="8baa3b91d59f004ee489f4f3fd3dd4e4" category="paragraph">Ab SRM 8.3 wird die Sicherung von VMs mit VVols Datastores unterstützt. SnapMirror Zeitpläne werden über den VASA Provider VM-Storage-Richtlinien ausgesetzt, wenn die VVols Replizierung im Einstellungsmenü der ONTAP Tools aktiviert ist, wie in den folgenden Screenshots dargestellt.</block>
  <block id="3c1a2fd7a7a41ca20efeae84dc73ba0a" category="paragraph">Im folgenden Beispiel wird die Aktivierung der VVols-Replizierung gezeigt.</block>
  <block id="fc7cd76ae57374916b5edd5a2dd19fee" category="paragraph">Im Gegensatz zu älteren VVols-Datastores müssen replizierte VVols Datastores von Anfang an bei aktivierter Replizierung erstellt werden. Dabei müssen sie Volumes verwenden, die vorab auf den ONTAP Systemen mit SnapMirror Beziehungen erstellt wurden. Hierfür sind vorab-Konfigurationen wie Cluster-Peering und SVM-Peering erforderlich. Diese Aktivitäten sollten von Ihrem ONTAP Administrator durchgeführt werden, da hierdurch die Zuständigkeiten zwischen denjenigen, die ONTAP Systeme an mehreren Standorten managen, und denjenigen, die hauptsächlich für vSphere Vorgänge verantwortlich sind, strikt getrennt werden können.</block>
  <block id="e2e7245b1b3cf4a13a6b703ead49ebee" category="paragraph">Für jedes Array-Paar wird ein Array-Manager erstellt. Zusammen mit SRM und ONTAP Tools erfolgt die Kopplung jedes Arrays mit dem Umfang einer SVM, auch wenn Cluster-Anmeldedaten verwendet werden. So können Sie DR-Workflows zwischen Mandanten segmentieren, basierend auf den ihnen zugewiesenen SVMs. Sie können mehrere Array-Manager für ein bestimmtes Cluster erstellen und diese asymmetrisch sein. Sie können Fan-out oder Fan-in zwischen verschiedenen ONTAP 9 Clustern. So können beispielsweise SVM-A und SVM-B auf Cluster-1 und damit auf SVM-C auf Cluster-2, SVM-D auf Cluster-3 oder umgekehrt genutzt werden.</block>
  <block id="3233e61306af256f9de7fd3086bc08bf" category="paragraph">Es gibt mehrere Faktoren, die bei Replizierungsgruppen berücksichtigt werden müssen und die Art und Weise, wie VMs über FlexVol Volumes verteilt werden. Das Gruppieren ähnlicher VMs im selben Volume kann die Storage-Effizienz in älteren ONTAP Systemen steigern, bei denen Deduplizierung auf Aggregatebene fehlt. Beim Gruppieren wird jedoch die Größe des Volumes vergrößert und die Volume-I/O-Parallelität verringert. Moderne ONTAP Systeme bieten ein optimales Verhältnis zwischen Performance und Storage-Effizienz, indem VMs über FlexVol Volumes im selben Aggregat verteilt werden. Dadurch wird die Deduplizierung auf Aggregatebene genutzt und die I/O-Parallelisierung über mehrere Volumes hinweg wird gesteigert. Sie können VMs in den Volumes zusammen wiederherstellen, da eine (nachfolgend erläutert) Sicherungsgruppe mehrere Replizierungsgruppen enthalten kann. Der Nachteil dieses Layouts besteht darin, dass Blöcke mehrmals über das Netzwerk übertragen werden können, da die Aggregat-Deduplizierung bei Volume SnapMirror nicht berücksichtigt wird.</block>
  <block id="670ea4d89ef5c48c4f2d840baf48688c" category="paragraph">So könnte Ihr Unternehmen beispielsweise eine geschäftskritische Tier-1-Applikation nutzen, die für seine Datenbank auf einen Microsoft SQL Server aufbaut. Sie entscheiden also, Ihre VMs in Prioritätsgruppe 1 einzufügen. Innerhalb der Prioritätsgruppe 1 beginnen Sie mit der Planung des Auftrages der Dienste. Sie möchten wahrscheinlich, dass Ihr Microsoft Windows Domain Controller vor Ihrem Microsoft SQL Server hochgefahren wird, was vor Ihrem Anwendungsserver online sein müsste, usw. Sie würden alle diese VMs der Prioritätsgruppe hinzufügen und dann die Abhängigkeiten festlegen, da Abhängigkeiten nur innerhalb einer bestimmten Prioritätsgruppe gelten.</block>
  <block id="512ee19e5020f3ee80a9e25f3f5a6468" category="paragraph">Als Best Practice empfiehlt es sich, immer einen Test-Failover durchzuführen, wenn die Konfiguration eines geschützten VM Storage geändert wird. Dadurch wird sichergestellt, dass Sie bei einem Notfall darauf vertrauen können, dass Site Recovery Manager Services innerhalb des erwarteten RTO-Ziels wiederherstellen kann.</block>
  <block id="4da3f855cd505741909cfd49d743e68e" category="paragraph">SRM ermöglicht es Ihnen auch, die Netzwerkkonfiguration einer VM wie das Recovery zu ändern. Diese Neukonfiguration umfasst Einstellungen wie IP-Adressen, Gateway-Adressen und DNS-Servereinstellungen. Verschiedene Netzwerkeinstellungen, die bei der Wiederherstellung auf einzelne VMs angewendet werden, können in den Einstellungen einer VM der Eigenschaft im Recovery-Plan angegeben werden.</block>
  <block id="e409450ce49f3558b068c69a77a9623a" category="paragraph">Nach dem Failback sollten Sie mit allen Stakeholdern bestätigen, dass ihre Dienste wieder in den Normalzustand gebracht wurden, bevor Sie erneut den Schutz erneut ausführen,</block>
  <block id="a42cf9beb4788dddb7d317e05cc08e23" category="sidebar">Datenbankdateien und Dateigruppen</block>
  <block id="8858bb6564a2efc189c9183c495ce545" category="sidebar">LUN-Ausrichtung</block>
  <block id="02bc4ad8a694d41c8b598dfbcb12f069" category="sidebar">Die LUN-Anzahl und die LUN-Größe</block>
  <block id="066f208a93617bd82090d42624ce63cc" category="sidebar">LUN-Größe</block>
  <block id="9eeba6fd02cf3b6d8aee35e0ce9bf8fe" category="sidebar">LVM-Striping</block>
  <block id="183bdb28f19bee640319f68825827aea" category="sidebar">RPO, RTO und SLAs</block>
  <block id="2172b5d51273924b537842b0db78bfd4" category="sidebar">Grundlagen von Backup und Recovery</block>
  <block id="3403aac944f3cb6e2d2c7f7d52bc44b9" category="paragraph">Mit dem exponentiellen Datenwachstum wird das Datenmanagement für Unternehmen komplexer. Dadurch steigen die Lizenz-, Betriebs-, Support- und Wartungskosten. Zur Senkung der Gesamtbetriebskosten empfiehlt sich der Wechsel von kommerziellen zu Open-Source-Datenbanken mit zuverlässigem, leistungsstarkem Back-End Storage.</block>
  <block id="dcac08882e3396bd30210108baa1641d" category="paragraph">ONTAP ist die ideale Plattform, da ONTAP buchstäblich für Datenbanken entworfen ist. Es wurden speziell für die Anforderungen von Datenbank-Workloads zahlreiche Funktionen wie die Optimierung der zufälligen I/O-Latenz bis hin zur erweiterten Quality of Service (QoS) und grundlegenden FlexClone-Funktionalität erstellt.</block>
  <block id="1429a98a71e16287326edd7fc1f999fd" category="paragraph">Zusätzliche Funktionen wie unterbrechungsfreie Upgrades (inkl. Storage-Austausch) stellen sicher, dass Ihre geschäftskritischen Datenbanken verfügbar bleiben. Darüber hinaus besteht die Möglichkeit zur sofortigen Disaster Recovery für große Umgebungen über MetroCluster oder zur Auswahl von Datenbanken mit SnapMirror Active Sync.</block>
  <block id="8df3fb35ff15e02683bfb1bc7f852c93" category="paragraph">Am wichtigsten ist jedoch, dass ONTAP eine unübertroffene Performance sowie die Möglichkeit bietet, die Lösung entsprechend Ihren spezifischen Anforderungen zu dimensionieren. Unsere High-End-Systeme bieten über 1 Mio. IOPS bei Latenzen im Mikrosekundenbereich. Wenn Sie jedoch nur 100.000 IOPS benötigen, können Sie Ihre Storage-Lösung mit einem kleineren Controller dimensionieren, auf dem immer noch genau dasselbe Storage-Betriebssystem ausgeführt wird.</block>
  <block id="97b4cda00dd31f5be69440f48c4da149" category="summary">Konfiguration der PostgreSQL-Datenbank mit ONTAP</block>
  <block id="c4f52498c0db572381512887d7584e53" category="summary">PostgreSQL-Datenbanken und Storage Snapshots</block>
  <block id="b0792163c858a7c221b236d84619b5ef" category="summary">PostgreSQL-Datenbanken NFS mit ONTAP</block>
  <block id="189950ed46e5e8b7e92941531a5bb88b" category="doc">PostgreSQL-Datenbanken mit NFS-Dateisystemen</block>
  <block id="6cabd4dcf3582d38c441228705da3bda" category="doc">PostgreSQL Datensicherung</block>
  <block id="383a6a5241ecfc558b9fec1d7cff5239" category="summary">PostgreSQL-Tablespaces</block>
  <block id="20d0e2519b3555a0a2927c8914c17280" category="summary">PostgreSQL Datensicherungssoftware</block>
  <block id="29e2aec511a8e307ecceacbd86c22f14" category="summary">PostgreSQL-Initialisierungsparameter</block>
  <block id="035ecd1be6cacde993302b440e396bc2" category="paragraph">Da sich die meisten Datenbankkunden nun für All-Flash-Arrays entscheiden, sind weitere Überlegungen anzustellen. Betrachten Sie beispielsweise Performance-Tests auf einem AFF A900 System mit zwei Nodes:</block>
  <block id="33f545d041b93838074263c0efd2e9da" category="list-text">Mit einem Lese-/Schreib-Verhältnis von 80/20 können zwei A900 Nodes über 1 Mio. zufällige Datenbank-IOPS liefern, bevor die Latenz sogar die 150µs-Marke überschreitet. Dies geht weit über die aktuellen Performance-Anforderungen der meisten Datenbanken hinaus, sodass sich die erwartete Verbesserung nur schwer vorhersagen lässt. Storage würde zu einem großen Teil als Engpass gelöscht werden.</block>
  <block id="0445195555faac96ebff962dbff126ea" category="summary">MySQL-Konfigurationsparameter</block>
  <block id="c0b7708ffcd66fd65a9d2a4801091563" category="paragraph">NetApp empfiehlt ein paar wichtige MySQL-Konfigurationsparameter, um eine optimale Performance zu erzielen.</block>
  <block id="10eb8b7e75da1815860da31490513841" category="summary">MySQL mit NFS</block>
  <block id="5249e05857621fbfae51371a73006c44" category="summary">Slot-Tabellen für MySQL und NFSv3</block>
  <block id="d058f95e3a8d4b7b46802139d6b95982" category="paragraph">Die NFSv3-Leistung unter Linux hängt von einem Parameter namens ab<block ref="453f2cc2326f82af1053f4aa4b75400b" prefix=" " category="inline-code"></block>.</block>
  <block id="bf2bdc980948fb9084a63963398846b2" category="paragraph">ONTAP ist eine ideale Plattform für MySQL-Datenbanken, da ONTAP buchstäblich für Datenbanken konzipiert ist. Es wurden speziell für die Anforderungen von Datenbank-Workloads zahlreiche Funktionen wie die Optimierung der zufälligen I/O-Latenz bis hin zur erweiterten Quality of Service (QoS) und grundlegenden FlexClone-Funktionalität erstellt.</block>
  <block id="1b3804b7e2292e18f5e66d54f7a9fafc" category="paragraph">Am wichtigsten ist jedoch, dass ONTAP eine unübertroffene Performance sowie die Möglichkeit bietet, die Lösung entsprechend Ihren spezifischen Anforderungen zu dimensionieren. Unsere High-End-Systeme bieten über 1 Mio. IOPS bei Latenzen im Mikrosekundenbereich. Wenn Sie jedoch nur 100.000 IOPS benötigen, können Sie die Größe Ihrer Storage-Lösung mit einem kleineren Controller anpassen, auf dem dennoch genau dasselbe Storage-Betriebssystem ausgeführt wird.</block>
  <block id="1339877637a3f7f59745a00e04f304fd" category="summary">MySQL und innodb_log_file_size</block>
  <block id="c1c4e722764621d56a075e42ce48d0c0" category="summary">MySQL und innodb_buffer_Pool_size</block>
  <block id="ae14ab6d1e5edd0e782d8897d1721eb4" category="summary">MySQL und innodb_doublewrite</block>
  <block id="689c52e3f5db5d2dafccd7d93cf1abc4" category="summary">MySQL und innodb_flush_log_at_trx_commi</block>
  <block id="639600080c225673dfdfa012fdbd6146" category="summary">MySQL und IO-Scheduler</block>
  <block id="32e3724e2b54692cbef969373fbfe040" category="doc">I/O-Planer und MySQL</block>
  <block id="72985ac51e91797345b2437088a4362e" category="summary">MySQL mit SAN</block>
  <block id="d5ef0a45fe252b03f538bfd134b168db" category="summary">MySQL und innodb_lru_Scan_depth</block>
  <block id="df657260453f6ef228ee0ca22dd7a415" category="summary">MySQL-Containerisierung</block>
  <block id="885b1b5a90f898c89b664ec58a42121c" category="doc">MySQL-Containerisierung</block>
  <block id="2828450fc6669c8df75f937e227996f1" category="doc">MySQL-Dateideskriptoren</block>
  <block id="9420010a756fb6d3ddeadfa63d2170f6" category="paragraph">Zum Ausführen benötigt der MySQL-Server Dateideskriptoren, und die Standardwerte reichen nicht aus.</block>
  <block id="62414a5a9c0e2fc12a8d028ad29164b8" category="summary">MySQL und innodb_io_Capacity</block>
  <block id="6219c45bc8c26437c0b7e1a69d4cbe58" category="summary">MySQL und innodb_flush_method</block>
  <block id="35d1a5d7b12999ec27f5233dccba043f" category="summary">MySQL und open_file_limits</block>
  <block id="c031febc35017c70bf3b526723756b2a" category="doc">ISCSI und NVMe/TCP</block>
  <block id="4da35d461815e71b97bda03476e89b7b" category="paragraph">Ein Host, der iSCSI oder NVMe/TCP verwendet, kann direkt mit einem Storage-System verbunden werden und ordnungsgemäß ausgeführt werden. Der Grund dafür ist Pathing. Direkte Verbindungen zu zwei verschiedenen Storage Controllern ergeben zwei unabhängige Pfade für den Datenfluss. Der Verlust von Pfad, Port oder Controller verhindert nicht, dass der andere Pfad verwendet wird.</block>
  <block id="40715d81cea527f13cdf470773fc3a65" category="paragraph">Direct-Connected NFS Storage kann genutzt werden, aber mit einer erheblichen Einschränkung - Failover funktioniert nicht ohne einen erheblichen Scripting-Aufwand, der in der Verantwortung des Kunden liegt.</block>
  <block id="d0b13bfd23c05f64222869de30f81000" category="paragraph">Der Grund, warum ein unterbrechungsfreier Failover mit direkt verbundenem NFS-Storage kompliziert ist, ist das Routing auf dem lokalen Betriebssystem. Angenommen, ein Host hat eine IP-Adresse von 192.168.1.1/24 und ist direkt mit einem ONTAP-Controller mit einer IP-Adresse von 192.168.1.50/24 verbunden. Während eines Failovers kann diese 192.168.1.50-Adresse ein Failover auf den anderen Controller durchführen, und sie wird für den Host verfügbar sein. Wie erkennt der Host jedoch sein Vorhandensein? Die ursprüngliche 192.168.1.1-Adresse ist noch auf der Host-NIC vorhanden, die keine Verbindung mehr zu einem Betriebssystem herstellt. Der für 192.168.1.50 bestimmte Datenverkehr würde weiterhin an einen nicht funktionsfähigen Netzwerkport gesendet.</block>
  <block id="288a0a5ebbc70fb3b968ec58d36d71ab" category="paragraph">Die zweite BS-NIC könnte als 19 konfiguriert werden 2.168.1.2 und wäre in der Lage, mit der Failed Over 192.168.1.50-Adresse zu kommunizieren, aber die lokalen Routing-Tabellen würden standardmäßig eine *und nur eine*-Adresse verwenden, um mit dem Subnetz 192.168.1.0/24 zu kommunizieren. Ein Sysadmin könnte ein Skript-Framework erstellen, das eine fehlerhafte Netzwerkverbindung erkennt und die lokalen Routing-Tabellen ändert oder Schnittstellen hoch- und herunterfahren würde. Das genaue Verfahren hängt vom verwendeten Betriebssystem ab.</block>
  <block id="d6ce8522932df7ae76aa0b963ad9ef7d" category="paragraph">In der Praxis haben NetApp-Kunden NFS direkt verbunden, aber normalerweise nur für Workloads, bei denen IO-Pausen während Failover akzeptabel sind. Wenn harte Mounts verwendet werden, sollte es während solcher Pausen keine IO-Fehler geben. Die E/A-Vorgänge sollten so lange hängen bleiben, bis Dienste wiederhergestellt werden, entweder durch ein Failback oder durch einen manuellen Eingriff, um IP-Adressen zwischen NICs auf dem Host zu verschieben.</block>
  <block id="3436a7073ac9f4071820b6720a7789fc" category="section-title">FC Direct Connect</block>
  <block id="b7981516813a09695906aa429b07f48f" category="paragraph">Es ist nicht möglich, einen Host direkt über das FC-Protokoll mit einem ONTAP Storage-System zu verbinden. Der Grund dafür ist die Verwendung von NPIV. Der WWN, der einen ONTAP FC-Port mit dem FC-Netzwerk identifiziert, verwendet eine Art Virtualisierung, die als NPIV bezeichnet wird. Jedes Gerät, das an ein ONTAP-System angeschlossen ist, muss einen NPIV-WWN erkennen können. Es gibt derzeit keine HBA-Anbieter, die einen HBA anbieten, der auf einem Host installiert werden kann, der ein NPIV-Ziel unterstützen könnte.</block>
  <block id="97db606ac6781989ea3b189d7be28bf2" category="section-title">Direct-Connect-Netzwerk</block>
  <block id="d078ae9a339971900f939db9e82f253d" category="paragraph">Storage-Administratoren ziehen es manchmal vor, ihre Infrastruktur zu vereinfachen, indem sie Netzwerk-Switches von der Konfiguration entfernen. Dies kann in einigen Szenarien unterstützt werden.</block>
  <block id="e817ecc2f82a2b78d828ec8704031da3" category="section-title">Direkte Netzwerkverbindung</block>
  <block id="87e6f97c8ec2f31df616ef84401ffaf4" category="paragraph">Es stehen mehrere Ressourcen zur Fehlerbehebung mit zusätzlichen Informationen zur Verfügung.</block>
  <block id="52d04863ac62a6fa67e1b49a31086928" category="paragraph">In den folgenden Abschnitten werden die betrieblichen Best Practices für VMware SRM und ONTAP Storage beschrieben.</block>
  <block id="f24ef4a42850a2b3e3d77e610ae46fee" category="doc">Ausfallsicherheit bei geplanten und ungeplanten Ereignissen</block>
  <block id="4bce11924453ef2ee7e40109f79dc66d" category="paragraph">NetApp MetroCluster und SnapMirror Active Sync sind leistungsstarke Tools, die die Hochverfügbarkeit und den unterbrechungsfreien Betrieb von NetApp Hardware und ONTAP Software verbessern.</block>
  <block id="52f49b6400100ac4558d31582b90ca4e" category="paragraph">Diese Tools bieten standortweiten Schutz für die gesamte Storage-Umgebung und stellen sicher, dass Ihre Daten immer verfügbar sind. Ob Sie Standalone-Server, hochverfügbare Server-Cluster, Docker Container oder virtualisierte Server verwenden: Die NetApp-Technologie sorgt nahtlos für die Storage-Verfügbarkeit im Falle eines totalen Ausfalls aufgrund von Strom-, Kühlungs- oder Netzwerkkonnektivität, Herunterfahren des Storage-Arrays oder Bedienungsfehlern.</block>
  <block id="d267775072875ddbb24c16a41c22d8a2" category="paragraph">MetroCluster und SnapMirror Active Sync bieten drei grundlegende Methoden für die Datenverfügbarkeit bei geplanten und ungeplanten Ereignissen:</block>
  <block id="1d3027d09fc1db3087f05f2163af2cb7" category="list-text">Redundante Komponenten zum Schutz vor dem Ausfall einer einzelnen Komponente</block>
  <block id="f3b83f3b77289efe3b53cf537b7b77f5" category="list-text">Lokaler HA-Takeover für Ereignisse, die sich auf einen einzelnen Controller auswirken</block>
  <block id="b6ed0cba165f6f9664dc1e57ac3419ec" category="list-text">Vollständiger Standortschutz: Schnelle Wiederaufnahme des Service durch Verschieben des Speicher- und Client-Zugriffs vom Quell-Cluster auf den Ziel-Cluster</block>
  <block id="16afcbe8821dfa582521cd62c1ec130d" category="paragraph">Das bedeutet, dass die Abläufe bei Ausfall einer einzelnen Komponente reibungslos fortgesetzt werden und beim Austausch der ausgefallenen Komponente automatisch in den redundanten Betrieb zurückkehren.</block>
  <block id="8fab78e8b214cad43d962b802bf568f8" category="paragraph">Alle ONTAP Cluster außer Cluster mit einem Node (in der Regel softwaredefinierte Versionen, wie beispielsweise ONTAP Select) verfügen über integrierte HA-Funktionen für Takeover und Giveback. Jeder Controller im Cluster wird mit einem anderen Controller gepaart, wodurch ein HA-Paar entsteht. Diese Paare stellen sicher, dass jeder Node lokal mit dem Speicher verbunden ist.</block>
  <block id="83e1749664841043ce8b1e894769a201" category="paragraph">Die Übernahme ist ein automatisierter Prozess, bei dem ein Node den Storage des anderen zur Aufrechterhaltung der Datenservices übernimmt. GiveBack bedeutet umgekehrter Prozess, der den normalen Betrieb wiederherstellt. Takeover können geplant werden, beispielsweise bei Hardware-Wartungsarbeiten oder ONTAP Upgrades oder aufgrund von Node-Panic- oder Hardware-Ausfällen.</block>
  <block id="834d83f4ff04b7e3948831f336559b7c" category="paragraph">Während einer Übernahme führen NAS-LIFs (Network Attached Storage Logical Interfaces) in MetroCluster-Konfigurationen automatisch ein Failover durch. Storage Area Network LIFs (SAN LIFs) führen jedoch keinen Failover durch. Sie verwenden weiterhin den direkten Pfad zu den Logical Unit Numbers (LUNs).</block>
  <block id="0b0403a9258293da1f6a5fc84d24a18e" category="inline-link">HA-Paar-Management – Übersicht</block>
  <block id="df5ca9bc3ee60e70aa9fd870a1ce1065" category="paragraph">Weitere Informationen zu HA-Takeover und Giveback finden Sie im<block ref="5a72801b431f75b7489a0e7de50680bf" category="inline-link-rx"></block>. Erwähnenswert ist, dass diese Funktion nicht spezifisch für MetroCluster oder SnapMirror für die aktive Synchronisierung ist.</block>
  <block id="cf654073ce9eb8f38d3f05292ed83cc5" category="paragraph">Eine Standortumschaltung mit MetroCluster erfolgt, wenn ein Standort offline ist oder als geplante Aktivität für die standortweite Wartung vorgesehen ist. Der verbleibende Standort übernimmt die Eigentümerschaft der Storage-Ressourcen (Festplatten und Aggregate) des Offline-Clusters, und die SVMs am ausgefallenen Standort werden online geschaltet und am Disaster-Standort neugestartet. Dabei bleibt die volle Identität für den Client- und Host-Zugriff erhalten.</block>
  <block id="4dd2305dcb9d7412784238dd42dcdc21" category="paragraph">Da beide Kopien gleichzeitig aktiv verwendet werden, arbeiten Ihre vorhandenen Hosts mit der aktiven SnapMirror Synchronisierung weiter. Der NetApp-Mediator ist erforderlich, um sicherzustellen, dass ein Standort-Failover korrekt ausgeführt wird.</block>
  <block id="823bd5c6c005e340599d915e134d851c" category="summary">VMware vSphere Lösungsübersicht</block>
  <block id="be26d39d86de44047c65c1fed157d529" category="paragraph">VCenter Server Appliance (VCSA) ist das leistungsstarke, zentralisierte Managementsystem und eine zentrale Konsole für vSphere, mit der Administratoren ESXi Cluster effizient betreiben können. Sie unterstützt wichtige Funktionen wie VM-Bereitstellung, vMotion Betrieb, Hochverfügbarkeit (HA), Distributed Resource Scheduler (DRS), Tanzu Kubernetes Grid und mehr. Sie ist eine wesentliche Komponente in VMware Cloud-Umgebungen und sollte im Hinblick auf die Service-Verfügbarkeit konzipiert werden.</block>
  <block id="8ba8694e23f6560a60d85c4de549122b" category="section-title">VSphere High Availability</block>
  <block id="4b5543b81af03f01e26fcb74614f7062" category="paragraph">Die Cluster-Technologie von VMware gruppiert ESXi Server in Pools mit gemeinsam genutzten Ressourcen für Virtual Machines und stellt vSphere High Availability (HA) bereit. VSphere HA bietet benutzerfreundliche, hohe Verfügbarkeit für Anwendungen, die auf virtuellen Maschinen ausgeführt werden. Wenn die HA-Funktion auf dem Cluster aktiviert ist, hält jeder ESXi-Server die Kommunikation mit anderen Hosts aufrecht, sodass ein ESXi-Host nicht mehr reagiert oder isoliert wird. der HA-Cluster kann die Wiederherstellung der Virtual Machines, die auf diesem ESXi-Host ausgeführt wurden, zwischen den noch intakten Hosts im Cluster aushandeln. Bei einem Ausfall eines Gastbetriebssystems startet vSphere HA die betroffene virtuelle Maschine auf demselben physischen Server neu. Mit vSphere HA werden geplante Ausfallzeiten reduziert, ungeplante Ausfallzeiten vermieden und eine schnelle Wiederherstellung nach Ausfällen ermöglicht.</block>
  <block id="01036edb3159652090a3f7149fe1ce2f" category="paragraph">VSphere HA Cluster Wiederherstellung von VMs aus ausgefallener Server.</block>
  <block id="15496c3917565cea1f21bbc7e6eba996" category="image-alt">VMSC-Diagramm</block>
  <block id="c1bb7aa3a002a819d935439c7e347826" category="paragraph">Es ist wichtig zu wissen, dass VMware vSphere nicht über NetApp MetroCluster oder SnapMirror Active Sync verfügt und alle ESXi Hosts im vSphere Cluster als berechtigte Hosts für HA-Clustervorgänge erkennt, je nach Host- und VM-Gruppenaffinitätskonfigurationen.</block>
  <block id="dcff37ab37f2cb6a72aeb3c30e3ed5aa" category="section-title">Erkennung Von Host-Ausfällen</block>
  <block id="7069d861b63dd4095cfb7b2ee30587f5" category="paragraph">Sobald der HA-Cluster erstellt ist, nehmen alle Hosts im Cluster an der Auswahl Teil, und einer der Hosts wird zum Master. Jeder Slave führt den Netzwerk-Heartbeat zum Master aus, und der Master wiederum führt den Netzwerk-Heartbeat auf allen Slave-Hosts aus. Der Master-Host eines vSphere HA-Clusters ist für die Erkennung des Ausfalls von Slave-Hosts verantwortlich.</block>
  <block id="73b629afd8ef39d8d0147696412538c8" category="paragraph">Je nach Art des erkannten Fehlers müssen die auf den Hosts ausgeführten virtuellen Maschinen möglicherweise ein Failover durchführen.</block>
  <block id="e12cf4015a45ff29f1f84ae1a3ee0390" category="paragraph">In einem vSphere HA-Cluster werden drei Arten von Host-Ausfällen erkannt:</block>
  <block id="4eac346eb6cdd3a1a98ee9bc71ebccaa" category="list-text">Fehler: Ein Host funktioniert nicht mehr.</block>
  <block id="3875a4c584a7715edf4a221ce53d45fc" category="list-text">Isolierung: Ein Host wird zu einem isolierten Netzwerk.</block>
  <block id="82fba11358d642111efbcf36273159f2" category="list-text">Partition: Ein Host verliert die Netzwerkverbindung mit dem Master-Host.</block>
  <block id="5f72ceebd161159fe623ff01ac0cf1b4" category="paragraph">Der Master-Host überwacht die Slave-Hosts im Cluster. Diese Kommunikation erfolgt durch den Austausch von Netzwerk-Heartbeats jede Sekunde. Wenn der Master-Host diese Heartbeats nicht mehr von einem Slave-Host empfängt, prüft er die Host-Lebendigkeit, bevor er den Host für fehlgeschlagen erklärt. Die Liveness-Prüfung, die der Master-Host durchführt, besteht darin festzustellen, ob der Slave-Host Heartbeats mit einem der Datastores austauscht. Außerdem prüft der Master-Host, ob der Host auf ICMP-Pings reagiert, die an seine Management-IP-Adressen gesendet werden, um festzustellen, ob er lediglich von seinem Master-Knoten isoliert oder vollständig vom Netzwerk isoliert ist. Dies erfolgt durch Ping an das Standard-Gateway. Eine oder mehrere Isolationsadressen können manuell angegeben werden, um die Zuverlässigkeit der Isolationsvalidierung zu erhöhen.</block>
  <block id="5546466ab10e5cbfbc4b286cd60b8b4a" category="section-title">_Best Practice_</block>
  <block id="0456a7ce7bf92fb68980571fdd2defa1" category="paragraph">NetApp empfiehlt, mindestens zwei zusätzliche Isolationsadressen anzugeben, und jede dieser Adressen sollte standortlokal sein. Dies erhöht die Zuverlässigkeit der Isolationsvalidierung.</block>
  <block id="19c262cd1e1bb07c491e359333f04a29" category="section-title">Antwort Der Hostisolation</block>
  <block id="7a2723aad08de3c674693adf221e4a48" category="paragraph">Die Isolationsantwort ist eine Einstellung in vSphere HA, die die Aktion bestimmt, die auf virtuellen Maschinen ausgelöst wird, wenn ein Host in einem vSphere HA-Cluster seine Verwaltungsnetzwerkverbindungen verliert, aber weiterhin ausgeführt wird. Für diese Einstellung gibt es drei Optionen: „Disabled“, „Shut Down and Restart VMs“ und „Power Off and Restart VMs“.</block>
  <block id="715e310d886b45856c3ab0f69b06241b" category="paragraph">„Herunterfahren“ ist besser als „Ausschalten“, das nicht die neuesten Änderungen auf Festplatte bereinigt oder Transaktionen festschreibt. Wenn virtuelle Maschinen nicht in 300 Sekunden heruntergefahren wurden, werden sie ausgeschaltet. Um die Wartezeit zu ändern, verwenden Sie die erweiterte Option das.isolationshutdowntimeout.</block>
  <block id="620b4ae663a26c98656d1c81bec1a6c2" category="paragraph">Bevor HA die Isolationsantwort initiiert, prüft es zunächst, ob der vSphere HA-Master-Agent den Datenspeicher besitzt, der die VM-Konfigurationsdateien enthält. Wenn dies nicht der Fall ist, löst der Host die Isolationsantwort nicht aus, da kein Master zum Neustart der VMs vorhanden ist. Der Host überprüft regelmäßig den Datastore-Status, um festzustellen, ob er von einem vSphere HA-Agent beansprucht wird, der die Master-Rolle besitzt.</block>
  <block id="6824ef1b1c5e58e8ea5ebc8b2cf94be2" category="paragraph">NetApp empfiehlt, die „Host-Isolationsantwort“ auf deaktiviert zu setzen.</block>
  <block id="6bd7b146f345eefac88b336a89aa3754" category="paragraph">Ein Split-Brain-Zustand kann auftreten, wenn ein Host vom vSphere HA-Master-Host isoliert oder partitioniert wird und der Master nicht über Heartbeat Datastores oder Ping kommunizieren kann. Der Master erklärt den isolierten Host für tot und startet die VMs auf anderen Hosts im Cluster neu. Eine Split-Brain-Bedingung besteht jetzt, weil zwei Instanzen der virtuellen Maschine ausgeführt werden, von denen nur eine die virtuellen Laufwerke lesen oder schreiben kann. Split-Brain-Bedingungen können jetzt durch die Konfiguration von VM Component Protection (VMCP) vermieden werden.</block>
  <block id="df05a421ed8ba49f97f18dc085495ee4" category="section-title">Schutz von VM-Komponenten (VMCP)</block>
  <block id="1418d127ee81cc7f3e114029e9ba6856" category="paragraph">Eine der Funktionsverbesserungen bei vSphere 6, relevant für HA, ist VMCP. VMCP bietet erweiterten Schutz vor All Paths Down (APD) und Permanent Device Loss (PDL) für Block (FC, iSCSI, FCoE) und File Storage (NFS).</block>
  <block id="fc35e15d00ef5b037a1a56c77ef3e17c" category="section-title">Permanenter Geräteverlust (PDL)</block>
  <block id="967b20f28092443585be2b9649d0ba93" category="paragraph">PDL ist ein Zustand, der auftritt, wenn ein Speichergerät dauerhaft ausfällt oder administrativ entfernt wird und nicht zurückgegeben werden soll. Das NetApp-Speicher-Array gibt ESXi einen SCSI-Sense-Code aus, der erklärt, dass das Gerät dauerhaft verloren geht. Im Abschnitt Fehlerbedingungen und VM-Reaktion von vSphere HA können Sie konfigurieren, wie die Antwort nach dem Erkennen einer PDL-Bedingung aussehen soll.</block>
  <block id="7cf4fdbfc8e1deb9f567848f2df19e91" category="paragraph">NetApp empfiehlt, die „Antwort für Datastore mit PDL“ auf „*Ausschalten und Neustart von VMs*“ zu setzen. Wenn dieser Zustand erkannt wird, wird eine VM sofort auf einem funktionierenden Host im vSphere HA-Cluster neu gestartet.</block>
  <block id="6d623de2ea8c5007def6f2349d9ad8b9" category="section-title">Alle Pfade nach unten (APD)</block>
  <block id="15d2d9b1f25022e7080482582e155265" category="paragraph">APD ist ein Zustand, der auftritt, wenn ein Speichergerät für den Host nicht mehr zugänglich ist und keine Pfade zum Array verfügbar sind. ESXi betrachtet dies als ein vorübergehendes Problem mit dem Gerät und erwartet, dass es wieder verfügbar wird.</block>
  <block id="f89caca297ba86d190d90ef30c409ccf" category="paragraph">Wenn eine APD-Bedingung erkannt wird, wird ein Timer gestartet. Nach 140 Sekunden wird der APD-Zustand offiziell deklariert und das Gerät als APD-Zeitabmeldung markiert. Nach Ablauf der 140 Sekunden zählt HA die Anzahl der Minuten, die in der Verzögerung für VM-Failover-APD angegeben sind. Wenn die angegebene Zeit verstrichen ist, startet HA die betroffenen virtuellen Maschinen neu. Sie können VMCP so konfigurieren, dass es bei Bedarf anders reagiert (deaktiviert, Ereignisse ausstellen oder VMs aus- und neu starten).</block>
  <block id="8f179a61d789dab9b994ecd0ca8c53cb" category="paragraph">NetApp empfiehlt, die „Antwort für Datastore mit APD“ auf „*Ausschalten und Neustart von VMs (konservativ)*“ zu konfigurieren.</block>
  <block id="76d29c3c5f57a16a7f080459356c3793" category="paragraph">Konservativ bezieht sich auf die Wahrscheinlichkeit, dass HA die VMs neu starten kann. Wenn sie auf Conservative gesetzt ist, startet HA nur die VM neu, die vom APD betroffen ist, wenn sie weiß, dass ein anderer Host sie neu starten kann. Im Fall von aggressive, versucht HA, die VM neu zu starten, selbst wenn sie den Status anderer Hosts nicht kennt. Dies kann dazu führen, dass VMs nicht neu gestartet werden, wenn kein Host mit Zugriff auf den Datenspeicher vorhanden ist, auf dem sich dieser befindet.</block>
  <block id="c64ef72a18526e28efe7c08b3ff3889a" category="paragraph">Wenn der APD-Status aufgelöst ist und der Zugriff auf den Speicher wiederhergestellt wird, bevor die Zeiteinstellung überschritten wurde, startet HA die virtuelle Maschine nicht unnötig neu, es sei denn, Sie konfigurieren sie ausdrücklich dafür. Wenn eine Antwort gewünscht wird, selbst wenn sich die Umgebung von der APD-Bedingung erholt hat, sollte die Antwort für APD-Wiederherstellung nach APD-Timeout so konfiguriert werden, dass die VMs zurückgesetzt werden.</block>
  <block id="580bc3953adcf3e3a20c5497f67f8b2c" category="paragraph">NetApp empfiehlt, die Antwort für die APD-Wiederherstellung nach der APD-Zeitüberschreitung auf deaktiviert zu konfigurieren.</block>
  <block id="a067a9a0e98c97f62fba4698bde30edc" category="section-title">VMware DRS Implementierung für NetApp MetroCluster</block>
  <block id="3bf3e67966e8b0e07b0812dae7b86ce7" category="paragraph">VMware DRS ist eine Funktion, die die Host-Ressourcen in einem Cluster aggregiert und hauptsächlich zum Lastausgleich innerhalb eines Clusters in einer virtuellen Infrastruktur verwendet wird. VMware DRS berechnet in erster Linie die CPU- und Arbeitsspeicherressourcen für den Lastausgleich in einem Cluster. Da vSphere das erweiterte Clustering nicht kennt, werden beim Lastausgleich alle Hosts an beiden Standorten berücksichtigt. Um standortübergreifenden Datenverkehr zu vermeiden, empfiehlt NetApp die Konfiguration der DRS Affinitätsregeln, um eine logische Trennung der VMs zu managen. So stellen Sie sicher, dass HA und DRS nur lokale Hosts verwenden, sofern es keinen vollständigen Standortausfall gibt.</block>
  <block id="b9e04f46a88c8139fbb91780a2ff9064" category="paragraph">Wenn Sie eine DRS-Affinitätsregel für Ihr Cluster erstellen, können Sie festlegen, wie vSphere diese Regel während eines Failover einer virtuellen Maschine anwendet.</block>
  <block id="f903d43a0acb7412db93e8201ab12be3" category="paragraph">Es gibt zwei Arten von Regeln, die Sie vSphere HA-Failover-Verhalten angeben können:</block>
  <block id="a96c8bab3a450bb828e3093d16f7e32c" category="list-text">VM-Anti-Affinitätsregeln zwingen bestimmte Virtual Machines dazu, bei Failover-Aktionen getrennt zu bleiben.</block>
  <block id="1fa3a04279011cbc6001c96c0f6f27fb" category="list-text">VM-Host-Affinitätsregeln platzieren angegebene Virtual Machines während Failover-Aktionen auf einem bestimmten Host oder einem Mitglied einer definierten Gruppe von Hosts.</block>
  <block id="225e3f7204b6e5bf6cd388af7bb16eb4" category="paragraph">Mithilfe der VM Host-Affinitätsregeln in VMware DRS lässt sich eine logische Trennung zwischen Standort A und Standort B erreichen, sodass die VM auf dem Host am selben Standort ausgeführt wird wie das Array, das als primärer Lese-/Schreib-Controller für einen bestimmten Datenspeicher konfiguriert ist. Zudem bleiben Virtual Machines gemäß den Regeln zur VM Host-Affinität lokal im Storage, wodurch wiederum die Virtual Machine-Verbindung im Falle von Netzwerkausfällen zwischen den Standorten hergestellt wird.</block>
  <block id="db03e2255e35df6fa9719c800850c397" category="paragraph">Nachfolgend finden Sie ein Beispiel für VM-Hostgruppen und Affinitätsregeln.</block>
  <block id="d400b2cf908bc5f979847146ff8ac816" category="paragraph">NetApp empfiehlt die Implementierung der „sollte“-Regeln statt der „müssen“-Regeln, da im Falle eines Ausfalls von vSphere HA gegen diese verstoßen wird. Der Einsatz von „Must“-Regeln kann potenziell zu Serviceausfällen führen.</block>
  <block id="e9503bc9ad84f6517ddc938c85541dd5" category="paragraph">Die Verfügbarkeit von Services sollte immer Vorrang vor der Leistung haben. Wenn ein vollständiges Datacenter ausfällt, müssen die „Must“-Regeln Hosts aus der VM-Host-Affinitätsgruppe auswählen. Wenn das Datacenter nicht verfügbar ist, werden die Virtual Machines nicht neu gestartet.</block>
  <block id="b3c45603f3738ac1e1f8e1e87569c00c" category="section-title">VMware Storage DRS Implementierung mit NetApp MetroCluster</block>
  <block id="b273633ed947791e90b7b83ad1bb2021" category="paragraph">Die VMware Storage DRS-Funktion ermöglicht die Aggregation von Datastores in eine einzige Einheit und gleicht Festplatten der virtuellen Maschine aus, wenn die Storage-I/O-Kontrollschwellenwerte überschritten werden.</block>
  <block id="4669dbcf4d42583b3fae1c79fa228078" category="paragraph">Die Storage-I/O-Steuerung ist bei DRS-Clustern mit Storage DRS standardmäßig aktiviert. Mit der Storage-I/O-Kontrolle kann ein Administrator die Menge an Storage-I/O steuern, die Virtual Machines bei I/O-Engpässen zugewiesen wird. Dadurch können wichtigeren Virtual Machines bei der I/O-Ressourcenzuweisung Vorrang vor weniger wichtigen Virtual Machines geben.</block>
  <block id="f06483bf36a941eebbb2f78a89b6bf68" category="paragraph">Storage DRS verwendet Storage vMotion, um die virtuellen Maschinen auf verschiedene Datastores innerhalb eines Datastore-Clusters zu migrieren. In einer NetApp MetroCluster Umgebung muss eine Migration von Virtual Machines innerhalb der Datenspeicher dieses Standorts gesteuert werden. Eine Virtual Machine A, die auf einem Host an Standort A ausgeführt wird, sollte idealerweise innerhalb der Datenspeicher der SVM an Standort A migriert werden Wenn dies nicht der Fall ist, wird die virtuelle Maschine weiterhin betrieben, jedoch mit verminderter Leistung, da das Lesen/Schreiben der virtuellen Festplatte von Standort B über standortübergreifende Links erfolgt.</block>
  <block id="e607d8c3dc392e521065e5494d8c85c6" category="paragraph">NetApp empfiehlt das Erstellen von Datastore-Clustern im Hinblick auf die Storage-Standortorientierung. Das heißt, Datastores mit Standortaffinität zu Standort A sollten nicht mit Datastore-Clustern mit Datastores mit Standortaffinität zu Standort B gemischt werden</block>
  <block id="63fe88e257ae2d072b6920d6dbcf0818" category="paragraph">Wenn eine virtuelle Maschine neu bereitgestellt oder mithilfe von Storage vMotion migriert wird, empfiehlt NetApp, alle für diese virtuellen Maschinen spezifischen VMware DRS-Regeln entsprechend manuell zu aktualisieren. Dadurch wird die Virtual Machine-Affinität auf Standortebene sowohl für Host als auch für Datenspeicher ermittelt und somit der Netzwerk- und Storage Overhead reduziert.</block>
  <block id="4adcd8d9323839ff088dc4f8f0bdcce2" category="paragraph">Zum Erstellen von VVols Storage für Ihre VMs sind verschiedene Schritte erforderlich.</block>
  <block id="2438f6a80b933cf058ad052d26ac0703" category="paragraph">Durch das Klonen eines Storage-Objekts können Sie schnell Kopien für andere Zwecke erstellen, beispielsweise zum Provisionieren weiterer VMs, für Backup- und Recovery-Vorgänge usw.</block>
  <block id="1a795345e422d7dbcca8fe38671a9371" category="summary">VMSC Design- und Implementierungsrichtlinien.</block>
  <block id="b73deaadc37ea1c7a13ad17af6730c42" category="doc">VMSC Design- und Implementierungsrichtlinien</block>
  <block id="3512760e2d3fc1a9dd2116e83bb0cada" category="paragraph">Dieses Dokument enthält Design- und Implementierungsrichtlinien für vMSC mit ONTAP Storage-Systemen.</block>
  <block id="556669df5b5073ab3d2ddcf638e942d9" category="section-title">NetApp-Speicherkonfiguration</block>
  <block id="7f4d4fbe08a8a895671d05c4a82b0c85" category="inline-link">MetroCluster-Dokumentation</block>
  <block id="002eb41870df47521a2b59424d251f60" category="inline-link">Überblick über die Business Continuity in SnapMirror</block>
  <block id="008c17a2bb74d3d9706c0f2c88499240" category="paragraph">Setup-Anweisungen für NetApp MetroCluster (als MCC-Konfiguration bezeichnet) finden Sie unter<block ref="2c736bccede87dae47dcf61108f083c4" category="inline-link-rx"></block>. Anweisungen für SnapMirror Active Sync finden Sie auch unter<block ref="14203840309dd1b6e13f73373477bb9d" category="inline-link-rx"></block>.</block>
  <block id="447566f62201eb825a1f6fb7a570873f" category="paragraph">Sobald Sie MetroCluster konfiguriert haben, ist die Verwaltung wie das Management einer herkömmlichen ONTAP-Umgebung. Sie können Storage Virtual Machines (SVMs) mithilfe verschiedener Tools wie Command Line Interface (CLI), System Manager oder Ansible einrichten. Sobald die SVMs konfiguriert sind, erstellen Sie logische Schnittstellen (LIFs), Volumes und LUNs (Logical Unit Numbers) auf dem Cluster, die für den normalen Betrieb verwendet werden. Diese Objekte werden automatisch über das Cluster-Peering-Netzwerk auf den anderen Cluster repliziert.</block>
  <block id="b5486be95ed9ae2e4681a39cf8fe8041" category="inline-link">Übersicht über Konsistenzgruppen</block>
  <block id="5440d8600dfca322be65444d14c4566f" category="paragraph">Wenn Sie MetroCluster nicht nutzen, können Sie SnapMirror Active Sync verwenden, was Datastores-granulare Sicherung und aktiv/aktiv-Zugriff über diverse ONTAP Cluster in verschiedenen Ausfall-Domains hinweg bietet. SnapMirror Active Sync verwendet Konsistenzgruppen, um die Konsistenz der Schreibreihenfolge zwischen einem oder mehreren Datastores sicherzustellen. Sie können je nach Applikations- und Datastore-Anforderungen mehrere Konsistenzgruppen erstellen. Konsistenzgruppen sind insbesondere für Applikationen nützlich, die eine Datensynchronisierung zwischen mehreren Datastores erfordern. SnapMirror Active Sync unterstützt außerdem Raw Device Mapping (RDMs) und über das Gastsystem verbundenen Storage mit iSCSI-Initiatoren in-Guest. Weitere Informationen zu Konsistenzgruppen finden Sie unter<block ref="8bba9b5b97dba092834d7a48a07f102d" category="inline-link-rx"></block>.</block>
  <block id="f9ac6f5e1bf01f1df914b714d5e6cb02" category="paragraph">Es gibt einen Unterschied beim Management einer vMSC Konfiguration mit aktiver SnapMirror Synchronisierung im Vergleich zu einer MetroCluster. Zunächst handelt es sich um eine reine SAN-Konfiguration. Es können keine NFS-Datenspeicher durch SnapMirror Active Sync geschützt werden. Als zweites müssen Sie Ihren ESXi-Hosts beide Kopien der LUNs zuordnen, damit sie auf die replizierten Datastores in beiden Ausfall-Domains zugreifen können.</block>
  <block id="c972b11c861e42787c05a2ee24828e3a" category="section-title">VMware vSphere HA</block>
  <block id="57166da54d8751e1ed084f00b3f169e3" category="section-title">Erstellen Sie einen vSphere HA-Cluster</block>
  <block id="829006223e7bdcfd478d2261f1d5ac40" category="inline-link">Wie erstellen und konfigurieren Sie Cluster im vSphere Client auf docs.vmware.com</block>
  <block id="2bbae4763835d2e2dd123c1b4b0ae73d" category="paragraph">Die Erstellung eines vSphere HA-Clusters ist ein mehrstufiger Prozess, der in vollständig dokumentiert ist<block ref="dd51ccd863dd3c9fbca770681f8e68a9" category="inline-link-rx"></block>. Kurz gesagt: Sie müssen zuerst einen leeren Cluster erstellen, dann mit vCenter Hosts hinzufügen und vSphere HA und andere Einstellungen des Clusters angeben.</block>
  <block id="d50e63b9f70efedde8dc093be8ec12b2" category="inline-link">Empfohlene Practices für VMware vSphere Metro Storage-Cluster</block>
  <block id="950954623b197b80e3ebb95b0adcc3de" category="paragraph">Führen Sie zum Konfigurieren eines HA-Clusters die folgenden Schritte aus:</block>
  <block id="1073102bd8c6cbd17340f2a19043583f" category="list-text">Stellen Sie eine Verbindung zur vCenter-Benutzeroberfläche her.</block>
  <block id="ebf69ea3a3e34739bde13d589e85ef10" category="list-text">Navigieren Sie unter Hosts und Cluster zum Rechenzentrum, in dem Sie Ihr HA-Cluster erstellen möchten.</block>
  <block id="d8fd85bbc0cd4746e5d80e291696b86c" category="list-text">Klicken Sie mit der rechten Maustaste auf das Datacenter-Objekt, und wählen Sie Neuer Cluster aus. Unter Grundlagen stellen Sie sicher, dass Sie vSphere DRS und vSphere HA aktiviert haben. Schließen Sie den Assistenten ab.</block>
  <block id="f2c634ca704e26f96be1723db6dda32a" category="image-alt">Ein Screenshot einer Computerbeschreibung wird automatisch generiert</block>
  <block id="f4f59bf1e324854573172ca2b5070d45" category="list-text">Wählen Sie den Cluster aus, und wechseln Sie zur Registerkarte Konfigurieren. Wählen Sie vSphere HA aus, und klicken Sie auf Bearbeiten.</block>
  <block id="329c69e52a7de6215aa4dbfa19940f2d" category="list-text">Wählen Sie unter Host-Überwachung die Option Host-Überwachung aktivieren aus.</block>
  <block id="9fc231ac455a53f2fabc59f29ab537ad" category="list-text">Wählen Sie auf der Registerkarte „Fehler und Antworten“ unter „VM-Überwachung“ die Option „nur VM-Überwachung“ oder „VM- und Anwendungsüberwachung“ aus.</block>
  <block id="58aea8c81a7e8b8160584574af309fbb" category="list-text">Legen Sie unter Admission Control die Option HA-Eintrittskontrolle auf Cluster-Ressourcenreserve fest. Verwenden Sie 50 % CPU/MEM.</block>
  <block id="d94a9f01bee7dcc2643864bf92ab4e59" category="list-text">Klicken Sie auf „OK“.</block>
  <block id="3f059eee365aa5c64c0be1d6dba81d69" category="list-text">Wählen Sie DRS und klicken Sie auf BEARBEITEN.</block>
  <block id="7b5f9be03fd1421a9da56c2c7677ad69" category="list-text">Setzen Sie den Automatisierungsgrad auf manuell, sofern dies nicht von Ihren Anwendungen erforderlich ist.</block>
  <block id="a8ee10582e28e3623629963a88ff5ec9" category="image-alt">Vmsc 3 5</block>
  <block id="256164bb48582e5267e408f2e67e1939" category="inline-link">docs.vmware.com</block>
  <block id="91fb89f03b846483f7108d3f1bfb0156" category="list-text">Aktivieren Sie den Schutz von VM-Komponenten, siehe<block ref="fc88c93ad20ad804a211f4e6fea63e56" category="inline-link-rx"></block>.</block>
  <block id="918c2d8e02dbd2c1f4cc3a0062690387" category="list-text">Die folgenden zusätzlichen vSphere HA-Einstellungen werden für vMSC mit MCC empfohlen:</block>
  <block id="d64ed3e9c10229648e069f56e32f4c8e" category="cell">Antwort</block>
  <block id="f3c1d0e4118d5d9501e1a7aeed19d224" category="cell">Host-Ausfall</block>
  <block id="45d583cd5692c76d45e96536edce2a0e" category="cell">Starten Sie die VMs neu</block>
  <block id="1c3a7ae924920b573baede481becd22f" category="cell">Host-Isolierung</block>
  <block id="2e011e74abc75a2823c627b7ee9e22a7" category="cell">Datenspeicher mit Permanent Device Loss (PDL)</block>
  <block id="49476a1ad365d5f8deb36464de7fe33b" category="cell">Schalten Sie die VMs aus und starten Sie sie neu</block>
  <block id="e7bbd35db169028c75e7c8ae655bf6ae" category="cell">Datastore mit All Paths Down (APD)</block>
  <block id="e7480b00a5e3efd904a08ca88804de45" category="cell">Der Gast ist nicht herzschlagend</block>
  <block id="e8896c595f3effde37ef29d7c6233703" category="cell">Setzt die VMs zurück</block>
  <block id="59695df3d45a3a21b8ff0bf57fda6a2b" category="cell">Richtlinie für den Neustart der VM</block>
  <block id="a150f56b87fd99bf9bf019f8447ba68b" category="cell">Bestimmt durch die Bedeutung der VM</block>
  <block id="370a480458a8c63a838940f1241e14ee" category="cell">Antwort für Host-Isolation</block>
  <block id="11f30f9be08cd4f9e500290222ddc552" category="cell">Fahren Sie die VMs herunter, und starten Sie sie neu</block>
  <block id="fb53fafa45c0de9bd41d368c7455c9f1" category="cell">Antwort für Datastore mit PDL</block>
  <block id="d7c3b9f5568eb60f08076b29490afcaf" category="cell">Antwort für Datenspeicher mit APD</block>
  <block id="423b2d98eae113ae3391aa0b12f11935" category="cell">VMs ausschalten und neu starten (konservativ)</block>
  <block id="132bbe6dbceb01227dba5ab04db7202b" category="cell">Verzögerung bei VM-Failover für APD</block>
  <block id="8d15ed7d27d83ed6229a66b1f44b7696" category="cell">3 Minuten</block>
  <block id="8c7b0644547f0ee3fe537e8ba441566d" category="cell">Antwort für APD-Wiederherstellung mit APD-Timeout</block>
  <block id="4dea08318b2824f845e9b889a6e17778" category="cell">Sensitivität für VM-Monitoring</block>
  <block id="f535a28adc173e28610c129d0dc578ae" category="cell">Voreinstellung hoch</block>
  <block id="83164803a765d1ea1f10d50dfdd26130" category="section-title">Konfigurieren Sie Datastores für Heartbeating</block>
  <block id="22c81b09ec3086b2dce8f68867a221e0" category="paragraph">VSphere HA verwendet Datastores, um Hosts und virtuelle Maschinen zu überwachen, wenn das Managementnetzwerk ausgefallen ist. Sie können konfigurieren, wie vCenter Heartbeat-Datenspeicher auswählt. Gehen Sie wie folgt vor, um Datastores für Heartbeating zu konfigurieren:</block>
  <block id="64809e5386c9dc53f894f772f27d1b44" category="list-text">Wählen Sie im Abschnitt Datastore Heartbeating die Option Datastores aus der angegebenen Liste verwenden aus und ergänzen Sie bei Bedarf automatisch.</block>
  <block id="fc66ba7317ceb4220dc88aa267db085d" category="list-text">Wählen Sie die Datastores aus, die vCenter von beiden Standorten verwenden soll, und drücken Sie OK.</block>
  <block id="1d2b14769d080b9214d9e407dfc92a64" category="section-title">Konfigurieren Sie Die Erweiterten Optionen</block>
  <block id="a22b9827e1a78c5fb9c939a78c116a05" category="paragraph">*Host-Fehlererkennung*</block>
  <block id="a9316d69e5abcfaffc56fc8f5e645412" category="paragraph">Isolierungsereignisse treten auf, wenn Hosts innerhalb eines HA-Clusters die Verbindung zum Netzwerk oder zu anderen Hosts im Cluster verlieren. Standardmäßig verwendet vSphere HA das Standard-Gateway für sein Managementnetzwerk als Standard-Isolationsadresse. Sie können jedoch zusätzliche Isolationsadressen für den Host angeben, um zu bestimmen, ob eine Isolationsantwort ausgelöst werden soll. Fügen Sie zwei isolierte IPs hinzu, die Ping-Daten senden können, eine pro Standort. Verwenden Sie nicht die Gateway-IP. Die erweiterte vSphere HA-Einstellung ist das.isolationaddress. Dazu können Sie ONTAP- oder Mediator-IP-Adressen verwenden.</block>
  <block id="102d7879ceca393c309ef492c958e19d" category="inline-link">core.vmware.com</block>
  <block id="27e2292ff6db48ed2e6d50b256eae43e" category="paragraph">Siehe<block ref="43057df4f3a6b5f2fffe8b7c5a490646" category="inline-link-rx"></block> Weitere Informationen__.__</block>
  <block id="3f44bb4a3b9420d495e400b9afbda2c7" category="paragraph">Das Hinzufügen einer erweiterten Einstellung namens das.heartbeatDsPerHost kann die Anzahl der Heartbeat-Datenspeicher erhöhen. Verwenden Sie vier Heartbeat Datastores (HB DSS) – zwei pro Standort. Verwenden Sie die Option „aus Liste auswählen, aber Kompliment“. Dies wird benötigt, da Sie bei Ausfall eines Standorts immer noch zwei HB DSS benötigen. Diese müssen jedoch nicht durch MCC oder SnapMirror Active Sync geschützt werden.</block>
  <block id="eaa1dfe3982e06655ce64e240c139b81" category="paragraph">VMware DRS Affinity zu NetApp MetroCluster</block>
  <block id="e32221c52eb3b0bbeb3ca79aad6c90f6" category="paragraph">In diesem Abschnitt erstellen wir DRS Gruppen für VMs und Hosts für jeden Standort\Cluster in der MetroCluster Umgebung. Anschließend konfigurieren wir VM\Host-Regeln, um die VM Host-Affinität mit lokalen Storage-Ressourcen auszurichten. Beispielsweise gehören Standort A VMs zur VM-Gruppe sitea_vms und Standort A Hosts zur Host-Gruppe sitea_Hosts. Als nächstes geben wir in VM\Host Rules an, dass sitea_vms auf Hosts in sitea_Hosts ausgeführt werden sollen.</block>
  <block id="5bfa0d712f66aef47f8a2ea582bc0b72" category="list-text">NetApp empfiehlt dringend die Spezifikation *sollte auf Hosts in Gruppe* laufen anstatt der Spezifikation *muss auf Hosts in Gruppe* ausgeführt werden. Im Falle eines Host-Ausfalls von Standort A müssen die VMs von Standort A über vSphere HA auf Hosts an Standort B neu gestartet werden. Bei der letzteren Spezifikation ist jedoch nicht möglich, dass HA die VMs auf Standort B neu starten, da es die harte Regel ist. Die frühere Spezifikation ist eine weiche Regel und wird im Falle von HA verletzt, wodurch die Verfügbarkeit anstatt die Leistung ermöglicht wird.</block>
  <block id="cc6a363013bd2a8c1e05304f1c6abd79" category="inline-link">VSphere Monitoring und Performance</block>
  <block id="124de7d2bc72d75be9e481c1cc56ada2" category="section-title">DRS-Host-Gruppen erstellen</block>
  <block id="0759e9476d3bf1868f88356025adafb2" category="paragraph">So erstellen Sie DRS Host-Gruppen speziell für Standort A und Standort B:</block>
  <block id="a69dff3dd4b284f60243dfd62fa4e57f" category="list-text">Klicken Sie im vSphere-Webclient mit der rechten Maustaste auf den Cluster in der Bestandsaufnahme, und wählen Sie Einstellungen aus.</block>
  <block id="a130b4af254ead99e9ea83044e54ea1d" category="list-text">Klicken Sie auf VM\Host Groups.</block>
  <block id="1bb250fbf1946dd1bd7ad228032f8803" category="list-text">Klicken Sie Auf Hinzufügen.</block>
  <block id="62c4d1538f957af2951f2c93ee191d0b" category="list-text">Geben Sie den Namen der Gruppe ein (z. B. sitea_Hosts).</block>
  <block id="df22605898fc890edbcb2a126aa202ce" category="list-text">Wählen Sie im Menü Typ die Option Host-Gruppe aus.</block>
  <block id="57c0028584902f6cee210dcc13bc9745" category="list-text">Klicken Sie auf Hinzufügen, wählen Sie die gewünschten Hosts von Standort A aus, und klicken Sie auf OK.</block>
  <block id="583e6d8c3b1006b0561d4b360e58994f" category="list-text">Wiederholen Sie diese Schritte, um eine weitere Host-Gruppe für Standort B hinzuzufügen</block>
  <block id="18d0ad44a1e563aaf9f5871e32535a6c" category="list-text">Klicken Sie auf OK.</block>
  <block id="154964ee7fc6d3261ec55811b0ff9972" category="section-title">DRS VM-Gruppen erstellen</block>
  <block id="40c1eaba5f178f243c57b595b6af4d5e" category="paragraph">So erstellen Sie DRS VM-Gruppen speziell für Standort A und Standort B:</block>
  <block id="9a209a741c26968fc13ed8192523df15" category="list-text">Geben Sie den Namen der Gruppe ein (z. B. sitea_vms).</block>
  <block id="f947c7c9f173289b1c2565b3297095f0" category="list-text">Wählen Sie im Menü Typ die Option VM-Gruppe aus.</block>
  <block id="40a2a21a084c8d16395113cb79a19ebb" category="list-text">Klicken Sie auf Hinzufügen, wählen Sie die gewünschten VMs von Standort A aus, und klicken Sie auf OK.</block>
  <block id="e2e7af699096077fe178ebf021fb2273" category="section-title">Erstellen Sie VM-Hostregeln</block>
  <block id="0a6075f3d3fdbe7c133268face0f5c82" category="paragraph">Gehen Sie wie folgt vor, um DRS-Affinitätsregeln für Standort A und Standort B zu erstellen:</block>
  <block id="df4f9bcca0ceb7a67ba25d886a18743b" category="list-text">Klicken Sie auf VM\Hostregeln.</block>
  <block id="876a25c102b5c301814921d1a9c0e625" category="list-text">Geben Sie den Namen der Regel ein (z. B. sitea_Affinity).</block>
  <block id="d4b5f57f4e2bd5c357ad9b1fbdc2cf8e" category="list-text">Überprüfen Sie, ob die Option Regel aktivieren aktiviert ist.</block>
  <block id="d1169f525bd934df7f1a7e3bcf401997" category="list-text">Wählen Sie im Menü Typ die Option Virtuelle Maschinen zu Hosts aus.</block>
  <block id="77328c16c697ce1d6c036c3f950e7542" category="list-text">Wählen Sie die VM-Gruppe aus (z.B. sitea_vms).</block>
  <block id="b36943ce975d4c5c6e9481a4ca396270" category="list-text">Wählen Sie die Host-Gruppe aus (z. B. sitea_Hosts).</block>
  <block id="32fbcb32cfae9a0656233943c28f7c94" category="list-text">Wiederholen Sie diese Schritte, um eine weitere VM\Host-Regel für Standort B hinzuzufügen</block>
  <block id="ff0ded925b170243e5154c9a687bb925" category="section-title">VMware vSphere Storage DRS für NetApp MetroCluster</block>
  <block id="b0cb5ad168b84040eff8132c70e41fdb" category="section-title">Datastore-Cluster Erstellen</block>
  <block id="488f70e76f3c9ddf155b693e31c5a1e1" category="paragraph">Führen Sie die folgenden Schritte aus, um ein Datastore-Cluster für jeden Standort zu konfigurieren:</block>
  <block id="1dfbc2b980c0b56a5d9889c6d9a25460" category="list-text">Navigieren Sie mithilfe des vSphere-Webclients zum Rechenzentrum, in dem sich der HA-Cluster unter Speicher befindet.</block>
  <block id="dfc332b252ad3de167b6a336d7e70d46" category="list-text">Klicken Sie mit der rechten Maustaste auf das Datacenter-Objekt, und wählen Sie Storage &gt; New Datastore Cluster aus.</block>
  <block id="7ce7f4ad7022187801efdf33d1fab92d" category="list-text">Wählen Sie die Option Storage DRS aktivieren aus, und klicken Sie auf Weiter.</block>
  <block id="6196801f8f77bab7132531adbb268512" category="list-text">Stellen Sie alle Optionen auf Keine Automatisierung (manueller Modus) ein, und klicken Sie auf Weiter.</block>
  <block id="cfcdd0a8a765af314fb460ad445fb375" category="list-text">NetApp empfiehlt, Storage DRS im manuellen Modus zu konfigurieren, sodass der Administrator entscheiden und kontrollieren kann, wann Migrationen stattfinden.</block>
  <block id="51a3bc726f294f3376fcaa12ccdf8b3b" category="list-text">Vergewissern Sie sich, dass das Kontrollkästchen E/A-Metrik für SDRS-Empfehlungen aktivieren aktiviert ist. Die metrischen Einstellungen können mit Standardwerten belassen werden.</block>
  <block id="829b62dbd6a4923b66591b2bff38ed5b" category="list-text">Wählen Sie das HA-Cluster aus, und klicken Sie auf Weiter.</block>
  <block id="30fe6cfa0a4c5fb23fccfe149cefc006" category="list-text">Wählen Sie die Datastores aus, die zu Standort A gehören, und klicken Sie auf Weiter.</block>
  <block id="4ea461b22638db4c94aa076d62a2065f" category="list-text">Überprüfen Sie die Optionen, und klicken Sie auf Fertig stellen.</block>
  <block id="7b79e97caff76847cf350f1197c49953" category="list-text">Wiederholen Sie diese Schritte, um das Datastore-Cluster an Standort B zu erstellen und sicherzustellen, dass nur Datastores von Standort B ausgewählt sind.</block>
  <block id="d8f83b09bd6b864f22076c109cfaae66" category="section-title">VCenter Server-Verfügbarkeit</block>
  <block id="605b8c1efcf2de9164e5d5425c67bdcd" category="paragraph">Ihre vCenter Server Appliances (VCSAs) sollten durch vCenter HA geschützt werden. Mit vCenter HA können Sie zwei VCSAs in einem aktiv/Passiv-HA-Paar implementieren. Einer in jeder Ausfall-Domäne. Weitere Informationen zu vCenter HA finden Sie im<block ref="173ca8434bdf39dd0a60efe576d066cd" category="inline-link-rx"></block>.</block>
  <block id="70901a02a148fd2dfc1ff5736f4f5336" category="list-text">Separater Storage-Netzwerk-Traffic aus anderen Netzwerken. Ein separates Netzwerk kann mithilfe eines dedizierten VLANs oder separater Switches für Storage eingerichtet werden. Falls im Storage-Netzwerk physische Pfade wie Uplinks geteilt werden, sind eventuell QoS oder zusätzliche Uplink-Ports erforderlich, um eine ausreichende Bandbreite sicherzustellen. Stellen Sie keine direkte Verbindung zwischen Hosts und Storage her. Verwenden Sie Switches, um redundante Pfade zu verwenden und VMware HA ohne Eingriff von Microsoft HA zu arbeiten. Siehe <block ref="2d54da766c3f840b00eab5923273fca5" category="inline-link-macro-rx"></block> Finden Sie weitere Informationen.</block>
  <block id="59cfc5aaa0678bcb883492b4397c71b8" category="cell">Ja (ONTAP 9.14.1)</block>
  <block id="a58c81d13cac4e9b37bba2bd7fc3a93d" category="paragraph">Der branchenführende vSphere-Hypervisor von VMware kann als erweiterbares Cluster, auch als vSphere Metro Storage-Cluster (vMSC) bezeichnet, implementiert werden.</block>
  <block id="e3224e391e08365c1593c07b9fe99cfa" category="section-title">Lösungen für kontinuierliche Verfügbarkeit für vSphere Umgebungen</block>
  <block id="ffa76ec5527e36c51f49fe091541cca6" category="paragraph">NetApp MetroCluster nutzt die NetApp HA-Funktion (Controller Failover oder CFO) zum Schutz vor Controller-Ausfällen. Außerdem beinhaltet es lokale SyncMirror Technologie, Cluster Failover bei Disaster (Controller Failover on Demand oder CFOD), Hardware-Redundanz und geografische Trennung, um ein hohes Maß an Verfügbarkeit zu erreichen. SyncMirror spiegelt Daten synchron auf die beiden Hälften der MetroCluster Konfiguration und schreibt sie in zwei Plexe: Der lokale Plex (auf dem lokalen Shelf), der aktiv Daten bereitstellt, und der Remote-Plex (auf dem Remote-Shelf), der normalerweise keine Daten bereitstellt. Für alle MetroCluster Komponenten wie Controller, Storage, Kabel, Switches (zur Verwendung mit Fabric MetroCluster) und Adapter besteht Hardwareredundanz.</block>
  <block id="d3e911c902a6b066f9c31d471d16fb21" category="paragraph">Um einen VMware HA/DRS Cluster über zwei Standorte zu erstellen, werden ESXi-Hosts von einer vCenter Server Appliance (VCSA) verwendet und gemanagt. Die vSphere-Management-, vMotion®- und Virtual Machine-Netzwerke sind über ein redundantes Netzwerk zwischen den beiden Standorten verbunden. Der vCenter Server, der den HA/DRS Cluster verwaltet, kann eine Verbindung zu den ESXi-Hosts an beiden Standorten herstellen und sollte über vCenter HA konfiguriert werden.</block>
  <block id="da76be2bc20d4e043d9a2019b817214f" category="inline-link">Wie erstellen und konfigurieren Sie Cluster im vSphere Client</block>
  <block id="e02f3e9258a23204655bd512ec53911d" category="paragraph">Siehe<block ref="c06ae090600e7aa086edfa28514435e7" category="inline-link-rx"></block> Um vCenter HA zu konfigurieren.</block>
  <block id="bdd88c75f085534f6e2cdc7726cf1dd6" category="paragraph">Weitere Informationen finden Sie unter<block ref="3a8301f60ad04eb873ae8dea15ee0495" category="inline-link-rx"></block>.</block>
  <block id="38910d3dab2318849fd5f1a1dfbe0152" category="inline-link">VMware Storage Compatibility Guide</block>
  <block id="eab65c9bc6c74eff9aee0d7f0c44276d" category="paragraph">VSphere Metro Storage Cluster (vMSC) ist eine zertifizierte Konfiguration, die Virtual Machines (VMs) und Container vor Ausfällen schützt. Dies wird erreicht, indem erweiterte Storage-Konzepte zusammen mit Clustern von ESXi Hosts genutzt werden, die auf verschiedene Ausfall-Domains verteilt sind, zum Beispiel Racks, Gebäude, Campus oder sogar Städte. Die NetApp MetroCluster und die Active Sync Storage-Technologien von SnapMirror sorgen für eine Sicherung von RPO=0 bzw. nahezu RPO=0 für die Host-Cluster. Mit der vMSC-Konfiguration ist sichergestellt, dass die Daten immer verfügbar sind, selbst wenn ein vollständiger physischer oder logischer „Standort“ ausfällt. Ein Speichergerät, das Teil der vMSC-Konfiguration ist, muss nach einer erfolgreichen vMSC-Zertifizierung zertifiziert werden. Alle unterstützten Speichergeräte finden Sie im<block ref="293ceee3268d7991d7fe245e02c1c5c9" category="inline-link-rx"></block>.</block>
  <block id="838b66b875df1b432ac420a98cba3db6" category="paragraph">Weitere Informationen zu den Designrichtlinien für vSphere Metro Storage Cluster finden Sie in der folgenden Dokumentation:</block>
  <block id="07aaf2aebfd07b8763284018d1ebbf57" category="inline-link">Unterstützung von VMware vSphere für NetApp MetroCluster</block>
  <block id="a4a5fd5b00daf98bd381d80257378acd" category="list-text"><block ref="a4a5fd5b00daf98bd381d80257378acd" category="inline-link-rx"></block></block>
  <block id="c3dc11e39d996a00b095022f4e56583d" category="inline-link">Unterstützung von VMware vSphere mit NetApp SnapMirror Business Continuity</block>
  <block id="8f675397d9c0ef95aa8b3f0ada9d601a" category="list-text"><block ref="f53214cfd764fa0267c92323be0d7337" category="inline-link-rx"></block> (Jetzt bekannt als SnapMirror Active Sync)</block>
  <block id="ee0a206d2287c89b2a7b780e07929b81" category="paragraph">Abhängig von den Überlegungen zur Latenz kann NetApp MetroCluster in zwei unterschiedlichen Konfigurationen für die Verwendung mit vSphere implementiert werden:</block>
  <block id="b553530def78406c0977f70d5e90b9e8" category="list-text">Stretch-MetroCluster</block>
  <block id="2d90abe978ed8084d304295ad0a8c8fd" category="list-text">Fabric MetroCluster</block>
  <block id="b03023e82bda6a71adc49c4834e3b6ab" category="paragraph">Im Folgenden finden Sie ein High-Level-Topologiediagramm von Stretch MetroCluster.</block>
  <block id="b0c20dc8226bb49f35f0f68f7ef77bba" category="image-alt">VMSC-Diagramm mit MCC</block>
  <block id="119be031ef4485d20c7683d291d0e9f6" category="inline-link">MetroCluster-Dokumentation</block>
  <block id="ad9122c6d75ebaa024669fde1c0bdfb8" category="paragraph">Siehe<block ref="d8a206747dbcec13122724b2aee18957" category="inline-link-rx"></block> Finden Sie spezifische Design- und Implementierungsinformationen für MetroCluster.</block>
  <block id="b47354fcd7d17ab4fbb3283f71677efe" category="paragraph">SnapMirror Active Sync kann darüber hinaus auf zwei verschiedene Arten implementiert werden.</block>
  <block id="314084a577b998d8089dff72c98c58b0" category="list-text">Asymmetrisch</block>
  <block id="40e2e641bad11918585c3dd4f76c2716" category="list-text">Symmetrisch (private Vorschau in ONTAP 9.14.1)</block>
  <block id="b9ca5ba97cf75d3dafb51f78e073770a" category="inline-link">NetApp Dokumente</block>
  <block id="1c71125ba0668792e46d93c17dc6c428" category="paragraph">Siehe<block ref="9ace5d3ebe2fe57037322de54ac9a869" category="inline-link-rx"></block> Für spezifische Design- und Implementierungsinformationen zu SnapMirror Active Sync</block>
  <block id="bca8b52c503406abc3e9e50c5352d0a2" category="paragraph">Dieses Dokument enthält eine Einführung in die ONTAP Lösung für VMware Site Recovery Manager (SRM), die branchenführende VMware Software für Disaster Recovery (DR), sowie in die neuesten Produktinformationen und Best Practices zur Optimierung der Bereitstellung, Risikominderung und Vereinfachung des fortlaufenden Managements.</block>
  <block id="4f46fffb0ac7017fd2b42c10d6aa03fc" category="paragraph">Zu den größten Stärken von ONTAP für vSphere zählt, dass Sie Ihre VMs sichern und schnell wiederherstellen können und dass Sie diese Funktion mit dem SnapCenter Plug-in für VMware vSphere einfach in vCenter managen können.</block>
  <block id="b69cd5a82aaf0c524a71d46ec6fdd29c" category="cell">Für das NFS v4.1 Session-Trunking ist ONTAP 9.14.1 und höher erforderlich</block>
  <block id="80303385ad0f7147caa1af6d2162852b" category="paragraph">Das Plug-in ist auch die Managementoberfläche für viele Funktionen von VASA Provider für ONTAP und unterstützt das richtlinienbasierte Storage-Management mit VVols. Nach der Registrierung von ONTAP Tools für VMware vSphere erstellen Sie damit Storage-Funktionsprofile, ordnen diesen Storage zu und stellen im Laufe der Zeit die Datastore-Compliance mit den Profilen sicher. Vasa Provider verfügt auch über eine Schnittstelle zum Erstellen und Managen von vVol Datastores.</block>
  <block id="d9c9eb44a58ee1ce90daa68527aa8930" category="summary">Ausfallszenarien für vMSC mit MCC</block>
  <block id="e7c7574b0826c9074f480de76c889ba8" category="paragraph">In den folgenden Abschnitten werden die erwarteten Ergebnisse verschiedener Ausfallszenarien mit vMSC- und NetApp MetroCluster-Systemen beschrieben.</block>
  <block id="c8a2eebb480fef69e6ed917ea7f7a9d3" category="section-title">Ausfall Eines Einzelnen Storage-Pfads</block>
  <block id="b99e8e694b032f593bbd01f6205b4a98" category="paragraph">Wenn in diesem Szenario Komponenten wie der HBA-Port, der Netzwerkport, der Front-End-Datenschalterport oder ein FC- oder Ethernet-Kabel ausfallen, wird dieser bestimmte Pfad zum Speichergerät vom ESXi-Host als „tot“ markiert. Wenn mehrere Pfade durch Ausfallsicherheit am HBA/Netzwerk/Switch Port für das Storage-Gerät konfiguriert sind, führt ESXi idealerweise eine Pfadumschaltung durch. Während dieser Zeit bleiben Virtual Machines ohne Beeinträchtigungen verfügbar, da für die Storage-Verfügbarkeit mehrere Pfade zum Storage-Gerät bereitgestellt werden.</block>
  <block id="80fc15dd1deb985234440745e8a78d4c" category="paragraph">In Umgebungen mit NFS/iSCSI-Volumes empfiehlt NetApp, mindestens zwei Netzwerk-Uplinks für den NFS-VMkernel-Port im Standard-vSwitch und dieselbe Port-Gruppe zu konfigurieren, bei der die NFS-VMkernel-Schnittstelle für den verteilten vSwitch zugeordnet ist. NIC-Teaming kann entweder aktiv-aktiv oder aktiv-Standby konfiguriert werden.</block>
  <block id="0c8694e8ef7f8935edbfac605753bc95" category="paragraph">Außerdem muss bei iSCSI LUNs Multipathing konfiguriert werden, indem die VMkernel-Schnittstellen an die iSCSI-Netzwerkadapter gebunden werden. Weitere Informationen finden Sie in der vSphere-Speicherdokumentation.</block>
  <block id="0fc808fdb968eb089fbc5124c0a8e15a" category="paragraph">In Umgebungen mit Fibre-Channel-LUNs empfiehlt NetApp die Verwendung von mindestens zwei HBAs, wodurch Ausfallsicherheit auf HBA-/Port-Ebene garantiert wird. NetApp empfiehlt für das Zoning von einem einzelnen Initiator außerdem als Best Practice zum Konfigurieren des Zoning.</block>
  <block id="238c3d1c7bf8b5b42237392383a4c7ab" category="paragraph">Sie sollten mithilfe der Virtual Storage Console (VSC) Multipathing-Richtlinien festlegen, da sie Richtlinien für alle neuen und vorhandenen NetApp Storage-Geräte definiert.</block>
  <block id="217811709dfb096c78e73934de01271c" category="section-title">Ausfall eines einzelnen ESXi-Hosts</block>
  <block id="93bc89a0478478dd84d8159a18f6c1a2" category="image-alt">Ausfall eines einzelnen Hosts</block>
  <block id="15023af3b440b4004f509e961bfe1e30" category="paragraph">Wenn in diesem Szenario ein ESXi-Host ausfällt, erkennt der Master-Node im VMware HA-Cluster den Host-Ausfall, da er keine Netzwerk-Heartbeats mehr empfängt. Um festzustellen, ob der Host wirklich ausgefallen ist oder nur eine Netzwerkpartition ist, überwacht der Master-Knoten die Datastore-Heartbeats und führt, falls sie nicht vorhanden sind, eine abschließende Prüfung durch, indem er die Management-IP-Adressen des ausgefallenen Hosts anpingt. Wenn alle Prüfungen negativ sind, erklärt der Master-Node diesen Host als ausgefallenen Host, und alle virtuellen Maschinen, die auf diesem ausgefallenen Host ausgeführt wurden, werden auf dem noch verbleibenden Host im Cluster neu gestartet.</block>
  <block id="9a11158d52f71012ae8722b09bf8bf29" category="paragraph">Wenn DRS VM und Host Affinity Regeln konfiguriert wurden (VMs in VM Gruppe sitea_vms sollten Hosts in Host Gruppe sitea_Hosts laufen lassen), dann prüft der HA Master zunächst auf verfügbare Ressourcen an Standort A. Wenn an Standort A keine verfügbaren Hosts vorhanden sind, versucht der Master, die VMs auf den Hosts an Standort B neu zu starten</block>
  <block id="7922c92ff860eb0508dc8cc8aae3ffdd" category="paragraph">Es ist möglich, dass die virtuellen Maschinen auf den ESXi-Hosts am anderen Standort gestartet werden, wenn am lokalen Standort eine Ressourcenbeschränkung vorhanden ist. Die definierten Regeln für die DRS-VM und Host-Affinität werden jedoch korrigiert, wenn Regeln verletzt werden, indem die virtuellen Maschinen zurück zu den noch verbleibenden ESXi-Hosts am lokalen Standort migriert werden. In Fällen, in denen DRS auf manuell festgelegt ist, empfiehlt NetApp, DRS zu aktivieren und die Empfehlungen anzuwenden, um die Platzierung der Virtual Machines zu korrigieren.</block>
  <block id="387b5a49326be3a47e081682e75e19b4" category="paragraph">Es gibt keine Änderung im MetroCluster Verhalten in diesem Szenario und alle Datenspeicher bleiben von ihren jeweiligen Seiten intakt.</block>
  <block id="a11ffa7d20dab792fe5aa9c27017493e" category="section-title">ESXi-Host-Isolierung</block>
  <block id="17e01652089acd413e26ea89cf98a2b8" category="image-alt">ESXi-Host-Isolierung</block>
  <block id="8a4b746274e9d7129602d54434624806" category="paragraph">Wenn in diesem Szenario das Managementnetzwerk des ESXi-Hosts ausgefallen ist, erhält der Master-Node im HA-Cluster keine Heartbeats, wodurch dieser Host im Netzwerk isoliert wird. Um festzustellen, ob es ausgefallen ist oder nur isoliert ist, beginnt der Master-Node mit der Überwachung des Datastore-Herzschlags. Wenn er vorhanden ist, wird der Host vom Master-Knoten isoliert deklariert. Je nach konfigurierter Isolationsantwort kann der Host sich entscheiden, die virtuellen Maschinen auszuschalten, herunterzufahren oder die virtuellen Maschinen sogar eingeschaltet zu lassen. Das Standardintervall für die Isolationsantwort beträgt 30 Sekunden.</block>
  <block id="a070f7923919fed39772c5d3f91bf482" category="section-title">Platten-Shelf-Fehler</block>
  <block id="e8acf0e4f25cdd641c9a047482d56ba5" category="paragraph">In diesem Szenario kommt es zu einem Ausfall von mehr als zwei Festplatten oder eines gesamten Shelf. Daten werden vom verbleibenden Plex ohne Unterbrechung der Datenservices bereitgestellt. Der Festplattenausfall kann sich auf einen lokalen oder einen Remote-Plex auswirken. Die Aggregate werden als degradierter Modus angezeigt, da nur ein Plex aktiv ist. Sobald die ausgefallenen Festplatten ersetzt wurden, werden die betroffenen Aggregate automatisch neu synchronisiert, um die Daten neu aufzubauen. Nach der Neusynchronisierung kehren die Aggregate automatisch in den normalen gespiegelten Modus zurück. Wenn mehr als zwei Laufwerke innerhalb einer einzelnen RAID-Gruppe ausgefallen sind, muss der Plex neu erstellt werden.</block>
  <block id="4f90d34a41c7ebc83024a5e3ca35ff3a" category="image-alt">Ausfall eines einzelnen Festplatten-Shelfs:</block>
  <block id="1c872dcd4bcebb1d7567f383901dfc0d" category="section-title">Ausfall Eines Einzelnen Storage Controllers</block>
  <block id="68badb604dbd29653979fdc7083cbffe" category="paragraph">In diesem Szenario fällt einer der beiden Storage Controller an einem Standort aus. Da an jedem Standort ein HA-Paar vorhanden ist, wird bei einem Ausfall eines Node automatisch ein Failover auf den anderen Node ausgelöst. Wenn beispielsweise Node A1 ausfällt, werden dessen Storage und Workloads automatisch auf Node A2 übertragen. Virtuelle Maschinen sind nicht betroffen, da alle Plexe verfügbar bleiben. Die Knoten des zweiten Standorts (B1 und B2) sind davon nicht betroffen. Außerdem führt vSphere HA keine Aktion durch, da der Master-Node im Cluster weiterhin Netzwerk-Heartbeats empfängt.</block>
  <block id="0ac25682ea568511724f2e0819127d89" category="image-alt">Ausfall eines einzelnen Nodes</block>
  <block id="e7b4c0523308b3fd46189b6ab4867fe5" category="paragraph">Wenn der Failover Teil eines rollierenden Disaster ist (Node A1 führt ein Failover auf A2 durch) und ein nachfolgender Ausfall von A2 oder ein vollständiger Ausfall von Standort A auftritt, kann an Standort B das Umschalten nach einem Ausfall stattfinden</block>
  <block id="7b590b52d9f3d4d40cda67758ff92ba1" category="section-title">Verbindungsfehler Zwischen Switches</block>
  <block id="9df08f82abe35d87c6f7c96b18dab755" category="section-title">Verbindungsfehler zwischen Switches im Managementnetzwerk</block>
  <block id="ed0711cbc7a050952e0e370bab079bff" category="image-alt">Verbindungsfehler zwischen Switches im Managementnetzwerk</block>
  <block id="a32c6c77bb8d407cf5825aec175bab4b" category="paragraph">In diesem Szenario können die ESXi-Hosts an Standort A nicht mit ESXi-Hosts an Standort B kommunizieren, wenn die ISL-Links am Front-End-Hostverwaltungsnetzwerk fehlschlagen Dies führt zu einer Netzwerkpartition, da ESXi-Hosts an einem bestimmten Standort die Netzwerk-Heartbeats nicht an den Master-Node im HA-Cluster senden können. Daher gibt es aufgrund der Partition zwei Netzwerksegmente, und in jedem Segment gibt es einen Master-Knoten, der die VMs vor Host-Ausfällen innerhalb des jeweiligen Standorts schützt.</block>
  <block id="628309883a8d1307131d205b200c56ae" category="section-title">Verbindungsfehler zwischen Switches im Speichernetzwerk</block>
  <block id="a9da7e2a43bcea673a22b949355e619d" category="image-alt">Fehler bei der Verbindung zwischen Switches im Speichernetzwerk</block>
  <block id="625db96c5ee1e9b5197b6cb3c1715021" category="paragraph">Wenn in diesem Szenario die ISL-Verbindungen im Back-End-Speichernetzwerk ausfallen, verlieren die Hosts an Standort A den Zugriff auf die Speicher-Volumes oder LUNs von Cluster B an Standort B und umgekehrt. Die VMware DRS Regeln sind so definiert, dass die Host-Storage-Standortaffinität die Ausführung der Virtual Machines ohne Auswirkungen auf den Standort erleichtert.</block>
  <block id="8d9609f32cbee8fa5080f95d71a2a142" category="paragraph">Während dieses Zeitraums bleiben die virtuellen Maschinen an ihren jeweiligen Standorten in Betrieb und es gibt keine Änderung im MetroCluster-Verhalten in diesem Szenario. Alle Datenspeicher bleiben von ihren jeweiligen Seiten intakt.</block>
  <block id="719831bfdda20be14c04e2b165cce85e" category="paragraph">Wenn aus irgendeinem Grund die Affinitätsregel verletzt wurde (z. B. VM1, das von Standort A ausgeführt werden sollte, wo sich seine Festplatten auf lokalen Cluster A-Knoten befinden, auf einem Host an Standort B ausgeführt wird), wird der Remote-Zugriff auf das Laufwerk der virtuellen Maschine über ISL-Links erfolgen. Aufgrund eines ISL-Verbindungsfehlers kann VM1, der an Standort B ausgeführt wird, nicht auf seine Festplatten schreiben, da die Pfade zum Storage-Volume ausgefallen sind und die jeweilige Virtual Machine nicht verfügbar ist. In diesen Situationen nimmt VMware HA keine Aktion vor, da die Hosts aktiv Heartbeats senden. Diese Virtual Machines müssen an den jeweiligen Standorten manuell ausgeschaltet und eingeschaltet werden. Die folgende Abbildung zeigt eine VM, die gegen eine DRS Affinitätsregel verstößt.</block>
  <block id="09ba89b1ea49214c15fbf8ea6f129cb0" category="image-alt">Eine VM, die gegen eine DRS Affinitätsregel verstößt, kann nach einem ISL-Ausfall nicht auf Festplatten schreiben</block>
  <block id="49f26967af2ad76cacf7888cbeeb0590" category="section-title">Alle Interswitch-Fehler oder komplette Rechenzentrumspartition</block>
  <block id="bc331d84348670dff6731555166fa378" category="paragraph">In diesem Szenario sind alle ISL-Verbindungen zwischen den Standorten ausgefallen und beide Standorte voneinander isoliert. Wie bereits in früheren Szenarien erläutert, wie z. B. ISL-Fehler im Managementnetzwerk und im Speichernetzwerk, werden die virtuellen Maschinen bei einem vollständigen ISL-Ausfall nicht beeinträchtigt.</block>
  <block id="a40e8cf2a5b5b1e22fec9f26cce85b9d" category="paragraph">Nachdem ESXi-Hosts zwischen Standorten partitioniert wurden, prüft der vSphere HA-Agent auf Datastore-Heartbeats. An jedem Standort sind die lokalen ESXi-Hosts in der Lage, die Datastore-Heartbeats auf ihr jeweiliges Lese-/Schreibvolumen/LUN zu aktualisieren. Hosts an Standort A gehen davon aus, dass die anderen ESXi-Hosts an Standort B ausgefallen sind, da keine Netzwerk-/Datastore-Heartbeats vorhanden sind. VSphere HA an Standort A versucht, die virtuellen Maschinen von Standort B neu zu starten. Dies schlägt schließlich fehl, da der Zugriff auf die Datenspeicher von Standort B aufgrund eines Fehlers in der Storage-ISL nicht möglich ist. Eine ähnliche Situation wiederholt sich in Standort B.</block>
  <block id="3543d47d1f3a8dc6989a32f62ce3ea77" category="image-alt">Alle ISL-Fehler oder vollständige Datacenter-Partition</block>
  <block id="3a096ef571101010add81b37081d3276" category="paragraph">NetApp empfiehlt, festzustellen, ob eine Virtual Machine gegen die DRS Regeln verstoßen hat. Alle virtuellen Maschinen, die von einem Remote-Standort aus ausgeführt werden, sind ausgefallen, da sie nicht auf den Datastore zugreifen können, und vSphere HA startet diese virtuelle Maschine am lokalen Standort neu. Nachdem die ISL-Links wieder online sind, wird die virtuelle Maschine, die am Remote-Standort ausgeführt wurde, abgebrochen, da es nicht zwei Instanzen virtueller Maschinen geben kann, die mit denselben MAC-Adressen ausgeführt werden.</block>
  <block id="87e9384cb464ff9a4ce385fc59a178a2" category="image-alt">Eine Datacenter-Partition, bei der VM1 gegen eine DRS-Affinitätsregel verstößt</block>
  <block id="0951e388ebea20c49e71af25ef84947d" category="section-title">Verbindungsfehler zwischen Switches auf beiden Fabrics in NetApp MetroCluster</block>
  <block id="1e9be78885a356d1a2ec0ab0d4a4cecb" category="paragraph">In einem Szenario, in dem ein oder mehrere ISLs ausfallen, wird der Datenverkehr über die verbleibenden Links fortgesetzt. Wenn alle ISLs auf beiden Fabrics ausfallen, sodass kein Link zwischen den Standorten für die Storage- und NVRAM-Replizierung vorhanden ist, stellt jeder Controller weiterhin seine lokalen Daten bereit. Bei der Wiederherstellung eines Minimums von einer ISL wird die Resynchronisierung aller Plexe automatisch durchgeführt.</block>
  <block id="715cbc6f6bcf5da6b73f10233847d3a1" category="paragraph">Alle Schreibvorgänge, die nach einem Ausfall aller ISLs stattfinden, werden nicht auf den anderen Standort gespiegelt. Bei einem Disaster-Switchover käme es, während sich die Konfiguration in diesem Zustand befindet, zu einem Verlust der nicht synchronisierten Daten. In diesem Fall ist ein manueller Eingriff für die Wiederherstellung nach der Umschaltung erforderlich. Wenn es wahrscheinlich ist, dass über einen längeren Zeitraum keine ISLs verfügbar sind, kann ein Administrator alle Datenservices herunterfahren, um bei Bedarf ein Switchover im Notfall zu verhindern, dass Daten verloren gehen. Die Durchführung dieser Maßnahme sollte mit der Wahrscheinlichkeit einer Katastrophe abgewogen werden, die eine Umschaltung erfordert, bevor mindestens eine ISL verfügbar wird. Wenn ISLs in einem kaskadierenden Szenario ausfallen, könnte ein Administrator alternativ eine geplante Umschaltung zu einem der Standorte auslösen, bevor alle Links fehlgeschlagen sind.</block>
  <block id="89f375bfe8be0893bf175b89ce754b8e" category="image-alt">Verbindungsfehler zwischen Switches auf beiden Fabrics in NetApp MetroCluster.</block>
  <block id="bc813a6fdbbeebf4d230b7e33d8c3a76" category="section-title">Verbindungsfehler Bei Peered Cluster</block>
  <block id="1919456d1613248a128c65a84b235fe5" category="paragraph">In einem Peering-Cluster-Link-Ausfallszenario, da die Fabric-ISLs noch aktiv sind, werden die Datenservices (Lese- und Schreibvorgänge) an beiden Standorten auf beiden Plexen fortgesetzt. Jegliche Änderungen an der Cluster-Konfiguration (beispielsweise das Hinzufügen einer neuen SVM, die Bereitstellung eines Volumes oder einer LUN in einer vorhandenen SVM) können nicht an den anderen Standort weitergegeben werden. Diese werden in den lokalen CRS-Metadaten-Volumes aufbewahrt und bei Wiederherstellung der Peering-Cluster-Verbindung automatisch auf das andere Cluster übertragen. Wenn eine erzwungene Umschaltung erforderlich ist, bevor der Peered Cluster-Link wiederhergestellt werden kann, werden ausstehende Cluster-Konfigurationsänderungen automatisch von der replizierten Remote-Kopie der Metadaten-Volumes am noch verbleibenden Standort im Rahmen der Umschaltung eingespielt.</block>
  <block id="77130a0b52e4379c6b6667cf7f8e155c" category="image-alt">Ein Verbindungsausfall bei einem Peered Cluster</block>
  <block id="9dc32d7afddc9a265c5c2713db55fc70" category="section-title">Kompletter Standortausfall</block>
  <block id="d0d6a03c382bc5412f1822e5c97d5142" category="paragraph">In einem kompletten Standort-A-Fehlerszenario erhalten die ESXi-Hosts an Standort B keinen Netzwerk-Heartbeat von den ESXi-Hosts an Standort A, weil sie ausgefallen sind. Der HA-Master an Standort B überprüft, ob die Datastore-Heartbeats nicht vorhanden sind, deklariert die Hosts an Standort A als fehlgeschlagen und versucht, die virtuellen Maschinen an Standort A an Standort B neu zu starten In diesem Zeitraum führt der Speicheradministrator eine Umschaltung durch, um die Dienste der ausgefallenen Nodes am noch intakten Standort wiederaufzunehmen. Dadurch werden alle Speicherservices von Standort A an Standort B wiederhergestellt Nachdem die Volumes oder LUNs an Standort A an Standort B verfügbar sind, versucht der HA-Master-Agent, die virtuellen Maschinen am Standort A an Standort B neu zu starten</block>
  <block id="23e5761401326cc21bcf9d108b9e3e7f" category="paragraph">Wenn der Versuch des vSphere HA Master-Agenten, eine VM neu zu starten, fehlschlägt (d. h. sie wird registriert und eingeschaltet), wird der Neustart nach einer Verzögerung erneut durchgeführt. Die Verzögerung zwischen den Neustarts kann auf maximal 30 Minuten konfiguriert werden. VSphere HA versucht diese Neustarts für eine maximale Anzahl von Versuchen (standardmäßig sechs Versuche).</block>
  <block id="085a4a3eba6c30fa589bfee20eb0c294" category="paragraph">Wenn Standort A umgeschaltet wurde, kann ein nachträglicher Ausfall eines der noch intakten Knoten Standort B nahtlos durch einen Failover auf den noch intakten Knoten bewältigt werden. In diesem Fall wird die Arbeit von vier Nodes jetzt nur von einem Node ausgeführt. Die Wiederherstellung würde in diesem Fall eine Rückgabe an den lokalen Knoten bedeuten. Wenn Standort A wiederhergestellt wird, wird ein Switchback-Vorgang durchgeführt, um den stabilen Konfigurationsbetrieb wiederherzustellen.</block>
  <block id="2b9b713afb0d80ef371a0b55e58366b6" category="image-alt">Kompletter Standortausfall</block>
  <block id="e6364c2f5314a32cd95be04b4c2112e6" category="paragraph">In den folgenden Abschnitten werden die Verfahren und Best Practices für die Verwendung von VMware VVols mit ONTAP Storage beschrieben.</block>
  <block id="a9da618400fb93b20c20f5343efaefa7" category="sidebar">VMware vSphere Metro Storage-Cluster mit ONTAP</block>
  <block id="2f58fa882a22b5af5f0741f03abaf611" category="sidebar">VSphere Metro Storage-Cluster mit ONTAP</block>
  <block id="4a884da785a62e8b1758932191ff42de" category="paragraph">Die Virtualisierung von Datenbanken mit VMware, Oracle OLVM oder KVM wird für NetApp-Kunden, die sich für Virtualisierung selbst für ihre geschäftskritischsten Datenbanken entschieden haben, immer häufiger eingesetzt.</block>
  <block id="3a8ba1ff8824968961222d6b447b4973" category="section-title">Instandhaltung</block>
  <block id="3231d83af741101eb02b16e5d7a6dffc" category="paragraph">Es gibt viele Missverständnisse bezüglich der Oracle Support-Richtlinien für Virtualisierung, insbesondere für VMware-Produkte. Es ist nicht ungewöhnlich, zu hören, dass Oracle die Virtualisierung nicht unterstützt. Diese Vorstellung ist falsch und führt zu verpassten Möglichkeiten, von der Virtualisierung zu profitieren. Oracle Doc ID 249212.1 behandelt die tatsächlichen Anforderungen und wird von Kunden selten als ein Problem betrachtet.</block>
  <block id="722bd611ee4f38780607b400c8a604c0" category="paragraph">Wenn ein Problem auf einem virtualisierten Server auftritt und das Problem dem Oracle Support bisher unbekannt war, wird der Kunde möglicherweise gebeten, das Problem auf physischer Hardware zu reproduzieren. Ein Oracle Kunde, der eine innovative Version eines Produkts ausführt, möchte möglicherweise wegen möglicher Supportprobleme keine Virtualisierung nutzen. Für Virtualisierungskunden, die allgemein verfügbare Oracle Produktversionen verwenden, war diese Situation jedoch keine echte Realität.</block>
  <block id="c8a4476ecddeda66dbd2c354c8fb2c6b" category="paragraph">Kunden, die eine Virtualisierung ihrer Datenbanken in Erwägung ziehen, sollten ihre Storage-Entscheidungen basierend auf ihren Geschäftsanforderungen treffen. Dies ist zwar für alle IT-Entscheidungen generell eine echte Aussage, ist aber vor allem bei Datenbankprojekten von Bedeutung, da sich Größe und Umfang der Anforderungen erheblich unterscheiden.</block>
  <block id="835ce293f862d9fb74e50f4cf928c56d" category="paragraph">Es gibt drei grundlegende Optionen für die Storage-Präsentation:</block>
  <block id="327813abbc0ae1e9d3abe282303bf00c" category="list-text">Virtualisierte LUNs auf Hypervisor-Datastores</block>
  <block id="1a1af5efffda8c766ead9f73fc782e0e" category="list-text">NFS-Dateisysteme, die von der VM gemountet wurden (nicht aus einem NFS-basierten Datastore)</block>
  <block id="f9bf9489147891f7ee7ba15126a27fda" category="list-text">Direkte Gerätezuordnungen. VMware RDMs werden von Kunden missbWege, aber physische Geräte werden immer noch oft ähnlich direkt mit KVM- und OLVM-Virtualisierung abgebildet.</block>
  <block id="69b0ac6bf6ea8e1ab4e4fc896294da01" category="paragraph">Die Methode zur Bereitstellung von Speicher für einen virtualisierten Gast hat in der Regel keine Auswirkungen auf die Leistung. Host-Betriebssysteme, virtualisierte Netzwerktreiber und Hypervisor-Datastore-Implementierungen sind allesamt hochoptimiert und können im Allgemeinen die gesamte verfügbare FC- oder IP-Netzwerkbandbreite zwischen dem Hypervisor und dem Storage-System nutzen, sofern grundlegende Best Practices befolgt werden. In einigen Fällen ist das Erlangen einer optimalen Performance unter Umständen mit einem Ansatz für die Storage-Präsentation im Vergleich zu einem anderen etwas einfacher, das Endergebnis sollte jedoch vergleichbar sein.</block>
  <block id="a2e8fb5328f6558e3a119d8c224a1897" category="paragraph">Der wichtigste Faktor bei der Entscheidung, wie Storage einem virtualisierten Gast zur Verfügung gestellt wird, ist die Fähigkeit, zu bestimmen. Es gibt keine richtige oder falsche Methode. Der beste Ansatz hängt von den Anforderungen, Fähigkeiten und Präferenzen der IT-Abteilung ab.</block>
  <block id="e4919ac0a074fa1bba5303f596a5f591" category="paragraph">Zu den berücksichtigenden Faktoren gehören:</block>
  <block id="c422e10bbf04c180f4c47f81ed9a1f7a" category="list-text">*Transparenz.* Wenn eine VM ihre Dateisysteme verwaltet, ist es für einen Datenbankadministrator oder einen Systemadministrator einfacher, die Quelle der Dateisysteme für ihre Daten zu identifizieren. Auf die Dateisysteme und LUNs wird nicht anders zugegriffen als auf einem physischen Server.</block>
  <block id="42de52ad09be3101fc535dea2aedce1d" category="list-text">*Konsistenz.* Wenn eine VM Eigentümer ihrer Dateisysteme ist, wirkt sich die Verwendung oder Nichtnutzung einer Hypervisor-Schicht auf die Verwaltbarkeit aus. Die gleichen Verfahren für die Bereitstellung, das Monitoring, die Datensicherung usw. können über den gesamten Bestand hinweg eingesetzt werden, einschließlich sowohl virtualisierter als auch nicht virtualisierter Umgebungen.</block>
  <block id="0562446bc5eefe9a584e23ffb3061cbf" category="paragraph">Andererseits könnte es in einem ansonsten zu 100 % virtualisierten Datacenter besser sein, Datastore-basierten Storage über den gesamten Platzbedarf hinweg zu nutzen – mit derselben Argumentation wie oben erwähnt – Konsistenz – die Fähigkeit, dieselben Verfahren für Bereitstellung, Sicherung, Überwachung und Datensicherung zu verwenden.</block>
  <block id="7269a37b557c66664c0377693f62ec8a" category="list-text">*Stabilität und Fehlerbehebung.* Wenn eine VM über ihre Dateisysteme verfügt, sind die Bereitstellung guter, stabiler Performance und Fehlerbehebungsprobleme einfacher, da der gesamte Speicher-Stack auf der VM vorhanden ist. Die einzige Rolle des Hypervisors besteht darin, FC- oder IP-Frames zu transportieren. Wenn ein Datastore in einer Konfiguration enthalten ist, wird die Konfiguration durch die Einführung eines weiteren Satzes von Timeouts, Parametern, Protokolldateien und potenziellen Bugs erschwert.</block>
  <block id="c160b6d86853c20a432e2407b82866e6" category="list-text">*Vendor Lock-in.* Nachdem die Daten in einem Datastore platziert wurden, wird es schwierig, einen anderen Hypervisor zu verwenden oder die Daten aus der virtualisierten Umgebung zu nehmen.</block>
  <block id="a42abd01a1e0c89cfa2112dc09378967" category="list-text">*Snapshot-Aktivierung.* herkömmliche Backup-Verfahren in einer virtualisierten Umgebung können wegen der relativ begrenzten Bandbreite zu einem Problem werden. Beispielsweise ist ein 10-GbE-Trunk mit vier Ports möglicherweise ausreichend, um den täglichen Leistungsbedarf vieler virtualisierter Datenbanken zu decken. Ein solcher Trunk wäre jedoch nicht ausreichend, um Backups mit RMAN oder anderen Backup-Produkten durchzuführen, die das Streaming einer vollständigen Kopie der Daten erfordern. So müssen in einer zunehmend konsolidierten virtualisierten Umgebung Backups über Storage Snapshots durchgeführt werden. Dadurch entfällt die Notwendigkeit, die Hypervisor-Konfiguration lediglich zu überbauen, um die Bandbreiten- und CPU-Anforderungen im Backup-Fenster zu unterstützen.</block>
  <block id="86c36175331b4a95c083fabdcca8b8a0" category="paragraph">Bei der Verwendung von Filesystemen, die sich im Besitz von Gästen befinden, ist es manchmal einfacher, Snapshot-basierte Backups und Restores zu nutzen, da die zu schützenden Storage-Objekte einfacher angepeißt werden können. Es gibt jedoch eine immer größere Anzahl an Datensicherungsprodukten für die Virtualisierung, die sich gut in Datenspeicher und Snapshots integrieren lassen. Die Backup-Strategie sollte vollständig berücksichtigt werden, bevor eine Entscheidung darüber getroffen wird, wie Speicher einem virtualisierten Host zur Verfügung gestellt werden kann.</block>
  <block id="8c19dda4c4cab5e52edf9463f196b97d" category="section-title">Datastore-Striping</block>
  <block id="8fa77493c52e71f201ccdb67db26446d" category="paragraph">Bei der Verwendung von Datenbanken mit Datastores muss ein entscheidender Faktor im Hinblick auf die Performance berücksichtigt werden: Striping.</block>
  <block id="53ab623a905e6ec84d908f0b99a8d1c8" category="paragraph">Datastore-Technologien wie VMFS können mehrere LUNs umfassen, sind jedoch keine Striping-Geräte. Die LUNs werden verkettet. Das Endergebnis kann LUN-Hotspots sein. Beispielsweise könnte eine typische Oracle-Datenbank eine ASM-Festplattengruppe mit 8 LUNs enthalten. Alle 8 virtualisierten LUNs konnten auf einem VMFS Datenspeicher mit 8 LUNs bereitgestellt werden, es gibt jedoch keine Garantie, auf welchen LUNs sich die Daten befinden. Die resultierende Konfiguration könnte alle 8 virtualisierten LUNs sein, die eine einzelne LUN innerhalb des VMFS-Datastore belegen. Das führt zu einem Performance-Engpass.</block>
  <block id="e6bb99e411f5495b03c1e0a4e75a1434" category="paragraph">Striping ist in der Regel erforderlich. Bei einigen Hypervisoren, einschließlich KVM, ist es möglich, wie beschrieben mit LVM Striping einen Datenspeicher zu erstellen <block ref="dd0ff2598ebb26feb9ff73a59186b79f" category="inline-link-macro-rx"></block>. Bei VMware sieht die Architektur etwas anders aus. Jede virtualisierte LUN muss in einem anderen VMFS-Datenspeicher platziert werden.</block>
  <block id="506c2c0c7f5b70af3df68c45c46f45a7" category="paragraph">Beispiel:</block>
  <block id="6ec151851a31c86edd358bc49f80908c" category="paragraph"><block ref="6ec151851a31c86edd358bc49f80908c" category="inline-image-macro-rx" type="image"></block></block>
  <block id="70ff2667d6b190dea65bae42217aa310" category="paragraph">Der Haupttreiber dieses Ansatzes ist nicht ONTAP, sondern er liegt an der inhärenten Beschränkung der Anzahl der Vorgänge, die eine einzelne VM oder Hypervisor-LUN parallel bedienen kann. Eine einzelne ONTAP-LUN kann im Allgemeinen deutlich mehr IOPS unterstützen, als ein Host anfordern kann. Das Performance-Limit für eine einzelne LUN ist fast universell ein Ergebnis des Host-Betriebssystems. Das Ergebnis: Die meisten Datenbanken benötigen zwischen 4 und 8 LUNs, um ihre Performance-Anforderungen zu erfüllen.</block>
  <block id="59f9d8d1b212a8ff6bd3476975e17620" category="paragraph">VMware Architekturen müssen ihre Architekturen sorgfältig planen, um sicherzustellen, dass sie keine Maxima für Datenspeicher und/oder LUN-Pfade aufweisen. Darüber hinaus ist keine Notwendigkeit für eine eindeutige Gruppe von VMFS-Datenspeichern für jede Datenbank erforderlich. Die primäre Anforderung besteht darin sicherzustellen, dass jeder Host über einen sauberen Satz von 4-8 I/O-Pfaden von den virtualisierten LUNs zu den Back-End-LUNs auf dem Speichersystem selbst verfügt. In seltenen Fällen können sogar noch mehr Daten für wirklich extreme Performance-Anforderungen von Vorteil sein, aber 4-8 LUNs sind im Allgemeinen für 95 % aller Datenbanken ausreichend. Ein einzelnes ONTAP Volume mit 8 LUNs kann bis zu 250,000 zufällige Oracle Block-IOPS mit einer typischen OS-/ONTAP-/Netzwerkkonfiguration unterstützen.</block>
  <block id="7564f732469e12963d8b416572cf4efb" category="section-title">Was ist vSphere Metro Storage-Cluster?</block>
  <block id="830d984ea50f71fa76636d0524167a3a" category="doc">Übersicht über vSphere Datastore- und Protokollfunktionen</block>
  <block id="1d1d9d816e63b6c7d3057732579e8889" category="list-text">Wenn das NetApp NFS-Plug-in für VMware VAAI verwendet wird, sollte das Protokoll auf eingestellt werden<block ref="2521ef5d58fc027d3121662b7d8f9ac2" prefix=" " category="inline-code"></block> Statt<block ref="61a52f7a75745796f63ec9e3ea2547b4" prefix=" " category="inline-code"></block> Wenn die Regel für die Exportrichtlinie erstellt oder geändert wird. Die Funktion für den Copy-Offload erfordert das NFSv4-Protokoll, selbst wenn das Datenprotokoll NFSv3 ist. Angeben des Protokolls als<block ref="2521ef5d58fc027d3121662b7d8f9ac2" prefix=" " category="inline-code"></block> Umfasst sowohl die NFSv3 als auch die NFSv4-Versionen.</block>
  <block id="cc0f7f705e4495c607aea288505e31bf" category="sidebar">VMSC-Übersicht</block>
  <block id="de9a829f775bc333b77fdacbc333f7a1" category="sidebar">VSphere HA-Lösung</block>
  <block id="70b29c8dabeb7a16884003c35e163f96" category="sidebar">VMSC-Design</block>
  <block id="6c4755219197ce467bf892a552c8663a" category="sidebar">VMSC-Ausfallsicherheit</block>
  <block id="13376e3e9b02a56731aa64aeb10947b2" category="sidebar">VMSC-Szenarien mit MCC</block>
  <block id="f0a36f02b824ef55cbd63aa2700a4ec9" category="summary">Snapshot-basierte Backups und Recovery für Oracle-Datenbanken</block>
  <block id="77243afc69ab08d8277d0e23dda813d9" category="doc">Online-Backups von Oracle Datenbanken</block>
  <block id="4ac7e99e454283880f5bdf93e9d59eb9" category="summary">FabricPool-Tiering für vollständige Dateien in der Oracle-Datenbank</block>
  <block id="3dd6786160916127fc2f6fc2ec955bbe" category="summary">ASM Reclamation Utility mit ONTAP Zero-Block Detection</block>
  <block id="89c60635365825dfa13d154785577592" category="summary">Oracle- und TCP/IP- sowie ethernet-Konfiguration</block>
  <block id="aaa024a70ffabce1e14fdc1a4b17fe8d" category="doc">TCP/IP- und ethernet-Konfiguration für Oracle-Datenbanken</block>
  <block id="665de76ea0059ac13982f04e19e74216" category="summary">Oracle Datenbanken mit Microsoft Windows</block>
  <block id="0d4fbbc8820271a0ce5d46a15e7bf90f" category="paragraph">Konfigurationsthemen für Oracle Database unter Microsoft Windows mit ONTAP..</block>
  <block id="422956aac4f76239935ded6a492144f2" category="summary">ONTAP wurde für Oracle Datenbanken entwickelt. Erfahren Sie, wie:</block>
  <block id="795da6575a33e265d7d9a5774cd10e6f" category="summary">Backups von Konsistenzgruppen für Oracle-Datenbanken auf ONTAP</block>
  <block id="33c729820ffa60f55745fa3c259e273c" category="doc">Backups der Konsistenzgruppe in der Oracle Datenbank</block>
  <block id="56abf0735f338713ce53e8cdecd6994e" category="paragraph">Für die einfachste mögliche Sicherung platzieren Sie einfach die gesamte Oracle-Datenbank in einem einzigen ONTAP-Volume</block>
  <block id="190d2bad1c6e231bc18c33e002e663c7" category="summary">Oracle-Datenbank und ONTAP-Konnektivität mit Direktverbindung</block>
  <block id="4a3f422632f7d08bf0e6c05affea24e4" category="summary">Oracle Database mit SyncMirror</block>
  <block id="5bbdf1e19bb3f7b70de51a3764fc5f19" category="doc">Oracle Database Storage Snapshot optimierte Backups</block>
  <block id="a962af081661ee46d4ef879de33c841e" category="paragraph">Snapshot-basiertes Backup und Recovery wurden vor der Veröffentlichung von Oracle 12c noch einfacher, da eine Datenbank nicht im Hot-Backup-Modus platziert werden muss. Daraus ergibt sich die Möglichkeit, Snapshot basierte Backups direkt auf einem Storage-System zu planen und dennoch eine vollständige oder zeitpunktgenaue Recovery durchzuführen.</block>
  <block id="fdf9dde4dc297fff7ac7b3c1458b0dad" category="doc">LVM Striping mit Oracle-Datenbanken</block>
  <block id="d3c8171a7167ce43c547aac8fdd86cfb" category="doc">Migration der Oracle-Datendatei</block>
  <block id="9718528cb14225785c79e3afc4152efc" category="doc">Größenbestimmung von LUNs und LUN-Anzahl bei der Oracle Datenbank</block>
  <block id="ea7df930bd6bec6222e23d2b3af02a50" category="paragraph">SnapMirror Active Sync unterstützt zwei Arten von Storage Failover-Operationen: Geplant und ungeplant, welche jeweils auf ein geringfügig unterschiedliches Verfahren funktionieren. Ein geplantes Failover wird manuell vom Administrator initiiert, um eine schnelle Umschaltung auf einen Remote-Standort zu ermöglichen, während der ungeplante Failover automatisch vom Mediator am dritten Standort initiiert wird. Der primäre Zweck eines geplanten Failovers ist die Durchführung inkrementeller Patches und Upgrades, Durchführung von Disaster-Recovery-Tests oder die Einführung einer formellen Richtlinie für den Wechsel zwischen Standorten während des ganzen Jahres, um die volle aktive Synchronisierungsfunktion zu belegen.</block>
  <block id="f7c8b0ba16967990d91c2ca78d655eaa" category="paragraph">Die Diagramme zeigen die Vorgänge während des normalen Failover und Failback-Vorgangs an. Zur einfachen Illustration zeigen sie eine replizierte LUN. In einer tatsächlichen Konfiguration mit SnapMirror für aktive Synchronisierung basiert die Replizierung auf Volumes, bei denen jedes Volume eine oder mehrere LUNs enthält. Zur Vereinfachung des Bilds wurde jedoch die Volume-Ebene entfernt.</block>
  <block id="cfa86460b638615878bbee89b1b6d4c7" category="paragraph">Die grüne Linie ist ein aktiver Pfad, aber es käme zu einer höheren Latenz, da I/O auf diesem Pfad über den aktiven SnapMirror Synchronisierungspfad geleitet werden müsste. Die zusätzliche Latenz hängt von der Geschwindigkeit der Verbindung zwischen Standorten ab, die für die aktive SnapMirror Synchronisierung verwendet wird.</block>
  <block id="175d350addf4717d454787f950938adf" category="paragraph"><block ref="175d350addf4717d454787f950938adf" category="inline-image-macro-rx" type="image"></block></block>
  <block id="033e21b9b80fa06986d25123f3025db5" category="paragraph"><block ref="033e21b9b80fa06986d25123f3025db5" category="inline-image-macro-rx" type="image"></block></block>
  <block id="f4d4f8b4d1153d0a9894be19e1f52e4e" category="paragraph"><block ref="f4d4f8b4d1153d0a9894be19e1f52e4e" category="inline-image-macro-rx" type="image"></block></block>
  <block id="22ea45ef5d49a1b807d571799d80613d" category="paragraph">Sobald das Quellsystem wieder einsatzbereit ist, kann die SnapMirror Active Sync Replizierung neu synchronisieren, aber in die andere Richtung ausführen. Die Konfiguration ist jetzt im Wesentlichen dieselbe wie der Startpunkt, mit Ausnahme, dass die Active-Mirror-Standorte gespiegelt wurden.</block>
  <block id="3dd37fdc28ae50bae909f461c8eae979" category="paragraph"><block ref="3dd37fdc28ae50bae909f461c8eae979" category="inline-image-macro-rx" type="image"></block></block>
  <block id="2702e101c97ce9a58738b1a4089319df" category="summary">Logische Architektur von MetroCluster und Oracle Datenbanken</block>
  <block id="784fdd6bf0ebb9c00eaefafc3f9a9cee" category="doc">RAID und Oracle Datenbanken</block>
  <block id="204d288bf0e9d1ee9912d68d6282d7a9" category="summary">Oracle Datenbanken mit AIX</block>
  <block id="90b3697759466f587c318ce55df76258" category="doc">Oracle Datenbanken mit IBM AIX</block>
  <block id="14ab10a24aa8f6c18bfdfd0d8a394dcb" category="paragraph">Konfigurationsthemen für Oracle Database auf IBM AIX mit ONTAP.</block>
  <block id="b67d277a8a8f29f7b869c9bdbff5fcd1" category="summary">Oracle Datenbanken und FabricPool Abrufrichtlinien</block>
  <block id="2e34d4efdcf01f27eb9407a010ca1621" category="summary">Oracle-Migration mit FLI – Umstellung</block>
  <block id="c5753b0ef1dfc2c724f24235a689d1ab" category="summary">Tiering-Richtlinien für Oracle Database FabricPool</block>
  <block id="57d15767e9302dbb67058ed0bd9eebac" category="doc">Oracle-Datenbanken und NVFAIL</block>
  <block id="4f03272fa32b53344924740fd30fc52d" category="summary">Oracle-Datenbanken mit Solaris</block>
  <block id="206960915d76763c590449d5ef03639a" category="summary">Oracle Datenbanken auf ONTAP und die Rolle von Snapshots</block>
  <block id="4bc550014af4fefad8f4cc8f0827c4b5" category="doc">Oracle-Datenbanken und Snapshot-basierte Backups</block>
  <block id="cf5bc9d838fb937caa7978b4a9b0f98a" category="paragraph">Die Grundlage der Datensicherung für Oracle-Datenbanken auf ONTAP ist die NetApp Snapshot Technologie.</block>
  <block id="ba5a23e78f51c1bcb1b82efbe1797502" category="summary">Oracle-Datenbankparameter - db_File_multiblock_read_count</block>
  <block id="ea84db84607112a0d40fca3ff45dfffa" category="summary">Management der Oracle Datenbank-Performance mit ONTAP QoS</block>
  <block id="0af83eb081b1a831789867d7f0867610" category="summary">Tools für das Management und die Automatisierung von Oracle Datenbanken</block>
  <block id="7c58d5a62a9c29d2a846b210deea95f2" category="paragraph">Der Hauptnutzen von ONTAP in einer Oracle Datenbankumgebung ergibt sich aus den zentralen ONTAP Technologien wie sofortige Snapshot Kopien, einfache SnapMirror Replizierung und die effiziente Erstellung von FlexClone Volumes.</block>
  <block id="591b3149d6cb247a04883c411dab3e24" category="summary">Prüfsummen und Oracle-Datenbankintegrität</block>
  <block id="56f014c6fc8fac3e242c9d2e42c654c0" category="paragraph">ONTAP und die unterstützten Protokolle umfassen mehrere Funktionen zum Schutz der Integrität der Oracle-Datenbank, darunter sowohl Daten im Ruhezustand als auch Daten, die über das Netzwerk übertragen werden.</block>
  <block id="31b3368edcb737b712ee02926409c837" category="doc">Oracle-Datenbanken, MetroCluster und NVFAIL</block>
  <block id="490e2f530e53cf85f8ce9b052a8336ec" category="paragraph">NVFAIL ist eine allgemeine Datenintegritätsfunktion in ONTAP, die darauf ausgelegt ist, die Datenintegrität in Datenbanken zu maximieren.</block>
  <block id="f2cf6b943d224a732c92223538580ee9" category="doc">Oracle Single Instance auf MetroCluster</block>
  <block id="ea1d91c32fcf0f4a1090fd0da341c014" category="paragraph">Im Gegensatz zu anderen Disaster Recovery-Lösungen für Storage bietet SnapMirror Active Sync asymmetrische Plattformflexibilität. Die Hardware an den einzelnen Standorten muss nicht identisch sein. Dank dieser Funktion können Sie die Größe der Hardware anpassen, die zur Unterstützung der SnapMirror Active Sync verwendet wird. Das Remote-Storage-System kann identisch mit dem primären Standort sein, wenn es einen vollständigen Produktions-Workload unterstützen muss. Wenn jedoch ein Ausfall zu einer Verringerung der I/O führt, könnte ein kleineres System am Remote-Standort kostengünstiger sein.</block>
  <block id="3873b13983c66ae61dd877b0443a3828" category="summary">Disaster Recovery für Oracle Datenbanken und Konsistenzgruppen</block>
  <block id="ede2d3fcaec1bb9dcda5a75c90f37368" category="summary">Maximieren Sie die Verfügbarkeit mit Oracle Database auf ONTAP</block>
  <block id="9688c9c1c1d15e2f91a3a5b8e5f14bd4" category="doc">Verfügbarkeit von Oracle Database mit ONTAP</block>
  <block id="44e95a6c3878f189a550b4c0314d3b33" category="paragraph">NetApp weiß, dass die geschäftskritischsten Daten in Datenbanken zu finden sind.</block>
  <block id="7aa93b5fc17b83fc9e4816155f8f59ab" category="summary">Performance-Tests für Oracle Database</block>
  <block id="0b3a030b64747fb3830c79e1ffbef7ee" category="summary">Unterbrechungen beim Zugriff auf Oracle Datenbanken und Objektspeicher</block>
  <block id="99a1755be2504777b7e266ed14b33168" category="doc">Oracle Datenbanken und ONTAP Controller-Failover/Switchover</block>
  <block id="8dbb58cb7fbd4c7db3f14b9674e7d4e6" category="summary">Schnelles Recovery von Oracle Database mit SnapRestore</block>
  <block id="fd0d4a1100bcbce28e8252537f7c4ef4" category="summary">Datensicherungs-SLAs für Oracle Datenbanken</block>
  <block id="48130805e35d0c7ab5afbcb71abb1ad0" category="doc">RTO-, RPO- und SLA-Planung für Oracle Database</block>
  <block id="f359cd007fda575e70269cac76fab4af" category="paragraph">Mit ONTAP können Sie in einfacher Weise eine Datensicherungsstrategie für Oracle Database an Ihre Geschäftsanforderungen anpassen.</block>
  <block id="05f9f35345d8f2164958f3b6b7098fa7" category="summary">Oracle FabricPool Tiering für partielle Dateien</block>
  <block id="48e2495d3eeb1b57ea7d073e06620063" category="doc">Migration der Oracle-Datenbank per Protokollversand</block>
  <block id="2cba4042f0a43491e4ea8f14238ae6e4" category="summary">Oracle und FabricPool Snapshot Tiering</block>
  <block id="6f7b1d0f996b7d49f60586ce7fbeca36" category="doc">Oracle mit FabricPool Snapshot Tiering</block>
  <block id="35b12c67685c33721c1a59e8b736fd61" category="summary">NFS Caching mit Oracle Datenbanken</block>
  <block id="1404b2da267c376dac716bd5455f4ff3" category="summary">Oracle-Migration mit FLI - Protokollkonvertierung</block>
  <block id="c37c1629ee55e6d553c16bbab86adbde" category="doc">Migration von Oracle Datenbanken auf ONTAP Storage-Systeme</block>
  <block id="f1d34b6e878aac7179a735d301e4fb76" category="paragraph">Die Nutzung der Leistungsfähigkeit einer neuen Storage-Plattform erfordert zwingend die Speicherung der Daten in dem neuen Storage-System. Mit ONTAP ist der Migrationsprozess denkbar einfach: Migrationen und Upgrades von ONTAP zu ONTAP, Import fremder LUNs und Verfahren für die direkte Nutzung des Host-Betriebssystems oder der Oracle Datenbanksoftware.</block>
  <block id="b7433f552fcf8a4e34f2ccc357037d4e" category="doc">Kopie der Oracle-Datenbank-Host-Daten</block>
  <block id="1cede70bc6077ecbac9a77b03d1bc6e1" category="summary">Planung der Datensicherung für Oracle Datenbanken</block>
  <block id="e2bc0913921cafeb725765069883b685" category="paragraph">Die richtige Datensicherungsarchitektur für Oracle-Datenbanken hängt von den geschäftlichen Anforderungen für die Datenaufbewahrung, Recovery-Fähigkeit und die Toleranz für Unterbrechungen bei verschiedenen Ereignissen ab.</block>
  <block id="c0b79db5f258f0c991a383cbdbf001dd" category="inline-link-macro">SnapMirror Active Sync</block>
  <block id="8a05777f407870cbc2668230c8117503" category="summary">Oracle-Datenbanken mit Linux und ASMlib/ASM Filter Driver</block>
  <block id="494900ae2c2cdb5abdba3806f4dfaeb0" category="doc">Oracle-Datenbanken mit ASMLib/AFD (ASM Filter Driver)</block>
  <block id="afae47e326854c63cbc85989720416e0" category="summary">Oracle Migration with FLI – Planung</block>
  <block id="85cef7bf4ac9d983cf8e1e11785eeef1" category="summary">Oracle Migration with FLI – Abschluss</block>
  <block id="0c5284e14535835121effc8afb702b28" category="summary">Disaster Recovery für Oracle-Datenbanken mit ONTAP</block>
  <block id="2ba21d57ee7f1d78a405b9e80c6aaf6b" category="paragraph">Bei den meisten Kunden benötigt DR mehr als nur einen Remote-Kopiervorgang, sondern muss in der Lage sein, diese Daten schnell zu nutzen. NetApp bietet zwei Technologien zur Erfüllung dieser Anforderungen: MetroCluster und SnapMirror Active Sync</block>
  <block id="107bcc34799abe752a088be24869419c" category="paragraph">Die aktive SnapMirror Synchronisierung basiert auf SnapMirror Synchronous. Mit MetroCluster ist jeder ONTAP Controller für die Replizierung seiner Laufwerksdaten an einen Remote-Standort verantwortlich. Bei SnapMirror Active Sync haben Sie im Grunde zwei verschiedene ONTAP-Systeme, die unabhängige Kopien Ihrer LUN-Daten führen, aber zusammenarbeiten, um eine einzige Instanz dieser LUN zu präsentieren. Auf Host-Ebene handelt es sich um eine einzelne LUN-Einheit.</block>
  <block id="50ae143b75fc238e91c09c750cf211c1" category="summary">Oracle-Datenbankparameter - filesystemio_options</block>
  <block id="cf451e0aed3c944d3f1c761387762e56" category="summary">Oracle-Datenbanken mit Linux</block>
  <block id="e712ed441aacc4eeecb5656d5493c99c" category="summary">Oracle Datenbanken mit HP-UX</block>
  <block id="c36ae718fe9d7978c8781b19d0b3dee0" category="paragraph">Konfigurationsthemen für Oracle Database on HP-UX with ONTAP.</block>
  <block id="45df37151f6a0fd8a3b55d0423459f82" category="doc">LUN-Ausrichtung für Oracle Database I/O</block>
  <block id="857a8adc9eda2d9e197a52323daca558" category="doc">FabricPool Tiering für Oracle Datenbanken – Überblick</block>
  <block id="fd73b1ca3b942a1514078a7e59c7433b" category="summary">Oracle-Datenbank und veraltete NFSv3-Sperren</block>
  <block id="102aeb54bf1a69403672f53090c52878" category="summary">DR für Oracle-Datenbank über Protokollversand</block>
  <block id="d470f5db851ef84aaa776600805af48b" category="summary">Einführung in die Virtualisierung von Oracle Datenbanken</block>
  <block id="b8fc1174bb3f1c65475334182829f75e" category="doc">Virtualisierung der Oracle Datenbank</block>
  <block id="b1b3f70e4333a9dbda75a98ceb78db39" category="summary">Physische Architektur von MetroCluster und Oracle Datenbanken</block>
  <block id="250bce9fbefc7872908d9c2708fc753b" category="doc">Backup-Tiering für Oracle Datenbank</block>
  <block id="4fbe2d000cdf5390ca980149d98eec4d" category="summary">Planung der Migration von Oracle Datenbanken</block>
  <block id="d11e22f56fe0c9dd85597038a25cd80d" category="doc">Oracle Datenbanken und Storage-Kapazitätsmanagement</block>
  <block id="1234e92308935cf0a5a22541f312c541" category="summary">Oracle Database mit SnapMirror und SyncMirror</block>
  <block id="a4cdac841b6453db2f51ce52a1303209" category="paragraph">Die SnapMirror-Replizierung und das Brechen der CG SnapMirror-Beziehung bewahrt die Konsistenz über Volumes hinweg, und SnapMirror Synchronous und SnapMirror Active Sync bewahren beide die Konsistenz über einzelne Volumes hinweg.</block>
  <block id="454ff7e5f4a8d37cf6b16b804deca2b6" category="summary">Blockgrößen für Oracle Database</block>
  <block id="eed7df980a6ae17f8e0fecdecf0e25f4" category="doc">Beispielskripts für das Oracle-Migrationsverfahren</block>
  <block id="fac6d6b61dd6bd3ee73d5b425bc5a3b7" category="doc">Thin Provisioning mit Oracle Datenbanken</block>
  <block id="35ebaedc3385b350dff13e0cfa869c9f" category="summary">Oracle RAC-Timeouts</block>
  <block id="710cf627a103721a0452cb72494ea742" category="admonition">In den nachfolgenden Abschnitten zu Thin Provisioning finden Sie eine Erläuterung des Wechselspiels zwischen Storage-Effizienz und fraktionaler Reservierung.</block>
  <block id="8bc1cc1880ea3a623cb9b80b8cbb7b72" category="paragraph">Es gibt verschiedene Möglichkeiten, Daten zu komprimieren. Viele Datenbanken verfügen über eigene Komprimierungsfunktionen, dies wird jedoch in Kundenumgebungen selten beobachtet. Der Grund dafür ist in der Regel die Performance-Einbußen bei einem *Wechsel* zu komprimierten Daten. Bei einigen Anwendungen fallen zudem hohe Lizenzierungskosten für die Komprimierung auf Datenbankebene an. Und schließlich gibt es noch die allgemeinen Performance-Auswirkungen auf die Datenbankvorgänge. Es macht wenig Sinn, für eine CPU, die Datenkomprimierung und -Dekomprimierung durchführt, hohe Lizenzkosten pro CPU zu zahlen, anstatt eine echte Datenbankarbeit zu erledigen. Eine bessere Option ist, die Komprimierungsarbeiten auf das Storage-System zu verlagern.</block>
  <block id="3e98696b9bb83904c14603458a8c3728" category="admonition">Die von der anpassungsfähigen Komprimierung verwendete Blockgröße kann auf bis zu 32 KB gesteigert werden. Dies kann die Speichereffizienz verbessern und sollte bei stillgelegten Dateien wie Transaktionsprotokollen und Backup-Dateien in Betracht gezogen werden, wenn eine große Menge solcher Daten auf dem Array gespeichert wird. In manchen Situationen profitieren aktive Datenbanken mit 16-KB- oder 32-KB-Blockgröße möglicherweise auch von der Erhöhung der Blockgröße der anpassungsfähigen Komprimierung. Wenden Sie sich an einen Mitarbeiter von NetApp oder einen unserer Partner, um Rat zu erhalten, ob diese Lösung für Ihren Workload geeignet ist.</block>
  <block id="0e6f69b7cb8364ed908c411686154808" category="admonition">Blockgrößen der Komprimierung von mehr als 8 KB sollten nicht zusammen mit der Deduplizierung an Streaming-Backup-Zielen verwendet werden. Der Grund dafür ist, dass kleine Änderungen an den gesicherten Daten das 32-KB-Komprimierungsfenster beeinflussen. Wenn sich das Fenster verschiebt, unterscheiden sich die resultierenden komprimierten Daten in der gesamten Datei. Die Deduplizierung erfolgt nach der Komprimierung. Das heißt, die Deduplizierungs-Engine sieht jedes komprimierte Backup unterschiedlich. Wenn eine Deduplizierung von Streaming-Backups erforderlich ist, sollte nur eine blockadaptive Komprimierung von 8 KB verwendet werden. Die adaptive Komprimierung ist vorzuziehen, da sie bei kleineren Blöcken arbeitet und die Deduplizierungseffizienz nicht stört. Aus ähnlichen Gründen wirkt sich die Host-seitige Komprimierung auch in die Effizienz der Deduplizierung aus.</block>
  <block id="e8a7060409e1a82af9dacfe72458ffac" category="paragraph">Ein Schreibvorgang von 8 KB in eine Datei wird beispielsweise nur komprimiert, wenn er an einer 8-KB-Grenze innerhalb des Dateisystems selbst ausgerichtet ist. Dieser Punkt bedeutet, dass er auf die ersten 8 KB der Datei, die zweiten 8 KB der Datei usw. fallen muss. Der einfachste Weg, um eine korrekte Ausrichtung zu gewährleisten, ist die Verwendung des korrekten LUN-Typs. Jede erstellte Partition sollte einen Offset vom Anfang des Geräts an haben, der ein Vielfaches von 8K ist, und eine Dateisystem-Blockgröße verwenden, die ein Vielfaches der Datenbank-Blockgröße ist.</block>
  <block id="96738983d52a374cb4a8c958e924836e" category="paragraph">Daten wie Backups oder Transaktions-Logs werden sequenziell geschrieben und umfassen mehrere Blöcke. Alle Blöcke werden komprimiert. Daher besteht keine Notwendigkeit, eine Ausrichtung zu erwägen. Das einzige I/O-Muster, das Bedenken aushinsichtlich des zufälligen Überschreibens von Dateien hat, ist das zufällige Überschreiben von Dateien.</block>
  <block id="1afe2c0821b67085f9ea2d2deeb2e92a" category="paragraph">Data-Compaction ist eine Technologie, die die Komprimierungseffizienz verbessert. Wie bereits erwähnt, erzielt die anpassungsfähige Komprimierung allein schon Einsparungen von 2:1, da sie auf das Speichern eines 8-KB-I/O-Blocks in einem 4-KB-WAFL-Block beschränkt ist. Komprimierungsmethoden mit größeren Blockgrößen verbessern die Effizienz. Sie sind jedoch nicht für Daten geeignet, die mit Überschreibungen kleiner Blöcke verbunden sind. Die Dekomprimierung von 32-KB-Dateneinheiten durch die Aktualisierung eines 8-KB-Abschnitts, die Datenkomprimierung und das Zurückschreiben auf die Laufwerke verursacht Overhead.</block>
  <block id="82116b39cd4d0d1920fab1ab4515603b" category="paragraph">Der Grad der erzielten Einsparungen variiert. Bereits komprimierte oder verschlüsselte Daten können in der Regel nicht weiter komprimiert werden. Daher profitieren diese Datensätze von der Data-Compaction nicht. Im Gegensatz dazu werden neu initialisierte Datendateien, die nur wenig mehr als Block-Metadaten und Nullen enthalten, mit bis zu 80 komprimiert.</block>
  <block id="2608473fa5767385d01f256eccbc1cc4" category="paragraph">Viele Arrays anderer Anbieter behaupten, Datenbanken unter der Annahme zu deduplizieren, dass eine Datenbank mehrfach kopiert wird. In dieser Hinsicht kann auch NetApp Deduplizierung eingesetzt werden, allerdings bietet ONTAP die bessere Option: NetApp FlexClone Technologie. Das Endergebnis ist das gleiche. Es werden mehrere Kopien einer Datenbank erstellt, die die meisten zugrunde liegenden physischen Blöcke nutzen. Ein Einsatz von FlexClone ist wesentlich effizienter, als Datenbankdateien zu kopieren und anschließend zu deduplizieren. Der Effekt ist die Nichtdeduplizierung und nicht die Deduplizierung, da ein Duplikat von vornirgends erstellt wird.</block>
  <block id="7e96c1b651af3f28c27b78807f0310e2" category="paragraph">Thin Provisioning wird nachdrücklich empfohlen, da es das Management vereinfachen und gleichzeitig eine deutliche Verbesserung der nutzbaren Kapazität mit den damit verbundenen Kosteneinsparungen ermöglichen kann. Der Grund hierfür ist einfach: Datenbankumgebungen enthalten oft viel leeren Speicherplatz, eine große Anzahl an Volumes und LUNs sowie komprimierbare Daten. Durch Thick Provisioning wird Speicherplatz auf Storage für Volumes und LUNs reserviert, für den Fall, dass sie eines Tages zu 100 % voll werden und 100 % nicht komprimierbare Daten enthalten. Das wird wohl nie passieren. Dank Thin Provisioning kann dieser Speicherplatz zurückgewonnen und an anderer Stelle verwendet werden. Das Kapazitätsmanagement kann auf dem Storage-System selbst basieren, anstatt auf vielen kleineren Volumes und LUNs.</block>
  <block id="0f55b43344575cc268bf51ae032d47c2" category="paragraph">NetApp empfiehlt Folgendes:</block>
  <block id="f67b854e2d644480b0ec40d7de2b773d" category="paragraph">Volumes, die auf ONTAP erstellt wurden und auf einem rein Flash-basierten AFF System ausgeführt werden, werden über Thin Provisioning mit allen Inline-Effizienzfunktionen bereitgestellt. Obwohl Datenbanken im Allgemeinen nicht von der Deduplizierung profitieren und nicht komprimierbare Daten enthalten können, sind die Standardeinstellungen dennoch für fast alle Workloads geeignet. ONTAP wurde mit dem Ziel entwickelt, alle Arten von Daten und I/O-Muster effizient zu verarbeiten. Dabei spielt es keine Rolle, ob es zu Einsparungen kommt oder nicht. Standardwerte sollten nur dann geändert werden, wenn die Gründe vollständig verstanden sind und es einen Vorteil gibt, dass sie abweichen.</block>
  <block id="649141c6b8267ea3df10949a853d22cb" category="list-text">Verwenden Sie für Datenbank-Backups nicht sowohl die 32-KB-Komprimierung als auch die Deduplizierung. Siehe Abschnitt <block ref="57ee2712afc5e59236f86db0537b05dc" category="inline-xref-macro-rx"></block> Entsprechende Details.</block>
  <block id="5059ada12bae6c7e7fac9338d88301b5" category="doc">Protokoll-Tiering für die Oracle-Datenbankarchivierung</block>
  <block id="b7e89b3a1ff13c93cc5c038440e48f58" category="paragraph">Um mehr zu erfahren, starten Sie den https://docs.netapp.com/us-en/ontap-automation/rest/rbac_overview.html[here.]</block>
  <block id="4abda684ecc952239d7800547dcf2995" category="sidebar">Oracle SI auf MetroCluster</block>
  <block id="bdebc59bb8c11012c00a4be61b8bdb5d" category="summary">Bereitstellung von Microsoft SQL Server Instanzen</block>
  <block id="030194212f0e1890126b205fdbcfe852" category="summary">Sichern von Microsoft SQL Server auf ONTAP</block>
  <block id="76e6e519036ed33c414d681e342c73be" category="paragraph">Die Sicherung einer SQL Server-Datenbankumgebung ist ein mehrdimensionaler Aufwand, der über das Management der Datenbank selbst hinausgeht. ONTAP bietet verschiedene einzigartige Funktionen, die den Storage-Aspekt Ihrer Datenbankinfrastruktur sichern sollen.</block>
  <block id="972d09960af01a5f3831f2eac1c75d12" category="summary">Disaster Recovery für Microsoft SQL Server mit ONTAP</block>
  <block id="db78788a721b9a9a669c4935d455a593" category="paragraph">Enterprise-Datenbanken und Applikationsinfrastrukturen erfordern oft Replizierung zum Schutz vor Naturkatastrophen oder unerwarteten Geschäftsunterbrechungen mit minimaler Ausfallzeit.</block>
  <block id="fad02ab7a45b438ba1a4c024b9edb2be" category="paragraph">Nachfolgend finden Sie Empfehlungen für SnapMirror für SQL Server:</block>
  <block id="ff1af055b96eb7cd0f0ab71b2cc80c3d" category="list-text">Aus Konsistenzgründen sollten Sie keine SnapMirror Updates von den Controllern planen. Stattdessen sollten Sie SnapMirror Updates von SnapCenter aktivieren, um SnapMirror zu aktualisieren, nachdem ein vollständiger Backup oder ein Protokoll-Backup abgeschlossen wurde.</block>
  <block id="60654048dbcae98cde53dc0449c8e47b" category="summary">Storage-Überlegungen für Microsoft SQL Server</block>
  <block id="71cc52562b241f23c0f5c74c589fc56c" category="paragraph">Aggregate sind die Storage-Container der niedrigsten Ebene für NetApp Storage-Konfigurationen. Im Internet gibt es eine ältere Dokumentation, die die Trennung von E/A auf verschiedene Sätze zugrunde liegender Laufwerke empfiehlt. Dies wird bei ONTAP nicht empfohlen. NetApp hat verschiedene I/O-Workload-Merkmalstests mit gemeinsam genutzten und dedizierten Aggregaten mit getrennten Datendateien und Transaktions-Log-Dateien durchgeführt. Tests zeigen, dass ein großes Aggregat mit mehr RAID-Gruppen und -Laufwerken die Storage Performance optimiert und verbessert und Administratoren aus zwei Gründen einfacher zu managen sind:</block>
  <block id="994dbcbb2492daba25572c539cc12968" category="list-text">Ein großes Aggregat macht die I/O-Funktionen aller Laufwerke für alle Dateien verfügbar.</block>
  <block id="8e6ab064c04f3cf4f6cfb70d8db3132c" category="list-text">Vermeiden Sie die gemeinsame Nutzung von Volumes zwischen Hosts. Beispielsweise wäre es möglich, 2 LUNs in einem einzelnen Volume zu erstellen und jede LUN mit einem anderen Host zu teilen. Dies sollte jedoch vermieden werden, da das Management dadurch komplizierter wird.</block>
  <block id="900188651b4134e36897865047c3fb5b" category="list-text">Setzen Sie den Wert der Snapshot-Reserve im Volume auf null, um die Überwachung aus betrieblicher Sicht zu vereinfachen.</block>
  <block id="27d9533edb3ad60370502d101cd38d2b" category="list-text">Snapshot Zeitpläne und Aufbewahrungsrichtlinien deaktivieren Stattdessen können Sie SnapCenter verwenden, um Snapshot Kopien der SQL Server-Daten-Volumes zu koordinieren.</block>
  <block id="a09a2d9023a555d2d5a72621bf3395b3" category="list-text">Platzieren Sie die SQL Server Systemdatenbanken auf einem dedizierten Volume.</block>
  <block id="7b3c6a2d089eb1b6f8959e7e61a50c07" category="section-title">LUNs</block>
  <block id="a2f15f0d2a5091a4b232a8fa89bb43f8" category="paragraph">Wenn eine Abfrage ausgeführt wird, versucht SQL Server, die optimale Speichergröße für eine effiziente Ausführung zuzuweisen.</block>
  <block id="a549d3d5e3e6aa1db52e782527a9664d" category="summary">Platzierung von Microsoft SQL Server-Datenbankdateien</block>
  <block id="329282f5b4d197877079b3b130756d76" category="paragraph">Die korrekte Platzierung von SQL Server-Datenbankdateien auf ONTAP ist in der ersten Implementierungsphase entscheidend. Dies sorgt für optimale Performance, Speicherplatz-Management, Backup- und Wiederherstellungszeiten, die Ihren geschäftlichen Anforderungen entsprechend konfiguriert werden können.</block>
  <block id="d8969d8106a5ac08f38277a3c0b813dc" category="paragraph">Die Fähigkeit, mehrere Datendateien innerhalb der Dateigruppe zu speichern, ermöglicht es Ihnen, die Last auf verschiedene Speichergeräte zu verteilen, wodurch die I/O-Performance des Systems verbessert wird. Der Kontrast für die Transaktionsprotokollanmeldung profitiert nicht von den mehreren Dateien, da SQL Server in sequenzieller Weise in das Transaktionsprotokoll schreibt.</block>
  <block id="bd87ed7f70b7f9173319df18e5b5b751" category="paragraph">Jedes Mal, wenn SQL Server Dateien vergrößert, füllt es neu zugewiesenen Speicherplatz mit Nullen. Dieser Prozess blockiert alle Sitzungen, die in die entsprechende Datei geschrieben werden müssen, oder generiert im Falle eines Wachstums des Transaktionsprotokolls Transaktionsprotokolle.</block>
  <block id="7a64f7e9c9b5737a434bdcc8f935c5ce" category="summary">Microsoft SQL Server und ONTAP Storage-Effizienz</block>
  <block id="20f0346a9d65f2fba1523d3343d7c6e7" category="doc">Microsoft SQL Server- und Storage-Effizienz</block>
  <block id="9df29e7c756637e30b37f014d02117ef" category="paragraph">SQL Server selbst verfügt auch über Funktionen zur Komprimierung und zum effizienten Management von Daten. SQL Server unterstützt derzeit zwei Arten der Datenkomprimierung: Row Compression und Page Compression.</block>
  <block id="09ddfa236e0fa1173f8d86a5ad0c148f" category="summary">Protokollverzeichnis von Microsoft SQL Server</block>
  <block id="cc2c23339c0ad05c5ae1704b4a0ecb37" category="paragraph">Das Protokollverzeichnis wird in SQL Server angegeben, um die Backup-Daten des Transaktionsprotokolls auf Hostebene zu speichern. Wenn Sie SnapCenter zum Sichern von Protokolldateien verwenden, muss für jeden von SnapCenter verwendeten SQL Server-Host ein Hostprotokollverzeichnis konfiguriert sein, um Protokollsicherungen durchzuführen. Bei SnapCenter gibt es ein Datenbank-Repository, sodass Metadaten, die mit Backup-, Restore- oder Klonvorgängen verbunden sind, in einem zentralen Datenbank-Repository gespeichert werden.</block>
  <block id="59412263456ecc0573b76fe1c40d55fb" category="paragraph">ONTAP bietet eine Sicherheits- und Performance-Lösung der Enterprise-Klasse für Ihre Microsoft SQL Server Datenbanken und stellt gleichzeitig erstklassige Tools für das Management Ihrer Umgebung bereit.</block>
  <block id="e8b13d430655a3ab009ba8e90e51ce5a" category="paragraph">NetApp geht davon aus, dass der Leser über die folgenden Kenntnisse verfügt:</block>
  <block id="c7f2945182947499933dea6fafeb22d3" category="summary">Microsoft SQL Server tempdb-Platzierung auf ONTAP</block>
  <block id="03162932e80c8f83c1ad9e36b16c1dcf" category="summary">Microsoft SQL Server CPU-Konfiguration</block>
  <block id="074e88cde37f7aba58ac120bbc874e77" category="paragraph">Es gibt zwei Optionen für die Lizenzierung von SQL Server. Die erste wird als Server + Client Access License (CAL)-Modell bezeichnet; die zweite ist das pro Prozessor-Core-Modell. Obwohl Sie mit der Server + CAL-Strategie auf alle in SQL Server verfügbaren Produktfunktionen zugreifen können, gibt es eine Hardwaregrenze von 20 CPU-Kernen pro Sockel. Selbst wenn Sie SQL Server Enterprise Edition + CAL für einen Server mit mehr als 20 CPU-Kernen pro Socket verwenden, kann die Anwendung nicht alle diese Kerne gleichzeitig auf dieser Instanz verwenden.</block>
  <block id="cc3873d6002920194cf7e9dd493c5863" category="paragraph">Es ist unwahrscheinlich, dass Sie die Standardeinstellungen für die Prozessoraffinität ändern müssen, es sei denn, Sie stoßen auf Leistungsprobleme, aber es lohnt sich immer noch zu verstehen, was sie sind und wie sie funktionieren.</block>
  <block id="4317f0d20ae625ae3139762a94175cbd" category="paragraph">Standardmäßig verwendet SQL Server während der Abfrageausführung alle verfügbaren CPUs, wenn die Prozessorkern-Lizenz ausgewählt wurde.</block>
  <block id="ff03ad08d7a38270e44c8c74372d0e7c" category="summary">Sichern von Microsoft SQL Server Datenbanken auf ONTAP mit SnapCenter- und T-SQL-Befehlen.</block>
  <block id="a72812111b6cfc428e19135820c476bc" category="paragraph">SnapCenter ist die NetApp Datensicherungssoftware für Enterprise-Applikationen. Mit dem SnapCenter Plug-in für SQL Server und den vom SnapCenter Plug-in für Microsoft Windows verwalteten Betriebssystemvorgängen können SQL Server Datenbanken schnell und einfach gesichert werden.</block>
  <block id="62c187efa0cd9f4191b84ae987d522c9" category="paragraph">Bei der SQL Server-Instanz kann es sich um eine eigenständige Einrichtung oder eine Failover-Cluster-Instanz handeln oder um eine Always-On-Verfügbarkeitsgruppe. Im Ergebnis können Datenbanken über eine zentrale Konsole geschützt, geklont und aus der primären oder sekundären Kopie wiederhergestellt werden. Mit SnapCenter lassen sich SQL Server Datenbanken sowohl vor Ort, in der Cloud als auch in hybriden Konfigurationen managen. Datenbankkopien können für Entwicklungszwecke oder für Berichte in wenigen Minuten auf dem ursprünglichen oder alternativen Host erstellt werden.</block>
  <block id="8a3aceb6ed4b21af2f2c00ecb032f75f" category="paragraph">In SQL Server 2022 hat Microsoft T-SQL Snapshots eingeführt, die einen Pfad zu Skripting und Automatisierung von Backup-Vorgängen bieten. Anstatt Kopien in voller Größe zu erstellen, können Sie die Datenbank für Snapshots vorbereiten. Sobald die Datenbank für das Backup bereit ist, können Sie Snapshots mithilfe der ONTAP REST-APIs erstellen.</block>
  <block id="eeeb4e610a7896c1b1f2e2ea9b3f52a0" category="list-text">Eine Datenbank mit dem Befehl ALTER fixieren. Dadurch wird die Datenbank auf einen konsistenten Snapshot auf dem zugrunde liegenden Speicher vorbereitet. Nach dem Einfrieren können Sie die Datenbank auftauen und den Snapshot mit dem BACKUP-Befehl aufzeichnen.</block>
  <block id="fd95362fecccbbb7b017317e8bede3b5" category="admonition">Diese Dokumentation zu ONTAP und der MySQL-Datenbank ersetzt die zuvor veröffentlichte _TR-4722: MySQL-Datenbank unter ONTAP Best Practices._</block>
  <block id="03591504f4566055ad54d9422f8a0488" category="paragraph">NetApp bietet mit Astra Trident erweiterte Management-Funktionen für Storage. Mit Astra Trident kann ein in Kubernetes erstellter Container automatisch seinen Storage auf der entsprechenden Tier bereitstellen, Richtlinien für den Export anwenden, Snapshot-Richtlinien festlegen und sogar einen Container auf einen anderen klonen. Weitere Informationen finden Sie im <block ref="2b8a155fc083396b96b18cee0ba5eab0" category="inline-link-macro-rx"></block>.</block>
  <block id="887757e2405372a0b9e17cc054ab38f3" category="paragraph">Der Grund, warum SnapRestore so schnell und effizient arbeitet, liegt in der Natur eines Snapshots, der im Wesentlichen eine parallele schreibgeschützte Ansicht der Inhalte eines Volumes zu einem bestimmten Zeitpunkt ist. Aktive Blöcke sind die realen Blöcke, die geändert werden können, während der Snapshot eine schreibgeschützte Ansicht des Status der Blöcke ist, die die Dateien und LUNs zum Zeitpunkt der Snapshot-Erstellung ausmachen.</block>
  <block id="f9b673452d1da030fef3f976bbbbb103" category="paragraph">In diesem Beispiel befindet sich die Quelldatenbank auf einem ONTAP-System. Die einfachste Methode, um ein Backup einer Datenbank zu erstellen, ist die Verwendung eines Snapshots. Die Datenbank wird für einige Sekunden in den Hot Backup-Modus versetzt, während ein<block ref="252ce13e6c150af4d3e7eea5fca266bc" prefix=" " category="inline-code"></block> Der Vorgang wird auf dem Volume ausgeführt, auf dem die Datendateien gehostet werden.</block>
  <block id="630a571b99791968d41f9caba4a3e39c" category="paragraph">Das Ergebnis ist ein Snapshot auf der Festplatte, der aufgerufen wird<block ref="a8b3bc9f12cd194b80d404c30e9015ee" prefix=" " category="inline-code"></block> Die ein Image der Datendateien enthält, während sie sich im Hot Backup-Modus befinden. Wenn die Daten in diesem Snapshot mit den entsprechenden Archivprotokollen konsistent sind, können sie als Grundlage für eine Wiederherstellung oder einen Klon verwendet werden. In diesem Fall wird sie auf den neuen Server repliziert.</block>
  <block id="96a1129a5a4b32943286cafd5e0ca2e1" category="paragraph">In diesem Beispiel wird SnapMirror verwendet, um das Snapshot Hot-Backup an einen neuen Speicherort zu replizieren.</block>
  <block id="0c4e214451012c456ece194f5465deb2" category="paragraph">SnapCenter umfasst Grundfunktionen wie Snapshot-basierte Backups und Restores, SnapMirror und SnapVault Replizierung sowie weitere Funktionen, die für den skalierten Betrieb von Großunternehmen erforderlich sind. Zu diesen erweiterten Funktionen gehören eine erweiterte Funktion zur rollenbasierten Zugriffssteuerung (RBAC), RESTful APIs zur Integration in Orchestrierungsprodukte von Drittanbietern, unterbrechungsfreies, zentrales Management von SnapCenter Plug-ins auf Datenbank-Hosts und eine Benutzeroberfläche für Cloud-skalierbare Umgebungen.</block>
  <block id="ee4b6ce05339d0d985309ece9b7d4372" category="inline-link-macro">TR-4067 NFS on ONTAP Best Practices</block>
  <block id="7a3ba6ac400bdd362b5db9daabcefca2" category="paragraph">Das NFS-Protokoll umfasst mehrere Versionen mit unterschiedlichen Anforderungen. Eine vollständige Beschreibung der NFS-Konfiguration mit ONTAP finden Sie unter <block ref="d594eac583da3f61bc51209142764c76" category="inline-link-macro-rx"></block>. In den folgenden Abschnitten werden einige der kritischeren Anforderungen und häufigen Benutzerfehler behandelt.</block>
  <block id="dabc72adea00cc712e7650b066f1d8c6" category="inline-link">TR-4067 NFS on ONTAP Best Practices</block>
  <block id="36911cfd2b35ef0ca97dab53c7e045f0" category="paragraph">Der Wechsel zu NFSv4 ist komplizierter als einfach die Mount-Optionen von vers=3 auf vers=4.1 zu ändern. Eine ausführlichere Erläuterung der NFSv4-Konfiguration mit ONTAP, einschließlich Anleitungen zur Konfiguration des Betriebssystems, finden Sie unter<block ref="92f446ddde3e320c4e32b449cbe3112d" category="inline-link-rx"></block>. Die folgenden Abschnitte dieses TR erklären einige der Grundvoraussetzungen für die Verwendung von NFSv4.</block>
  <block id="a80438f1b3354b284cf31082c45b33b3" category="summary">Hyper-V Storage-Infrastruktur mit ONTAP</block>
  <block id="cfbb26e347d5746581822f8046be20c4" category="paragraph">Eine Hyper-V Storage-Infrastruktur kann auf ONTAP Storage-Systemen gehostet werden. Speicher für Hyper-V zur Speicherung der VM-Dateien und ihrer Festplatten kann mithilfe von NetApp-LUNs oder NetApp-CIFS-Freigaben bereitgestellt werden, wie in der folgenden Abbildung dargestellt.</block>
  <block id="25aa60030552ee28c6e802f74d96f111" category="inline-image-macro">Hyper-V Storage-Infrastruktur auf NetApp,width=624,height=338</block>
  <block id="818a20c74e71223c5a644895a6dd4095" category="paragraph"><block ref="818a20c74e71223c5a644895a6dd4095" category="inline-image-macro-rx" type="image"></block></block>
  <block id="a9c5672e01a7d0a8e068c2573fbdd4c2" category="section-title">Hyper-V-Speicher auf NetApp-LUNs</block>
  <block id="1d9f855d005777411eca330fd3c883a1" category="inline-link-macro">Bereitstellung in SAN-Umgebungen</block>
  <block id="d02671c8f4f293bf5688d34f44e5b87b" category="list-text">Stellen Sie eine NetApp-LUN auf dem Hyper-V-Servercomputer bereit. Weitere Informationen finden Sie im Abschnitt „<block ref="9659d1fb07c4af2143cd165cb4b6822e" category="inline-link-macro-rx"></block>.“</block>
  <block id="2771a58b129233810f1c389eedd2207f" category="list-text">Öffnen Sie Hyper-V Manager im Abschnitt Tools von Server Manager.</block>
  <block id="ee15a170a42b1f37e0d70bf3b367f2ee" category="list-text">Wählen Sie den Hyper-V-Server aus, und klicken Sie auf Hyper-V-Einstellungen.</block>
  <block id="b9cc0276ae5386d0ed5e9ed31ceb340d" category="list-text">Geben Sie den Standardordner an, in dem die VM und ihre Festplatte als LUN gespeichert werden sollen. Dadurch wird der Standardpfad als LUN für den Hyper-V-Speicher festgelegt. Wenn Sie den Pfad für eine VM explizit angeben möchten, können Sie dies bei der Erstellung der VM tun.</block>
  <block id="72feffd29972ad4bc2fb9f5705e60864" category="inline-link-macro">Bereitstellung in SMB-Umgebungen</block>
  <block id="ace87f7890ab6204f4ef6abd0afaf28a" category="paragraph">Bevor Sie mit den in diesem Abschnitt aufgeführten Schritten beginnen, lesen Sie den Abschnitt „<block ref="f9d723fa339e7968f40a99719593036c" category="inline-link-macro-rx"></block>.“ Gehen Sie wie folgt vor, um Hyper-V-Speicher auf der NetApp-CIFS-Freigabe zu konfigurieren:</block>
  <block id="d3f9aa7fc4289533bdeb015bbd645a22" category="list-text">Geben Sie den Standardordner an, in dem die VM und ihr Laufwerk als CIFS-Freigabe gespeichert werden sollen. Dadurch wird der Standardpfad als CIFS-Freigabe für den Hyper-V-Speicher festgelegt. Wenn Sie den Pfad für eine VM explizit angeben möchten, können Sie dies bei der Erstellung der VM tun.</block>
  <block id="a531c67c9d6be93b7864c891efc5480a" category="paragraph">Jede VM in Hyper-V kann wiederum mit den NetApp LUNs und CIFS-Freigaben bereitgestellt werden, die dem physischen Host zur Verfügung gestellt wurden. Dieses Verfahren ist das gleiche wie für jeden physischen Host. Mit den folgenden Methoden kann Storage für eine VM bereitgestellt werden:</block>
  <block id="b7756c6e1b92d15f0367db534aa4fdc2" category="list-text">Hinzufügen einer Storage-LUN mithilfe des FC-Initiators in der VM</block>
  <block id="dda418820dd5a8b649a681b73da4ba97" category="list-text">Hinzufügen einer Storage-LUN mithilfe des iSCSI-Initiators in der VM</block>
  <block id="ee0537a6d83f8b83e99ac0b7b3d0af82" category="list-text">Hinzufügen einer physischen Pass-Through-Festplatte zu einer VM</block>
  <block id="a842c008b079c30c444ab4c9d78524e6" category="list-text">Hinzufügen von VHD/VHDX zu einer VM vom Host aus</block>
  <block id="50fba9282607c757ab39ad110cd0fde4" category="list-text">Wenn eine VM und die zugehörigen Daten im NetApp Storage gespeichert sind, empfiehlt NetApp, die NetApp Deduplizierung regelmäßig auf Volume-Ebene durchzuführen. Wenn identische VMs auf einer CSV- oder SMB-Freigabe gehostet werden, lassen sich erhebliche Platzeinsparungen erzielen. Die Deduplizierung wird auf dem Storage-Controller ausgeführt und das Host-System und die VM-Performance werden nicht beeinträchtigt.</block>
  <block id="6421e175d1ef977d73a52a6a90626eb8" category="list-text">Wenn Sie iSCSI-LUNs für Hyper-V verwenden, stellen Sie sicher, dass aktiviert ist<block ref="0a6a9fc5db5706ee3ac704f21780cb07" prefix=" " category="inline-code"></block> Und<block ref="d8ed6ba87266cde713ed7ac6c907ff7e" prefix=" " category="inline-code"></block> In den Firewall-Einstellungen auf dem Hyper-V-Host. Auf diese Weise kann iSCSI-Datenverkehr zum und vom Hyper-V-Host und dem NetApp-Controller geleitet werden.</block>
  <block id="6b4ecf09c5310b6b905c86b910a57012" category="list-text">NetApp empfiehlt, die Option Verwaltungs-Betriebssystem zulassen, diesen Netzwerkadapter für den virtuellen Hyper-V-Switch gemeinsam zu nutzen, zu deaktivieren. Dadurch wird ein dediziertes Netzwerk für die VMs erstellt.</block>
  <block id="d39c100d166d09a8e573d5c4980c0674" category="list-text">Die Bereitstellung einer VM mithilfe von virtuellem Fibre Channel erfordert einen N_Port ID Virtualizationâ€“enabled FC HBA. Es werden maximal vier FC-Ports unterstützt.</block>
  <block id="8f20505d01de58b8e00048581d2d7244" category="list-text">Wenn das Hostsystem mit mehreren FC-Ports konfiguriert und der VM vorgelegt wird, muss MPIO in der VM installiert werden, um Multipathing zu aktivieren.</block>
  <block id="e003acc3eceda748125d4c7bf41052e7" category="list-text">Pass-Through-Festplatten können nicht auf dem Host bereitgestellt werden, wenn MPIO auf diesem Host verwendet wird, da Pass-Through-Festplatten MPIO nicht unterstützen.</block>
  <block id="aa7c5f68a034b43fa3bafa90ea6a6c2b" category="list-text">Für VHD/VHDX-Dateien verwendete Festplatten sollten zur Zuweisung eine 64-KB-Formatierung verwenden.</block>
  <block id="bc8be72fdc9d6054f356bb2b7f80fd12" category="inline-link">NetApp Interoperabilitätsmatrix</block>
  <block id="34de012edea0d70cf76edda88efae407" category="list-text">Weitere Informationen zu FC-HBAs finden Sie im<block ref="03bbc27528de80852aa5d0d1250f8442" category="inline-link-rx"></block>.</block>
  <block id="5aa08a9ab39b4ff0ad3ec476f9c0a7a9" category="inline-link">Hyper-V Virtual Fibre Channel – Überblick</block>
  <block id="17d2fe1a1b42dbc4c914a82ff9a06cbf" category="list-text">Weitere Informationen zu virtuellem Fibre Channel finden Sie unter Microsoft<block ref="ffefda586e5fabc71f0e43393b116d4c" category="inline-link-rx"></block> Seite.</block>
  <block id="6736b7040d05d12679268b13fae93377" category="paragraph">Mit ODX ist es schneller und effizienter, Dateien innerhalb der SMB-Freigaben, innerhalb der LUNs sowie zwischen SMB-Freigaben und LUNs zu kopieren, wenn sich diese sich in demselben Volume befinden. Dieser Ansatz ist insbesondere in Szenarien hilfreich, in denen mehrere Kopien des Golden Image eines Betriebssystems (VHD/VHDX) innerhalb desselben Volumes erforderlich sind. Es können mehrere Kopien desselben goldenen Images in deutlich kürzerer Zeit erstellt werden, wenn sich Kopien innerhalb desselben Volumes befinden. ODX wird auch bei der Hyper-V Storage Live Migration für die Verschiebung von VM Storage eingesetzt.</block>
  <block id="53003aec095203ae2075a8150e8ead4e" category="paragraph">Wenn Kopien über Volumes hinweg erstellt werden, ergeben sich möglicherweise keine nennenswerten Performance-Steigerungen im Vergleich zu hostbasierten Kopien.</block>
  <block id="80f704dacc785527bc57a7851889f8b3" category="paragraph">Führen Sie die folgenden CLI-Befehle auf dem NetApp-Speichercontroller aus, um die ODX-Funktion auf CIFS zu aktivieren:</block>
  <block id="9c8ee5398727fdb2573f25e0535d686d" category="list-text">Aktivieren Sie ODX für CIFS.
#Setzen Sie die Berechtigungsebene auf Diagnose
Cluster::&gt; set -Privilege Diagnostics</block>
  <block id="574eac1c36a4e6832136e6cf051cc147" category="list-text">Führen Sie zum Aktivieren der ODX-Funktion auf dem SAN die folgenden CLI-Befehle auf dem NetApp-Speicher-Controller aus:
#Setzen Sie die Berechtigungsebene auf Diagnose
Cluster::&gt; set -Privilege Diagnostics</block>
  <block id="ea782640f98557c065767874aded89e7" category="list-text">Für CIFS ist ODX nur verfügbar, wenn sowohl der Client als auch der Speicherserver SMB 3.0 und die ODX-Funktion unterstützen.</block>
  <block id="c2dca5b1810c92e856cc04f6f2a88586" category="list-text">In SAN-Umgebungen ist ODX nur verfügbar, wenn sowohl der Client als auch der Speicherserver die ODX-Funktion unterstützen.</block>
  <block id="f41987326c80680c60b359697adfa4cc" category="inline-link">Verbesserung Der Microsoft Remote Copy Performance</block>
  <block id="fcb65106bb1fd73c88cf9528cb579ed5" category="inline-link">Microsoft Offloaded Data Transfers</block>
  <block id="65994fb0563b49bfe0e59359edf5513d" category="paragraph">Informationen zu ODX finden Sie unter<block ref="29f9e15a4017cdf9ae06e098a7a28381" category="inline-link-rx"></block> Und<block ref="f884f78ec6d36ebebd3d9fc57e8c3734" category="inline-link-rx"></block> .</block>
  <block id="20029977789a74e480e6f8d14c07b48a" category="paragraph">Failover-Cluster bieten Hochverfügbarkeit und Skalierbarkeit für Hyper-V Server. Ein Failover-Cluster ist eine Gruppe unabhängiger Hyper-V Server, die gemeinsam die Verfügbarkeit und Skalierbarkeit der VMs erhöhen.</block>
  <block id="1b5f8c43b61e9249f3c82e39f9649160" category="section-title">Cluster Shared Volumes</block>
  <block id="c3e01cf065f02a4feb72ac75d534fa2e" category="paragraph">CSVs ermöglichen mehreren Knoten in einem Failover-Cluster gleichzeitig Lese-/Schreibzugriff auf dieselbe NetApp-LUN, die als NTFS- oder ReFS-Volume bereitgestellt wird. Mit CSVs können geclusterte Rollen schnell ein Failover von einem Node auf einen anderen durchführen, ohne dass eine Änderung des Festplatteneigentums erforderlich ist oder ein Volume aus- und wieder gemountet werden muss. CSVs vereinfachen außerdem das Management einer potenziell großen Anzahl von LUNs in einem Failover-Cluster. CSVs stellen ein universell einsetzbare Cluster-Dateisystem bereit, das über NTFS oder ReFS geschichtet ist.</block>
  <block id="92f30d2ddb9cc7fbf92029307c6dbe98" category="inline-image-macro">Hyper-V Failover Cluster und NetApp,width=624,height=271</block>
  <block id="873c6f06eca771e8f918112aaf233170" category="paragraph"><block ref="873c6f06eca771e8f918112aaf233170" category="inline-image-macro-rx" type="image"></block></block>
  <block id="7c1d777ad1068fcefc91b140eba543bf" category="list-text">NetApp empfiehlt, die Cluster-Kommunikation im iSCSI-Netzwerk zu deaktivieren, um zu verhindern, dass interne Cluster-Kommunikation und CSV-Datenverkehr über dasselbe Netzwerk übertragen werden.</block>
  <block id="b53334a2710cb18a4e717240b312807f" category="list-text">NetApp empfiehlt zur Gewährleistung von Ausfallsicherheit und QoS redundante Netzwerkpfade (mehrere Switches).</block>
  <block id="f251ad17cf6806e0e3667148cda40e43" category="list-text">Für CSV verwendete Laufwerke müssen mit NTFS oder ReFS partitioniert werden. Mit FAT oder FAT32 formatierte Festplatten können nicht für CSV verwendet werden.</block>
  <block id="161432666d89990b3c1112afe633ff8e" category="list-text">Für CSVs verwendete Festplatten sollten eine 64K-Formatierung für die Zuweisung verwenden.</block>
  <block id="36cf7806cbb55a382b64bd9b65b2939b" category="inline-link-macro">Implementieren Sie Hyper-V Cluster</block>
  <block id="390167e852a9bfc97d6ed31462a05b2f" category="paragraph">Informationen zum Bereitstellen eines Hyper-V-Clusters finden Sie in Anhang B: <block ref="8a150f92c22920868769b8738759e23d" category="inline-link-macro-rx"></block>.</block>
  <block id="32730f5882545bbe6a81ac49049968f0" category="section-title">Hyper-V Live Migration: Migration von VMs</block>
  <block id="d7f2f7b4116921a3d8d4b8e5892420a7" category="paragraph">Manchmal ist es während der Lebensdauer der VMs erforderlich, sie auf einen anderen Host auf dem Windows-Cluster zu verschieben. Dies kann erforderlich sein, wenn dem Host die Systemressourcen ausgehen oder der Host aus Wartungsgründen neu gestartet werden muss. Gleichermaßen kann es erforderlich sein, eine VM auf eine andere LUN- oder SMB-Freigabe zu verschieben. Dies kann erforderlich sein, wenn die aktuelle LUN oder Share über zu viel Speicherplatz verfügt oder eine niedrigere Performance erzielt als erwartet. Live-Migration mit Hyper-V verschiebt laufende VMs von einem physischen Hyper-V Server auf einen anderen, ohne dass die VM-Verfügbarkeit für Benutzer darunter ist. Sie können VMs zwischen Hyper-V-Servern, die Teil eines Failover-Clusters sind, oder zwischen unabhängigen Hyper-V-Servern, die nicht Teil eines Clusters sind, live migrieren.</block>
  <block id="92829881718abd402c6c73b34be944da" category="section-title">Live-Migration in einer Cluster-Umgebung</block>
  <block id="e8881f62af8c1495a1f4869b97e95505" category="paragraph">VMs können nahtlos zwischen den Nodes eines Clusters verschoben werden. Die VM-Migration erfolgt unmittelbar, da alle Nodes im Cluster denselben Storage teilen und Zugriff auf die VM und die Festplatte haben. Die folgende Abbildung zeigt die Live-Migration in einer Cluster-Umgebung.</block>
  <block id="8cda6697809dd81e112a12c43f6f89f4" category="inline-image-macro">Live-Migration in einer Cluster-Umgebung,width=580,height=295</block>
  <block id="d668981cacbbf412643956687c03fc0d" category="paragraph"><block ref="d668981cacbbf412643956687c03fc0d" category="inline-image-macro-rx" type="image"></block></block>
  <block id="4797d6b2727c9902e114fa95ed076fc2" category="list-text">Verfügen über einen dedizierten Port für den Datenverkehr von Live-Migrationen.</block>
  <block id="7c021f26d9ab2ec9934fc24d167ceaa4" category="list-text">Nutzen Sie ein dediziertes Host-Live-Migrationsnetzwerk, um netzwerkbezogene Probleme während der Migration zu vermeiden.</block>
  <block id="a725857d2338d476a409fd1fb67b1f56" category="inline-link-macro">Anhang C: Bereitstellung von Hyper-V Live-Migration in einer Cluster-Umgebung</block>
  <block id="45f7134e494aae2cba3b60b3fe769caa" category="paragraph">Informationen zur Bereitstellung von Live-Migration in einer Cluster-Umgebung finden Sie unter <block ref="b584c2d96a6b139fbe05976580afae94" category="inline-link-macro-rx"></block>.</block>
  <block id="07cb5e9c694ee3a8dcc475a38371ad5c" category="paragraph">Sie können eine VM zwischen zwei nicht geclusterten, unabhängigen Hyper-V Servern migrieren. Bei diesem Prozess kann entweder eine Live-Migration ohne gemeinsame Nutzung oder ohne gemeinsame Nutzung verwendet werden.</block>
  <block id="87b94a12a79a6dbc08decd540810bb26" category="inline-image-macro">Shared Live-Migration in einer nicht geclusterten Umgebung,width=331,height=271</block>
  <block id="f442dd66197cc65f49ad8ec9c50e8076" category="paragraph"><block ref="f442dd66197cc65f49ad8ec9c50e8076" category="inline-image-macro-rx" type="image"></block></block>
  <block id="207d1150dff7488c7f8bfafe7036a222" category="list-text">Bei der Live-Migration ohne Shared-Ressourcen verfügt jeder Hyper-V-Server über einen eigenen lokalen Storage (ein SMB-Share, eine LUN oder das), und der Storage der VM befindet sich lokal auf seinem Hyper-V Server. Bei der Live-Migration einer VM wird der Storage der VM über das Client-Netzwerk auf den Zielserver gespiegelt und dann die VM migriert. Die auf das, einer LUN oder einer SMB/CIFS-Freigabe gespeicherte VM kann zu einem SMB/CIFS-Share auf einem anderen Hyper-V Server verschoben werden, wie in der folgenden Abbildung dargestellt. Sie kann auch auf eine LUN verschoben werden, wie in der zweiten Abbildung dargestellt.</block>
  <block id="8df9231bc9de1239aa92c6a9d1116db4" category="inline-image-macro">Shared-Nothing Live-Migration in einer nicht Cluster-Umgebung zu SMB-Shares,width=624,height=384</block>
  <block id="c6acea552c059bcc9ccb207ad6b8cee1" category="paragraph"><block ref="c6acea552c059bcc9ccb207ad6b8cee1" category="inline-image-macro-rx" type="image"></block></block>
  <block id="559b3e5c1d4ea7b8f3eaf8cbc0e4155e" category="inline-image-macro">Shared-Nothing-Live-Migration in einer nicht geclusterten Umgebung zu LUNs,width=624,height=384</block>
  <block id="3d0d53528ff4699abb6fab11a6bf756d" category="paragraph"><block ref="3d0d53528ff4699abb6fab11a6bf756d" category="inline-image-macro-rx" type="image"></block></block>
  <block id="b19cfe6ea61be3a5ed442a8a975a6406" category="inline-link-macro">Anhang D: Implementierung von Hyper-V Live-Migration außerhalb einer Cluster-Umgebung</block>
  <block id="b7c375b5c0ef95eecd45b0a4614c2c68" category="paragraph">Informationen zur Bereitstellung von Live-Migration außerhalb einer Cluster-Umgebung finden Sie unter <block ref="52490817f607e6265e62f91bb3c3799a" category="inline-link-macro-rx"></block>.</block>
  <block id="061731a1a3266df0eda8efe87e649dbb" category="section-title">Hyper-V Storage Live-Migration</block>
  <block id="ecaa2ddd351a3f5a9704aaea9a13abaf" category="paragraph">Während der Nutzungsdauer einer VM müssen Sie möglicherweise den VM Storage (VHD/VHDX) auf eine andere LUN oder SMB-Freigabe verschieben. Dies kann erforderlich sein, wenn die aktuelle LUN oder Share über zu viel Speicherplatz verfügt oder eine niedrigere Performance erzielt als erwartet.</block>
  <block id="1f976b42d4bb2c18a943d132252997d6" category="paragraph">Die LUN oder die Freigabe, die derzeit als Host für die VM fungiert, kann jedoch nicht mehr genügend Speicherplatz haben, mit einer neuen Verwendung zugewiesen werden oder die Performance beeinträchtigen. Unter diesen Umständen kann die VM ohne Ausfallzeit auf eine andere LUN oder auf eine andere Share in einem anderen Volume, Aggregat oder Cluster verschoben werden. Dieser Prozess läuft schneller ab, wenn das Storage-System Copy-Offload-Funktionen verfügt. NetApp Storage-Systeme sind in CIFS- und SAN-Umgebungen standardmäßig für die Copy-Offload-Funktion aktiviert.</block>
  <block id="bbaae8d50419225b92db654852107b29" category="paragraph">Die ODX-Funktion erstellt Kopien von vollständigen oder untergeordneten Dateien zwischen zwei Verzeichnissen auf Remote-Servern. Eine Kopie wird durch Kopieren von Daten zwischen den Servern (oder dem gleichen Server, wenn sich sowohl die Quell- als auch die Zieldateien auf demselben Server befinden) erstellt. Die Kopie wird erstellt, ohne dass der Client die Daten von der Quelle liest oder auf das Ziel schreibt. Dieser Prozess reduziert die Prozessor- und Speichernutzung für den Client oder Server und minimiert die Netzwerk-I/O-Bandbreite. Die Kopie ist schneller, wenn sie sich innerhalb des gleichen Volumes befindet. Wenn Kopien über Volumes hinweg erstellt werden, ergeben sich möglicherweise keine nennenswerten Performance-Steigerungen im Vergleich zu hostbasierten Kopien. Bevor Sie mit einem Kopiervorgang auf dem Host fortfahren, vergewissern Sie sich, dass die Einstellungen für den Copy-Offload im Storage-System konfiguriert sind.</block>
  <block id="843c69ad55fb8bac002321dee51a4770" category="paragraph">Wenn die VM Storage Live-Migration von einem Host aus initiiert wird, werden Quelle und Ziel identifiziert und die Kopieraktivität wird zum Storage-System verlagert. Da die Aktivität vom Storage-System durchgeführt wird, wird die Host-CPU, der Arbeitsspeicher oder das Netzwerk nicht wesentlich genutzt.</block>
  <block id="27d2fda0da241bb37583fba1af25cd09" category="paragraph">NetApp Storage Controller unterstützen die folgenden ODX Szenarien:</block>
  <block id="48a53ae81ff0c821cba0e11bd2383bf3" category="list-text">*IntraSVM.* die Daten befinden sich im Besitz derselben SVM:</block>
  <block id="ec3d44fd176b5d51f4f60dff319cea6b" category="list-text">*Intravolume, Intranode.* die Quell- und Zieldateien oder LUNs befinden sich innerhalb des gleichen Volumes. Die FlexClone Dateitechnologie ermöglicht die Erstellung der Kopie. Damit profitieren Sie von weiteren Performance-Vorteilen bei Remote-Kopien.</block>
  <block id="980ca02c9855a39b86e9fbf5acd955b4" category="list-text">*Intervolume, Intranode.* die Quell- und Zieldateien bzw. LUNs befinden sich auf verschiedenen Volumes, die sich auf demselben Knoten befinden.</block>
  <block id="8a4d33d47a803a1796b431a0df5a8016" category="list-text">*Intervolume, Internodes.* die Quell- und Zieldateien oder LUNs befinden sich auf verschiedenen Volumes, die sich auf verschiedenen Knoten befinden.</block>
  <block id="81d5c73c98489729a8f91afe4936dbd2" category="list-text">*InterSVM.* die Daten sind Eigentum verschiedener SVMs.</block>
  <block id="ea9799ac312b9050a36864c98be8f73f" category="list-text">*Intervolume, Internodes.* die Quell- und Zieldateien oder LUNs befinden sich auf verschiedenen Volumes, die sich auf verschiedenen Knoten befinden.</block>
  <block id="3693f5a47e6bfc0762c5b2c174e653be" category="list-text">*Intercluster.* ab ONTAP 9.0 wird ODX auch für Cluster-LUN-Transfers in SAN-Umgebungen unterstützt. Intercluster ODX wird nur für SAN-Protokolle unterstützt, nicht für SMB.</block>
  <block id="8e459b300b9b5f521f0cc464bc1a7137" category="paragraph">Nach Abschluss der Migration müssen die Backup- und Replizierungsrichtlinien neu konfiguriert werden, um das neue Volume, in dem die VMs enthalten sind, zu berücksichtigen. Alle zuvor erstellten Backups können nicht verwendet werden.</block>
  <block id="9cabe6ff649f754acc3a863e852828e8" category="paragraph">VM Storage (VHD/VHDX) kann zwischen den folgenden Storage-Typen migriert werden:</block>
  <block id="49a63af944b72883bd718eb8c8ff01c8" category="list-text">DAS und die SMB-Freigabe</block>
  <block id="85fbd1c03217c7c62157aac97d16ecf9" category="list-text">DAS und LUN</block>
  <block id="876c6424f49d57e8be596b8caadf5a61" category="list-text">Eine SMB-Freigabe und eine LUN</block>
  <block id="1518cb86dc78f6083ed8b1d292b87ed8" category="list-text">Zwischen LUNs durchgeführt</block>
  <block id="7257cd316393b74da62868c27652c6c0" category="list-text">Zwischen SMB-Freigaben</block>
  <block id="17785c3ef4bbda06e9db9c15908dc0c6" category="inline-image-macro">Live-Migration von Hyper-V-Speicher,width=339,height=352</block>
  <block id="eccf932baf8f6019c69035ea09e26c2c" category="paragraph"><block ref="eccf932baf8f6019c69035ea09e26c2c" category="inline-image-macro-rx" type="image"></block></block>
  <block id="319572d5872182625e2834b8615517f2" category="inline-link-macro">Anhang E: Implementieren von Hyper-V Storage Live-Migration</block>
  <block id="04a136595cb35e5c42e3ea764e9cd259" category="paragraph">Informationen zur Bereitstellung der Live-Migration von Speicher finden Sie unter <block ref="e0d213bcf75379722df0c1b3af5063a2" category="inline-link-macro-rx"></block>.</block>
  <block id="86174aa4d77b1853f72b6e1a6f32c753" category="paragraph">Hyper-V Replica repliziert die Hyper-V VMs von einem primären Standort auf die VMs an einem sekundären Standort und stellt so das Disaster Recovery für die VMs asynchron zur Verfügung. Der Hyper-V-Server am primären Standort, der die VMs hostet, wird als primärer Server bezeichnet; der Hyper-V-Server am sekundären Standort, der replizierte VMs empfängt, wird als Replikatserver bezeichnet. Ein Beispielszenario für Hyper-V-Replika wird in der folgenden Abbildung dargestellt. Sie können Hyper-V Replica für VMs zwischen Hyper-V-Servern verwenden, die Teil eines Failover-Clusters sind, oder zwischen unabhängigen Hyper-V-Servern, die nicht Teil eines Clusters sind.</block>
  <block id="28e90920ae557bd81a8056f93fb4b0ee" category="inline-image-macro">Hyper-V Replica, Breite=624, Höhe=201</block>
  <block id="4fc821418df9e4488323cc70692d5083" category="paragraph"><block ref="4fc821418df9e4488323cc70692d5083" category="inline-image-macro-rx" type="image"></block></block>
  <block id="8c340dc334134096f68b880b42a8692c" category="section-title">Replizierung</block>
  <block id="b9bce4677e5b3ea1c310fddca6123c9f" category="paragraph">Nachdem das Hyper-V-Replikat für eine VM auf dem primären Server aktiviert wurde, erstellt die erste Replikation eine identische VM auf dem Replikatserver. Nach der ersten Replikation verwaltet Hyper-V Replica eine Protokolldatei für die VHDs der VM. Die Protokolldatei wird in umgekehrter Reihenfolge auf die Replikat-VHD in Übereinstimmung mit der Replikationsfrequenz wiedergegeben. Dieses Protokoll und die Verwendung der umgekehrten Reihenfolge stellen sicher, dass die neuesten Änderungen gespeichert und asynchron repliziert werden. Wenn die Replikation nicht der erwarteten Häufigkeit entspricht, wird eine Warnmeldung ausgegeben.</block>
  <block id="ec9fc8b78968b1d0daadf7598a4e2ab6" category="paragraph">Hyper-V Replica unterstützt erweiterte Replikation, bei der ein sekundärer Replikatserver für die Disaster Recovery konfiguriert werden kann. Ein sekundärer Replikatserver kann so konfiguriert werden, dass der Replikatserver die Änderungen an den Replikat-VMs empfängt. In einem erweiterten Replikationsszenario werden die Änderungen an den primären VMs auf dem primären Server auf den Replikatserver repliziert. Anschließend werden die Änderungen auf den erweiterten Replikatserver repliziert. Die VMs können nur dann ein Failover auf den erweiterten Replikatserver durchgeführt werden, wenn sowohl der primäre als auch der Replikatserver ausfallen.</block>
  <block id="a3f36244dba1d116dac91134dda3b9db" category="paragraph">Failover ist nicht automatisch; der Prozess muss manuell ausgelöst werden. Es gibt drei Arten von Failover:</block>
  <block id="f8487dcb10cf762a4d3f28da4635bd2a" category="list-text">*Test Failover.* dieser Typ wird verwendet, um zu überprüfen, ob eine ReplikatVM erfolgreich auf dem Replikatserver gestartet werden kann und auf der ReplikatVM initiiert wird. Durch diesen Prozess wird während des Failovers eine Test-VM doppelt erstellt und die regelmäßige Produktionsreplikation wird nicht beeinträchtigt.</block>
  <block id="09f4056fddf2108616fe7bf3194540ba" category="list-text">*Geplante Ausfallsicherung.* dieser Typ wird verwendet, um VMs während geplanter Ausfallzeiten oder erwarteter Ausfälle zu überführen. Dieser Prozess wird auf der primären VM gestartet, die auf dem primären Server ausgeschaltet werden muss, bevor ein geplantes Failover ausgeführt wird. Nach dem Failover der Maschine startet Hyper-V Replica die Replikat-VM auf dem Replikatserver.</block>
  <block id="0ad8df7e141c378e7ad1debc5721cecf" category="list-text">*Ungeplantes Failover.* dieser Typ wird verwendet, wenn unerwartete Ausfälle auftreten. Dieser Prozess wird auf der Replikat-VM initiiert und sollte nur verwendet werden, wenn der primäre Computer ausfällt.</block>
  <block id="d8afbc541b39d23648c823057cffe3a5" category="section-title">Recovery</block>
  <block id="53b169078c7c6ecee01802b49b675c54" category="paragraph">Wenn Sie die Replikation für eine VM konfigurieren, können Sie die Anzahl der Wiederherstellungspunkte angeben. Wiederherstellungspunkte stellen Zeitpunkte dar, aus denen Daten von einem replizierten Rechner wiederhergestellt werden können.</block>
  <block id="31e96602ee944674a6ee07052bc6552a" category="inline-link-macro">Bereitstellung von Hyper-V Replica außerhalb einer Cluster-Umgebung</block>
  <block id="fd09d6344b235cdd2863e5027a2716e8" category="list-text">Informationen zur Bereitstellung von Hyper-V Replica außerhalb einer Cluster-Umgebung finden Sie im Abschnitt „<block ref="376f647d5b86bfed7dfffbd7c50e7d1e" category="inline-link-macro-rx"></block>.“</block>
  <block id="23b6fa788d78e992f84931ddd3cef629" category="inline-link-macro">Bereitstellung von Hyper-V Replica in einer Cluster-Umgebung</block>
  <block id="a3538f75015c27ee42ac1f1c0b535a61" category="list-text">Informationen zur Bereitstellung von Hyper-V Replica in einer Cluster-Umgebung finden Sie im Abschnitt „<block ref="e81baea973ee37d0bb61473f872cfb84" category="inline-link-macro-rx"></block>.“</block>
  <block id="57a0eb5139789611b6b213a5f3fa5412" category="summary">In diesem Anhang wird die Bereitstellung eines Hyper-V-Clusters mit hoher Verfügbarkeit auf NetApp-Speicher beschrieben.</block>
  <block id="e286734bc8843e9f46f2812455917ea5" category="paragraph">In diesem Anhang wird das Bereitstellen eines Hyper-V-Clusters beschrieben.</block>
  <block id="fa0edb5816386e7e049e5c77d9cdb367" category="list-text">Mindestens zwei Hyper-V-Server sind miteinander verbunden.</block>
  <block id="a7bad6853c4b367ee245a88ab3195af0" category="list-text">Auf jedem Hyper-V-Server ist mindestens ein virtueller Switch konfiguriert.</block>
  <block id="ebdc83ef4c8d2993b67ae667bcb0cd50" category="list-text">Die Failover-Cluster-Funktion ist auf jedem Hyper-V-Server aktiviert.</block>
  <block id="65e98c7fa4c69ce7bd0d182ede13991a" category="list-text">SMB-Freigaben oder CSVs werden als Shared Storage verwendet, um VMs und ihre Festplatten für das Hyper-V Clustering zu speichern.</block>
  <block id="22f4b86bcfebce25bb61ddd3ee8d6f7f" category="list-text">Storage sollte nicht zwischen verschiedenen Clustern gemeinsam genutzt werden. Pro Cluster sollte nur eine CSV-/CIFS-Freigabe vorhanden sein.</block>
  <block id="c2377be670723e586c1324a4a12607a5" category="list-text">Wenn die SMB-Freigabe als freigegebener Speicher verwendet wird, müssen Berechtigungen für die SMB-Freigabe konfiguriert werden, um Zugriff auf die Computerkonten aller Hyper-V-Server im Cluster zu gewähren.</block>
  <block id="ea355214fd4bc7c57f471bd92918879b" category="section-title">Einsatz</block>
  <block id="3627e168149dcf185918371a7adb5680" category="list-text">Melden Sie sich bei einem der Windows Hyper-V-Server als Mitglied der Administratorgruppe an.</block>
  <block id="0cc618f3a272a1fc35b193fc6efa1122" category="list-text">Starten Sie Server Manager**.**</block>
  <block id="f30124a5c887cea53cc6397d6c9e40a6" category="list-text">Klicken Sie im Abschnitt Extras auf Failover Cluster Manager.</block>
  <block id="54be8be618f1803d7fc1579e2fad9a6c" category="list-text">Klicken Sie im Menü Aktionen auf Cluster erstellen.</block>
  <block id="6bcb4b141d153c7cee9fc6b2633a703c" category="list-text">Geben Sie Details für den Hyper-V-Server an, der Teil dieses Clusters ist.</block>
  <block id="58ee4cb30b6b0dd4665be3c60c65e0f1" category="list-text">Validieren der Cluster-Konfiguration. Wählen Sie Ja, wenn Sie zur Validierung der Cluster-Konfiguration aufgefordert werden, und wählen Sie die erforderlichen Tests aus, um zu überprüfen, ob die Hyper-V-Server die Voraussetzungen erfüllen, um Teil des Clusters zu sein.</block>
  <block id="8a2ec0e7f4a4ecb848794b1aba47839b" category="list-text">Nachdem die Validierung erfolgreich war, wird der Assistent Cluster erstellen gestartet. Geben Sie im Assistenten den Cluster-Namen und die Cluster-IP-Adresse für das neue Cluster an. Anschließend wird ein neuer Failover-Cluster für den Hyper-V-Server erstellt.</block>
  <block id="ce73c2e33e5d228466b28690400c8c14" category="list-text">Klicken Sie im Failover Cluster Manager auf den neu erstellten Cluster, und verwalten Sie ihn.</block>
  <block id="904aa93ec80c2a5ca93ed8ace0e52220" category="list-text">Definieren Sie den gemeinsam genutzten Storage, der für das Cluster verwendet werden soll. Es kann sich entweder um eine SMB-Freigabe oder ein CSV handeln.</block>
  <block id="d69f1a0923232e84331ec6d30315124c" category="list-text">Die Verwendung einer SMB-Freigabe als Shared Storage erfordert keine besonderen Schritte.</block>
  <block id="3b045123cc5f216c3aa18614a4cabf45" category="list-text">Konfigurieren Sie eine CIFS-Freigabe auf einem NetApp-Speicher-Controller. Hierzu siehe Abschnitt „<block ref="f9d723fa339e7968f40a99719593036c" category="inline-link-macro-rx"></block>„.</block>
  <block id="c0edb31f94845e5a1f67d2f79db4408f" category="list-text">Führen Sie die folgenden Schritte aus, um ein CSV als gemeinsam genutzten Speicher zu verwenden:</block>
  <block id="dfd86a662700832b7931201c3abdf13f" category="list-text">Konfigurieren Sie LUNs auf einem NetApp Storage Controller. Hierzu finden Sie im Abschnitt „Provisionierung in SAN-Umgebungen“.</block>
  <block id="49873862ecc43076ac3a4de327f3efdb" category="list-text">Stellen Sie sicher, dass alle Hyper-V-Server im Failover Cluster die NetApp-LUNs sehen können. Um dies für alle Hyper-V-Server zu tun, die Teil des Failover-Clusters sind, stellen Sie sicher, dass ihre Initiatoren der Initiatorgruppe im NetApp Storage hinzugefügt werden. Stellen Sie auch sicher, dass ihre LUNs erkannt werden, und stellen Sie sicher, dass MPIO aktiviert ist.</block>
  <block id="4fe4609b22418e84ea85667368c2e742" category="list-text">Führen Sie auf einem der Hyper-V-Server im Cluster die folgenden Schritte aus:</block>
  <block id="de824a62ee353b5481bb4eeb5c13d899" category="list-text">Nehmen Sie die LUN online, initialisieren Sie die Festplatte, erstellen Sie ein neues einfaches Volume und formatieren Sie sie mit NTFS oder ReFS.</block>
  <block id="340f3a4d7fad2c45eadfe69d5b56ce38" category="list-text">Erweitern Sie in Failover Cluster Manager den Cluster, erweitern Sie Speicher, klicken Sie mit der rechten Maustaste auf Festplatten, und klicken Sie dann auf Festplatten hinzufügen. Dadurch wird der Assistent Festplatten zu einem Cluster hinzufügen geöffnet, in dem die LUN als Festplatte angezeigt wird. Klicken Sie auf OK, um die LUN als Festplatte hinzuzufügen.</block>
  <block id="a685089b1d550296b9407033a7411217" category="list-text">Nun wird die LUN mit dem Namen „Cluster Disk“ bezeichnet und unter „Disks“ als „Available Storage“ angezeigt.</block>
  <block id="2970ece9496766e632c3e2c23145cfcf" category="list-text">Klicken Sie mit der rechten Maustaste auf die LUN (Cluster Disk) und klicken Sie auf Add to Cluster Shared Volumes. Nun wird die LUN als CSV angezeigt.</block>
  <block id="c473a447596255e12e2e772012c42b17" category="list-text">Der CSV ist von allen Hyper-V Servern des Failover-Clusters an seinem lokalen Standort C:\ClusterStorage\ gleichzeitig sichtbar und zugänglich.</block>
  <block id="b735c11dd95d1b42d173adb43b3a1df2" category="list-text">Erstellen einer hochverfügbaren VM:</block>
  <block id="27f977a1c170f87899be67daf203a268" category="list-text">Wählen Sie in Failover Cluster Manager den zuvor erstellten Cluster aus, und erweitern Sie ihn.</block>
  <block id="569f3bea800216a760ac935540b72c1e" category="list-text">Klicken Sie auf Rollen und anschließend auf Virtuelle Maschinen in Aktionen. Klicken Sie Auf Neue Virtuelle Maschine.</block>
  <block id="4380a9c5c56776fad0f4492a0d3c63f5" category="list-text">Wählen Sie den Node aus dem Cluster aus, auf dem sich die VM befinden soll.</block>
  <block id="a8be7e73ae88a070ad15a4a6f6d515fe" category="list-text">Stellen Sie im Assistenten für die Erstellung virtueller Maschinen den gemeinsam genutzten Speicher (SMB-Freigabe oder CSV) als Pfad zum Speichern der VM und ihrer Festplatten bereit.</block>
  <block id="9da4e10b0da989c65844cd59ed2a0991" category="list-text">Verwenden Sie Hyper-V Manager, um den gemeinsam genutzten Speicher (SMB-Freigabe oder CSV) als Standardpfad festzulegen, um die VM und ihre Festplatten für einen Hyper-V-Server zu speichern.</block>
  <block id="064cd683bf50cb65205de6b9298201b3" category="list-text">Testen Sie ein ungeplantes Failover. Stoppen Sie den Cluster-Service auf dem Server, auf dem die VM gehört.</block>
  <block id="c7bfaead2ee1aea320b1d91da7ba31d7" category="summary">Erfahren Sie mehr über NetApp Storage- und Windows Server-Umgebung</block>
  <block id="5c8c98e7f403562b754941b4d0c17f65" category="paragraph">Wie im erwähnt <block ref="ba92ae7cf3bb4a058d2b231c16067714" category="inline-link-macro-rx"></block>, NetApp Storage Controller bieten eine echte Unified Architecture, die Datei-, Block- und Objektprotokolle unterstützt. Dazu zählen SMB/CIFS, NFS, NVMe/TCP, NVMe/FC, iSCSI FC (FCP) und S3 zugreifen. Sie erstellen außerdem einen einheitlichen Client- und Host-Zugriff. Derselbe Storage Controller kann gleichzeitig Block-Storage-Service in Form von SAN-LUNs und Fileservices wie NFS und SMB/CIFS bereitstellen. ONTAP ist auch als All-SAN-Array (ASA) verfügbar, das den Hostzugriff über symmetrisches aktiv/aktiv-Multipathing mit iSCSI und FCP optimiert, während die Unified ONTAP-Systeme asymmetrisches aktiv/aktiv-Multipathing verwenden. In beiden Modi verwendet ONTAP ANA für NVMe over Fabrics (NVMe-of) Multipath-Management.</block>
  <block id="b39d47dbcb48e2064ea0978dea0793ae" category="list-text">VMs werden auf kontinuierlich verfügbaren SMB 3.0-Freigaben gehostet</block>
  <block id="be1d5dd432dd053b97f14401fc060813" category="list-text">VMs, die auf LUNs für Cluster Shared Volume (CSV) gehostet werden und auf iSCSI oder FC ausgeführt werden</block>
  <block id="fc27a7625c9ffb2fe9dd539f1960c978" category="list-text">SQL Server-Datenbanken auf SMB 3.0-Freigaben</block>
  <block id="658f98073361780ad39f8c0d45eca2f8" category="list-text">SQL Server-Datenbanken auf NVMe-of, iSCSI oder FC</block>
  <block id="d2ed21657f98def30346884514f11cce" category="list-text">Anderen Applikations-Workloads</block>
  <block id="b7da44d6d6578097c611512549b04555" category="paragraph">Darüber hinaus bietet NetApp Storage-Effizienzfunktionen wie Deduplizierung, NetApp FlexClone Kopien, NetApp Snapshot Technologie, Thin Provisioning, Komprimierung, Storage Tiering kommt bei Workloads, die auf Windows Server ausgeführt werden, erheblichen Nutzen aus.</block>
  <block id="b76d8ce5a4b5758745257d0223494701" category="section-title">ONTAP Datenmanagement</block>
  <block id="37b74b1418c82d2f0f3a093c4da8e7de" category="paragraph">ONTAP ist eine Management-Software, die auf einem NetApp Storage Controller ausgeführt wird. Ein als Node bezeichnet NetApp Storage Controller ist ein Hardwaregerät mit Prozessor, RAM und NVRAM. Der Node kann mit SATA-, SAS- oder SSD-Festplattenlaufwerken oder einer Kombination dieser Laufwerke verbunden werden.</block>
  <block id="16e8274a6170bfb8f7a4b80869edaca0" category="paragraph">Mehrere Nodes werden in einem Cluster-System zusammengefasst. Die Nodes im Cluster kommunizieren kontinuierlich mit einander, um Cluster-Aktivitäten zu koordinieren. Außerdem können die Nodes Daten transparent von Nodes zu Nodes verschieben. Hierzu werden redundante Pfade zu einem dedizierten Cluster-Netzwerk mit zwei 10-Gbit-Ethernet-Switches verwendet. Die Nodes im Cluster können sich gegenseitig übernehmen, um in jedem Failover-Fall für Hochverfügbarkeit zu sorgen. Cluster werden über einen vollständigen Cluster statt pro Node verwaltet. Die Daten werden von einer oder mehreren Storage Virtual Machines (SVMs) bereitgestellt. Ein Cluster muss über mindestens eine SVM verfügen, um Daten bereitzustellen.</block>
  <block id="a93b9d090163c0668a090fac714f7578" category="paragraph">Die Basiseinheit eines Clusters ist der Node, und die Nodes werden dem Cluster als Teil eines Hochverfügbarkeitspaars (HA) hinzugefügt. HA-Paare ermöglichen Hochverfügbarkeit durch Kommunikation untereinander über einen HA Interconnect (getrennt vom dedizierten Cluster-Netzwerk) und durch Beibehalten redundanter Verbindungen zu den Festplatten des HA-Paars. Festplatten werden nicht von HA-Paaren gemeinsam genutzt, obwohl Shelfs Festplatten enthalten können, die zu einem der beiden Mitglieder eines HA-Paars gehören. Die folgende Abbildung zeigt die Bereitstellung von NetApp Storage in einer Windows Server-Umgebung.</block>
  <block id="4472647bb1219b68f4b76e737a20ce52" category="inline-image-macro">Bereitstellung von NetApp Storage in einer Windows Server-Umgebung,width=624,height=479</block>
  <block id="79ea3c938053f84aab6e3d9a963056f6" category="paragraph"><block ref="79ea3c938053f84aab6e3d9a963056f6" category="inline-image-macro-rx" type="image"></block></block>
  <block id="7df39ad25b0e96b5a09cafef15c90e83" category="paragraph">Eine ONTAP SVM ist ein logischer Storage-Server, der den Datenzugriff auf LUNs und/oder einen NAS-Namespace über eine oder mehrere logische Schnittstellen (LIFs) ermöglicht. Damit ist die SVM die grundlegende Einheit der Storage-Segmentierung, die eine sichere Mandantenfähigkeit in ONTAP ermöglicht. Jede SVM ist so konfiguriert, eigene Storage Volumes zu besitzen, die über ein physisches Aggregat bereitgestellt werden, und logische Schnittstellen (LIFs), die einem physischen Ethernet-Netzwerk oder FC-Zielports zugewiesen sind.</block>
  <block id="ded35f53d6a4c12f9aa23622a273290c" category="paragraph">Logische Festplatten (LUNs) oder CIFS Shares werden innerhalb der Volumes einer SVM erstellt und Windows Hosts und Clustern zugeordnet, um ihnen Speicherplatz zur Verfügung zu stellen, wie in der folgenden Abbildung dargestellt. SVMs sind Node-unabhängig und Cluster-basiert. Sie können physische Ressourcen wie Volumes oder Netzwerk-Ports im gesamten Cluster verwenden.</block>
  <block id="66bfba6c82925242c27d2f07403b627e" category="inline-image-macro">ONTAP Storage Virtual Machine, Breite=572, Höhe=443</block>
  <block id="9bcb5e161ae4c106ccd0f2dc96989097" category="paragraph"><block ref="9bcb5e161ae4c106ccd0f2dc96989097" category="inline-image-macro-rx" type="image"></block></block>
  <block id="8f64c369fcdc9c990f5540ce53430671" category="paragraph">In SAN- und NAS-Umgebungen kann Windows Server Storage bereitstellen. In einer SAN-Umgebung wird der Storage als Disks von LUNs auf einem NetApp-Volume als Block-Storage zur Verfügung gestellt. In einer NAS-Umgebung wird der Storage als CIFS/SMB-Freigaben auf NetApp Volumes als File-Storage bereitgestellt. Diese Laufwerke und Freigaben können in Windows Server wie folgt angewendet werden:</block>
  <block id="8572daf67b8cf4929b8a104909d42a03" category="list-text">Storage für Windows Server-Hosts für Applikations-Workloads</block>
  <block id="460622219d2e0a7fe2d7339d37894127" category="list-text">Speicher für Nano Server und Container</block>
  <block id="24148b9d626dc3494e0f1a07f5ddf180" category="list-text">Storage für einzelne Hyper-V Hosts zum Speichern von VMs</block>
  <block id="c9d2a95802a3dc0a7c1033928d3cfc3f" category="list-text">Shared Storage für Hyper-V Cluster in Form von CSVs zum Speichern von VMs</block>
  <block id="76c319d8818e93b50538dff2f4427cbc" category="list-text">Storage für SQL Server-Datenbanken</block>
  <block id="13a4fb282282699692756c149d94bbbe" category="paragraph">Verwenden Sie eine der folgenden Methoden, um NetApp-Speicher von Windows Server 2016 aus zu verbinden, zu konfigurieren und zu verwalten:</block>
  <block id="81ef8774b45c60d7387b0e91d9c4eb05" category="list-text">*Secure Shell (SSH).* Verwenden Sie einen beliebigen SSH-Client auf dem Windows-Server, um NetApp-CLI-Befehle auszuführen.</block>
  <block id="935df6062271011aad64300fe0f8a6c7" category="list-text">*System Manager.* Dies ist das GUI-basierte Manageability-Produkt von NetApp.</block>
  <block id="85c7991eb4ed1078ecb54aacffe3a3a2" category="list-text">*NetApp PowerShell Toolkit.* Dies ist das NetApp PowerShell Toolkit zur Automatisierung und Implementierung von benutzerdefinierten Skripts und Workflows.</block>
  <block id="1fb25b443082bb79703b1237630cd9b6" category="section-title">NetApp PowerShell Toolkit</block>
  <block id="a5038d870bfecae7a8fb1fb0d8ba892b" category="list-text">NetApp unterstützt keine Storage Spaces im Windows Server. Storage Spaces werden nur für JBOD verwendet (nur ein paar Disks) und funktionieren nicht mit irgendeiner Art von RAID (Direct-Attached Storage [das] oder SAN).</block>
  <block id="b37ae3efea13417cff3fdb198db41274" category="list-text">Cluster-Speicherpools in Windows Server werden von ONTAP nicht unterstützt.</block>
  <block id="436593f3429fec64588705fe44025823" category="list-text">NetApp unterstützt das gemeinsam genutzte Virtual Hard Disk Format (VHDX) für Gastclustering in Windows SAN-Umgebungen.</block>
  <block id="a3a862b96d666cb7005617313f703378" category="list-text">Windows Server unterstützt nicht das Erstellen von Speicherpools mit iSCSI- oder FC-LUNs.</block>
  <block id="570d38aae09bbf84ba37f9219909a7cb" category="list-text">Weitere Informationen zum NetApp PowerShell Toolkit finden Sie im<block ref="12f72d9ce1eab5b7cd58718fecdd145c" category="inline-link-rx"></block>.</block>
  <block id="699f0a44719fb3de5404046fc6caa8af" category="inline-link">TR-4475: Best Practices-Leitfaden für das NetApp PowerShell Toolkit</block>
  <block id="532d077f114744273b1a7cd2a4794077" category="list-text">Informationen zu Best Practices für das NetApp PowerShell Toolkit finden Sie unter<block ref="87b47631a9123c9d2a382039a9ba503e" category="inline-link-rx"></block>.</block>
  <block id="64f3248fbae79dd41841377bf7dd37c6" category="paragraph">Ethernet-Netzwerke können in die folgenden Gruppen unterteilt werden:</block>
  <block id="0047d4a23ffec6d4afe912d0a2598e58" category="list-text">Ein Client-Netzwerk für die VMs</block>
  <block id="d6cf6da705ce58c7b649fea1cd43c470" category="list-text">Noch ein Storage-Netzwerk (iSCSI oder SMB, das mit den Storage-Systemen verbunden ist)</block>
  <block id="0d8658721aaab8eb43f9b6d72283b8f8" category="list-text">Ein Cluster-Kommunikationsnetzwerk (Heartbeat und andere Kommunikation zwischen den Nodes des Clusters)</block>
  <block id="338c97ec16c135df1c972ba5b92db129" category="list-text">Ein Managementnetzwerk (zur Überwachung und Fehlerbehebung des Systems)</block>
  <block id="7b6753ef35b59e8866faf1b43b213520" category="list-text">Ein Migrationsnetzwerk (für Host-Live-Migration)</block>
  <block id="0f31fc2f15105bd45773e0fe5a33814a" category="list-text">VM-Replizierung (ein Hyper-V Replikat)</block>
  <block id="11ef3cd9dc7173f009417493a7a51f57" category="list-text">NetApp empfiehlt für jede der oben genannten Funktionen dedizierte physische Ports zur Netzwerkisolierung und zur Performance.</block>
  <block id="84f729d0d6ae07e1ea7697e418337328" category="list-text">Für jede der oben genannten Netzwerkanforderungen (mit Ausnahme der Speicheranforderungen) können mehrere physische Netzwerkports aggregiert werden, um die Last zu verteilen oder eine Fehlertoleranz bereitzustellen.</block>
  <block id="89831505b6fb0eb3bfb78852c3e8838a" category="list-text">NetApp empfiehlt die Erstellung eines dedizierten virtuellen Switches auf dem Hyper-V Host für die Verbindung zum Gast-Storage innerhalb der VM.</block>
  <block id="301b0a1048a669cb6b56215ce25f83d2" category="list-text">Stellen Sie sicher, dass die Hyper-V-Host- und iSCSI-Datenpfade verschiedene physische Ports und virtuelle Switches zur sicheren Isolation zwischen dem Gast und dem Host verwenden.</block>
  <block id="38d14a6f7d54295d21bfb890bbfac145" category="list-text">NetApp empfiehlt, NIC-Teaming für iSCSI-NICs zu vermeiden.</block>
  <block id="3b5eab23ccdc37978e26fa584647f2f5" category="list-text">NetApp empfiehlt die Verwendung von ONTAP Multipath Input/Output (MPIO), der auf dem Host für Storage-Zwecke konfiguriert ist.</block>
  <block id="6395caf6c767d5f8026b684721c1e27e" category="list-text">NetApp empfiehlt die Verwendung von MPIO innerhalb einer Gast-VM, wenn Sie iSCSI-Gastinitiatoren verwenden. Die MPIO-Verwendung im Gastsystem muss vermieden werden, wenn Sie Pass-Through-Festplatten verwenden. In diesem Fall sollte die Installation von MPIO auf dem Host ausreichen.</block>
  <block id="55892ded4e0eac09cd2bd256ea8e4f49" category="list-text">NetApp empfiehlt, keine QoS-Richtlinien auf den virtuellen Switch anzuwenden, der dem Storage-Netzwerk zugewiesen ist.</block>
  <block id="9e33949565ff57eed3e484a27adca687" category="list-text">NetApp empfiehlt, keine automatische private IP-Adressierung (APIPA) auf physischen NICs zu verwenden, da APIPA nicht routingfähig ist und nicht im DNS registriert ist.</block>
  <block id="b06f6cf7b5c4e8ac8117fe6238a15184" category="list-text">NetApp empfiehlt die Aktivierung von Jumbo Frames für CSV-, iSCSI- und Live-Migrationsnetzwerke, um den Durchsatz zu erhöhen und CPU-Zyklen zu reduzieren.</block>
  <block id="d34fac956ae63cd93b204c541399ec3f" category="list-text">NetApp empfiehlt, die Option Management Operating System zur Freigabe dieses Netzwerkadapters für den virtuellen Hyper-V-Switch deaktivieren, um ein dediziertes Netzwerk für die VMs zu erstellen.</block>
  <block id="0d3e51fc3c7ecb31cb6dcf3ea104060c" category="list-text">NetApp empfiehlt die Erstellung redundanter Netzwerkpfade (mehrere Switches) für die Live-Migration und das iSCSI-Netzwerk, um Ausfallsicherheit und QoS zu gewährleisten.</block>
  <block id="72a46482f366b350b6b0215baa631137" category="summary">Zusätzliche Ressourcen für Microsoft Windows und Hyper-V</block>
  <block id="4bb047f8c530785002e490ef17fa725e" category="doc">Wo Sie weitere Informationen finden</block>
  <block id="0650b34324cf33e3bf1ba6d5db9aa14c" category="list-text">Was ist neu in Hyper-V auf Windows Server +
<block ref="8508224d602483e88bde8402b5d1f2d1" category="inline-link-rx"></block></block>
  <block id="1139a59d4aa15acd59f31d05c5a9d642" category="summary">In diesem Anhang wird die Bereitstellung von Hyper-V Replica außerhalb einer Clusterumgebung beschrieben.</block>
  <block id="be9cfe5b3b6e5354686be0cb736fe70d" category="list-text">Sie benötigen eigenständige Hyper-V-Server, die sich an demselben oder einem anderen geografischen Standort befinden und als primäre und Replikatserver dienen.</block>
  <block id="427aef80e259ae74b835c2b646e8fba8" category="list-text">Wenn separate Standorte verwendet werden, muss die Firewall an jedem Standort so konfiguriert werden, dass die Kommunikation zwischen dem primären und dem Replikatserver möglich ist.</block>
  <block id="3ac92c1d06a47463c6ea9f2c4289a625" category="list-text">Der Replikatserver muss über genügend Speicherplatz zum Speichern der replizierten Workloads verfügen.</block>
  <block id="6db575d6d23958eefd722c2559c2ae2b" category="list-text">Konfigurieren Sie den Replikatserver.</block>
  <block id="f348eb2699776a23b340068fa13ba378" category="list-text">Führen Sie das folgende PowerShell-Cmdlet aus, damit die Regeln der eingehenden Firewall eingehenden Replikationsverkehr zulassen:</block>
  <block id="b3741a0bf4f272b05520af3145ce6b20" category="list-text">Klicken Sie in Aktionen auf Hyper-V-Einstellungen.</block>
  <block id="0b81c69f5b40344714e859c333f7e46d" category="list-text">Klicken Sie auf Replikationskonfiguration und wählen Sie Diesen Computer als Replikatserver aktivieren aus.</block>
  <block id="23e173753ad1fc5a192335afcd91aaa2" category="list-text">Wählen Sie im Abschnitt Authentifizierung und Ports die Authentifizierungsmethode und den Port aus.</block>
  <block id="c67571ac7fd00493d3ca7147b7756914" category="list-text">Geben Sie im Abschnitt Autorisierung und Speicher den Speicherort für die replizierten VMs und Dateien an.</block>
  <block id="c6f470c780ac40391a73366709e36fae" category="list-text">Aktivieren Sie die VM-Replikation für VMs auf dem primären Server. VM-Replikation wird pro VM und nicht für den gesamten Hyper-V-Server aktiviert.</block>
  <block id="84f81856434a0c817bfd8ec7ba575d63" category="list-text">Klicken Sie in Hyper-V Manager mit der rechten Maustaste auf eine VM, und klicken Sie auf Replikation aktivieren, um den Assistenten Replikation aktivieren zu öffnen.</block>
  <block id="bb27dca619d145c6538ad523a5c5aec7" category="list-text">Geben Sie den Namen des Replikatservers an, auf dem die VM repliziert werden muss.</block>
  <block id="368e47c642802a224da6a1e9196c2345" category="list-text">Geben Sie den Authentifizierungstyp und den Port des Replikatservers an, der für den Empfang von Replikationsdatenverkehr auf dem Replikatserver konfiguriert wurde.</block>
  <block id="df839b9a847de722b03fb6f8db4abd3d" category="list-text">Wählen Sie die zu replizierenden VHDs aus.</block>
  <block id="7e2b2defa62bab724dad332e0efe93d4" category="list-text">Wählen Sie die Häufigkeit (Dauer), mit der die Änderungen an den Replikatserver gesendet werden.</block>
  <block id="f1358329d5a84d751f47c4d4e4882286" category="list-text">Konfigurieren Sie Wiederherstellungspunkte, um die Anzahl der Wiederherstellungspunkte anzugeben, die auf dem Replikatserver beibehalten werden sollen.</block>
  <block id="13bb9737452c217970585237bf9a3c67" category="list-text">Wählen Sie Initial Replication Method, um die Methode anzugeben, mit der die erste Kopie der VM-Daten auf den Replikatserver übertragen werden soll.</block>
  <block id="e7773df7e8bec8a84ededaaba6cc3b5c" category="list-text">Überprüfen Sie die Zusammenfassung, und klicken Sie auf Fertig stellen.</block>
  <block id="eaa408dd6135876f9368b7e9e8447340" category="list-text">Durch diesen Prozess wird ein VM-Replikat auf dem Replikatserver erstellt.</block>
  <block id="c6c9686c38a42b00345f77c5ddbe044a" category="list-text">Führen Sie einen Test-Failover aus, um sicherzustellen, dass die Replikat-VM auf dem Replikatserver ordnungsgemäß funktioniert. Der Test erstellt eine temporäre VM auf dem Replikatserver.</block>
  <block id="4b07a5223e241d6b4d7935c2282616a5" category="list-text">Melden Sie sich beim Replikatserver an.</block>
  <block id="baaaeb9b824a8922645198d46a10e27a" category="list-text">Klicken Sie in Hyper-V Manager mit der rechten Maustaste auf eine Replikat-VM, klicken Sie auf Replikation, und klicken Sie auf Failover testen.</block>
  <block id="b9950331695e861c821d48077fe8b1f1" category="list-text">Wählen Sie den zu verwendenden Wiederherstellungspunkt aus.</block>
  <block id="b1d034cdaa7ef0d2bc761b948c78838a" category="list-text">Bei diesem Vorgang wird eine VM mit dem gleichen Namen erstellt, die mit -Test angehängt wird.</block>
  <block id="8c95c5ef4d5593fe860313c283fab784" category="list-text">Überprüfung der VM zur Gewährleistung der guten Funktionsweise</block>
  <block id="e5c73308bdacf8978516ccda5462e80a" category="list-text">Nach dem Failover wird die Test-VM des Replikats gelöscht, wenn Sie für sie die Option „Test-Failover anhalten“ auswählen.</block>
  <block id="c4eacda4a6022cafe1bfc0d735075dce" category="list-text">Führen Sie ein geplantes Failover aus, um die letzten Änderungen an der primären VM auf die Replikat-VM zu replizieren.</block>
  <block id="7c3c0f256f21e1f117a98a5afa47dd56" category="list-text">Melden Sie sich beim primären Server an.</block>
  <block id="2d6fd035cbd8062dcc29935ff448420e" category="list-text">Schalten Sie die VM aus, für die ein Failover durchgeführt werden soll.</block>
  <block id="582c3456f1d3f75fd04a3dfcdb20c132" category="list-text">Klicken Sie in Hyper-V Manager mit der rechten Maustaste auf die ausgeschalteten VM, klicken Sie auf Replikation, und klicken Sie auf geplante Failover.</block>
  <block id="4ed0c56504eba0632689dfc8056e3c49" category="list-text">Klicken Sie auf Failover, um die letzten VM-Änderungen auf den Replikatserver zu übertragen.</block>
  <block id="f490046b13ec9105a4adb8f885b5688f" category="list-text">Führen Sie bei Ausfall der primären VM ein ungeplantes Failover aus.</block>
  <block id="c01438bcded7cda6d1ee326fa268f93c" category="list-text">Klicken Sie in Hyper-V Manager mit der rechten Maustaste auf eine Replikat-VM, klicken Sie auf Replikation und dann auf Failover.</block>
  <block id="94faab2d3840f3d8320e20ad4a90e6cd" category="list-text">Klicken Sie auf Failover, um ein Failover der VM durchzuführen.</block>
  <block id="4be45953e45cfa5fc987ba108c1e5793" category="summary">In diesem Anhang wird die Bereitstellung von Live-Migration in einer Cluster-Umgebung beschrieben.</block>
  <block id="388b92a1de3321595dcfbdc1c67ce749" category="paragraph">Gehen Sie wie folgt vor, um die Live-Migration in einer Cluster-Umgebung zu nutzen:</block>
  <block id="b66a6c742469f1d08a2d11bcf4f211ff" category="list-text">Wählen Sie in Failover Cluster Manager den Cluster aus, und erweitern Sie ihn. Wenn der Cluster nicht angezeigt wird, klicken Sie auf Failover Cluster Manager, klicken Sie auf mit Cluster verbinden, und geben Sie den Cluster-Namen ein.</block>
  <block id="e3883376513ba0aeb2f3d190d4e0377d" category="list-text">Klicken Sie auf Rollen, in der alle in einem Cluster verfügbaren VMs aufgeführt sind.</block>
  <block id="e164db40505184b94012bbbd4aa4c1a2" category="list-text">Klicken Sie mit der rechten Maustaste auf die VM und klicken Sie auf Verschieben. Dadurch stehen Ihnen drei Optionen zur Verfügung:</block>
  <block id="6a1e101231f8f4bcd4a75390cc0eddc2" category="list-text">*Live Migration.* Sie können einen Knoten manuell auswählen oder dem Cluster erlauben, den besten Knoten auszuwählen. Bei der Live-Migration kopiert das Cluster den von der VM verwendeten Arbeitsspeicher vom aktuellen Node auf einen anderen Node. Wenn die VM zu einem anderen Node migriert wird, sind die von der VM benötigten Arbeitsspeicher- und Statusinformationen bereits für die VM vorhanden. Diese Migrationsmethode erfolgt nahezu ohne Verzögerung, aber nur eine VM kann gleichzeitig live migriert werden.</block>
  <block id="4374953843b8a5507e79ee5145fd88cd" category="list-text">*Schnelle Migration.* Sie können einen Knoten manuell auswählen oder dem Cluster erlauben, den besten Knoten auszuwählen. Bei einer schnellen Migration kopiert das Cluster den von einer VM genutzten Speicher auf eine Festplatte im Storage. Wenn die VM zu einem anderen Node migriert wird, können daher die von der VM benötigten Arbeitsspeicher- und Statusinformationen schnell von der Festplatte des anderen Node gelesen werden. Durch die schnelle Migration können mehrere VMs gleichzeitig migriert werden.</block>
  <block id="f1321d73ebfb5a92e710d313fd53c21c" category="list-text">*Migration des virtuellen Maschinenspeichers.* Diese Methode verwendet den Assistenten zum Verschieben des virtuellen Maschinenspeichers. Mit diesem Assistenten können Sie die VM-Festplatte zusammen mit anderen Dateien auswählen, die an einen anderen Speicherort verschoben werden sollen. Dabei kann es sich um eine CSV- oder SMB-Freigabe handeln.</block>
  <block id="36397d251acf4232a7e91edc7ae649c7" category="summary">ONTAP NAS-Storage für Hyper-V mit SMB3</block>
  <block id="a06ae7fcee90c7ab1e1321d0dd8b2242" category="paragraph">ONTAP bietet unter Verwendung des SMB3-Protokolls einen stabilen und hochperformanten NAS Storage für Hyper-V Virtual Machines.</block>
  <block id="2efbb65fec3bad963652820bc484fe59" category="paragraph">Wenn eine SVM mit dem CIFS-Protokoll erstellt wird, wird ein CIFS-Server auf der SVM ausgeführt, die Teil der Windows Active Directory Domain ist. SMB-Freigaben können für ein Home Directory verwendet und Hyper-V und SQL Server Workloads hosten. Die folgenden Funktionen von SMB 3.0 werden in ONTAP unterstützt:</block>
  <block id="55339f66c60fe6973920932c368d5b51" category="list-text">Persistente Handles (kontinuierlich verfügbare Dateifreigaben)</block>
  <block id="94713d3b3a69a2fd695bffeb252007d3" category="list-text">Witness-Protokoll</block>
  <block id="adb115272d11e09afdfd8702a652d626" category="list-text">Cluster-Client-Failover</block>
  <block id="f5025b9fcd1b24276af8ec2da3aa4cd7" category="list-text">Erkennung von horizontaler Skalierbarkeit</block>
  <block id="81abcea6e16bc538d9843e8808b2066b" category="list-text">ODX</block>
  <block id="f4ad9c4d51155dbb3fb746c7c497c145" category="list-text">Remote-VSS</block>
  <block id="56a26f5f5f602966c3a3088fc05c7383" category="paragraph">Für die Verwendung von NetApp Storage in NAS-Umgebungen in Windows Server gelten folgende Anforderungen:</block>
  <block id="5f3a2953a5f6d187ed4babba3c754b95" category="list-text">ONTAP Cluster verfügen über eine gültige CIFS-Lizenz.</block>
  <block id="2744237d602ca5ed3f1a6370ed8ebe91" category="list-text">Mindestens ein Aggregat wird erstellt.</block>
  <block id="89917a1bea2d5c48569d9ed0a9526f74" category="list-text">Eine logische Datenschnittstelle (LIF) wird erstellt und die Datenschnittstelle muss für CIFS konfiguriert werden.</block>
  <block id="87a2c188e55f400a5936550cc2f91148" category="list-text">Ein DNS-konfigurierter Windows Active Directory-Domänenserver und Domänenadministratoranmeldeinformationen sind vorhanden.</block>
  <block id="da920c4d42bb062c9dbfd877c84ef133" category="list-text">Jeder Knoten im NetApp-Cluster ist mit dem Windows-Domänencontroller zeitsynchronisiert.</block>
  <block id="f04e0c239705cd7c6e249d3ed6991627" category="section-title">Active Directory-Domänencontroller</block>
  <block id="2e0bc40db7949b4af757ea11110ae955" category="paragraph">Ein NetApp Storage Controller kann einem Active Directory ähnlich wie einem Windows Server angeschlossen und innerhalb dessen betrieben werden. Während der Erstellung der SVM können Sie den DNS konfigurieren, indem Sie den Domain-Namen und die Details des Name Servers angeben. Die SVM versucht, nach einem Active Directory-Domänencontroller zu suchen, indem sie den DNS nach einem Active Directory-/Lightweight Directory Access Protocol-(LDAP-)Server in einer Weise abfragt, die Windows Server ähnelt.</block>
  <block id="dee2aa3e7614e2fc47c6fbbb5cd123b5" category="paragraph">Damit das CIFS-Setup ordnungsgemäß funktioniert, müssen die NetApp Storage Controller mit dem Windows Domain Controller synchronisiert werden. NetApp empfiehlt eine Zeitskew zwischen dem Windows Domain Controller und dem NetApp Storage Controller von maximal fünf Minuten. Es empfiehlt sich, den NTP-Server (Network Time Protocol) für die Synchronisierung des ONTAP-Clusters mit einer externen Zeitquelle zu konfigurieren. Führen Sie zum Konfigurieren des Windows-Domänencontrollers als NTP-Server den folgenden Befehl auf dem ONTAP-Cluster aus:</block>
  <block id="2d60b04429a0c7bcf1dc0021d1c01046" category="list-text">Erstellen Sie eine neue SVM mit aktiviertem NAS-Protokoll CIFS. Eine neue SVM kann mit einer der folgenden Methoden erstellt werden:</block>
  <block id="26ed02718973e705cd69ee6e109400e0" category="list-text">System Manager</block>
  <block id="fc05c3b0a7017fa6841b856230e365bb" category="list-text">Das NetApp PowerShell Toolkit</block>
  <block id="bbee3ec15cf30d6e65d6a020f6d4af1e" category="list-text">Konfigurieren Sie das CIFS-Protokoll</block>
  <block id="8de4bddcd1cd8e4614e52342eef41752" category="list-text">Geben Sie den CIFS-Servernamen an.</block>
  <block id="a817c1f7bf7af809fd90c63b2a8e4447" category="list-text">Geben Sie das Active Directory an, mit dem der CIFS-Server verbunden werden muss. Sie müssen über die Anmeldeinformationen des Domänenadministrators verfügen, um dem CIFS-Server das Active Directory beizutreten.</block>
  <block id="bf8cf0b24d936c2fe2e3681004650bf0" category="list-text">Zuweisung der SVM mit LIFs auf jedem Cluster-Node</block>
  <block id="5d77fb569194f93fead7154a8d1d1bc3" category="list-text">Starten Sie den CIFS-Service auf der SVM.</block>
  <block id="547edc4d2fb156c3e495e6325eae5a4a" category="list-text">Erstellen Sie ein Volume mit NTFS-Sicherheitsstil aus dem Aggregat.</block>
  <block id="1418b945a2d25e4f6514fc52e9d6c7ef" category="list-text">Erstellen Sie auf dem Volume einen qtree (optional).</block>
  <block id="f0bbe023b40d4d11ed83d36a68894cae" category="list-text">Erstellen Sie Shares, die dem Volume oder qtree-Verzeichnis entsprechen, sodass über Windows Server auf diese zugegriffen werden kann. Wählen Sie kontinuierliche Verfügbarkeit für Hyper-V während der Erstellung der Freigabe aktivieren, wenn die Freigabe für Hyper-V-Speicher verwendet wird. Auf diese Weise ist Hochverfügbarkeit für Dateifreigaben möglich.</block>
  <block id="9ef47a6b1a5d19245750aa732054173a" category="list-text">Bearbeiten Sie die erstellte Freigabe, und ändern Sie die Berechtigungen, die für den Zugriff auf die Freigabe erforderlich sind. Die Berechtigungen für die SMB-Freigabe müssen so konfiguriert werden, dass sie den Zugriff auf die Computerkonten aller Server gewährt, die auf diese Freigabe zugreifen.</block>
  <block id="053003e418b94e54d509f16e6a5ac54f" category="paragraph">Führen Sie die folgenden Schritte aus, um die zuvor mit Windows Server erstellte CIFS-Freigabe zu ermitteln:</block>
  <block id="9eb306535570bd14746bad4f0768684f" category="list-text">Melden Sie sich bei Windows Server als Mitglied der Administratorgruppe an.</block>
  <block id="4cac21133029fce3cd01fb6d42f8bedb" category="list-text">Gehen Sie zu run.exe und geben Sie den vollständigen Pfad der CIFS-Freigabe ein, die für den Zugriff auf die Freigabe erstellt wurde.</block>
  <block id="cf48b5bae22bbcd1956b57470865f653" category="list-text">Um die Freigabe dauerhaft auf dem Windows Server zuzuordnen, klicken Sie mit der rechten Maustaste auf Diesen PC, klicken Sie auf Netzwerklaufwerk zuordnen und geben Sie den Pfad der CIFS-Freigabe an.</block>
  <block id="304c9374426bb10ac275b8d050d77a72" category="list-text">Um die MMC in Windows Server zu öffnen, klicken Sie im Abschnitt Extras des Server Managers auf Computerverwaltung.</block>
  <block id="e731a6a9193d81f0140801ba9fb7c316" category="list-text">Klicken Sie auf Weitere Aktionen und Verbinden mit einem anderen Computer. Daraufhin wird das Dialogfeld Computer auswählen geöffnet.</block>
  <block id="92b70da99a18efe7530353abe1820089" category="list-text">Geben Sie den Namen des CIFS-Servers oder die IP-Adresse der SVM-LIF ein, um eine Verbindung zum CIFS-Server herzustellen.</block>
  <block id="2d6dbdc8809506c631cd711d19c21958" category="list-text">Erweitern Sie System-Tools und freigegebene Ordner, um geöffnete Dateien, Sitzungen und Freigaben anzuzeigen und zu verwalten.</block>
  <block id="3a38167e840fd7939f34e194afadd957" category="list-text">Um sicherzustellen, dass es keine Ausfallzeiten gibt, wenn ein Volume von einem Node auf einen anderen oder im Fall eines Node-Ausfalls verschoben wird, empfiehlt NetApp, die Option für die kontinuierliche Verfügbarkeit der Dateifreigabe zu aktivieren.</block>
  <block id="7a98f276625e5966a817f0783982ef86" category="list-text">Bei der Bereitstellung von VMs für Hyper-V über SMB-Umgebungen empfiehlt NetApp, den Copy-Offload auf dem Storage-System zu aktivieren. Auf diese Weise wird die Bereitstellungszeit der VMs verkürzt.</block>
  <block id="8bb66ec4b6ef9437bd1c4c88328028b9" category="list-text">Wenn das Storage-Cluster mehrere SMB-Workloads wie SQL Server, Hyper-V und CIFS-Server hostet, empfiehlt NetApp, verschiedene SMB-Workloads auf separaten SVMs in separaten Aggregaten zu hosten. Diese Konfiguration ist von Vorteil, da für jede dieser Workloads ein einzigartiges Storage-Netzwerk- und Volume-Layout erforderlich ist.</block>
  <block id="ce424317fa59851e16cfbe4cbac13386" category="list-text">Wenn VMs von einer SMB 3.0-Freigabe zu einer anderen migriert werden, empfiehlt NetApp die Aktivierung der CIFS-Offloaded-Funktion auf dem Storage-System, damit die Migration schneller erfolgt.</block>
  <block id="be7159d8a65544f2712c72dc0254f9be" category="list-text">Wenn Sie Volumes für SMB-Umgebungen bereitstellen, müssen die Volumes mit dem NTFS-Sicherheitsstil erstellt werden.</block>
  <block id="bb17c7edf27266c09bc64edcc1299db5" category="list-text">Die Zeiteinstellungen für Knoten im Cluster sollten entsprechend eingerichtet werden. Verwenden Sie NTP, wenn der NetApp-CIFS-Server an der Windows Active Directory-Domäne teilnehmen muss.</block>
  <block id="261b72d9729025ebf489205324a048d5" category="list-text">Persistente Handles funktionieren nur zwischen Nodes in einem HA-Paar.</block>
  <block id="e185fb0d3a49c7a3318c954d8aa25c05" category="list-text">Das Witness-Protokoll funktioniert nur zwischen Nodes in einem HA-Paar.</block>
  <block id="67d7fd26a92bba839041dd45a19cec56" category="list-text">Kontinuierlich verfügbare File Shares werden nur für Hyper-V und SQL Server Workloads unterstützt.</block>
  <block id="fa2e04a9f0cdcb5ad0e9f1e5ad1ada82" category="list-text">Der Multichannel SMB wird ab ONTAP 9.4 unterstützt.</block>
  <block id="f357368b2f195de4f8b4463eed2bf004" category="list-text">RDMA wird nicht unterstützt.</block>
  <block id="f55cb235b18bf3d499f9276cd235bfb6" category="list-text">ReFS wird nicht unterstützt.</block>
  <block id="fb97dc994a0600d8610daca021159b33" category="paragraph">Nano Server benötigt keine zusätzliche Client-Software, um auf Daten auf der CIFS-Freigabe auf einem NetApp-Speicher-Controller zuzugreifen.</block>
  <block id="2be09e0be799992463b5c9542ce31dd8" category="paragraph">Um Dateien von Nano Server auf eine CIFS-Freigabe zu kopieren, führen Sie die folgenden Cmdlets auf dem Remote-Server aus:</block>
  <block id="2e0d4f41db110bf64a66493b31c9b168" category="list-text"><block ref="6f88c516ba3d9ab0cd23b81fc43dd697" prefix="" category="inline-code"></block> Ist die CIFS-Freigabe auf dem NetApp-Speicher-Controller.</block>
  <block id="5e017ad7a7df43934bccc7c27031a140" category="list-text">Führen Sie das folgende Cmdlet aus, um Dateien in Nano Server zu kopieren:</block>
  <block id="d153ba0c2e04ac16dcece009a842216e" category="paragraph">Um den gesamten Inhalt eines Ordners zu kopieren, geben Sie den Ordnernamen an und verwenden Sie den Parameter -Recurse am Ende des Cmdlet.</block>
  <block id="c9f4515194bc7b2927aa17b7d9b4aa25" category="summary">So konfigurieren Sie die Live-Migration von Hyper-V-Speicher</block>
  <block id="a49764977574be237e816e868b16bd9e" category="doc">Bereitstellung von Hyper-V Storage Live Migration</block>
  <block id="587db979ac1f69c45774cb09d6d6670b" category="paragraph">Erfahren Sie, wie Sie die Hyper-V-Speicher-Live-Migration konfigurieren</block>
  <block id="0146445d75059a401441fd65f1bae1e0" category="list-text">Sie müssen über einen Standalone-Hyper-V-Server mit unabhängigem Storage (das oder LUN) oder SMB-Storage (lokal oder von anderen Hyper-V Servern gemeinsam genutzt) verfügen.</block>
  <block id="71c2ecf4798ba7bf00669c37826cb55b" category="inline-link-macro">Live-Migration außerhalb einer Cluster-Umgebung</block>
  <block id="4a8052295fad176b01067c772f7c7e5d" category="list-text">Der Hyper-V-Server muss für die Live-Migration konfiguriert sein. Lesen Sie den Abschnitt zur Bereitstellung in <block ref="0e7f526c94c2d5b9ef424b7a8a1c9587" category="inline-link-macro-rx"></block>.</block>
  <block id="43434644e024fa3073cc9c64dba551d0" category="list-text">Öffnen Sie Hyper-V Manager.</block>
  <block id="3999a6e17f16c7f048684f76a1a59e38" category="list-text">Klicken Sie mit der rechten Maustaste auf eine VM und klicken Sie auf Verschieben.</block>
  <block id="1d838d1a72e03329477d7f4024eec108" category="list-text">Wählen Sie Speicher der virtuellen Maschine verschieben.</block>
  <block id="45d2d7245527e5e6e9be62e79d5a46d8" category="list-text">Wählen Sie Optionen zum Verschieben des Speichers nach Ihren Präferenzen aus.</block>
  <block id="13d0ba1c0320910efd0d040f2d461527" category="list-text">Geben Sie den neuen Speicherort für die VM-Elemente an.</block>
  <block id="6efadc6e298c560f1699100ac97c7378" category="list-text">Überprüfen Sie die Zusammenfassung, und klicken Sie auf OK, um den VM-Speicher zu verschieben.</block>
  <block id="b2d949a2d7f46bb1e39037d38eea14e4" category="summary">ONTAP Storage-Effizienz mit Microsoft Hyper-V</block>
  <block id="bd2eab4ffe0ab602b60386dfdd3ef913" category="paragraph">ONTAP bietet branchenführende Storage-Effizienz für virtualisierte Umgebungen, einschließlich Microsoft Hyper-V. NetApp bietet darüber hinaus Garantie-Programme für Storage-Effizienz.</block>
  <block id="e957a0e8016b502f05bb1966b2ec6064" category="paragraph">NetApp Deduplizierung entfernt Blockduplikate auf Storage Volume-Ebene und speichert nur eine physische Kopie, unabhängig von der Anzahl der logischen Kopien. Daher erzeugt die Deduplizierung die Illusion, dass es mehrere Kopien dieses Blocks gibt. Deduplizierung entfernt automatisch doppelte Datenblöcke auf 4-KB-Blockebene über ein gesamtes Volume. Bei diesem Prozess wird Storage neu beansprucht, um Speicherplatz und potenzielle Performance-Einsparungen zu erzielen, indem die Anzahl der physischen Schreibvorgänge auf die Festplatte reduziert wird. Die Deduplizierung kann in Hyper-V Umgebungen zu einer Platzeinsparung von mehr als 70 % führen.</block>
  <block id="44ee5e4157e5291ffc8bc5d1d9b5959d" category="paragraph">Thin Provisioning bietet eine effiziente Möglichkeit, Storage bereitzustellen, da der Storage nicht vorab zugewiesen wird. Das bedeutet, dass bei der Erstellung eines Volume oder einer LUN mit Thin Provisioning der Speicherplatz im Storage-System nicht genutzt wird. Der Speicherplatz bleibt ungenutzt, bis die Daten auf die LUN oder das Volume geschrieben werden und nur so viel Speicherplatz verwendet wird, wie zur Speicherung der Daten notwendig ist. NetApp empfiehlt die Aktivierung von Thin Provisioning auf dem Volume und die Deaktivierung der LUN-Reservierung.</block>
  <block id="8bc14f0cb78851f59e85c1d447c2c99f" category="section-title">Quality of Service</block>
  <block id="5f4c07a9f526ef861fd661ab7d801ef0" category="paragraph">Mit Storage QoS in Clustered ONTAP können Sie Storage-Objekte gruppieren und Durchsatzbegrenzungen in der Gruppe festlegen. Storage QoS kann verwendet werden, um den Durchsatz auf Workloads zu begrenzen und die Workload-Performance zu überwachen. Storage-Administratoren können so Workloads je nach Organisation, Applikation, Geschäftsbereich oder Produktions- oder Entwicklungsumgebung trennen.</block>
  <block id="8649c492f1adfda9b6ab44990a1a7580" category="paragraph">In Enterprise-Umgebungen bietet Storage QoS folgende Vorteile:</block>
  <block id="ca944817219b4a5d2cb25894ee7bac2d" category="list-text">Verhindert, dass sich Benutzer-Workloads gegenseitig beeinträchtigen</block>
  <block id="1d5819479882e9b1b8133fda618801e5" category="list-text">Sichert kritische Applikationen mit spezifischen Reaktionszeiten, die in IT-als-Service-Umgebungen (ITaaS) erfüllt werden müssen.</block>
  <block id="b06d1116eb0db3e3c8c51d1707dfcf99" category="list-text">Verhindert, dass sich Mandanten gegenseitig beeinträchtigen.</block>
  <block id="1d7dd784ada3e9fe62630025fa82dc80" category="list-text">Vermeidung von Performance-Einbußen durch Hinzufügen neuer Mandanten</block>
  <block id="3c7577ceba90e087950e69f357ce949b" category="paragraph">Mit QoS können Sie die Menge der an eine SVM, ein flexibles Volume, eine LUN oder eine Datei gesendeten I/O begrenzen. Die I/O-Operationen können durch die Anzahl der Operationen oder den Datendurchsatz begrenzt werden.</block>
  <block id="0ac164a0cdedc812bd371348407d8df2" category="paragraph">In der folgenden Abbildung wird SVM mit einer eigenen QoS-Richtlinie dargestellt, die ein maximales Durchsatzlimit durchsetzt.</block>
  <block id="4cd97525569a4c4f332a1fbadea31367" category="inline-image-macro">Storage Virtual Machine mit einer eigenen QoS-Richtlinie,width=319,height=341</block>
  <block id="11751e265ace69b487d325224006c7e5" category="paragraph"><block ref="11751e265ace69b487d325224006c7e5" category="inline-image-macro-rx" type="image"></block></block>
  <block id="42674eda4c194d913efb361684c8b8c6" category="paragraph">Führen Sie zum Konfigurieren einer SVM mit einer eigenen QoS-Richtlinie und für das Monitoring der Richtliniengruppe die folgenden Befehle auf Ihrem ONTAP-Cluster aus:</block>
  <block id="2398ac10b8f12f8d175464af41b6cc27" category="summary">Erfahren Sie mehr über die Bereitstellung von Microsoft Windows Nano Server</block>
  <block id="60aa20a9330bfedece833eb6496e61dc" category="doc">Stellen Sie Nano Server Bereit</block>
  <block id="751f4ae88fb7c0e83c557b594f24f467" category="paragraph">Erfahren Sie mehr über die Bereitstellung von Microsoft Windows Nano Server.</block>
  <block id="ffd8df9c68fc3e1710ed9dc15dec9225" category="paragraph">Um einen Nano-Server als Hyper-V-Host bereitzustellen, führen Sie die folgenden Schritte aus:</block>
  <block id="2c8cf6c7616e7bf4d627844c10ef6914" category="list-text">Kopieren Sie den Ordner NanoServerImageGenerator aus dem Ordner \NanoServer im Windows Server ISO auf die lokale Festplatte.</block>
  <block id="e93afc2a7bab8945ac834e9f955ab2c8" category="list-text">Gehen Sie wie folgt vor, um eine Nano Server VHD/VHDX zu erstellen:</block>
  <block id="f0b4451826a1b48587dce5ccbdf7a1e7" category="list-text">Starten Sie Windows PowerShell als Administrator, navigieren Sie zum kopierten NanoServerImageGenerator-Ordner auf der lokalen Festplatte und führen Sie das folgende Cmdlet aus:</block>
  <block id="3b515a3daeec18784951623c4ef02332" category="list-text">Erstellen Sie eine VHD für den Nano Server als Hyper-V-Host, indem Sie das folgende PowerShell-Cmdlet ausführen. Mit diesem Befehl werden Sie zur Eingabe eines Administratorkennworts für die neue VHD aufgefordert.</block>
  <block id="2ef6d24f20fa6986cbd70ea844d3e16e" category="list-text">Im folgenden Beispiel erstellen wir eine Nano Server VHD mit der Funktion Hyper-V Host mit aktiviertem Failover Clustering. In diesem Beispiel wird eine Nano Server VHD von einem ISO erstellt, das bei f:\ gemountet ist. Die neu erstellte VHD wird in einem Ordner namens NanoServer im Ordner abgelegt, von dem aus das Cmdlet ausgeführt wird. Der Computername ist NanoServer und die resultierende VHD enthält die Standard-Edition von Windows Server.</block>
  <block id="7f165cd40e3ab8b56d69cfdf799ebb23" category="list-text">Konfigurieren Sie mit dem Cmdlet New-NanoServerImage Parameter, die die IP-Adresse, die Subnetzmaske, das Standard-Gateway, den DNS-Server, den Domänennamen, und so weiter.</block>
  <block id="9ba9f17628c7c8e123a1c4d0b154e1e0" category="list-text">Verwenden Sie die VHD in einer VM oder einem physischen Host, um Nano Server als Hyper-V-Host bereitzustellen:</block>
  <block id="7482916c1c66ae16c274920ca01eda80" category="list-text">Erstellen Sie für die Bereitstellung auf einer VM eine neue VM im Hyper-V Manager, und verwenden Sie die in Schritt 3 erstellte VHD.</block>
  <block id="8a1f197a1a3903636fe46d151b55f1a4" category="list-text">Kopieren Sie zur Bereitstellung auf einem physischen Host die VHD auf den physischen Computer, und konfigurieren Sie sie so, dass sie von dieser neuen VHD gestartet wird. Zuerst mounten Sie die VHD, führen Sie bcdboot e:\Windows (wo die VHD unter E:\ gemountet ist), unmounten Sie die VHD, starten Sie den physischen Computer neu, und starten Sie den Nano Server.</block>
  <block id="1a2fbb26db89a11f849f5a17170449b5" category="list-text">Verbinden Sie den Nano Server mit einer Domain (optional):</block>
  <block id="fa966a71fb7e787a036cc8ae2df719c2" category="list-text">Melden Sie sich an einem beliebigen Computer in der Domäne an und erstellen Sie einen Daten-Blob, indem Sie das folgende PowerShell Cmdlet ausführen:</block>
  <block id="5e83141959914643e6e309d32cb02675" category="list-text">Kopieren Sie die odjBLOB-Datei auf den Nano Server, indem Sie die folgenden PowerShell-Cmdlets auf einem Remote-Computer ausführen:</block>
  <block id="9493ca9bfebee08582b48c95f6ad1768" category="list-text">Starten Sie den Nano Server neu.</block>
  <block id="24d48ec9863eca883370d9e471e7dec1" category="section-title">Verbindung mit Nano Server herstellen</block>
  <block id="11e7dceca10309920a26c38df3c5f421" category="paragraph">Gehen Sie wie folgt vor, um eine Remote-Verbindung mit dem Nano Server über PowerShell herzustellen:</block>
  <block id="08d7b9e67b4bbd447b1fa3d9103976e8" category="list-text">Fügen Sie den Nano Server als vertrauenswürdigen Host auf dem Remotecomputer hinzu, indem Sie das folgende Cmdlet auf dem Remoteserver ausführen:</block>
  <block id="0b1f1a420d35761f550f4a9f3b9e2ecb" category="list-text">Wenn die Umgebung sicher ist und Sie alle hinzuzufügenden Hosts als vertrauenswürdige Hosts auf einem Server festlegen möchten, führen Sie den folgenden Befehl aus:</block>
  <block id="06879aa989dffc1c877a92441f72d474" category="list-text">Starten Sie die Remote-Sitzung, indem Sie das folgende Cmdlet auf dem Remote-Server ausführen. Geben Sie das Passwort für den Nano Server an, wenn Sie dazu aufgefordert werden.</block>
  <block id="c5363e6f2cab660a02c04316d65097b5" category="paragraph">Um eine Remote-Verbindung mit dem Nano Server über GUI-Verwaltungstools von einem Remote-Windows-Server herzustellen, führen Sie die folgenden Befehle aus:</block>
  <block id="d97c0d550aa0ad1af548a89c27d9f7ea" category="list-text">Melden Sie sich beim Windows Server als Mitglied der Administratorgruppe an.</block>
  <block id="ff621b1b3a7bcf389fc4c97a55bf9505" category="list-text">Um einen Nano Server Remote vom Server Manager aus zu verwalten, klicken Sie mit der rechten Maustaste auf Alle Server, klicken Sie auf Server hinzufügen, geben Sie die Informationen des Nano Servers an und fügen Sie sie hinzu. Der Nano Server ist nun in der Serverliste aufgelistet. Wählen Sie den Nano Server aus, klicken Sie mit der rechten Maustaste darauf, und beginnen Sie mit der Verwaltung mit den verschiedenen verfügbaren Optionen.</block>
  <block id="2343c80f9e6b008ea95cc15026c9c094" category="list-text">Um Dienste auf einem Nano Server Remote zu verwalten, führen Sie die folgenden Schritte aus:</block>
  <block id="1430c8ff3134db821cc65ac53e010d4d" category="list-text">Öffnen Sie Services im Abschnitt „Tools“ von Server Manager.</block>
  <block id="358fe30f1157bd64dad7a3aba4e40f94" category="list-text">Klicken Sie mit der rechten Maustaste auf Dienste (Lokal).</block>
  <block id="72fa515e487f796b536e03179e63e7db" category="list-text">Klicken Sie auf mit Server verbinden.</block>
  <block id="50ec7cc625733992673123275ff834d6" category="list-text">Geben Sie die Details des Nano-Servers an, um die Dienste auf dem Nano-Server anzuzeigen und zu verwalten.</block>
  <block id="f322fcdc841439901a45a176928f80e0" category="list-text">Wenn die Hyper-V-Rolle auf dem Nano Server aktiviert ist, führen Sie die folgenden Schritte aus, um sie Remote vom Hyper-V Manager zu verwalten:</block>
  <block id="14994643f0c569708527514d0405c1e5" category="list-text">Klicken Sie mit der rechten Maustaste auf Hyper-V Manager.</block>
  <block id="8e0877f5a8a5d540837664b9de1070ca" category="list-text">Klicken Sie auf mit Server verbinden und geben Sie die Details zum Nano Server ein. Jetzt kann der Nano Server als Hyper-V Server verwaltet werden, um darüber hinaus VMs zu erstellen und zu verwalten.</block>
  <block id="b2c5e9024429f1b7f8581d1747d25fd6" category="list-text">Wenn die Failover Clustering-Rolle auf dem Nano Server aktiviert ist, führen Sie die folgenden Schritte aus, um sie vom Failover Cluster Manager aus Remote zu verwalten:</block>
  <block id="31b3efaeee0b35b5bde4f1353f947b93" category="list-text">Öffnen Sie Failover Cluster Manager im Abschnitt „Tools“ von Server Manager.</block>
  <block id="2cb85a898a7a975600dcc1bea85df531" category="list-text">Führen Sie mit dem Nano Server Cluster-bezogene Vorgänge durch.</block>
  <block id="2810ba8ba9d5e4319af85df66ddf2286" category="summary">ONTAP Storage-Sicherheit mit Hyper-V</block>
  <block id="2fae32629d4ef4fc6341f1751b405e45" category="doc">Sicherheit</block>
  <block id="6ca21561032e8c064bc8514ab2e92740" category="paragraph">ONTAP stellt ein sicheres Storage-System für das Windows Betriebssystem bereit.</block>
  <block id="615e826f5576798f024d7959ccefc794" category="section-title">Windows Defender Antivirus</block>
  <block id="2861d3b5f4351fd51979bf98547a050b" category="paragraph">Windows Defender ist eine Anti-Malware-Software, die standardmäßig auf Windows Server installiert und aktiviert ist. Diese Software schützt Windows Server aktiv vor bekannter Malware und kann Malwaredefinitionen regelmäßig über Windows Update aktualisieren. NetApp-LUNs und SMB-Freigaben können mit Windows Defender gescannt werden.</block>
  <block id="a86f08bae2bbd68aa81b64f748f3ccb8" category="inline-link">Windows Defender – Übersicht</block>
  <block id="4b51dd417e2be3fdc9a793c3290e3a63" category="paragraph">Weitere Informationen finden Sie im<block ref="d66127569e4d44a2d0a35bc9d20b1ea7" category="inline-link-rx"></block>.</block>
  <block id="ce71fc775b37c52dabd938808de8d0f9" category="section-title">BitLocker</block>
  <block id="8d9091beebb755bb6e68a4f04d3288f6" category="paragraph">Die BitLocker-Laufwerkverschlüsselung ist eine Datenschutzfunktion, die von Windows Server 2012 fortgesetzt wird. Diese Verschlüsselung schützt physische Festplatten, LUNs und CSVs.</block>
  <block id="0494cad48bf6f5dd2105391f0426af1a" category="paragraph">Vor der Aktivierung von BitLocker muss die CSV-Datei in den Wartungsmodus versetzt werden. Daher empfiehlt NetApp, vor dem Erstellen von VMs auf der CSV-Datei Entscheidungen zur BitLocker-basierten Sicherheit zu treffen, um Ausfallzeiten zu vermeiden.</block>
  <block id="e2e4ca1a6c3bcca61ed5ec3ea10c90c0" category="summary">Erfahren Sie, wie Sie Hyper-V-Replikate mit Windows Server Failover Cluster bereitstellen und konfigurieren.</block>
  <block id="5674fdbee301f444dd0ca3836c682957" category="list-text">Wenn separate Standorte verwendet werden, muss die Firewall an jedem Standort so konfiguriert werden, dass die Kommunikation zwischen dem primären und dem Replikatcluster möglich ist.</block>
  <block id="6cc66ed27b17f9c380f924d533e633d6" category="list-text">Das Replikat-Cluster muss über genügend Speicherplatz zum Speichern der replizierten Workloads verfügen.</block>
  <block id="ff71768b9e039ee5012c181eb29bdc39" category="list-text">Aktivieren Sie Firewall-Regeln für alle Knoten eines Clusters. Führen Sie das folgende PowerShell-Cmdlet mit Administratorrechten auf allen Knoten sowohl im primären als auch im Replikatcluster aus.</block>
  <block id="ac005fe375c3903e88ed3801a8646b0a" category="list-text">Konfigurieren Sie das Replikat-Cluster.</block>
  <block id="3a57da55cdc319b0580af329b8776df6" category="list-text">Konfigurieren Sie den Hyper-V Replica Broker mit einem NetBIOS-Namen und einer IP-Adresse, die als Verbindungspunkt zu dem Cluster verwendet werden sollen, der als Replikatcluster verwendet wird.</block>
  <block id="643ce0b9206c6c138b36b79315a468f2" category="list-text">Öffnen Sie Failover Cluster Manager.</block>
  <block id="52243c461a19e74a8e4b64d801979142" category="list-text">Erweitern Sie den Cluster, klicken Sie auf Rollen, und klicken Sie im Bereich Aktionen auf Rolle konfigurieren.</block>
  <block id="f87ee1c8e6945b31ba45ee813ea7c124" category="list-text">Wählen Sie auf der Seite Rolle auswählen die Option Hyper-V Replica Broker aus.</block>
  <block id="fdbea454936dca227734d3f978334254" category="list-text">Geben Sie den NetBIOS-Namen und die IP-Adresse an, die als Verbindungspunkt zum Cluster (Client-Zugriffspunkt) verwendet werden sollen.</block>
  <block id="bb8627cc57d25f02546b85f38d97ae04" category="list-text">Dieser Prozess erstellt eine Hyper-V Replica Broker-Rolle. Stellen Sie sicher, dass sie erfolgreich online ist.</block>
  <block id="9b11f91c42607554f82ca0c4a5739d14" category="list-text">Konfigurieren Sie die Replikationseinstellungen.</block>
  <block id="662af7cfca9243c46223a416df05d774" category="list-text">Klicken Sie mit der rechten Maustaste auf den Replikatbroker, der in den vorherigen Schritten erstellt wurde, und klicken Sie auf Replikationseinstellungen.</block>
  <block id="45375ee44c681920240b13f25bf53d9e" category="list-text">Wählen Sie Diesen Cluster als Replikatserver aktivieren aus.</block>
  <block id="88aeceb8128e3b94092a3e66ac2236db" category="list-text">Wählen Sie im Abschnitt Autorisierung und Speicher die Server aus, die VMs auf dieses Cluster replizieren dürfen. Geben Sie außerdem den Standardspeicherort an, an dem die replizierten VMs gespeichert werden.</block>
  <block id="3d9241b7079c08f25c850a7d8ac5eebd" category="inline-link-macro">Replikat außerhalb einer Cluster-Umgebung</block>
  <block id="3077be518a570235f65497aa5b227e9b" category="paragraph">Die Replikation ähnelt dem im Abschnitt beschriebenen Prozess <block ref="85d27647a4d4ffb042b0371e9654ea4c" category="inline-link-macro-rx"></block>.</block>
  <block id="f8b7d7bb25e13b6b771e58da1fa078cb" category="summary">Bereitstellung von ONTAP Storage für Windows und Hyper-V in SAN-Umgebungen</block>
  <block id="24294828f9d26e2b73b5b1e0eda7a2d0" category="paragraph">ONTAP SVMs unterstützen die Blockprotokolle iSCSI und FC. Wenn eine SVM mit dem Blockprotokoll iSCSI oder FC erstellt wird, erhält die SVM entweder einen iSCSI Qualified Name (IQN) oder einen FC Worldwide Name (WWN). Diese Kennung stellt Hosts, die auf den NetApp-Block-Storage zugreifen, ein SCSI-Ziel dar.</block>
  <block id="7f33f1b99e752967fce41fe3dec9f051" category="section-title">Bereitstellung von NetApp-LUNs auf Windows Server</block>
  <block id="025d3d39bb1937ffe40a12b1917e28b6" category="paragraph">Der Einsatz von NetApp Storage in SAN-Umgebungen in Windows Server hat folgende Anforderungen:</block>
  <block id="3e306b32d1acc6cfcca681b93d8d57cd" category="list-text">Ein NetApp Cluster ist mit einem oder mehreren NetApp Storage Controllern konfiguriert.</block>
  <block id="3e0ea625c6f590606b5242be6455347c" category="list-text">Der NetApp-Cluster oder die Storage-Controller verfügen über eine gültige iSCSI-Lizenz.</block>
  <block id="5bf7c7c03ed76506f8822badb526f0a8" category="list-text">Es sind iSCSI- und/oder FC-konfigurierte Ports verfügbar.</block>
  <block id="46434438c1896350bc085c1a67785198" category="list-text">FC-Zoning wird auf einem FC-Switch für FC durchgeführt.</block>
  <block id="1e704db41d570618bd723b41d0db395e" category="list-text">Eine SVM sollte über eine LIF pro Ethernet-Netzwerk oder Fibre Channel Fabric auf jedem Storage Controller verfügen, der Daten über iSCSI oder Fibre Channel bereitstellen soll.</block>
  <block id="daa700de515c2e157d34b0e12c154478" category="list-text">Erstellen einer neuen SVM mit aktivierter Blockprotokoll-iSCSI und/oder FC Eine neue SVM kann mit einer der folgenden Methoden erstellt werden:</block>
  <block id="ec642b047305fbfe6db973f313768eae" category="list-text">CLI-Befehle auf NetApp Storage</block>
  <block id="43736124eda42b8dec979b188b9d891b" category="list-text">Konfigurieren Sie das iSCSI- und/oder FC-Protokoll.</block>
  <block id="51dbb9050f89d2c5cbfda6a46bb48963" category="list-text">Starten Sie den iSCSI- und/oder FC-Service auf der SVM.</block>
  <block id="5058f1af8388633f609cadb75a75dc9d" category="paragraph">.</block>
  <block id="80967cdacf7a48c49276c07097765c46" category="list-text">Erstellen Sie iSCSI- und/oder FC-Port-Sets mit den SVM LIFs.</block>
  <block id="2d59cbd9d3169fc27e47164a0bf60c42" category="list-text">Erstellen Sie eine iSCSI- und/oder FC-Initiatorgruppe für Windows mithilfe des erstellten Portgruppe.</block>
  <block id="b6efc79e22c0381d9fcc8c3f40ff674f" category="list-text">Fügen Sie der Initiatorgruppe einen Initiator hinzu. Der Initiator ist der IQN für iSCSI und der WWPN für FC. Sie können von Windows Server abgefragt werden, indem das PowerShell Cmdlet Get-InitiatorPort ausgeführt wird.</block>
  <block id="ab39c1b30c8045ee5b2419b682b64e5c" category="paragraph">Der IQN für iSCSI auf Windows Server kann auch in der Konfiguration der iSCSI-Initiator-Eigenschaften geprüft werden.</block>
  <block id="83f0120a854049de17bd46bb97e5c5fa" category="list-text">Erstellen Sie eine LUN mit dem Assistenten zum Erstellen einer LUN und verknüpfen Sie sie mit der erstellten Initiatorgruppe.</block>
  <block id="186c31b1cbf72f8c54f9c6a0edca1cfe" category="paragraph">Windows Server verwendet Asymmetrical Logical Unit Access (ALUA) Extension MPIO, um direkte und indirekte Pfade zu LUNs zu ermitteln. Obwohl jede LIF, die einer SVM gehört, Lese-/Schreibanforderungen für ihre LUNs akzeptiert, sind tatsächlich nur einer der Cluster-Nodes Eigentümer der Festplatten, die diese LUN zu einem beliebigen Zeitpunkt sichern. Dadurch werden die verfügbaren Pfade zu einer LUN in zwei Typen unterteilt, direkt oder indirekt, wie in der folgenden Abbildung dargestellt.</block>
  <block id="7dc177b120da05827161978955c8104e" category="paragraph">Ein direkter Pfad für eine LUN ist ein Pfad, auf dem sich die LIFs einer SVM und die LUN, auf die zugegriffen wird, auf demselben Node befinden. Um von einem physischen Ziel-Port zur Festplatte zu wechseln, muss das Cluster-Netzwerk nicht durchlaufen werden.</block>
  <block id="b456728920702bd70bc705a1af06be72" category="paragraph">Bei den indirekten Pfaden handelt es sich um Datenpfade, auf denen sich die LIFs einer SVM und die aufgerufene LUN auf unterschiedlichen Nodes befinden. Um von einem physischen Ziel-Port auf die Festplatte zu gelangen, müssen Daten das Cluster-Netzwerk durchlaufen.</block>
  <block id="6b08a0deb0bef5f6429e5fb7bab4da29" category="inline-image-macro">Mehrere Pfade in einer SAN-Umgebung,width=624,height=232</block>
  <block id="e5106eaf1ca3b8ff0d6a4555b38845de" category="paragraph"><block ref="e5106eaf1ca3b8ff0d6a4555b38845de" category="inline-image-macro-rx" type="image"></block></block>
  <block id="7e6c7f4878b6fa562299bd45be030c4d" category="section-title">MPIO</block>
  <block id="90881f9a41c6c14a994dd6d410b5eae3" category="section-title">Aktivieren Sie MPIO</block>
  <block id="1f4614ecf24e855792cb4a72e2ca4a86" category="paragraph">Führen Sie die folgenden Schritte aus, um MPIO auf Windows Server zu aktivieren:</block>
  <block id="bb35cb72ffe2e8dd1a02cf65b86c6cd3" category="list-text">Starten Sie Server Manager.</block>
  <block id="d580309cd21fec5dd6a0fc9e095dacbe" category="list-text">Klicken Sie im Abschnitt Verwalten auf Rollen und Funktionen hinzufügen.</block>
  <block id="19c0c759d805032bc93817553dbc8ef9" category="list-text">Wählen Sie auf der Seite Funktionen auswählen die Option Multipath-E/A aus</block>
  <block id="63c2082592ff76eac0541130a24e7670" category="section-title">Konfigurieren Sie MPIO</block>
  <block id="655995aa16b525227f20b4e6b55a22cf" category="paragraph">Wenn Sie das iSCSI-Protokoll verwenden, müssen Sie Windows Server anweisen, Multipath-Unterstützung auf iSCSI-Geräte in den MPIO-Eigenschaften anzuwenden.</block>
  <block id="3cdb8cf9c5fe85e070ceba959169e83f" category="paragraph">Führen Sie die folgenden Schritte aus, um MPIO auf Windows Server zu konfigurieren:</block>
  <block id="bd27f39c0a288d0682b789a806d1eec7" category="list-text">Melden Sie sich bei Windows Server als Mitglied der Administratorgruppe an.</block>
  <block id="a267872bfc142a132ea159cb0904930f" category="list-text">Klicken Sie im Abschnitt Extras auf MPIO.</block>
  <block id="46205f4b1d4b254565cae2fd6970a3a6" category="list-text">Wählen Sie unter MPIO-Eigenschaften auf Mehrpfade ermitteln die Option Support für iSCSI-Geräte hinzufügen aus, und klicken Sie auf Hinzufügen. Sie werden anschließend aufgefordert, den Computer neu zu starten.</block>
  <block id="8c372bcf6ff4084f9e072d698d449b18" category="list-text">Starten Sie Windows Server neu, um das MPIO-Gerät im Abschnitt MPIO-Geräte der MPIO-Eigenschaften anzuzeigen.</block>
  <block id="28f21ae88e9953ac3d80686cfb49832b" category="section-title">Konfigurieren Sie iSCSI</block>
  <block id="a12e2e6108a8b8cb032c190ebe798d79" category="paragraph">Führen Sie die folgenden Schritte aus, um iSCSI-Blockspeicher auf Windows Server zu erkennen:</block>
  <block id="623f636e32ba5729ab907ad3eeeb5adb" category="list-text">Klicken Sie im Abschnitt Extras auf iSCSI-Initiator.</block>
  <block id="d259349ad54c81b69ffb57d045a7c135" category="list-text">Klicken Sie auf der Registerkarte Ermittlung auf Portal ermitteln.</block>
  <block id="9055f8c4230cf1268018d9b81d1c30e0" category="list-text">Geben Sie die IP-Adresse der LIFs für die SVM an, die für das NetApp-Storage-Protokoll für SAN erstellt wurden. Klicken Sie auf Erweitert, konfigurieren Sie die Informationen auf der Registerkarte Allgemein, und klicken Sie auf OK.</block>
  <block id="56fe767f1eb1a3401bb37455b6e1c804" category="list-text">Der iSCSI-Initiator erkennt das iSCSI-Ziel automatisch und listet es auf der Registerkarte Ziele auf.</block>
  <block id="f6357f1aad932cb8ef210a786e72e1e9" category="list-text">Wählen Sie das iSCSI-Ziel unter ermittelte Ziele aus. Klicken Sie auf Verbinden, um das Fenster mit Ziel verbinden zu öffnen.</block>
  <block id="28d26404caba20cb28577f61c51a7e1f" category="list-text">Sie müssen mehrere Sitzungen vom Windows Server-Host zu den Ziel-iSCSI-LIFs auf dem NetApp-Storage-Cluster erstellen. Um das zu tun, führen Sie folgende Schritte aus:</block>
  <block id="3aef6deb3c61c368bc66313781c8e351" category="list-text">Wählen Sie im Fenster mit Ziel verbinden die Option MPIO aktivieren aus, und klicken Sie auf Erweitert.</block>
  <block id="d754cac9ca73d1113c5fa5908ca2dc97" category="list-text">Wählen Sie unter Erweiterte Einstellungen auf der Registerkarte Allgemein den lokalen Adapter als Microsoft iSCSI-Initiator aus und wählen Sie Initiator-IP und Zielportal-IP aus.</block>
  <block id="fdcb9587e9c4d393ea337f94fb0e1f5f" category="list-text">Sie müssen auch über den zweiten Pfad eine Verbindung herstellen. Wiederholen Sie daher Schritt 5 bis Schritt 8, wählen Sie jedoch dieses Mal die Initiator-IP und die Ziel-Portal-IP für den zweiten Pfad aus.</block>
  <block id="cb35c153406b8cdeb5963bd9a0ceb501" category="list-text">Wählen Sie das iSCSI-Ziel im Hauptfenster iSCSI-Eigenschaften unter ermittelte Ziele aus, und klicken Sie auf Eigenschaften.</block>
  <block id="2a3977b58760a2b4c9e557175a5e494e" category="list-text">Das Fenster Eigenschaften zeigt an, dass mehrere Sitzungen erkannt wurden. Wählen Sie die Sitzung aus, klicken Sie auf Geräte, und klicken Sie dann auf MPIO, um die Load-Balancing-Richtlinie zu konfigurieren. Alle für das Gerät konfigurierten Pfade werden angezeigt und alle Load-Balancing-Richtlinien werden unterstützt. NetApp empfiehlt im Allgemeinen Round Robin mit Teilmenge. Diese Einstellung ist der Standard für Arrays mit aktiviertem ALUA. Round Robin ist der Standard für aktiv-aktiv-Arrays, die ALUA nicht unterstützen.</block>
  <block id="7b7b8262c038321869fc17ba522bbe07" category="paragraph">Führen Sie die folgenden Schritte aus, um iSCSI- oder FC-Blockspeicher auf Windows Server zu erkennen:</block>
  <block id="7554b29976a023cca6089775e3a7fd73" category="list-text">Klicken Sie im Abschnitt Extras des Server-Managers auf Computerverwaltung.</block>
  <block id="b7e726cedaa814db32dbfe4395773cc4" category="list-text">Klicken Sie in der Computerverwaltung im Abschnitt Speicherverwaltung auf Datenträgerverwaltung, und klicken Sie dann auf Weitere Aktionen und Datenträger erneut scannen. Dadurch werden die RAW-iSCSI-LUNs angezeigt.</block>
  <block id="8fd13d2e47a19c784864b40dc7e4621f" category="list-text">Klicken Sie auf die ermittelte LUN, und stellen Sie sie online. Wählen Sie anschließend Datenträger mit der MBR- oder GPT-Partition initialisieren aus. Erstellen Sie ein neues einfaches Volume, indem Sie die Volume-Größe und den Laufwerksbuchstaben angeben und es mit FAT, FAT32, NTFS oder dem Resilient File System (ReFS) formatieren.</block>
  <block id="31ef353687ab7c0fb280206b2eba67d0" category="list-text">NetApp empfiehlt die Aktivierung von Thin Provisioning auf den Volumes, auf denen die LUNs gehostet werden.</block>
  <block id="de065768c0f0b724b1926b25d92fdcef" category="list-text">Um Multipathing-Probleme zu vermeiden, empfiehlt NetApp, entweder alle 10-GB-Sitzungen oder alle 1-GB-Sitzungen für eine bestimmte LUN zu verwenden.</block>
  <block id="ed4eb5407c23e138e99781446b4170d8" category="list-text">NetApp empfiehlt, dass Sie bestätigen, dass ALUA auf dem Storage-System aktiviert ist. ALUA ist auf ONTAP standardmäßig aktiviert.</block>
  <block id="df0f70c4f2bca2f5f0216ab40982d08a" category="list-text">Aktivieren Sie auf dem Windows-Server-Host, dem die NetApp-LUN zugeordnet ist, iSCSI-Dienst (TCP-in) für Inbound- und iSCSI-Dienst (TCP-out) für Outbound in den Firewall-Einstellungen. Mit diesen Einstellungen kann iSCSI-Datenverkehr zum und vom Hyper-V-Host und NetApp-Controller geleitet werden.</block>
  <block id="defe090ca065bb89790c1a0b2d533ad0" category="section-title">Bereitstellung von NetApp-LUNs auf dem Nano Server</block>
  <block id="a7b1fc24d300b03d303a4e66fd5eaf19" category="inline-link-macro">Stellen Sie Nano Server Bereit.</block>
  <block id="c56e09ae358fe6206c8e4059eb76252b" category="paragraph">Zusätzlich zu den im vorherigen Abschnitt genannten Voraussetzungen muss die Speicherrolle von der Nano-Server-Seite aus aktiviert werden. Beispielsweise muss Nano Server mit der Option -Storage bereitgestellt werden. Informationen zum Bereitstellen von Nano Server finden Sie im Abschnitt „<block ref="00a4a5119b1e43564baa188fc895d5ff" category="inline-link-macro-rx"></block>„</block>
  <block id="152dbede177bc5347922fdc2b306adf2" category="paragraph">Gehen Sie wie folgt vor, um NetApp-LUNs auf einem Nano-Server bereitzustellen:</block>
  <block id="fe02f402a046a8784957e096b11835e3" category="list-text">Stellen Sie eine Remote-Verbindung zum Nano Server her, indem Sie die Anweisungen im Abschnitt „<block ref="60e7ef5a283935545371795472ceb490" category="inline-link-macro-rx"></block>.“</block>
  <block id="1b3c1a51c3603283ec4f7ab199a89ad3" category="list-text">Führen Sie zum Konfigurieren von iSCSI die folgenden PowerShell-Cmdlets auf dem Nano Server aus:</block>
  <block id="5e885af04f617a09b29600bd064e0a82" category="list-text">Fügen Sie der Initiatorgruppe einen Initiator hinzu.</block>
  <block id="8a73c7acbdf9fc31a87cebe8de47a176" category="list-text">Konfigurieren Sie MPIO.</block>
  <block id="8e08d5639ddc5bf9f71aeea2f9df6706" category="list-text">Block-Storage erkennen</block>
  <block id="ad04ba98fa2a26c6e2a4f71314b1c4c7" category="section-title">Booten über das SAN</block>
  <block id="e29512aa6180bca309f145289f08337c" category="paragraph">Ein physischer Host (Server) oder eine Hyper-V-VM kann das Windows-Serverbetriebssystem direkt von einer NetApp-LUN starten, anstatt von der internen Festplatte. Beim Ansatz „vom SAN booten“ befindet sich das BS-Image, von dem aus gebootet werden soll, auf einer NetApp-LUN, die mit einem physischen Host oder einer physischen VM verbunden ist. Bei einem physischen Host ist der HBA des physischen Hosts so konfiguriert, dass er die NetApp-LUN zum Booten verwendet. Bei einer VM wird die NetApp-LUN zum Booten als Pass-Through-Disk angehängt.</block>
  <block id="2cf07cb81d5366158ac32b0d02e3bf39" category="paragraph">Mithilfe der NetApp FlexClone Technologie können Boot-LUNs mit einem Betriebssystem-Image sofort geklont und mit den Servern und VMs verbunden werden, um schnell saubere Betriebssystem-Images zu liefern, wie in der folgenden Abbildung dargestellt.</block>
  <block id="e60c0a021a1a11ca0fd66050c818b441" category="inline-image-macro">Booten von LUNs mit NetApp FlexClone,width=561,height=357</block>
  <block id="79f4f54e05eb2f1c2084e0542e66f182" category="paragraph"><block ref="79f4f54e05eb2f1c2084e0542e66f182" category="inline-image-macro-rx" type="image"></block></block>
  <block id="738e6a96e2bf795b59ac62d4827a779e" category="list-text">Der physische Host (Server) verfügt über einen geeigneten iSCSI- oder FC-HBA.</block>
  <block id="7957c1b3b06b3ca5c17a05da5e59c9ec" category="list-text">Sie haben einen geeigneten HBA-Gerätetreiber für den Server heruntergeladen, der Windows Server unterstützt.</block>
  <block id="1810f78aa22a9972644e65ac2cdad107" category="list-text">Der Server verfügt über ein geeignetes CD/DVD-Laufwerk oder ein virtuelles Medium zum Einlegen des Windows Server-ISO-Images, und der HBA-Gerätetreiber wurde heruntergeladen.</block>
  <block id="0af744e0f07c6b198e478b2ba83aca3a" category="list-text">Eine NetApp iSCSI- oder FC-LUN wird auf dem NetApp Storage Controller bereitgestellt.</block>
  <block id="30e2bee2ccadbae82d2d5aeddd5b4078" category="paragraph">So konfigurieren Sie das Booten von SAN für einen physischen Host:</block>
  <block id="7d90eb9f56cbfbfe2930a2d651f51f3a" category="list-text">Aktivieren Sie BootBIOS auf dem Server-HBA.</block>
  <block id="034cb1858f7a3762e67e18bff38aa62a" category="list-text">Konfigurieren Sie für iSCSI-HBAs die Initiator-IP, den iSCSI-Knotennamen und den Adapter-Startmodus in den Boot-BIOS-Einstellungen.</block>
  <block id="aeae1f3a4453e2d3e333de58d3f28c4a" category="list-text">Wenn Sie auf einem NetApp Storage Controller eine Initiatorgruppe für iSCSI und/oder FC erstellen, fügen Sie der Gruppe den Server-HBA-Initiator hinzu. Der HBA-Initiator des Servers ist der WWPN für den FC-HBA oder den iSCSI-Knotennamen für iSCSI-HBA.</block>
  <block id="1ec419dd357b1eacb28eec8cfd657908" category="list-text">Erstellen Sie eine LUN auf dem NetApp Storage Controller mit der LUN-ID 0 und verknüpfen Sie sie mit der Initiatorgruppe, die im vorherigen Schritt erstellt wurde. Diese LUN dient als Boot-LUN.</block>
  <block id="52ec2612ed3c17b60d5c662122410366" category="list-text">Beschränken Sie den HBA auf einen einzelnen Pfad zur Boot-LUN. Nach der Installation von Windows Server auf der Boot-LUN können zusätzliche Pfade hinzugefügt werden, um die Multipathing-Funktion auszunutzen.</block>
  <block id="0fa69ac9076a9031393e6f5d4aa909d5" category="list-text">Konfigurieren Sie die LUN mithilfe des HBA-BootBIOS-Dienstprogramms als Startgerät.</block>
  <block id="58347d33bbc0409dce93e92c5983619a" category="list-text">Starten Sie den Host neu, und rufen Sie das Host-BIOS-Dienstprogramm auf.</block>
  <block id="1b9013d7a416c62e05336b3791451d0d" category="list-text">Konfigurieren Sie das Host-BIOS so, dass die Start-LUN zum ersten Gerät in der Startreihenfolge wird.</block>
  <block id="3472e7b0e2c4389366cf3e0f09f6376d" category="list-text">Starten Sie über die Windows Server-ISO die Installation.</block>
  <block id="8423662d97649ae1012d665b6594f514" category="list-text">Wenn die Installation fragt: „Wo möchten Sie Windows installieren?“, klicken Sie unten im Installationsbildschirm auf Treiber laden, um die Seite Treiber für Installation auswählen zu starten. Geben Sie den Pfad des zuvor heruntergeladenen HBA-Gerätetreibers an, und beenden Sie die Installation des Treibers.</block>
  <block id="4d0de28cb0fe5a517374a0c7fdaaff27" category="list-text">Nun muss die zuvor erstellte Boot-LUN auf der Windows-Installationsseite sichtbar sein. Wählen Sie die Start-LUN für die Installation von Windows Server auf der Boot-LUN aus, und beenden Sie die Installation.</block>
  <block id="0dbc8e7afe6136a3b34471c047b28f83" category="paragraph">Gehen Sie wie folgt vor, um das Booten über das SAN für eine VM zu konfigurieren:</block>
  <block id="6af97eac32eef9a62687eac41262ae48" category="list-text">Wenn Sie eine Initiatorgruppe für iSCSI oder FC auf einem NetApp-Speichercontroller erstellen, fügen Sie dem Controller den IQN für iSCSI oder den WWN für FC des Hyper-V-Servers hinzu.</block>
  <block id="c18de5a7c8558c6b9b749ea9d61336da" category="list-text">Erstellen Sie LUNs oder LUN-Klone auf dem NetApp Storage Controller und verknüpfen Sie sie mit der Initiatorgruppe, die im vorherigen Schritt erstellt wurde. Diese LUNs dienen als Boot-LUNs für die VMs.</block>
  <block id="dac33c93e3d9b5c90a34c39aa579711a" category="list-text">Erkennen Sie die LUNs auf dem Hyper-V-Server, schalten Sie sie online und initialisieren Sie sie.</block>
  <block id="441d10873880501fa764914d0bf1f044" category="list-text">Versetzen Sie die LUNs in den Offline-Modus.</block>
  <block id="0490d2908eb59e831010f6c50ebdf6ea" category="list-text">Erstellen Sie VMs mit der Option Virtuelle Festplatte später anhängen auf der Seite Virtuelle Festplatte verbinden.</block>
  <block id="2b2b3b281225dec976273bad7f4dacf0" category="list-text">Fügen Sie eine LUN als Pass-Through-Disk zu einer VM hinzu.</block>
  <block id="ab82d3a85d9187f45c1cac0bd2922647" category="list-text">Öffnen Sie die VM-Einstellungen.</block>
  <block id="d2e03542bea91031de7772d6cb0b7b13" category="list-text">Klicken Sie auf IDE-Controller 0, wählen Sie Festplatte aus, und klicken Sie auf Hinzufügen. Wenn Sie IDE Controller 0 auswählen, ist diese Festplatte das erste Startgerät für die VM.</block>
  <block id="1904d13d92aac636b46007dec34eb357" category="list-text">Wählen Sie in den Festplattenoptionen physische Festplatte aus, und wählen Sie eine Festplatte aus der Liste als Pass-Through-Disk aus. Bei den Festplatten handelt es sich um die in den vorherigen Schritten konfigurierten LUNs.</block>
  <block id="19e47f17e14687afa5f6c07833dbd7aa" category="list-text">Installieren Sie Windows Server auf dem Pass-Through-Datenträger.</block>
  <block id="30f837081d56a5f1605e52592a38d5f4" category="list-text">Stellen Sie sicher, dass die LUNs offline sind. Andernfalls kann die Festplatte nicht als Pass-Through-Disk zu einer VM hinzugefügt werden.</block>
  <block id="9865f9e3af02eaca8b480229a22515f4" category="list-text">Wenn mehrere LUNs vorhanden sind, achten Sie darauf, die Datenträgernummer der LUN in der Datenträgerverwaltung zu notieren. Dies ist notwendig, da für die VM aufgeführte Festplatten mit der Festplattennummer aufgeführt werden. Außerdem basiert die Auswahl der Festplatte als Pass-Through-Disk für die VM auf dieser Plattennummer.</block>
  <block id="15b11e66e58b25dffc74ff36f6529d4d" category="list-text">NetApp empfiehlt die Verwendung von ONTAP MPIO, das auf dem Host für Storage-Zwecke konfiguriert ist.</block>
  <block id="154e178d8f948264ecf34dcfd8ad3112" category="summary">In diesem Anhang wird die Verwendung der Hyper-V Live-Migration außerhalb einer Cluster-Umgebung beschrieben</block>
  <block id="3a37015d23bf5b95b8aaeeb60e7149f6" category="paragraph">In diesem Abschnitt wird die Bereitstellung der Hyper-V Live-Migration außerhalb einer Cluster-Umgebung beschrieben.</block>
  <block id="34076d85579973b175bda646666488f5" category="list-text">Standalone Hyper-V Server mit unabhängigem Storage oder Shared SMB Storage.</block>
  <block id="6d3867adf71fde2bd2e28c3530f980e2" category="list-text">Die Hyper-V-Rolle, die sowohl auf den Quell- als auch auf den Zielservern installiert ist.</block>
  <block id="860a24a5665c79d337512cf7091c1f27" category="list-text">Beide Hyper-V-Server gehören zur gleichen Domäne oder zu Domänen, die sich gegenseitig vertrauen.</block>
  <block id="599b01a5eeab6af8dcd5d22203b25b3b" category="paragraph">Um eine Live-Migration in einer Umgebung ohne Cluster durchzuführen, konfigurieren Sie Hyper-V Quell- und Zielserver so, dass sie Live-Migrationsvorgänge senden und empfangen können. Führen Sie auf beiden Hyper-V-Servern die folgenden Schritte aus:</block>
  <block id="5c8aa736f4d71f22237d67e44338e090" category="list-text">Klicken Sie unter Aktionen auf Hyper-V-Einstellungen.</block>
  <block id="146cc3da35b3cb4807f19e5c342f3d0b" category="list-text">Klicken Sie auf Live-Migrationen, und wählen Sie eingehende und ausgehende Live-Migrationen aktivieren aus.</block>
  <block id="000104d6631c201d8476c728987584e9" category="list-text">Wählen Sie aus, ob Live-Migrationsverkehr auf einem beliebigen verfügbaren Netzwerk oder nur in bestimmten Netzwerken zugelassen werden soll.</block>
  <block id="bbba185068a3a5aa887f5064ee5fc3de" category="list-text">Optional können Sie das Authentifizierungsprotokoll und die Leistsoptionen im Abschnitt „Erweitert“ von Live-Migrationen konfigurieren.</block>
  <block id="0d91ef025d958a7706b6abf25f2160a5" category="list-text">Wenn CredSSP als Authentifizierungsprotokoll verwendet wird, müssen Sie sich vom Hyper-V-Zielserver beim Hyper-V-Quellserver anmelden, bevor Sie die VM verschieben.</block>
  <block id="4671ab6d9ed8f69134f49812ab7a7c35" category="list-text">Wenn Kerberos als Authentifizierungsprotokoll verwendet wird, konfigurieren Sie die eingeschränkte Delegierung. Hierfür ist der Zugriff auf den Active Directory-Domänencontroller erforderlich. Führen Sie zum Konfigurieren der Delegierung die folgenden Schritte aus:</block>
  <block id="3c3a519ac45fd3a37ae321645897af94" category="list-text">Melden Sie sich beim Active Directory-Domänencontroller als Administrator an.</block>
  <block id="15be526c72b91c88e16f64f6c832eb6b" category="list-text">Klicken Sie im Abschnitt Extras auf Active Directory-Benutzer und -Computer.</block>
  <block id="444f95dff8ac9b4c8c6335d66d6ae4a7" category="list-text">Erweitern Sie die Domäne, und klicken Sie auf Computer.</block>
  <block id="a07ecc7f9ab368b241540687975a30aa" category="list-text">Wählen Sie den Hyper-V-Quellserver aus der Liste aus, klicken Sie mit der rechten Maustaste darauf, und klicken Sie auf Eigenschaften.</block>
  <block id="a3a23c7b6ce827c11b3cdb698108c307" category="list-text">Wählen Sie auf der Registerkarte Delegation die Option Diesen Computer nur für die Delegation an bestimmte Dienste vertrauen aus.</block>
  <block id="4070350eb1a20fec8173f5ee230b8566" category="list-text">Wählen Sie Nur Kerberos Verwenden Aus.</block>
  <block id="36d92877123542287dffc5912e198436" category="list-text">Klicken Sie auf Hinzufügen, um den Assistenten zum Hinzufügen von Services zu öffnen.</block>
  <block id="2a206fdec4cfdbdd63ff3dd2265bbb34" category="list-text">Klicken Sie unter Dienste hinzufügen auf Benutzer und Computer, um Benutzer oder Computer auswählen** zu öffnen.**</block>
  <block id="6fb0c1f70349caf1f37bdb1c7b73efe4" category="list-text">Geben Sie den Hyper-V-Zielservernamen ein, und klicken Sie auf OK.</block>
  <block id="261db7be66ebd6db420e928cb4b402f7" category="list-text">Um VM-Speicher zu verschieben, wählen Sie CIFS aus.</block>
  <block id="f4288db12d8f66e52ec2ac189f491e82" category="list-text">Um VMs zu verschieben, wählen Sie den Microsoft Virtual System Migration Service aus.</block>
  <block id="9149d965e2ccf728685f671fb1046ab9" category="list-text">Klicken Sie auf der Registerkarte Delegation auf OK.</block>
  <block id="9bc766cbcc4911cf673963ca303576bc" category="list-text">Wählen Sie im Ordner Computer den Hyper-V-Zielserver aus der Liste aus, und wiederholen Sie den Vorgang. Geben Sie unter Benutzer oder Computer auswählen den Hyper-V-Quellservernamen an.</block>
  <block id="5868e6c6d4568bb2c1245c731ab1f681" category="list-text">Verschieben Sie die VM.</block>
  <block id="ab102f10122268c6b0663d464d3192da" category="list-text">Wählen Sie „Virtuelle Maschine verschieben“.</block>
  <block id="8b95cb0524fdc80c99bdf8cb976556d2" category="list-text">Geben Sie den Hyper-V-Zielserver für die VM an.</block>
  <block id="25b70b154b120ba061f03c292637a9a4" category="list-text">Wählen Sie die Optionen zum Verschieben. Wählen Sie für Shared Live Migration nur die virtuelle Maschine verschieben. Wählen Sie für „Shared Nothing Live Migration“ je nach Ihren Einstellungen eine der beiden anderen Optionen aus.</block>
  <block id="8c60fc97ae35b3af22d644ab2b9186e7" category="list-text">Geben Sie den Speicherort für die VM auf dem Hyper-V-Zielserver basierend auf Ihren Einstellungen an.</block>
  <block id="dd714dbbaab89194e2aefdf439089cff" category="list-text">Überprüfen Sie die Zusammenfassung, und klicken Sie auf OK, um die VM zu verschieben.</block>
  <block id="8ab11353f8fda3f130084c6ff3c1f32e" category="summary">Überblick über die Virtualisierung von Microsoft Windows und Hyper-V mit ONTAP</block>
  <block id="e905fd86484ea1bf5b665b34fc9e24b4" category="paragraph">Microsoft Windows Server ist ein Betriebssystem der Enterprise-Klasse, das Netzwerke, Sicherheit, Virtualisierung, Private Cloud, Hybrid Cloud, Virtual Desktop Infrastructure, Zugriffsschutz, Informationsschutz, Webservices, Anwendungsplattform Infrastruktur, und vieles mehr.</block>
  <block id="16b5970810caafa8d6f89bee6e0398f2" category="admonition">*Diese Dokumentation ersetzt die zuvor veröffentlichten technischen Berichte _TR-4568: NetApp-Bereitstellungsrichtlinien und bewährte Speichermethoden für Windows Server_*</block>
  <block id="dbf037f8e99a1d5b52fe58af78568561" category="list-text">Eine Unified Architecture, die Datei-, Objekt- und Blockprotokolle unterstützt Auf diese Weise können die Storage-Controller sowohl als NAS- und SAN-Geräte als auch als Objektspeicher agieren</block>
  <block id="23748de0bc860150d3c6b27a4d472042" category="list-text">Ein All-SAN-Array (ASA), das sich nur auf Blockprotokolle konzentriert und die I/O-Wiederaufnahme-Zeiten (IORT) optimiert, indem symmetrisches aktiv/aktiv-Multipathing für connect Hosts hinzugefügt wird</block>
  <block id="d0114359083804284c542907b2f8cd60" category="list-text">Eine softwaredefinierte Unified Architecture</block>
  <block id="30063e1e8c837a5b52057909e747b2d8" category="list-text">ONTAP Select auf VMware vSphere oder KVM</block>
  <block id="0f76eddbf2cd8ceb8e89447591876ab9" category="list-text">Cloud Volumes ONTAP wird als Cloud-native Instanz ausgeführt</block>
  <block id="f7d26dbdc8e2b21b211abf2157e73536" category="list-text">First-Party-Angebote von Hyperscale-Cloud-Providern</block>
  <block id="4d82a1ec1af02725042c8c785564ee7a" category="list-text">Amazon FSX für NetApp ONTAP</block>
  <block id="7450cfde7058dc5e1f7909d0280fd7ae" category="list-text">Azure NetApp Dateien</block>
  <block id="1215f8190533dfdf63d2224a6d88266f" category="list-text">Google Cloud NetApp Volumes</block>
  <block id="7ad18871bdb08a3899a3f7ed4e463284" category="paragraph">ONTAP bietet NetApp Funktionen zur Steigerung der Storage-Effizienz, beispielsweise die NetApp Snapshot Technologie, Klonen, Deduplizierung, Thin Provisioning, Thin Replication, Komprimierung, Virtual Storage Tiering und vieles mehr mit verbesserter Performance und Effizienz.</block>
  <block id="538a662f2ecbeba6af56e41386484f67" category="paragraph">Zusammen können Windows Server und ONTAP in großen Umgebungen betrieben werden und Datacenter-Konsolidierung und Private oder Hybrid Cloud-Implementierungen einen enormen Mehrwert bringen. Diese Kombination bietet auch effiziente unterbrechungsfreie Workloads und unterstützt nahtlose Skalierbarkeit.</block>
  <block id="8f0442ea7c58fe8ab4000f5aaeac415b" category="paragraph">Dieses Dokument richtet sich an System- und Storage-Architekten, die NetApp Storage-Lösungen für Windows Server entwerfen.</block>
  <block id="f6f27d1607ed17a4b610676f16f68357" category="paragraph">Wir gehen in diesem Dokument von folgenden Annahmen aus:</block>
  <block id="e9d690b9eb25093e460ea369baa55496" category="inline-link">Systemadministrationshandbuch für Clusteradministratoren</block>
  <block id="d54e01b4234443b582b1d0cc44ce1c19" category="list-text">Der Leser hat allgemeine Kenntnisse über NetApp Hardware- und Softwarelösungen. Siehe<block ref="87244016000a25428f0fcae2d3732d46" category="inline-link-rx"></block> Entsprechende Details.</block>
  <block id="c501659fe58b766ec2ead9a0c974e530" category="inline-link">Clustered Data ONTAP SAN-Management</block>
  <block id="75ac79ee68c2245f6d8b8cc32a3eae9e" category="inline-link">NAS-Management</block>
  <block id="48fa880bcded409e494bd8e2e8c64a97" category="list-text">Der Leser verfügt über allgemeine Kenntnisse zu Block-Zugriffsprotokollen wie iSCSI, FC und dem Dateizugriffsprotokoll SMB/CIFS. Siehe<block ref="123a6e61ead9566ba44616c8f29f1ba3" category="inline-link-rx"></block> Für SAN-bezogene Informationen. Siehe<block ref="9933d9f07fd1633df051103d1b03340e" category="inline-link-rx"></block> Für CIFS/SMB-bezogene Informationen.</block>
  <block id="02279515531a2de57bfdffa958d40813" category="list-text">Der Leser hat allgemeine Kenntnisse über das Betriebssystem Windows Server und Hyper-V.</block>
  <block id="e1e99607c6efe5e3439e886c50500015" category="paragraph">Eine vollständige, regelmäßig aktualisierte Matrix getesteter und unterstützter SAN- und NAS-Konfigurationen finden Sie im<block ref="5555d80b97794db0ba7e8564a4b85ca4" category="inline-link-rx"></block> Auf der NetApp Support-Website. Mithilfe des IMT können Sie die genauen Produkt- und Funktionsversionen ermitteln, die für Ihre spezifische Umgebung unterstützt werden. NetApp IMT definiert die Produktkomponenten und -Versionen, die mit von NetApp unterstützten Konfigurationen kompatibel sind. Die dort angezeigten Ergebnisse basieren auf der spezifischen Infrastruktur des jeweiligen Kunden bzw. auf den technischen Daten der in dieser Infrastruktur enthaltenen Komponenten.</block>
  <block id="3eb0aeddd8b10d858bd2028c6f983308" category="summary">Best Practices für den Betrieb von VMware SRM und ONTAP Storage</block>
  <block id="35d022831f6b84d57c0d19770bd37b82" category="list-text">ONTAP 9 kann so konfiguriert werden, dass Snapshots automatisch entfernt werden, um die Uptime aufrechtzuerhalten, falls ein Speicherplatz nicht ausreicht, wenn Autosize nicht in der Lage ist, eine ausreichende Notfallkapazität zur Verfügung zu stellen. In der Standardeinstellung für diese Funktion werden die von SnapMirror erstellten Snapshots nicht automatisch gelöscht. Wenn SnapMirror Snapshots gelöscht werden, kann NetApp SRA die Replizierung für das betroffene Volume nicht rückgängig machen und erneut synchronisieren. Um zu verhindern, dass ONTAP SnapMirror Snapshots löscht, konfigurieren Sie die Funktion für automatisches Löschen von Snapshots für den Versuch.</block>
  <block id="70ca1a895029f273d425b715900ff37a" category="inline-image-macro">VVols Replizierung</block>
  <block id="0c1274faebf0b61ccee15ede78b00b55" category="paragraph"><block ref="0c1274faebf0b61ccee15ede78b00b55" category="inline-image-macro-rx" type="image"></block></block>
  <block id="c2c5aade1021a3c4cd296db5e8cca90b" category="inline-image-macro">SnapMirror Zeitpläne</block>
  <block id="a1066290411892a896db86292d410eb3" category="paragraph"><block ref="a1066290411892a896db86292d410eb3" category="inline-image-macro-rx" type="image"></block></block>
  <block id="1079afba0a1986bf5bb313da40d31a49" category="inline-image-macro">Richtlinienzuordnung</block>
  <block id="32e0e862ac943abe75cff3d58b3e5d2b" category="paragraph"><block ref="32e0e862ac943abe75cff3d58b3e5d2b" category="inline-image-macro-rx" type="image"></block></block>
  <block id="3dd63afe980253ca98d9e32784c2f1fc" category="paragraph">Dafür muss der vSphere Administrator eine neue Anforderung erfüllen. Da Volumes außerhalb der ONTAP Tools erstellt werden, ist es nicht bekannt, dass die Änderungen, die Ihr ONTAP-Administrator bis zur regelmäßigen planmäßigen Neuerfassungszeit vorgenommen hat. Daher ist es eine Best Practice, immer wieder neu zu ermitteln, wenn Sie eine Volume- oder SnapMirror Beziehung erstellen, die mit VVols verwendet werden soll. Klicken Sie einfach mit der rechten Maustaste auf den Host oder den Cluster und wählen Sie ONTAP Tools &gt; Host- und Speicherdaten aktualisieren aus, wie im folgenden Screenshot dargestellt.</block>
  <block id="2802aa9f39078d087d6ee0f565b18e51" category="inline-image-macro">Host- und Speicherdaten aktualisieren</block>
  <block id="416223ee42c638c719e40efc871fe8a1" category="paragraph"><block ref="416223ee42c638c719e40efc871fe8a1" category="inline-image-macro-rx" type="image"></block></block>
  <block id="06b2d4b91b5c9eaa8c20a1c270f95b3c" category="inline-image-macro">Cluster</block>
  <block id="6c0893d9a45aaff00c1ef0daf0867ada" category="paragraph"><block ref="6c0893d9a45aaff00c1ef0daf0867ada" category="inline-image-macro-rx" type="image"></block></block>
  <block id="820b71b9a8a55fe48690a5f973c5480e" category="paragraph">Das Software Engineering mit ONTAP Tools für VMware vSphere nutzt die folgenden sicheren Entwicklungsaktivitäten:</block>
  <block id="ecdc76c84c9674a3148f3d464298469c" category="paragraph">Die ONTAP Tools für VMware vSphere beinhalten in jeder Version die folgenden Sicherheitsfunktionen.</block>
  <block id="0785fa6b3221e7345916be824259dc2f" category="paragraph">Wenn Sie SDRS mit ONTAP Tools für VMware vSphere verwenden, müssen Sie zuerst einen Datastore mit dem Plug-in erstellen, das Datastore-Cluster mithilfe von vCenter erstellen und diesem dann den Datastore hinzufügen. Nach der Erstellung des Datastore-Clusters können diesem direkt aus dem Assistenten für die Datastore-Bereitstellung auf der Seite „Details“ weitere Datastores hinzugefügt werden.</block>
  <block id="043d75b2a25cf6f38bccaf9211c5bcc5" category="summary">Auf dieser Seite werden die Best Practices zur Implementierung einer ONTAP Storage-Lösung in einer VMware vSphere Umgebung beschrieben.</block>
  <block id="760f70bd45ae12af9fa6bee90f402eeb" category="cell">Siehe NFS.MaxQueueDepth in <block ref="1f38ac07598dbb4b06912539aafa4211" category="inline-link-macro-rx"></block>.</block>
  <block id="647fe6e64f77a1f54f9bb61154ab2a4f" category="summary">Mehr VVols Ressourcen</block>
  <block id="852ee2d26adc87d2379ffeb9287f7642" category="inline-image-macro">ONTAP Tools für das VVols Dashboard von VMware vSphere 9.8</block>
  <block id="12947c7ceaf237f27fd206edb07cc0c1" category="paragraph"><block ref="12947c7ceaf237f27fd206edb07cc0c1" category="inline-image-macro-rx" type="image"></block></block>
  <block id="6e2c48e84c0d02dd4e236da05038bd9a" category="paragraph">ONTAP unterstützt sowohl VMFS als auch NFS vVol Datastores. Bei gemeinsamer Verwendung von VVols und SAN-Datastores profitieren Sie von einigen der Vorteile von NFS, beispielsweise von Granularität auf VM-Ebene. Im Folgenden werden einige der zu berücksichtigende Best Practices beschrieben. Weitere Informationen finden Sie unter <block ref="7be3ce63e842612697a6d91b19b8afcd" category="inline-link-macro-rx"></block>:</block>
  <block id="436b5d96360742275889cb237fe2ab70" category="inline-image-macro">VVols Komponenten</block>
  <block id="1706c14e85f152d61bab2f65d32810a1" category="paragraph"><block ref="1706c14e85f152d61bab2f65d32810a1" category="inline-image-macro-rx" type="image"></block></block>
  <block id="43f38d7beb04aa98b65d2317c2f1659f" category="inline-image-macro">ONTAP Tools für vSphere</block>
  <block id="aa5b1d896aebe8a04ceac7c2a16b2cd3" category="paragraph"><block ref="aa5b1d896aebe8a04ceac7c2a16b2cd3" category="inline-image-macro-rx" type="image"></block></block>
  <block id="c4b0e8af5e898d44e394489b0c629abb" category="image-alt">VM Host-Gruppen und Affinitätsregeln</block>
  <block id="9a2f41e25337439aa55e14c7e16bf211" category="summary">Die ONTAP-Lösung für VMware Site Recovery Manager (SRM)</block>
  <block id="a2fca395a283b61cb3e85ebc206c6274" category="paragraph">ONTAP ist seit seiner Einführung in das moderne Datacenter im Jahr 2002 eine der führenden Storage-Lösungen für VMware vSphere Umgebungen und wird kontinuierlich um innovative Funktionen erweitert, die nicht nur zur Vereinfachung des Managements, sondern auch zu Kostensenkungen beitragen.</block>
  <block id="f98dd9d12416a648450df8c99d10878a" category="summary">Dies ist eine Übersicht über VMware vSphere Virtual Volumes (VVols) mit ONTAP</block>
  <block id="674c41ba00ec77fc9d294abbba7e7b7d" category="paragraph">ONTAP ist seit über zwei Jahrzehnten eine der führenden Storage-Lösungen für VMware vSphere Umgebungen und wird kontinuierlich mit innovativen Funktionen erweitert, die nicht nur zur Vereinfachung des Managements, sondern auch zu Kostensenkungen beitragen.</block>
  <block id="ae7454da5e20b82b7fe67eb1ff88c923" category="admonition">Diese Dokumentation ersetzt zuvor veröffentlichte technische Berichte _TR-4400: VMware vSphere Virtual Volumes (VVols) durch ONTAP_</block>
  <block id="ee82b7a98212a75a2397dd9c5f9b1ab4" category="paragraph">ONTAP unterstützt die VASA Spezifikation seit ihrer ersten Version im Jahr 2012. Während andere NetApp Storage-Systeme VASA unterstützen, konzentriert sich dieses Dokument auf die derzeit unterstützten Versionen von ONTAP 9.</block>
  <block id="669b30b7e6d5ef1d862fe65880b816a8" category="paragraph">Neben ONTAP 9 auf AFF, ASA und FAS Systemen unterstützt NetApp VMware-Workloads auf ONTAP Select, Amazon FSX for NetApp mit VMware Cloud on AWS, Azure NetApp Files mit der Azure VMware Lösung, Cloud Volumes Service mit Google Cloud VMware Engine und NetApp Private Storage in Equinix, die spezifische Funktionalität kann jedoch je nach Dienstanbieter und verfügbarer Netzwerkverbindung variieren. Es ist auch möglich, von vSphere Gasts auf Daten zuzugreifen, die in diesen Konfigurationen sowie auf Cloud Volumes ONTAP gespeichert sind.</block>
  <block id="afd715f07b1ec06fe77bf23a2d7bf185" category="paragraph">_Weitere Informationen zu den Best Practices von ONTAP und VMware vSphere finden Sie unter <block ref="5dbb4949ffa7cb97fe4eaffc517a1111" category="inline-link-macro-rx"></block>_</block>
  <block id="bfdc61fd18bd1a23d9660561bc8999b7" category="list-text">*Cloud Volumes ONTAP.* die NetApp Cloud Volumes ONTAP Datenmanagement-Software bietet Kontrolle, Schutz, Flexibilität und Effizienz für Ihre Unternehmensdaten in der gewünschten Cloud. Cloud Volumes ONTAP ist eine Cloud-native Datenmanagement-Software auf der Basis von ONTAP Storage. Nutzen Sie diese Technologie zusammen mit Cloud Manager, um Cloud Volumes ONTAP Instanzen gemeinsam mit Ihren lokalen ONTAP Systemen zu implementieren und zu managen. Nutzen Sie erweiterte NAS- und iSCSI SAN-Funktionen mit einheitlichem Datenmanagement einschließlich Snapshots und SnapMirror Replizierung.</block>
  <block id="e88c947a5d8f4c0daec271a8595604a7" category="inline-image-macro">Ansicht der virtuellen Maschinen von Active IQ Unified Manager</block>
  <block id="9078040b61fc8a9bd0215e8f35ff788c" category="paragraph"><block ref="9078040b61fc8a9bd0215e8f35ff788c" category="inline-image-macro-rx" type="image"></block></block>
  <block id="0ef2f850d3f17cacfbd404bade7eed5b" category="inline-image-macro">AIQUM erweiterte Topologie</block>
  <block id="75817aa97084cb93e0292a1f0549551f" category="paragraph"><block ref="75817aa97084cb93e0292a1f0549551f" category="inline-image-macro-rx" type="image"></block></block>
  <block id="cbd6cdb70073007e002f1ae45c2f7e4a" category="summary">Replizierungstopologien unter Einsatz von ONTAP mit SnapMirror und VMware SRM</block>
  <block id="4e749568996a3331860532ac7a470fdb" category="inline-image-macro">Layout der SnapMirror Beziehung</block>
  <block id="3d2c169d59a49a498b2d7e714c5a6ce2" category="paragraph"><block ref="3d2c169d59a49a498b2d7e714c5a6ce2" category="inline-image-macro-rx" type="image"></block></block>
  <block id="706bdd824ec69e00026180eaf2801d2e" category="paragraph"><block ref="706bdd824ec69e00026180eaf2801d2e" category="inline-image-macro-rx" type="image"></block></block>
  <block id="ba1aebb9b3c7be6b408863c12bbc2844" category="paragraph"><block ref="ba1aebb9b3c7be6b408863c12bbc2844" category="inline-image-macro-rx" type="image"></block></block>
  <block id="ea07c4b81c3f00ae8bbef49ac8562d1c" category="paragraph"><block ref="ea07c4b81c3f00ae8bbef49ac8562d1c" category="inline-image-macro-rx" type="image"></block></block>
  <block id="12a757dd793cbe082927530094a72d26" category="inline-image-macro">Array-Paare</block>
  <block id="263506cdbbc7c35e461fc5f1044b9c79" category="paragraph"><block ref="263506cdbbc7c35e461fc5f1044b9c79" category="inline-image-macro-rx" type="image"></block></block>
  <block id="b26873df0260bdc332a7511f6823cf63" category="inline-image-macro">Nicht unterstützte Konfigurationen</block>
  <block id="120528a3f7d5a39fdd0d67fb0e2b94a4" category="paragraph"><block ref="120528a3f7d5a39fdd0d67fb0e2b94a4" category="inline-image-macro-rx" type="image"></block></block>
  <block id="160a69c7a04198d870353b06dbf714a9" category="paragraph"><block ref="160a69c7a04198d870353b06dbf714a9" category="inline-image-macro-rx" type="image"></block></block>
  <block id="742a3ab0c9a97e5a438c8edfb1b271d2" category="paragraph"><block ref="742a3ab0c9a97e5a438c8edfb1b271d2" category="inline-image-macro-rx" type="image"></block></block>
  <block id="d0a05359b36f0932509cfa007b46e853" category="inline-image-macro">Kaskadierung von SnapMirror Beziehungen</block>
  <block id="2057708d80974a7573d92b3e0d8a9258" category="paragraph"><block ref="2057708d80974a7573d92b3e0d8a9258" category="inline-image-macro-rx" type="image"></block></block>
  <block id="cd92dffb12f75e4b81ad67d8668ae698" category="inline-image-macro">SnapMirror auf SnapVault Kaskadierung</block>
  <block id="13ece5df0d16f65585acbfb9ebad113c" category="paragraph"><block ref="13ece5df0d16f65585acbfb9ebad113c" category="inline-image-macro-rx" type="image"></block></block>
  <block id="c74d71595c83ecb05581a89554ca8bb2" category="inline-image-macro">SnapMirror to SnapVault Kaskadierung rückwärts</block>
  <block id="5bb35c000e450925b37eff3feca75a24" category="paragraph"><block ref="5bb35c000e450925b37eff3feca75a24" category="inline-image-macro-rx" type="image"></block></block>
  <block id="2ab367df5e87b988bc086e5ee534793e" category="summary">Auf dieser Seite werden die Best Practices für die Implementierung von VMware vSphere mit ONTAP und mit NFS verbundenen Datastores beschrieben.</block>
  <block id="0e768c8183bcbfc87718f59d9dfd61eb" category="inline-link-macro">Datenspeicher</block>
  <block id="e6a172fa04a2b585ad34762a7e8991da" category="list-text">Weitere Informationen finden Sie in den Anmerkungen zur Interoperabilität von NFS v4.1 <block ref="6a9c495bd8eb9d28b9471f77a4372a63" category="inline-link-macro-rx"></block> Für bestimmte ESXi-Patch-Level, die zur Unterstützung erforderlich sind.</block>
  <block id="622b235a9123d972c612737c81eb55a1" category="inline-link-macro">NFSv3 nConnect Funktion mit NetApp und VMware</block>
  <block id="6185cefe4b2be85244dfd9d4fce6f2e4" category="list-text">VMware unterstützt nconnect mit NFSv3 ab vSphere 8.0U2. Weitere Informationen zu nconnect finden Sie im <block ref="390300af711b584cff3623a0769fdf9b" category="inline-link-macro-rx"></block></block>
  <block id="d94a770977f68948a4930f4a2edbabb9" category="image-alt">Neuer Cluster</block>
  <block id="f5c7047b98efcb8c7eef2db6ef31faf4" category="image-alt">Aktivieren der Host-Überwachungsoption</block>
  <block id="239aaf78fcb42b02d3709057d2a7978a" category="image-alt">VM-Monitoring</block>
  <block id="a7765f8868404f7c3cb890ce051f7de4" category="image-alt">Zugangskontrolle</block>
  <block id="807dfd0ff3fd0db4a48087c18683dc6b" category="image-alt">SDRS-Empfehlungen</block>
  <block id="e5bd6e5564d38885fef18a28e48861b4" category="image-alt">HA-Cluster</block>
  <block id="8e00c91ecf143f76c94e3aa1668fc92b" category="inline-image-macro">ONTAP-Klonen</block>
  <block id="9c7295440be885852a164f5880627146" category="paragraph"><block ref="9c7295440be885852a164f5880627146" category="inline-image-macro-rx" type="image"></block></block>
  <block id="1eed6756a7834e04fb7716ee8d830425" category="inline-link-macro"><block ref="1eed6756a7834e04fb7716ee8d830425" category="inline-link-rx"></block></block>
  <block id="0e4a75553a081fffff029a37e55f5d71" category="inline-link-macro"><block ref="0e4a75553a081fffff029a37e55f5d71" category="inline-link-rx"></block></block>
  <block id="2e633a7cd484b8152bb61b1eee51006b" category="paragraph">Die ONTAP FlexClone Lizenz (in ONTAP One enthalten) und die ONTAP Tools Appliance sind die einzigen zusätzlichen Produkte, die für die Verwendung von VVols in Verbindung mit ONTAP erforderlich sind. Die neuesten Versionen der ONTAP Tools werden als einzige vereinheitlichte Appliance bereitgestellt und auf ESXi ausgeführt. Sie bieten die Funktionen von ehemals drei verschiedenen Appliances und Servern. Bei VVols ist es wichtig, die vCenter UI-Erweiterungen der ONTAP Tools oder REST-APIs als allgemeine Managementtools und Benutzeroberflächen für ONTAP Funktionen mit vSphere zusammen mit dem VASA Provider, der bestimmte VVols Funktionen bereitstellt, zu verwenden. Die SRA Komponente ist für herkömmliche Datastores enthalten, doch VMware Site Recovery Manager verwendet keine SRA für VVols. Stattdessen werden neue Services in SRM 8.3 und höher implementiert, bei denen der VASA-Provider für die VVols-Replizierung genutzt wird.</block>
  <block id="32fd58743bd2ce579949a47db1b95ca8" category="summary">Die VVols Replizierung mit VASA bietet im Vergleich zu SRA und herkömmlichen Datastores einzigartige Funktionen.</block>
  <block id="daf6970022e96957e3a4ea5b6a599a27" category="list-text">ONTAP Tools und SRA unterstützen zwar Anmeldedaten auf Cluster- und SVM-Ebene, doch unterstützt der VASA Provider nur Zugangsdaten auf Cluster-Ebene für Storage-Systeme. Der Grund dafür ist, dass viele der für VVols verwendeten APIs nur auf Cluster-Ebene verfügbar sind. Wenn Sie daher VVols verwenden möchten, müssen Sie die ONTAP-Cluster mit den Anmeldedaten für den Cluster-Umfang hinzufügen.</block>
  <block id="570bfadf5783125790b6c87f55842fb2" category="inline-image-macro">SnapCenter Implementierung</block>
  <block id="71ca4878111d253f9883f471e5de9594" category="paragraph"><block ref="71ca4878111d253f9883f471e5de9594" category="inline-image-macro-rx" type="image"></block></block>
  <block id="bd90e11d4c9fc73f37adda8029276215" category="summary">Auf dieser Seite werden die Best Practices zur Implementierung einer ONTAP Storage-Lösung in einer VMware vSphere Umgebung beschrieben.</block>
  <block id="4cc4b32d72a64364862dd657b7eee15e" category="paragraph">Sowohl NMP als auch ONTAP unterstützen Asymmetric Logical Unit Access (ALUA) zur Ermittlung optimierter und nicht optimierter Pfade. In ONTAP folgt ein ALUA-optimierter Pfad auf einen direkten Datenpfad. Dabei wird ein Zielport auf dem Node verwendet, der die LUN hostet, auf die zugegriffen wird. ALUA ist sowohl in vSphere als auch in ONTAP standardmäßig aktiviert. NMP erkennt das ONTAP Cluster als ALUA-fähig und verwendet ein ALUA Storage-Array-Plug-in <block ref="d4f7bd3bf4872a2f8cd0cbe400fb23d0" prefix="(" category="inline-code"></block>) Und wählt das Round-Robin-Pfadauswahl-Plug-in aus <block ref="4646bb781f9a95f64c182af129e43c7c" prefix="(" category="inline-code"></block>).</block>
  <block id="b05dc829eda4798f22045fd6851afaaf" category="list-text">Für LUNs, die vor Data ONTAP 8.3 erstellt wurden, wenden Sie SLM manuell an, indem Sie den ausführen<block ref="886fd385b633a0746a8a08f8f00c67cd" prefix=" " category="inline-code"></block> Befehl, um die LUN-Nodes für die Berichterstellung zu entfernen und den LUN-Zugriff auf den LUN-Eigentümer-Node und seinen HA-Partner zu beschränken.</block>
  <block id="5983b6c329fc9e16157db158f01b3bb1" category="list-text">Nutzen Sie für iSCSI-Netzwerke mehrere VMkernel Netzwerkschnittstellen für verschiedene Subnetze mit NIC-Teaming, wenn mehrere virtuelle Switches vorhanden sind. Darüber hinaus können Sie mehrere physische NICs nutzen, die mit mehreren physischen Switches verbunden sind, um Hochverfügbarkeit und einen höheren Durchsatz bereitzustellen. Die folgende Abbildung zeigt ein Beispiel für Multipath-Konnektivität. Verwenden Sie in ONTAP eine Single-Mode-Schnittstellengruppe mit mehreren Links zu verschiedenen Switches oder LACP mit Multimode-Schnittstellengruppen für hohe Verfügbarkeit und Vorteile bei der Link-Aggregation.</block>
  <block id="f0634c16da2019d5a4bad40e20aea8a7" category="list-text">Weitere Informationen zur Verwendung von NVMe/FC mit vSphere 7.0 finden Sie im hier<block ref="6b4ed262d14e47ef641cd57b65c32c38" category="inline-link-rx"></block> Und<block ref="7a29b2f31035a451d5b068728d2b79a7" category="inline-link-rx"></block>. In der folgenden Abbildung ist die Multipath-Konnektivität von einem vSphere Host zu einer ONTAP-LUN dargestellt.</block>
  <block id="ab906277df9ce070b1e11724f8ccf589" category="list-text">Verwenden einer einzelnen logischen Schnittstelle (LIF) für jede SVM auf jedem Node im ONTAP-Cluster Die bisherigen Empfehlungen eines LIF pro Datenspeicher sind nicht mehr erforderlich. Der direkte Zugriff (LIF und Datastore auf demselben Node) ist zwar am besten, aber indirekte Zugriffe müssen sich keine Sorgen machen, da die Performance-Auswirkungen im Allgemeinen minimal sind (Mikrosekunden).</block>
  <block id="522483df083cc02904b0eec18cef8e4e" category="list-text">Alle aktuell unterstützten Versionen von VMware vSphere können sowohl NFS v3 als auch v4.1 verwenden. Die offizielle Unterstützung für nconnect wurde in vSphere 8.0 Update 2 für NFS v3 hinzugefügt. Für NFS v4.1 unterstützt vSphere weiterhin Session-Trunking, Kerberos-Authentifizierung und Kerberos-Authentifizierung mit Integrität. Beachten Sie, dass für das Session-Trunking ONTAP 9.14.1 oder eine neuere Version erforderlich ist. Sie können mehr über die nconnect-Funktion und wie sie die Leistung verbessert unter erfahren <block ref="390300af711b584cff3623a0769fdf9b" category="inline-link-macro-rx"></block>.</block>
  <block id="b01b10b0c784ff1a4d4f05add231b52a" category="paragraph">Erwähnenswert ist, dass NFSv3 und NFSv4.1 verschiedene Sperrmechanismen verwenden. NFSv3 verwendet „Client-side locking“, während in NFSv4.1 „Server-side locking“ verwendet wird. Ein ONTAP Volume kann zwar mit beiden Protokollen exportiert werden, doch ESXi kann einen Datastore nur durch ein Protokoll mounten. Dies bedeutet jedoch nicht, dass andere ESXi-Hosts nicht denselben Datastore über eine andere Version mounten können. Um Probleme zu vermeiden, ist es wichtig, die beim Mounten verwendete Protokollversion anzugeben, um sicherzustellen, dass alle Hosts dieselbe Version und somit auch denselben Sperrungsstil anwenden. Es ist entscheidend, zu vermeiden, dass NFS-Versionen über Hosts hinweg gemischt werden. Wenn möglich, verwenden Sie Hostprofile, um die Compliance zu überprüfen.
** Da keine automatische Datastore-Konvertierung zwischen NFSv3 und NFSv4.1 stattfindet, erstellen Sie einen neuen Datastore für NFSv4.1 und migrieren Sie die VMs mithilfe von Storage vMotion zum neuen Datastore.
** Bitte beachten Sie die Hinweise in der Tabelle NFS v4.1 Interoperability im <block ref="6a9c495bd8eb9d28b9471f77a4372a63" category="inline-link-macro-rx"></block> Für bestimmte ESXi-Patch-Level, die zur Unterstützung erforderlich sind.
* NFS-Exportrichtlinien werden verwendet, um den Zugriff durch vSphere-Hosts zu steuern. Sie können eine Richtlinie für mehrere Volumes (Datastores) nutzen. Bei NFSv3 verwendet ESXi den Sicherheitsstil „sys“ (UNIX). Zur Ausführung von VMs ist dabei die Root-Mount-Option erforderlich. In ONTAP wird diese Option als Superuser bezeichnet. Wenn die Option Superuser verwendet wird, ist es nicht erforderlich, die anonyme Benutzer-ID anzugeben. Beachten Sie, dass Exportrichtlinien mit unterschiedlichen Werten für gelten<block ref="ea72d12ecaa06afdb0c8a3b15e485e95" prefix=" " category="inline-code"></block> Und<block ref="df809a941c1287d18c08bb5927b639dc" prefix=" " category="inline-code"></block> Die ONTAP-Tools können zu Problemen bei der SVM-Erkennung führen. Hier sehen Sie eine Beispielrichtlinie:
** Access Protocol: nfs3
** Client-Match-Spezifikation: 192.168.42.21
** RO-Zugriffsregel: Sys
** RW-Zugriffsregel: Sys
** Anonyme UID
** Superuser: Sys
* Wenn das NetApp-NFS-Plugin für VMware VAAI verwendet wird, sollte das Protokoll auf eingestellt werden<block ref="2521ef5d58fc027d3121662b7d8f9ac2" prefix=" " category="inline-code"></block> Wenn die Regel für die Exportrichtlinie erstellt oder geändert wird. Damit der Copy-Offload funktioniert, wird das NFSv4-Protokoll benötigt und das Protokoll als angegeben<block ref="2521ef5d58fc027d3121662b7d8f9ac2" prefix=" " category="inline-code"></block> Beinhaltet automatisch sowohl die NFSv3- als auch die NFSv4-Versionen.
* NFS-Datastore-Volumes werden aus dem Root-Volume der SVM heraus verbunden. Daher muss ESXi zum Navigieren und Mounten von Datastore Volumes auch Zugriff auf das Root-Volume haben. Die Exportrichtlinie für das Root-Volume und für alle anderen Volumes, in denen die Verbindung des Datastore Volumes geschachtelt ist, muss eine oder mehrere Regeln für die ESXi Server einschließen, die ihnen schreibgeschützten Zugriff gewähren. Hier sehen Sie eine Beispielrichtlinie für das Root-Volume, bei der auch das VAAI Plug-in genutzt wird:
** Access Protocol: nfs (schließt NFSv3 und NFSv4 ein)
** Client-Match-Spezifikation: 192.168.42.21
** RO-Zugriffsregel: Sys
** RW Access Rule: Never (höchste Sicherheit für Root-Volume)
** Anonyme UID
** Superuser: Sys (auch für Root-Volume mit VAAI erforderlich)
* Verwenden Sie ONTAP-Tools für VMware vSphere (die wichtigste Best Practice):
** Verwenden Sie ONTAP Tools für VMware vSphere zur Bereitstellung von Datastores, da es das Management von Richtlinien für den Export automatisch vereinfacht.
** Wenn Sie Datastores für VMware-Cluster mit dem Plug-in erstellen, wählen Sie das Cluster anstelle eines einzigen ESX-Servers aus. Bei dieser Auswahl mountet der Datastore automatisch auf alle Hosts im Cluster.
** Verwenden Sie die Plug-in Mount-Funktion, um vorhandene Datastores auf neue Server anzuwenden.
** Wenn Sie keine ONTAP-Tools für VMware vSphere verwenden, verwenden Sie eine einzige Exportrichtlinie für alle Server oder für jeden Cluster von Servern, bei dem eine zusätzliche Zugriffskontrolle erforderlich ist.
* Obwohl ONTAP eine flexible Namespace-Struktur für Volumes bietet, in der Volumes mithilfe von Verbindungen in einer Baumstruktur angeordnet werden können, ist dieser Ansatz für vSphere nicht geeignet. Für jede VM im Root-Verzeichnis des Datastores wird unabhängig von der Namespace-Hierarchie des Storage ein Verzeichnis erstellt. Daher besteht die Best Practice darin, den Verbindungspfad für Volumes für vSphere im Root-Volume der SVM zu erstellen. Dies entspricht auch der Art und Weise, wie ONTAP Tools für VMware vSphere Datastores bereitstellt. Ohne geschachtelte Verbindungspfade besteht bei Volumes zudem nur eine Abhängigkeit zum Root-Volume. Wenn ein Volume dann offline geschaltet oder sogar absichtlich zerstört wird, wirkt sich dies also nicht auf den Pfad zu den anderen Volumes aus.
* Eine Blockgröße von 4.000 ist für NTFS-Partitionen auf NFS-Datastores in Ordnung. In der folgenden Abbildung ist die Konnektivität eines vSphere Hosts zu einem ONTAP NFS-Datastore dargestellt.</block>
  <block id="8dc90de75c19a4114217f443b0ca2866" category="inline-image-macro">Konnektivität von einem vSphere-Host zu einem ONTAP-NFS-Datastore</block>
  <block id="36b9a8da16a1d2fad06b788e662b7c4a" category="paragraph"><block ref="36b9a8da16a1d2fad06b788e662b7c4a" category="inline-image-macro-rx" type="image"></block></block>
  <block id="bb6d2d52dfe07f9662232c444256aece" category="paragraph">Ein Host, der iSCSI oder NVMe/TCP verwendet, kann direkt mit einem Storage-System verbunden werden und ordnungsgemäß ausgeführt werden. Der Grund dafür ist Pathing. Direkte Verbindungen zu zwei verschiedenen Storage Controllern ergeben zwei unabhängige Pfade für den Datenfluss. Der Verlust von Pfad, Port oder Controller verhindert nicht, dass der andere Pfad verwendet wird.</block>
  <block id="0d761ccb2787bad60b77ac971f6d3914" category="summary">Auf dieser Seite werden die Best Practices für die Implementierung von ONTAP FlexGroup Volumes in einer VMware vSphere Umgebung beschrieben.</block>
  <block id="adc6ae60e2ce9c7497e70727d5232a2d" category="paragraph">Verwenden Sie ONTAP und FlexGroup Volumes mit VMware vSphere für einfache und skalierbare Datastores, die das volle Potenzial eines gesamten ONTAP Clusters ausschöpfen.</block>
  <block id="53c3353b925071c58c705888c2ba7f14" category="paragraph">Neben ONTAP 9.8 und den ONTAP Tools für VMware vSphere 9.8 und dem SnapCenter Plug-in für VMware 4.4 Versionen wurde auch die Unterstützung für Volume-gestützte FlexGroup Datastores in vSphere hinzugefügt. FlexGroup Volumes vereinfachen die Erstellung großer Datenspeicher und erstellen automatisch die erforderlichen verteilten zusammengehörigen Volumes im gesamten ONTAP Cluster, um die maximale Performance eines ONTAP Systems zu erzielen.</block>
  <block id="3f004d1e872dca374feb3f23565997c6" category="paragraph">Verwenden Sie FlexGroup Volumes mit vSphere, wenn Sie einen einzelnen, skalierbaren vSphere-Datastore mit der Leistung eines vollständigen ONTAP Clusters benötigen oder wenn sehr große Klon-Workloads vorhanden sind, die von dem neuen FlexGroup-Klonmechanismus profitieren können.</block>
  <block id="55e0e7799b68cd76cf68be0ee82fef7b" category="section-title">Copy-Offload</block>
  <block id="6dda3ca3c4d8fee4a726d8e72402d30a" category="paragraph">Zusätzlich zu umfangreichen Systemtests mit vSphere Workloads hat ONTAP 9.8 einen neuen Copy-Offload-Mechanismus für FlexGroup Datastores hinzugefügt. Das neue System verwendet eine verbesserte Copy Engine, um Dateien zwischen Komponenten im Hintergrund zu replizieren und gleichzeitig Zugriff auf Quelle und Ziel zu ermöglichen. Dieser lokale Cache wird dann verwendet, um VM-Klone bei Bedarf schnell instanziieren zu können.</block>
  <block id="aa27f776099657de8054550adc71803c" category="inline-link">Konfigurieren von ONTAP FlexGroups zum Zulassen von VAAI Copy-Offload</block>
  <block id="52b5c3310b38cd7b3015770aa7de4aa7" category="paragraph">Informationen zum Aktivieren des für FlexGroup optimierten Copy-Offload finden Sie unter<block ref="cbb2bc98314efa36f692ba86393a7074" category="inline-link-rx"></block></block>
  <block id="0db4fc806176bc768d44fa59cf3c36aa" category="paragraph">Wenn Sie VAAI klonen, aber nicht genug klonen, um den Cache warm zu halten, können Sie feststellen, dass Ihre Klone möglicherweise nicht schneller als eine Host-basierte Kopie sind. In diesem Fall können Sie das Cache-Timeout auf Ihre Bedürfnisse abstimmen.</block>
  <block id="1e8e371d8058e71f2444bc0cd42517eb" category="paragraph">Bei jedem neuen Klonjob, der ein Volume erhält, wird die Zeitüberschreitung zurückgesetzt. Wenn ein konstituierendes Volume in der Beispiel-FlexGroup vor dem Timeout keine Klonanforderung erhält, wird der Cache für diese bestimmte VM gelöscht und das Volume muss erneut ausgefüllt werden. Wenn sich auch die Quelle des ursprünglichen Klons ändert (z. B. Sie haben die Vorlage aktualisiert), wird der lokale Cache jeder Komponente ungültig, um Konflikte zu vermeiden. Wie bereits erwähnt, kann der Cache an die Anforderungen Ihrer Umgebung angepasst werden.</block>
  <block id="6bcf203b609d41b14c06e319cede61d7" category="section-title">QoS-Einstellungen</block>
  <block id="9ab9903a16c07ff38214a043719957fd" category="paragraph">Das Konfigurieren von QoS auf FlexGroup-Ebene mit ONTAP System Manager oder der Cluster Shell wird unterstützt, allerdings bietet es keine VM-Erkennung oder vCenter-Integration.</block>
  <block id="d9450aaad95d69e8cc75ddc0a69a9026" category="paragraph">QoS (IOPS-Maximum/Min.) kann auf einzelnen VMs oder auf allen VMs in einem Datastore eingerichtet werden. Zu diesem Zeitpunkt in der vCenter UI oder über REST-APIs mithilfe von ONTAP Tools. Die Festlegung der QoS auf allen VMs ersetzt alle separaten Einstellungen pro VM. Einstellungen erweitern nicht auch künftig auf neue oder migrierte VMs. Sie können entweder QoS auf den neuen VMs festlegen oder QoS neu auf alle VMs im Datastore anwenden.</block>
  <block id="43a70a1e984a991b55dc01f4e4c6bda0" category="paragraph">Beachten Sie, dass VMware vSphere alle I/O-Vorgänge für einen NFS-Datastore als eine einzige Warteschlange pro Host behandelt. Die QoS-Drosselung einer VM kann die Performance anderer VMs im selben Datastore beeinträchtigen. Dies steht im Gegensatz zu VVols, die ihre QoS-Richtlinieneinstellungen beibehalten können, wenn sie zu einem anderen Datastore migriert werden und bei einer Drosselung die I/O anderer VMs nicht beeinträchtigen.</block>
  <block id="f32c3edaacea72c0ddb30ecf0135c4de" category="section-title">Metriken</block>
  <block id="ed7927d72c6154d79bedf6b19ea8fc83" category="paragraph">ONTAP 9.8 hat außerdem neue dateibasierte Performance-Kennzahlen (IOPS, Durchsatz und Latenz) für FlexGroup-Dateien hinzugefügt. Diese Metriken können über das Dashboard von ONTAP Tools für VMware vSphere sowie VM-Berichte eingesehen werden. Die ONTAP Tools für VMware vSphere Plug-in ermöglichen Ihnen darüber hinaus die Festlegung von QoS-Regeln (Quality of Service) über eine Kombination aus dem Maximum und/oder dem Minimum von IOPS. Diese können über alle VMs in einem Datenspeicher oder individuell für bestimmte VMs hinweg festgelegt werden.</block>
  <block id="c2ffe02d76c4f089648f1647b43e4ee5" category="section-title">Best Practices in sich vereint</block>
  <block id="43fc84d75bb64307120a48d004166174" category="inline-link-macro">Datenspeicher und Protokolle – NFS</block>
  <block id="b43cf2a2c13b864c3b2547368e95b2c5" category="list-text">Erstellen Sie mit den ONTAP Tools FlexGroup Datastores, damit Ihre FlexGroup optimal erstellt wird und die Exportrichtlinien entsprechend Ihrer vSphere Umgebung konfiguriert werden. Nachdem Sie jedoch das FlexGroup Volume mit ONTAP Tools erstellt haben, wird festgestellt, dass alle Nodes im vSphere-Cluster eine einzige IP-Adresse zum Mounten des Datenspeichers verwenden. Dies kann zu einem Engpass am Netzwerkport führen. Um dieses Problem zu vermeiden, mounten Sie den Datastore ab und mounten Sie ihn dann mit dem standardmäßigen vSphere Datastore-Assistenten unter Verwendung eines Round-Robin-DNS-Namens, der die Last über LIFs auf der SVM verteilt. Nach der erneuten Montage können ONTAP Tools den Datastore wieder managen. Wenn keine ONTAP-Tools verfügbar sind, verwenden Sie die FlexGroup-Standardeinstellungen, und erstellen Sie entsprechend den Richtlinien in Ihre Exportrichtlinie <block ref="1dcda1ccb6404fd747bdfa52cd9e0cc2" category="inline-link-macro-rx"></block>.</block>
  <block id="689367b2abf9b4f7fdef5f8589689dbc" category="list-text">Beachten Sie bei der Dimensionierung eines FlexGroup-Datenspeichers, dass die FlexGroup aus mehreren kleineren FlexVol-Volumes besteht, die einen größeren Namespace erstellen. Daher sollten Sie die Größe des Datenspeichers mindestens 8x (bei Annahme der 8 Standard-Komponenten) der Größe Ihrer größten VMDK-Datei plus 10 bis 20 % ungenutzte Reserven aufweisen, um Flexibilität bei der Ausbalancierung zu ermöglichen. Wenn Sie beispielsweise eine 6 TB VMDK in Ihrer Umgebung haben, müssen Sie den FlexGroup Datenspeicher nicht kleiner als 52,8 TB (6 x 8 + 10 %) dimensionieren.</block>
  <block id="098d1f2818677f5cc4dd7c08e40b3f41" category="list-text">VMware und NetApp unterstützen das NFSv4.1 Session Trunking ab ONTAP 9.14.1. In den Hinweisen zur NetApp NFS 4.1 Interoperabilitäts-Matrix finden Sie spezifische Versionsdetails. NFSv3 unterstützt nicht mehrere physische Pfade zu einem Volume, sondern beginnend mit vSphere 8.0U2 nconnect. Weitere Informationen zu nconnect finden Sie im <block ref="390300af711b584cff3623a0769fdf9b" category="inline-link-macro-rx"></block>.</block>
  <block id="ac08f555cdca1e9107c33a3e4f5eca2b" category="list-text">Nutzen Sie das NFS-Plug-in für VMware VAAI für den Offloaded Data Transfer. Beachten Sie, dass das Klonen innerhalb eines FlexGroup-Datastore verbessert wird, wie bereits erwähnt, aber ONTAP beim Kopieren von VMs zwischen FlexVol und/oder FlexGroup Volumes keine wesentlichen Performance-Vorteile gegenüber ESXi Hostkopien bietet. Berücksichtigen Sie daher beim Einsatz von VAAI oder FlexGroups Ihre Klon-Workloads. Die Änderung der Anzahl zusammengebender Volumes ist eine Möglichkeit zur Optimierung des FlexGroup-basierten Klonens. Ebenso wie die Anpassung der zuvor erwähnten Cache-Zeitüberschreitung.</block>
  <block id="0906d77ff43b190c03c912f8e47c3230" category="list-text">Verwenden Sie ONTAP Tools für VMware vSphere 9.8 oder höher, um die Performance von FlexGroup VMs mithilfe von ONTAP Kennzahlen (Dashboard und VM-Berichte) zu überwachen und QoS für einzelne VMs zu managen. Diese Metriken sind derzeit nicht über ONTAP-Befehle oder APIs verfügbar.</block>
  <block id="315f464c64861565200e97577a6eabde" category="list-text">Das SnapCenter Plug-in für VMware vSphere Version 4.4 und höher unterstützt das Backup und die Recovery von VMs in einem FlexGroup Datastore auf dem primären Storage-System. SCV 4.6 bietet zusätzliche SnapMirror Unterstützung für FlexGroup-basierte Datastores. Array-basierte Snapshots und Replizierung sind die effizienteste Methode zum Schutz Ihrer Daten.</block>
  <block id="6c5f0256b71da4e103acbeed0dfcc379" category="summary">In diesem Dokument werden die Sicherheitsaspekte des SnapCenter Plug-ins für VMware beschrieben.</block>
  <block id="3fd4fd8640d78da436f762569919841f" category="list-text">*Aktion zur Reaktion auf Produktsicherheitsvorfälle.* Sicherheitsschwachstellen werden sowohl intern als auch extern im Unternehmen entdeckt und können ein ernsthaftes Risiko für den Ruf von NetApp darstellen, wenn sie nicht rechtzeitig behoben werden. Zur Erleichterung dieses Prozesses meldet ein PSIRT (Product Security Incident Response Team) die Sicherheitsanfälligkeiten und verfolgt diese.</block>
  <block id="a2132633bc2804871caed9697d50a10d" category="summary">Funktionen, Grenzen und VVols unterstützen mit ONTAP Tools.</block>
  <block id="2fa76de41291847c8d191ea664c53395" category="summary">Erfahren Sie mehr über vSphere Metro Storage-Cluster mit ONTAP</block>
  <block id="0ecd760f5e697e57a60b2f1597be9d54" category="admonition">Weitere Informationen zu VMware vSphere Virtual Volumes, SPBM und ONTAP finden Sie unter <block ref="2e6ec19fbf81081d6cc68364fb2fb656" category="inline-link-macro-rx"></block>.</block>
  <block id="431a88aaf8eee90b865aec0ca23cfc8c" category="paragraph">Sowohl NMP als auch ONTAP unterstützen Asymmetric Logical Unit Access (ALUA) zur Ermittlung optimierter und nicht optimierter Pfade. In ONTAP folgt ein ALUA-optimierter Pfad auf einen direkten Datenpfad. Dabei wird ein Zielport auf dem Node verwendet, der die LUN hostet, auf die zugegriffen wird. ALUA ist sowohl in vSphere als auch in ONTAP standardmäßig aktiviert. NMP erkennt das ONTAP Cluster als ALUA-fähig und verwendet ein ALUA Storage-Array-Plug-in <block ref="d4f7bd3bf4872a2f8cd0cbe400fb23d0" prefix="(" category="inline-code"></block>) Und wählt das Plug-in zur Auswahl des Round-Robin-Pfads aus <block ref="4646bb781f9a95f64c182af129e43c7c" prefix="(" category="inline-code"></block>).</block>
  <block id="7dbbfa4d228c6ffe9a64c938bc057b0d" category="inline-image-macro">Multipath-Konnektivität</block>
  <block id="4525956d57ef52f4b2a4f341fbeec28e" category="paragraph"><block ref="4525956d57ef52f4b2a4f341fbeec28e" category="inline-image-macro-rx" type="image"></block></block>
  <block id="8692ff2ea8c35b5c4c7cea90481e416c" category="summary">In diesem Dokument werden die empfohlenen Speicher- und Netzwerkeinstellungen kategorisiert und aufgelistet.</block>
  <block id="ff6fdc6e7749c69219fc26d54c625bd5" category="paragraph">NetApp hat optimale ESXi Host-Einstellungen für NFS- und Blockprotokolle entwickelt. Des Weiteren finden Sie spezielle Anleitungen zu Multipathing- und HBA-Zeitüberschreitungseinstellungen, um ein angemessenes Verhalten gegenüber ONTAP auf der Grundlage interner Tests von NetApp und VMware zu gewährleisten.</block>
  <block id="bc25cf0638b9454338db449fa254150f" category="paragraph">Diese Werte lassen sich mit den ONTAP-Tools für VMware vSphere problemlos einstellen: Klicken Sie im Dashboard „Zusammenfassung“ im Portlet „Hostsysteme“ auf „Einstellungen bearbeiten“ oder klicken Sie mit der rechten Maustaste auf den Host in vCenter. Navigieren Sie dann zu „ONTAP-Tools“ &gt; „Empfohlene Werte festlegen“.</block>
  <block id="8b31cb1f15553175f0c1621a39bdbb79" category="paragraph">Im Folgenden finden Sie die derzeit empfohlenen Host-Einstellungen für die Versionen 9.8-9.13.</block>
  <block id="baee97fad9d55c529dc7a7eb07fd1ba4" category="cell">VSphere 6.0 oder höher: Auf 256 einstellen
Alle anderen NFS-Konfigurationen auf 64 eingestellt.</block>
  <block id="f52dc930699add83bc126704edf5400b" category="list-text">In vSphere 6.7 Update 1 hat VMware einen neuen Lastausgleichsmechanismus für das Round Robin PSP System eingeführt. Bei der Auswahl des optimalen Pfads für I/O berücksichtigt die neue Option die I/O-Bandbreite und die Pfadlatenz Möglicherweise profitieren Sie von der Verwendung in Umgebungen mit nicht gleichwertiger Pfadverbindung, z. B. bei mehr Netzwerk-Hops auf einem Pfad als auf einem anderen oder bei Verwendung eines NetApp All-SAN-Array-Systems. Siehe<block ref="f5ecc69f2970e2e9c8638ad278ec38f7" category="inline-link-rx"></block> Finden Sie weitere Informationen.</block>
  <block id="b3fd5cebc3e5891cccea8f8df97003e0" category="paragraph">In den folgenden Abschnitten werden die Best Practices für die Implementierung mit ONTAP und VMware SRM beschrieben.</block>
  <block id="c89f9fd1a8019dc4d7a2559a982d3aa8" category="inline-link-macro">S3-Konfigurationsübersicht</block>
  <block id="f14de37f621abb28f0bf089cae467a29" category="paragraph">Ursprünglich wurde in diesem einheitlichen Ansatz erwähnt, dass sowohl NAS- als auch SAN-Protokolle auf einem Storage-System unterstützt werden sollten. ONTAP ist dabei weiterhin eine der führenden Plattformen für SAN und bietet in Bezug auf NAS die ursprünglichen Stärken. ONTAP unterstützt jetzt auch S3-Objektprotokolle. Obwohl S3 nicht für Datastores verwendet wird, können Sie es für in-Guest-Applikationen verwenden. Weitere Informationen zur Unterstützung des S3-Protokolls in ONTAP finden Sie unter <block ref="9a0a48164e4b196191c2134bda6f342d" category="inline-link-macro-rx"></block>.</block>
  <block id="8eb06904cba6440e6130ac3e32b68aba" category="paragraph">*HINWEIS:* Weitere Informationen zu SVMs, Unified Storage und Client-Zugriff finden Sie unter <block ref="9e7286a7ad3ab81fa024704c7a52e505" category="inline-link-macro-rx"></block> Im Dokumentationszentrum ONTAP 9.</block>
  <block id="d96a9839a4f7a8d356a868bd23816516" category="summary">ONTAP ist seit fast zwei Jahrzehnten eine der führenden Storage-Lösungen für VMware vSphere Umgebungen und wird kontinuierlich mit innovativen Funktionen erweitert, die nicht nur zur Vereinfachung des Managements, sondern auch zu Kostensenkungen beitragen. Dieses Dokument bietet eine Einführung in die ONTAP Lösung für vSphere sowie in die neuesten Produktinformationen und Best Practices zur Optimierung der Implementierung, Risikominderung und Vereinfachung des Managements.</block>
  <block id="93fbb268e5e551ae95265cfe3b3b6c7a" category="paragraph">PostgreSQL wird mit Varianten wie PostgreSQL, PostgreSQL Plus und EDB Postgres Advanced Server (EPAS) geliefert. PostgreSQL wird typischerweise als Back-End-Datenbank für Multi-Tier-Applikationen implementiert. Es wird von gängigen Middleware-Paketen (wie PHP, Java, Python, Tcl/TK, ODBC, Und JDBC) und war in der Vergangenheit eine beliebte Wahl für Open-Source-Datenbankmanagementsysteme. ONTAP ist eine ausgezeichnete Wahl für die Ausführung von PostgreSQL-Datenbanken aufgrund seiner Zuverlässigkeit, seiner hohen Performance und seiner effizienten Datenmanagementfunktionen.</block>
  <block id="bbc2d665b02dc421c5c756db4b973610" category="sidebar">Microsoft Hyper-V</block>
  <block id="672553790d8814b967eb8623e754ea84" category="sidebar">Implementierungsrichtlinien und Storage Best Practices</block>
  <block id="c5865855c43d734069987786177edd98" category="sidebar">Hyper-V</block>
  <block id="230dea8fdf5e53eb32282fb3f8d4d9f6" category="sidebar">Implementierungsrichtlinien und Best Practices für Storage</block>
  <block id="3c0fa9c86e8d92c7a146b2d71467ab2e" category="sidebar">NetApp Storage- und Windows Server-Umgebung</block>
  <block id="1ed211700a6cc31a2b45adb662dcbb8a" category="sidebar">Bereitstellung in SAN-Umgebungen</block>
  <block id="41ffc5ff950f2260951b6b6150c36f6b" category="sidebar">Bereitstellung in SMB-Umgebungen</block>
  <block id="e5f52b64058cb937fdd114369219d380" category="sidebar">Hyper-V Storage-Infrastruktur auf NetApp</block>
  <block id="4380100fa910735712157068aeec0575" category="sidebar">Bereitstellung von Hyper-V Live Migration in einer Cluster-Umgebung</block>
  <block id="0726f4e9c3c98564af638f1b4cd17a9e" category="sidebar">Bereitstellung von Hyper-V Live Migration außerhalb einer Cluster-Umgebung</block>
  <block id="1bbd09497cd931a02f8dc0ee3cfcf19d" category="sidebar">Bereitstellung von Hyper-V-Replikaten außerhalb einer Cluster-Umgebung</block>
  <block id="832a116d7f9184d8880a539553a0cb52" category="sidebar">Bereitstellung von Hyper-V-Replikaten in einer Cluster-Umgebung</block>
  <block id="1a85c0d242c43eb56cc500e60d323331" category="paragraph">VMFS ist ein hochperformantes geclustertes Filesystem, das Datastores bereitstellt, bei denen es sich um Shared-Storage-Pools handelt. VMFS Datastores können mit LUNs konfiguriert werden, auf die über FC, iSCSI, FCoE zugegriffen wird. Zudem können NVMe-Namespaces, auf die über NVMe/FC- oder NVMe/TCP-Protokolle zugegriffen wird, verwendet werden. Bei VMFS können alle ESX Server in einem Cluster gleichzeitig auf den Speicher zugreifen. Die maximale LUN-Größe beträgt normalerweise 128 TB ab ONTAP 9.12.1P2 (und früher bei ASA Systemen). Daher kann ein VMFS 5 oder ein Datastore mit einer maximalen Größe von 6 TB mit einer einzigen LUN erstellt werden.</block>
  <block id="5d7771be7242d0b5e76913e2ea5d3dad" category="paragraph">ESXi 6 unterstützt bis zu 256 LUNs und insgesamt bis zu 1,024 Pfade zu LUNs. ESXi erkennt keine LUNs oder Pfade, die über diese Grenzen hinausgehen. Ausgehend von dieser maximalen Anzahl an LUNs lässt das Pfadlimit vier Pfade pro LUN zu. In einem größeren ONTAP Cluster ist es möglich, dass das Pfadlimit vor dem LUN-Limit erreicht wird. Zur Beseitigung dieser Beschränkung unterstützt ONTAP ab Version 8.3 die selektive LUN-Zuordnung (Selective LUN Map, SLM).</block>
  <block id="8983ee0717807e5385f3c8b2c70a177c" category="list-text">TR-4597: VMware vSphere für ONTAP
<block ref="0bff808225ac084b8184e7670c17aa52" category="inline-link-macro-rx"></block></block>
  <block id="1f7ff87198d4a8563ac0d42354d47052" category="list-text">TR-4400: VMware vSphere Virtual Volumes with ONTAP
<block ref="57ea579fe6c0d333a496008248ad03e2" category="inline-link-macro-rx"></block></block>
  <block id="ef8c76bd50596fa93881a13dac3f23b3" category="inline-link-macro"><block ref="ef8c76bd50596fa93881a13dac3f23b3" category="inline-link-rx"></block></block>
  <block id="6e5923e0b2d91653ee7dd35aa5f48141" category="list-text">TR-4015 SnapMirror Configuration Best Practice Guide für ONTAP 9
<block ref="6a851721cd10ebeb2f9cfa107f963956" category="inline-link-macro-rx"></block></block>
  <block id="1e626dbc797733633cd43ac045564b36" category="list-text">RBAC Benutzer-Creator für ONTAP
<block ref="51416f5c8b6f7981eb23be678ab313ad" category="inline-link-macro-rx"></block></block>
  <block id="eaef75afe923a97ffd4ccab86876f8da" category="list-text">ONTAP Tools für VMware vSphere Ressourcen
<block ref="9d52dd6d1c195e015d4baef3146522a8" category="inline-link-macro-rx"></block></block>
  <block id="58b39d11ffaef440efc4e3c365487610" category="list-text">VMware Site Recovery Manager - Dokumentation
<block ref="18c037ca2f7fe7180f506030fe7e514c" category="inline-link-macro-rx"></block></block>
  <block id="bc11d67bfced1da7778c2cabee9e7615" category="paragraph">Siehe <block ref="de89165a46abdf575eb9a1ba0995e131" category="inline-link-macro-rx"></block> Überprüfen Sie auf der NetApp Support-Website, ob die in diesem Dokument angegebenen Produktversionen und Funktionen in Ihrer IT-Umgebung unterstützt werden. Das NetApp IMT definiert die Produktkomponenten und -Versionen, die für von NetApp unterstützte Konfigurationen verwendet werden können. Die dort angezeigten Ergebnisse basieren auf der spezifischen Infrastruktur des jeweiligen Kunden bzw. auf den technischen Daten der in dieser Infrastruktur enthaltenen Komponenten.</block>
  <block id="e651877073d877e1ee090d986eafc550" category="list-text">Sie müssen Hyper-V-Cluster auf demselben oder an verschiedenen geografischen Standorten haben, die als primäre und Replikatcluster dienen. Prüfen <block ref="8a150f92c22920868769b8738759e23d" category="inline-link-macro-rx"></block> Entnehmen.</block>
  <block id="8762adae157b34ee542f2e614401603a" category="list-text">ONTAP-Konzepte
<block ref="836022e29667128997850a15d18e8759" category="inline-link-rx"></block></block>
  <block id="e7b3912abd11fa36935a211d0189ebc2" category="list-text">Best Practices für modernes SAN
<block ref="49df8abaa6163afdde37b69f7f59185c" category="inline-link-rx"></block></block>
  <block id="05c54431756e1b1a7114938bc6f4b199" category="list-text">Datenverfügbarkeit und Integrität von All-SAN-Arrays der NetApp mit der NetApp ASA
<block ref="8f140ecff12b563e3c96a8cff991aedb" category="inline-link-rx"></block></block>
  <block id="bc96f334e3988ce96701a7082c0e23b5" category="list-text">Erste Schritte mit Nano Server +
<block ref="abdd6de84336a7f761fbe384605879e9" category="inline-link-rx"></block></block>
  <block id="0e660b82d6e28b6935041a5223ed1aed" category="paragraph">Für die Bereitstellung der Live-Migration müssen Hyper-V-Server in einem Failover-Cluster mit Shared Storage konfiguriert sein. Prüfen <block ref="8a150f92c22920868769b8738759e23d" category="inline-link-macro-rx"></block> Entnehmen.</block>
  <block id="1dd1e97374852e1f244963d9fdb0c414" category="list-text">Testen Sie das geplante Failover. Verschieben Sie VMs mithilfe von Live-Migration, schneller Migration oder Storage-Migration (Verschieben) auf einen anderen Node. Prüfen <block ref="ba8afff2d0e4662eeb47d28935e79828" category="inline-link-macro-rx"></block> Entnehmen.</block>
  <block id="8737a5d2bc43b0eb76ab5674d0fcb094" category="sidebar">SRM mit ONTAP – Überblick</block>
  <block id="76f6daa4bfab6ca8a7350e46d3d6718f" category="sidebar">Zusätzliche Informationen zu SRM</block>
  <block id="3ebf71fd7bd7392466a892c2228b2f58" category="section-title">Bereitstellen von SMB-Freigaben auf Windows Server</block>
  <block id="674b9c5bbf2379d6c48343d029972e53" category="section-title">Host-Integration</block>
  <block id="16ce478a465011bacaa6e508063c2648" category="section-title">Dinge, die Sie sich merken sollten</block>
  <block id="83f04319244758f7531b398c51aa9f38" category="section-title">Bereitstellung von SMB-Freigaben auf Nano Server</block>
  <block id="2323e97f7cf2da464750911f1f3e1af6" category="doc">Bereitstellung von Hyper-V-Replikaten in einer Cluster-Umgebung</block>
  <block id="36b10adb4505559b63b091b53614af1f" category="section-title">NetApp Deduplizierung</block>
  <block id="1ba5c1747d37a3d7ae84fd1dcac77603" category="section-title">Weitere Informationen</block>
  <block id="50802d3e5a25b93d471686a10da03dd8" category="section-title">Best Practices in sich</block>
  <block id="67bd1e9f90c0abcd3fe89dce2e8b6307" category="doc">Implementierung von Hyper-V Live Migration außerhalb einer Cluster-Umgebung</block>
  <block id="f2b84afd0523a6df7547c404aa3647d8" category="section-title">Zielgruppe</block>
  <block id="458ed20ed5d1729b427e233e0e52f797" category="section-title">Bereitstellung von NetApp Storage für Windows Server</block>
  <block id="e648c74a53b161f90c8e915624ce6135" category="section-title">Managen von NetApp Storage</block>
  <block id="9292ee6bce21e93911c38cd0a1c2e209" category="section-title">Best Practices für die Netzwerkumgebung</block>
  <block id="381400d1babe5b80783d5e46896d832e" category="doc">Einsatz von Nano-Server</block>
  <block id="02676c7c18a8d411999242a1f5731de2" category="section-title">Hyper-V-Speicher auf NetApp CIFS</block>
  <block id="00a66fbc09172147ac7be9c85b708a6b" category="section-title">Verlagerte Datenübertragung</block>
  <block id="39d0bfdf27f1fc555ae2f030e21f8818" category="section-title">Hyper-V Clustering: Hohe Verfügbarkeit und Skalierbarkeit für virtuelle Maschinen</block>
  <block id="10fd9c0040ca7b99a67758c3cd638746" category="section-title">Live-Migration in einer Cluster-Umgebung</block>
  <block id="ed7cd01cb26e2db33a36eaa87ac85ff1" category="section-title">Live-Migration außerhalb einer Cluster-Umgebung</block>
  <block id="023196611fee49915adf49422b3289e6" category="section-title">Hyper-V Replica: Disaster Recovery für virtuelle Maschinen</block>
  <block id="e78f6d0414a6dde66f4e12a3e218620b" category="section-title">Erweiterte Replizierung</block>
  <block id="e43f3be13a49d6eeab4304d80790eaa0" category="section-title">Block-Storage erkennen</block>
  <block id="adc95a9707f2f351740aa0d52f08981e" category="section-title">NetApp FlexClone</block>
  <block id="0bee1618a00f62535bd1e3f03af6a8f0" category="section-title">Booten vom SAN für physischen Host</block>
  <block id="68468d5f213be162fe50260787894f8f" category="section-title">Starten Sie von SAN für die virtuelle Maschine</block>
  <block id="5b6253395cb699e828a4e33bbe3ad99e" category="doc">Bereitstellung von Hyper-V Replica außerhalb einer Cluster-Umgebung</block>
  <block id="c56bd6d44fe37df3133f12e09059b492" category="doc">Implementieren des Hyper-V-Clusters</block>
  <block id="284558d4613b1e72103300f1b8973b3b" category="paragraph">Bei der Erstinstallation verfügt ESX über keine vorkonfigurierten Funktionen, wie z. B. das Hosten eines Gastbetriebssystems oder die Unterstützung einer Endbenutzeranwendung. Es ist ein leerer Container, bis eine Virtual Machine (VM) definiert ist. ONTAP ist ähnlich. Die erste Installation von ONTAP umfasst keine Datenserverfunktionen, bis eine SVM erstellt wurde. Die Datenservices werden von der SVM-Persönlichkeit definiert.</block>
  <block id="2737c9012caaebb1cbb9e526400a2345" category="paragraph">In einer mandantenfähigen Umgebung können die Daten jedes Mandanten eine dedizierte SVM zugewiesen werden. Die Obergrenze für die Anzahl der SVMs und LIFs pro Cluster, HA-Paar und Node ist abhängig vom verwendeten Protokoll, dem Node-Modell und der Version von ONTAP.  Konsultieren Sie die <block ref="41f66f34b3a905f864500c9319fdff8f" category="inline-link-macro-rx"></block> Für diese Grenzwerte.</block>
  <block id="50812cc1b8d48579d382b689e4393be6" category="paragraph">Einige Daten enthalten keine Datenduplikate. Ein Oracle-Block enthält beispielsweise einen Header, der global nur für die Datenbank gilt, und einen Trailer, der fast einzigartig ist. Aus diesem Grund führt die Deduplizierung einer Oracle Database selten zu Einsparungen von mehr als 1 %. Die Deduplizierung mit MS SQL Datenbanken ist etwas besser, aber eindeutige Metadaten auf Blockebene stellen immer noch eine Einschränkung dar.</block>
  <block id="e237dd01ca20b300861ea50239fefbf9" category="paragraph">Dazu gehören typische Planungsmaßnahmen wie die Sicherstellung einer ausreichenden Bandbreite auf dem SAN zwischen dem Host und dem Speichersystem, die Überprüfung, ob alle SAN-Pfade zwischen allen erforderlichen Geräten vorhanden sind, unter Verwendung der FC-Port-Einstellungen, die Ihr FC-Switch-Anbieter benötigt, um ISL-Konflikte zu vermeiden, und ordnungsgemäße Überwachung des SAN-Fabrics verwenden.</block>
  <block id="bab63c3a8b4ae009c3596c809e27d73b" category="paragraph">Die MetroCluster-Replizierung basiert auf der NetApp SyncMirror Technologie, mit der effizient in den synchronen Modus bzw. aus dem synchronen Modus gewechselt werden kann. Diese Funktion erfüllt die Anforderungen von Kunden, die synchrone Replizierung benötigen, aber auch Hochverfügbarkeit für ihre Datenservices benötigen. Wenn zum Beispiel die Verbindung zu einem Remote-Standort unterbrochen wird, ist es in der Regel besser, dass das Speichersystem weiterhin in einem nicht replizierten Zustand betrieben wird.</block>
  <block id="bf4d44f407dab4b746f1eaeb06e897af" category="paragraph">Viele Lösungen zur synchronen Replizierung können nur im synchronen Modus betrieben werden. Diese Art der alles-oder-nichts-Replikation wird manchmal Domino-Modus genannt. Solche Storage-Systeme stellen keine Daten mehr bereit, statt die lokalen und Remote-Kopien der Daten unsynchronisiert zu lassen. Wenn die Replikation gewaltsam unterbrochen wird, kann die Resynchronisierung äußerst zeitaufwendig sein und einen Kunden während der Wiederherstellung der Spiegelung einem vollständigen Datenverlust aussetzen.</block>
  <block id="8f076395fe8f8f46c03efb655db313e6" category="section-title">FC-Direktverbindung</block>
  <block id="722830ffc5e46dbe206670f85cd1d889" category="paragraph">ONTAP und einige andere NetApp Produkte unterstützen jetzt Multi-Faktor-Authentifizierung (MFA) anhand verschiedener Methoden. Das Ergebnis ist, dass ein kompromittierter Benutzername/Passwort allein kein Sicherheitsthread ohne die Daten des zweiten Faktors, wie z. B. eine FOB oder eine Smartphone-App, ist.</block>
  <block id="37c43820ab88f1340855c30955eec330" category="section-title">Verifizierung durch mehrere Administratoren (Multi-Admin Verification, MAV)</block>
  <block id="1910dc5a02cb649cc24df5670c631e34" category="paragraph"><block ref="1910dc5a02cb649cc24df5670c631e34" category="inline-image-macro-rx" type="image"></block></block>
  <block id="416d0d3efc5f3919abcdaaa2364738d9" category="paragraph"><block ref="416d0d3efc5f3919abcdaaa2364738d9" category="inline-image-macro-rx" type="image"></block></block>
  <block id="28c6217e46797f76e427c591070cf50c" category="paragraph"><block ref="28c6217e46797f76e427c591070cf50c" category="inline-image-macro-rx" type="image"></block></block>
  <block id="d03e09c345dd723011486de3855c13b2" category="list-text">SMB-Dokumentation
<block ref="409ef354547d1e3428907ed7fdd2dc6e" category="inline-link-rx"></block></block>
  <block id="834873443aaaf2c09b851c8b0f7b2497" category="section-title">Datenbankkomprimierung</block>
  <block id="2ee3c7318caa2523e9c749e4e2152266" category="paragraph">Ein verbreiteter Verwechslungspunkt bei Kunden, die neu bei ONTAP sind, ist die Verwendung von FlexVols, die allgemein als „Volumes“ bezeichnet werden.</block>
  <block id="ff2dabe8a40c84d01b0584c50319a79e" category="paragraph">Ein Volume ist keine LUN. Diese Begriffe werden synonym mit vielen Produkten anderer Anbieter verwendet, darunter auch Cloud-Provider. ONTAP Volumes sind einfach Management-Container. Sie dienen nicht allein der Bereitstellung von Daten, noch belegen sie Speicherplatz. Sie sind Container für Dateien oder LUNs und sollen die Managebarkeit verbessern und vereinfachen, insbesondere bei großen Umgebungen.</block>
  <block id="92a5781e05d0d9b7044f44e2ef7bff84" category="section-title">Volumes und LUNs</block>
  <block id="faf1e8eebcac2d18d4ac10e75f416a3f" category="paragraph">Zugehörige LUNs befinden sich normalerweise in einem einzelnen Volume. Beispiel: Bei einer Datenbank, die 10 LUNs benötigt, sind normalerweise alle 10 LUNs auf demselben Volume platziert.</block>
  <block id="3a5bf2895b95fa2b89fce4dcf94fd2f9" category="list-text">Die Verwendung eines 1:1:1-Verhältnisses von LUNs zu Volumes, was einer LUN pro Volume entspricht, ist *nicht* eine formale Best Practice.</block>
  <block id="1886f4ac4aa2f64bf8224f178a36ba92" category="list-text">Stattdessen sollten Volumes als Container für Workloads oder Datensätze angesehen werden. Es kann eine einzelne LUN pro Volume geben oder viele. Die richtige Antwort hängt von den Anforderungen an die Managebarkeit ab.</block>
  <block id="0de05608a70e8ec5863afc41ba150178" category="list-text">Die Streuung von LUNs über eine unnötige Anzahl von Volumes kann zu zusätzlichem Overhead und Zeitplanungsproblemen bei Vorgängen wie Snapshot-Vorgängen führen, eine übermäßige Anzahl von Objekten, die in der UI angezeigt werden, und das Erreichen der Plattform-Volume-Grenzen führen, bevor das LUN-Limit erreicht wird.</block>
  <block id="0955dbd01a9f8194cad5a6876b931f5a" category="section-title">Volumes, LUNs und Snapshots</block>
  <block id="9bbd4f2fca3a1e1aaa76abf8cb43351c" category="paragraph">Snapshot-Richtlinien und Zeitpläne werden auf dem Volume statt auf der LUN platziert. Ein Datensatz, der aus 10 LUNs besteht, würde nur eine einzige Snapshot-Politik erfordern, wenn diese LUNs auf demselben Volume Co-lokalisiert sind.</block>
  <block id="f34f28bb7a1701ded173c2ab504f5c20" category="paragraph">Darüber hinaus sorgt das Co-Lokalisieren aller verwandten LUNs für einen bestimmten Datensatz in einem einzelnen Volume für atomare Snapshot-Vorgänge. Beispielsweise könnte eine Datenbank, die auf 10 LUNs residierte, oder eine VMware-basierte Applikationsumgebung mit 10 verschiedenen Betriebssystemen als einzelnes, konsistentes Objekt gesichert werden, wenn alle zugrunde liegenden LUNs auf einem einzelnen Volume platziert werden. Wenn sie auf verschiedenen Volumes platziert werden, können die Snapshots zu 100% synchron sein, auch wenn sie zur gleichen Zeit geplant sind.</block>
  <block id="fe7797cb1dbe36223357d4c25ff30c9e" category="paragraph">In manchen Fällen muss ein verwandter Satz von LUNs aufgrund von Recovery-Anforderungen in zwei verschiedene Volumes aufgeteilt werden. Beispielsweise könnte eine Datenbank vier LUNs für Datendateien und zwei LUNs für Protokolle haben. In diesem Fall könnte ein Datendatei-Volume mit 4 LUNs und ein Protokoll-Volume mit 2 LUNs die beste Option sein. Der Grund dafür ist eine unabhängige Wiederherstellbarkeit. Beispielsweise könnte das Datendatei-Volume selektiv in einen früheren Zustand zurückgesetzt werden. Dies bedeutet, dass alle vier LUNs auf den Status des Snapshot zurückgesetzt werden, während das Protokoll-Volume mit seinen kritischen Daten davon unberührt bleibt.</block>
  <block id="66383d632fe33f1b7c20af9c1c142028" category="section-title">Volumes, LUNs und SnapMirror</block>
  <block id="8fb29c5f40886ece62abcd994f1aa202" category="paragraph">SnapMirror Richtlinien und Operationen werden wie Snapshot-Vorgänge auf dem Volume, nicht auf der LUN durchgeführt.</block>
  <block id="7d8d0afc8d9998a7e6457c6251b4defc" category="paragraph">Durch die Lokalisierung verwandter LUNs in einem einzelnen Volume können Sie eine einzelne SnapMirror Beziehung erstellen und alle enthaltenen Daten mit einem einzigen Update aktualisieren. Wie bei Snapshots wird auch das Update eine atomare Operation sein. Das SnapMirror Ziel würde garantiert über ein einzelnes Point-in-Time-Replikat der Quell-LUNs verfügen. Wenn die LUNs auf mehrere Volumes verteilt waren, können die Replikate miteinander konsistent sein oder nicht.</block>
  <block id="e45ffde45b5f4090fe8c92a49d38bcb2" category="section-title">Volumes, LUNs und QoS</block>
  <block id="756a4c619fb81eafd00bc072334a5877" category="paragraph">QoS kann selektiv auf einzelne LUNs angewendet werden, doch eine Festlegung auf Volume-Ebene ist in der Regel einfacher. So könnten beispielsweise alle LUNs, die die Gäste in einem bestimmten ESX Server nutzen, auf einem einzelnen Volume platziert werden, und anschließend könnte eine anpassungsfähige QoS-Richtlinie von ONTAP angewendet werden. Das Ergebnis ist ein selbst skalierendes Limit für IOPS pro TB, das für alle LUNs gilt.</block>
  <block id="fb219ca81e7a635609b70d290a7a3aa2" category="paragraph">Auch wenn eine Datenbank 100.000 IOPS benötigte und 10 LUNs belegte, wäre es einfacher, für ein einzelnes Volume eine einzige 100.000 IOPS-Grenze festzulegen, als 10 individuelle IOPS-Grenzwerte für 10.000 IOPS festzulegen, also eine für jede LUN.</block>
  <block id="56739402ea66bd29b7303b69ee165d5b" category="section-title">Multi-Volume-Layouts</block>
  <block id="c509cb5f7def30cc77b15b0e91ab39a4" category="paragraph">In einigen Fällen kann es von Vorteil sein, LUNs über mehrere Volumes zu verteilen. Der primäre Grund ist Controller-Striping. Ein HA-Storage-System kann beispielsweise eine einzige Datenbank hosten, für die das volle Verarbeitungs- und Caching-Potenzial jedes Controllers erforderlich ist. In diesem Fall würde ein typisches Design bedeuten, die Hälfte der LUNs in einem einzelnen Volume auf Controller 1 und die andere Hälfte der LUNs in einem einzelnen Volume auf Controller 2 zu platzieren.</block>
  <block id="ad328d1793d6c3621e91d83556548a01" category="paragraph">Ebenso kann das Controller-Striping für den Lastausgleich verwendet werden. Ein HA-System, das 100 Datenbanken mit jeweils 10 LUNs hostet, kann so konzipiert werden, dass jede Datenbank auf jedem der beiden Controller ein 5-LUN-Volume erhält. Wenn zusätzliche Datenbanken bereitgestellt werden, wird die symmetrische Auslastung jedes Controllers gewährleistet.</block>
  <block id="4cd2d5d8d873ad1f707d88334324153a" category="paragraph">Keines dieser Beispiele bezieht jedoch ein 1:1 Volume-zu-LUN-Verhältnis mit ein. Das Ziel bleibt die Optimierung des Managements durch Co-lokalisieren zugehöriger LUNs in Volumes.</block>
  <block id="a9fd7be1f3363701ad1b35b16af02c9e" category="paragraph">Ein Verhältnis von 1:1 LUNs zu Volumes ist beispielsweise die Containerisierung, wobei jede LUN tatsächlich einen einzelnen Workload darstellt und individuell gemanagt werden muss. In solchen Fällen kann ein Verhältnis von 1:1 optimal sein.</block>
  <block id="64e5a77897fe6326f14436eaaaf5432b" category="section-title">Zuweisung von freiem Speicherplatz und LVM-Speicherplatz</block>
  <block id="702db8f3db66a0eec35817fe5d2e4544" category="inline-link-macro">ASMRU</block>
  <block id="096ca5ad0b3774fee4699e2da1bfa884" category="paragraph">Die Effizienz des Thin Provisioning von aktiven LUNs in einer Filesystem-Umgebung kann nach und nach verloren gehen, wenn Daten gelöscht werden. Es sei denn, dass gelöschte Daten entweder mit Nullen überschrieben werden (siehe auch <block ref="5b58c4ec517e475a59d0a785f87cabdc" category="inline-link-macro-rx"></block> Oder der Speicherplatz wird mit TRIM/UNMAP-Platzreklamation freigegeben, die „gelöschten“ Daten belegen mehr und mehr nicht zugewiesene Leerzeichen im Dateisystem. Darüber hinaus kommt Thin Provisioning für aktive LUNs in vielen Datenbankumgebungen nur eingeschränkt zum Einsatz, da Datendateien zum Zeitpunkt der Erstellung auf ihre volle Größe initialisiert werden.</block>
  <block id="ceb8ec6ec593174c2c6f33be2ca966eb" category="summary">Platzierung von Oracle und ONTAP Volumes und LUNs</block>
  <block id="0724b56ced27e0a66fb4692a57fb1436" category="doc">Platzierung von Oracle Database LUNs</block>
  <block id="f595bf3bf71ab0c91a5503ba4efb8ea4" category="paragraph">Die optimale Platzierung von Datenbank-LUNs in ONTAP Volumes hängt in erster Linie davon ab, wie verschiedene ONTAP-Funktionen verwendet werden.</block>
  <block id="05a1ffdb277b8c7ac1dca78adae0017e" category="sidebar">LUN-Platzierung</block>
  <block id="9d5c13d45bbf74999764cfe8d4798562" category="summary">Security Hardening Guide for ONTAP Tools for VMware vSphere, HTTPS Certificate Management.</block>
  <block id="d6a1787b51b08315bfcc44f3a9bb4b2f" category="paragraph">Standardmäßig verwendet ONTAP-Tools ein selbstsigniertes Zertifikat, das bei der Installation automatisch erstellt wird, um den HTTPS-Zugriff auf die Web-Benutzeroberfläche zu sichern. Funktionen der ONTAP Tools:</block>
  <block id="1eddd0f3479da31541c39e5ddddfa930" category="list-text">HTTPS-Zertifikat neu generieren</block>
  <block id="61bf3033b18d202012951f5bf7be4fa9" category="paragraph">Während der Installation der ONTAP-Tools wird ein HTTPS-CA-Zertifikat installiert und das Zertifikat wird im Keystore gespeichert. Der Benutzer hat die Möglichkeit, das HTTPS-Zertifikat über die maint-Konsole neu zu generieren.</block>
  <block id="b363a018e3596ec66080579ce5bd4c2d" category="paragraph">Auf die oben genannten Optionen kann in der _maint_-Konsole zugegriffen werden, indem Sie zu _'Anwendungskonfiguration' → 'Zertifikate erneut generieren' navigieren._</block>
  <block id="43d609906e5ac2fd9406eddd67d6f82a" category="summary">Security Hardening Guide for ONTAP Tools for VMware vSphere, user Access Points.</block>
  <block id="3562b87b566382dc56e8b6fb2b849126" category="doc">ONTAP Tools für VMware vSphere Access Points (User)</block>
  <block id="c015e2af845448c5e88873c4e51c8cac" category="paragraph">Mit der Installation der ONTAP Tools für VMware vSphere werden drei Benutzertypen erstellt und verwendet:</block>
  <block id="6399e228248ba3e1f084750197938b67" category="list-text">Systembenutzer: Das root-Benutzerkonto</block>
  <block id="2a408bd40a0a3130a390340955a699ef" category="list-text">Anwendungsbenutzer: Administratorbenutzer, Benutzerkonten und db-Benutzerkonten</block>
  <block id="5d212a60c02bc59306c8863b31342276" category="list-text">Support-Benutzer: Das diag-Benutzerkonto</block>
  <block id="3c2b7b0ce42bb5eb8141643a9729f4ee" category="section-title">1. Systembenutzer</block>
  <block id="b277c4882e7365ab0ac88d56cd6154b3" category="paragraph">System(root) Benutzer wird von ONTAP-Tools-Installation auf dem zugrunde liegenden Betriebssystem (Debian) erstellt.</block>
  <block id="155b9191864da17a7eaaf07b64d596d8" category="list-text">Ein Standardsystembenutzer "root" wird auf Debian von der Installation der ONTAP-Tools erstellt. Der Standardwert ist deaktiviert und kann per Ad-hoc-Funktion über die „Wartung“-Konsole aktiviert werden.</block>
  <block id="159efbffccd1957bc8c4c78b2a74c085" category="section-title">2. Anwendungsbenutzer</block>
  <block id="170cab8b15d5b37a17e43144afe7308d" category="paragraph">Der Anwendungsbenutzer wird in ONTAP-Tools als lokaler Benutzer benannt. Diese Benutzer wurden in der Anwendung ONTAP Tools erstellt. In der folgenden Tabelle sind die Typen von Anwendungsbenutzern aufgeführt:</block>
  <block id="e01ca242f9b1bef82800de4c209149c7" category="cell">* Benutzer*</block>
  <block id="f14c276c07197ebef1394a5a219087a1" category="cell">*Beschreibung*</block>
  <block id="9eb92414db7b53ac16645af9b0f64e58" category="cell">Administratorbenutzer</block>
  <block id="ccf6887baccb4288ae3579b56fac8d2a" category="cell">Es wird während der Installation von ONTAP Tools erstellt, und Benutzer bietet die Anmeldeinformationen während der Bereitstellung der ONTAP Tools. Benutzer haben die Möglichkeit, das 'Passwort' in der 'Wartung'-Konsole zu ändern. Das Passwort läuft in 90 Tagen ab, und Benutzer werden davon ausgehen, dass es sich um dasselbe Passwort handelt.</block>
  <block id="55df75b800db0f96de770b3c38f676b4" category="cell">Wartungsbenutzer</block>
  <block id="a2bc121b09cae43d6c478870f5298ae5" category="cell">Es wird während der Installation von ONTAP Tools erstellt, und Benutzer bietet die Anmeldeinformationen während der Bereitstellung der ONTAP Tools. Benutzer haben die Möglichkeit, das 'Passwort' in der 'Wartung'-Konsole zu ändern. Dies ist ein Wartungsbenutzer und wird zur Ausführung der Wartungskonsolenoperationen erstellt.</block>
  <block id="b124032a093d41b9efbb43f40c538bcb" category="cell">Datenbankbenutzer</block>
  <block id="86a670227d3e81e8fa34eb4aede3129a" category="section-title">3. Support user(diag user)</block>
  <block id="e482d8f0e37161e7916c7a78d2cf7210" category="paragraph">Während der Installation der ONTAP-Tools wird ein Support-Benutzer erstellt. Dieser Benutzer kann für den Zugriff auf ONTAP-Tools bei Problemen oder Ausfällen im Server und zum Sammeln von Protokollen verwendet werden. Standardmäßig ist dieser Benutzer deaktiviert, kann aber per Ad-hoc-Funktion über die „Wartung“-Konsole aktiviert werden. Beachten Sie, dass dieser Benutzer nach einem bestimmten Zeitraum automatisch deaktiviert wird.</block>
  <block id="562596399deefc1e2a6cc32eb489976c" category="summary">Security Hardening Guide for ONTAP Tools for VMware vSphere.</block>
  <block id="80dd7fd00106118cf9c10abf911b7111" category="paragraph">In dieser Liste sind die erforderlichen Ports und Protokolle aufgeführt, die die Kommunikation zwischen ONTAP Tools für VMware vSphere Server und anderen Einheiten wie gemanagten Storage-Systeme, Servern und anderen Komponenten ermöglichen.</block>
  <block id="d40dfcfdaecc4e8015535acd5de00398" category="section-title">Für OTV erforderliche ein- und ausgehende Ports</block>
  <block id="834b74ce890c61455ad330759d44eff2" category="paragraph">Bitte beachten Sie die folgende Tabelle, in der die ein- und ausgehenden Ports aufgeführt sind, die für das ordnungsgemäße Funktionieren der ONTAP-Tools erforderlich sind. Es ist wichtig sicherzustellen, dass nur die in der Tabelle genannten Ports für Verbindungen von Remotecomputern geöffnet sind, während alle anderen Ports für Verbindungen von Remotecomputern gesperrt werden sollten. Dadurch wird die Sicherheit und Sicherheit Ihres Systems gewährleistet.</block>
  <block id="5732d3577c35dfcce968ba967a9a6149" category="cell">*TCP v4/v6-Port #*</block>
  <block id="e71a4b7488a97c0e5d8b3ab3efe619e4" category="cell">*Richtung*</block>
  <block id="bc437eb0df24b836c84a74f47b391093" category="cell">*Funktion*</block>
  <block id="576fe92cf4e45714b61ed10e5c856aa5" category="cell">HTTPS-Verbindungen +
Wird für SOAP über HTTPS-Verbindungen verwendet +
Dieser Port muss geöffnet werden, damit ein Client eine Verbindung zum ONTAP Tools API-Server herstellen kann.</block>
  <block id="4e31bd5bdc8239bb7c98471d3500d951" category="cell">HTTPS-Verbindungen - VP und SRA +
Wird für SOAP-Verbindungen über HTTPS verwendet</block>
  <block id="7e6b20d014d1c659e4eaf20dc2dcd7eb" category="cell">8443</block>
  <block id="aaa2d7fe63bee55de8ffca52cb61fbd5" category="cell">Remote-Plug-In</block>
  <block id="25fa62922a9b3c1c2763bbf88014e217" category="cell">Derby-Datenbank-Port, nur zwischen diesem Computer und sich selbst, externe Verbindungen nicht akzeptiert - nur interne Verbindungen</block>
  <block id="c68bd9055776bf38d8fc43c0ed283678" category="cell">8150</block>
  <block id="b482bc4bf0e383b0858c5d9eb0e96ba9" category="cell">Der Protokollintegritätsservice wird auf dem Port ausgeführt</block>
  <block id="9956361dfa33899fc0efed6ee3946512" category="section-title">Steuern des Remote-Zugriffs auf die Derby-Datenbank</block>
  <block id="58b7326b56820f4b18dfd73dc9707831" category="paragraph">Administratoren können mit den folgenden Befehlen auf die derby-Datenbank zugreifen. Der Zugriff ist über die lokale VM der ONTAP-Tools sowie über einen Remote-Server mit den folgenden Schritten möglich:</block>
  <block id="296068c08046bbf197300e38c4e851cb" category="paragraph">*[.underline]#Beispiel:#*</block>
  <block id="10046bc1eb8463e6ca6ae51bd3b7a574" category="inline-image-macro">Derby, width=468, height=136</block>
  <block id="36434da35edb72a1f85cbbcc1ff2b197" category="paragraph"><block ref="36434da35edb72a1f85cbbcc1ff2b197" category="inline-image-macro-rx" type="image"></block></block>
  <block id="f21dd7ffc271f1301878c3078a59692d" category="summary">Security Hardening Guide for ONTAP Tools for VMware vSphere, Anmeldebanner.</block>
  <block id="67f78f0bab8fe81a2b6971543c716d3d" category="paragraph">Das folgende Anmeldebanner wird angezeigt, nachdem der Benutzer einen Benutzernamen in die Anmeldeaufforderung eingegeben hat. Beachten Sie bitte, dass SSH standardmäßig deaktiviert ist und nur einmalige Anmeldungen zulässt, wenn sie über die VM-Konsole aktiviert werden.</block>
  <block id="ae20daf991a940f9de7033ff0a1af410" category="paragraph">Nachdem der Benutzer die Anmeldung über den SSH-Kanal abgeschlossen hat, wird der folgende Text angezeigt:</block>
  <block id="8f1b6f904292913013b1f6e578d7b074" category="summary">Security Hardening Guide for ONTAP Tools for VMware vSphere, Integrity Verification.</block>
  <block id="cedd87fff2ad5101cd8565e33950e1fb" category="doc">Überprüfen der Integrität der ONTAP Tools für VMware vSphere Installationspakete</block>
  <block id="aeeb9f5277554716592480db544d25f3" category="paragraph">Es gibt zwei Methoden, mit denen Kunden die Integrität ihrer Installationspakete für ONTAP-Tools überprüfen können.</block>
  <block id="126ba7383e25522e071a963ddd6f220f" category="list-text">Überprüfen der Prüfsummen</block>
  <block id="deccc645a0c8869a44e645f2f4247dbd" category="list-text">Überprüfen der Signatur</block>
  <block id="e5f3970dd7fabab1726b53d58d636de2" category="paragraph">Prüfsummen werden auf den Download-Seiten der OTV-Installationspakete zur Verfügung gestellt. Benutzer müssen die Prüfsummen der heruntergeladenen Pakete anhand der auf der Download-Seite angegebenen Prüfsumme überprüfen.</block>
  <block id="7452fe022f94726684aeea655673eb0f" category="section-title">Überprüfen der Signatur der ONTAP-Tools OVA</block>
  <block id="373d060b52f94a014177952dcca0c4e9" category="paragraph">Das vApp-Installationspaket wird in Form eines Tarballs geliefert. Dieser Tarball enthält Zwischen- und Root-Zertifikate für das virtuelle Gerät sowie eine README-Datei und ein OVA-Paket. Die README-Datei führt Benutzer dazu, wie die Integrität des vApp OVA-Pakets überprüft wird.</block>
  <block id="48fefdb76906b256f183e6f68b23eba1" category="paragraph">Kunden müssen auch das bereitgestellte Root- und Intermediate-Zertifikat auf vCenter Version 7.0U3E und höher hochladen.  Bei vCenter-Versionen zwischen 7.0.1 und 7.0.U3E wird die Funktion zur Überprüfung des Zertifikats von VMware nicht unterstützt. Kunden müssen kein Zertifikat für vCenter Version 6.x hochladen</block>
  <block id="84a005842347a9560b170cfcd79c5203" category="section-title">Hochladen des vertrauenswürdigen Stammzertifikats in vCenter</block>
  <block id="57f60da4817dc5d71c45890bb182781e" category="list-text">Melden Sie sich mit dem VMware vSphere Client beim vCenter Server an.</block>
  <block id="72282f16a7aca6b61acfc6fe27ce55c7" category="list-text">Geben Sie den Benutzernamen und das Kennwort für administrator@vsphere.local oder ein anderes Mitglied der vCenter Single Sign-On-Administratorgruppe an. Wenn Sie während der Installation eine andere Domäne angegeben haben, melden Sie sich als Administrator@mydomain an.</block>
  <block id="9470c9e64c285d5b66a039ab668ab64a" category="list-text">Navigieren Sie zur Benutzeroberfläche Zertifikatverwaltung: a. Wählen Sie im Home-Menü die Option Administration. B. Klicken Sie unter Zertifikate auf Zertifikatverwaltung.</block>
  <block id="43a75b176810226695d2db6f52c87b73" category="list-text">Wenn Sie vom System aufgefordert werden, geben Sie die Anmeldedaten Ihres vCenter-Servers ein.</block>
  <block id="5f51c5b3cb2f48fd1d670bf384d1006d" category="list-text">Klicken Sie unter Vertrauenswürdige Stammzertifikate auf Hinzufügen.</block>
  <block id="55c3b759c54a437c7ced6432f1c62de0" category="list-text">Klicken Sie auf Durchsuchen, und wählen Sie den Speicherort der .pem-Zertifikatdatei (OTV_OVA_INTER_ROOT_CERT_CHAIN.pem) aus.</block>
  <block id="8acdedaaa4134bb6ae49472b4bc268a7" category="list-text">Klicken Sie Auf Hinzufügen. Das Zertifikat wird dem Store hinzugefügt.</block>
  <block id="e2200da08ea15bdea0f97dbfc4e9f1c7" category="inline-link-macro">Fügen Sie dem Zertifikatspeicher ein vertrauenswürdiges Stammzertifikat hinzu</block>
  <block id="6eeecddd909e7e017ba2732e46dc739c" category="paragraph">Siehe <block ref="b8e45b03638af10d852f253eba7e63e3" category="inline-link-macro-rx"></block> Finden Sie weitere Informationen. Während der Bereitstellung einer vApp (mithilfe der OVA-Datei) kann die digitale Signatur für das vApp-Paket auf der Seite „Details überprüfen“ überprüft werden. Wenn es sich bei dem heruntergeladenen vApp-Paket um ein Originalprodukt handelt, wird in der Spalte „Publisher“ (Herausgeber) die Option „Vertrauenswürdiges Zertifikat“ angezeigt (wie im folgenden Screenshot).</block>
  <block id="a13b63a149750821fb588cd3cb41c209" category="inline-image-macro">Vertrauenswürdiges Zertifikat</block>
  <block id="a83d8109f7a0cf9c4a04c65c40ecd354" category="paragraph"><block ref="a83d8109f7a0cf9c4a04c65c40ecd354" category="inline-image-macro-rx" type="image"></block></block>
  <block id="61dc5ac976a8bd4a8744d8890305d26e" category="section-title">Überprüfen der Signatur der ONTAP-Tools ISO und SRA tar.gz</block>
  <block id="d4f26c1c073b64ba200e3fa71762c256" category="paragraph">NetApp teilt sein Code Signing-Zertifikat mit Kunden auf der Produkt-Download-Seite, zusammen mit den Produkt-Zip-Dateien für OTV-ISO und SRA.tgz.</block>
  <block id="a7ac1e7561da5cc70126cfea891de159" category="paragraph">Aus dem Code-Signing-Zertifikat können Benutzer den öffentlichen Schlüssel wie folgt extrahieren:</block>
  <block id="5f4345a6f62f7a3926b2c3d92f5574f6" category="paragraph">Dann sollte der öffentliche Schlüssel verwendet werden, um die Signatur für iso und tgz Produkt zip wie unten zu überprüfen:</block>
  <block id="81eeab9506186e2dca8faefa78d54067" category="paragraph">Beispiel:</block>
  <block id="c6666237b03fcb86e315a0a06e73b6ef" category="summary">Security Hardening Guide for ONTAP Tools for VMware vSphere, Mutual TLS Encryption for Storage Management connections.</block>
  <block id="53893182b7e17fb2da37935ee3ffda55" category="paragraph">ONTAP Version 9.7 und höher unterstützen die gegenseitige TLS-Kommunikation. Ab ONTAP Tools für VMware und vSphere 9.12 wird wechselseitiges TLS für die Kommunikation mit neu hinzugefügten Clustern verwendet (je nach ONTAP Version).</block>
  <block id="1be712dee4c47804142318ffb739e46b" category="paragraph">Für alle zuvor hinzugefügten Speichersysteme: Während eines Upgrades werden alle hinzugefügten Speichersysteme automatisch vertrauenswürdig und die zertifikatbasierten Authentifizierungsmechanismen werden konfiguriert.</block>
  <block id="d39adf53eaace0fbba30d4e13760a974" category="paragraph">Wie im Screenshot unten gezeigt, zeigt die Cluster-Setup-Seite den Status von Mutual TLS (Certificate-Based Authentication) an, die für jeden Cluster konfiguriert wurden.</block>
  <block id="c036889d83926f57571c8b12e4f7fb6f" category="inline-image-macro">bild2,Breite=468,Höhe=158</block>
  <block id="095064767a4b9cadb4016dfc2788237c" category="paragraph"><block ref="095064767a4b9cadb4016dfc2788237c" category="inline-image-macro-rx" type="image"></block></block>
  <block id="53f061d401dd80027634ccca2a19cd8d" category="section-title">*Cluster Hinzufügen*</block>
  <block id="7ce4b2757e70979f49bcf8f64ba36363" category="paragraph">Wenn das hinzugefügte Cluster MTLS unterstützt, wird MTLS während des Cluster-Add-Workflows standardmäßig konfiguriert. Der Benutzer muss hierfür keine Konfiguration vornehmen. Im Screenshot unten wird der Bildschirm angezeigt, der dem Benutzer beim Hinzufügen von Cluster angezeigt wird.</block>
  <block id="3c1556a297200019d3a0b8045c0956b9" category="inline-image-macro">Storage-System hinzufügen, Breite=450, Höhe=400</block>
  <block id="3a225aabc3c5c25670ad0b4d131da24f" category="paragraph"><block ref="3a225aabc3c5c25670ad0b4d131da24f" category="inline-image-macro-rx" type="image"></block></block>
  <block id="987de6e025f6af1fc4451710295ca76f" category="inline-image-macro">Storage-System hinzufügen, Breite=468, Höhe=416</block>
  <block id="576a9128888c73fc82869b3ae173c7d2" category="paragraph"><block ref="576a9128888c73fc82869b3ae173c7d2" category="inline-image-macro-rx" type="image"></block></block>
  <block id="307eba22b8070bb93e642827620f8588" category="paragraph"><block ref="307eba22b8070bb93e642827620f8588" category="inline-image-macro-rx" type="image"></block></block>
  <block id="71cfed326a2cabd3b505c6580123a459" category="inline-image-macro">Storage-System hinzufügen, Breite=468, Höhe=516</block>
  <block id="a3a8f9da50b3f15d69f3186129f159f6" category="paragraph"><block ref="a3a8f9da50b3f15d69f3186129f159f6" category="inline-image-macro-rx" type="image"></block></block>
  <block id="3b3ebbeea272602ddec07bb77e34668a" category="section-title">Cluster-Bearbeitung</block>
  <block id="d2562e450657937f981a45d9d18ff4ec" category="paragraph">Während der Cluster-Bearbeitung gibt es zwei Szenarien:</block>
  <block id="500bcb89776517849e2e07073aad4eca" category="list-text">Wenn das ONTAP-Zertifikat abläuft, muss der Benutzer das neue Zertifikat erhalten und hochladen.</block>
  <block id="edf2227e3378b5b0864ef73e58d39d77" category="list-text">Wenn das OTV-Zertifikat abläuft, kann der Benutzer es durch Aktivieren des Kontrollkästchens neu generieren.</block>
  <block id="6db8cdf80049c7cf629fe6f8f6e3a9c0" category="list-text">_Generieren Sie ein neues Client-Zertifikat für ONTAP._</block>
  <block id="acba923a9524c642724d6ff34f5dac93" category="inline-image-macro">Speichersystem ändern, Breite=468, Höhe=461</block>
  <block id="6a59df9970674c274e20b1d91be2df8a" category="paragraph"><block ref="6a59df9970674c274e20b1d91be2df8a" category="inline-image-macro-rx" type="image"></block></block>
  <block id="75545011d3e6abe8a2cd8cd5bc21c6fd" category="paragraph"><block ref="75545011d3e6abe8a2cd8cd5bc21c6fd" category="inline-image-macro-rx" type="image"></block></block>
  <block id="14a73da2f5fe352620bed1b49a3319ed" category="summary">Security Hardening Guide for ONTAP Tools for VMware vSphere, Netzwerksicherheitsschutz gegen DOS-Angriffe.</block>
  <block id="8f7e7bd942f453f0550dc4f907e88810" category="paragraph">Standardmäßig beträgt die maximale Anzahl gleichzeitiger Anfragen pro Benutzer 48. Der Root-Benutzer in ONTAP-Tools kann diesen Wert je nach den Anforderungen seiner Umgebung ändern. *Dieser Wert sollte nicht auf einen sehr hohen Wert gesetzt werden, da er einen Mechanismus gegen Denial-of-Service (DOS) Angriffe bietet.*</block>
  <block id="99c1088ea27d16b42c6b45d794f59d3f" category="paragraph">Benutzer können die Anzahl der maximalen gleichzeitigen Sessions und andere unterstützte Parameter in der Datei *_/opt/netapp/vscserver/etc/dosfilterParams.json_* ändern.</block>
  <block id="5e7c907630f5a432ac6b39f0b8c04c28" category="paragraph">Wir können den Filter mit folgenden Parametern konfigurieren:</block>
  <block id="a35f2f4ccb228969a85b8719ea25b047" category="list-text">*_delayMs_*: Die Verzögerung in Millisekunden, die allen Anfragen über das Limit der Rate gegeben wird, bevor sie berücksichtigt werden. Geben Sie -1, um die Anfrage einfach abzulehnen.</block>
  <block id="f1b9d5fde713093a3a80f140d1439d37" category="list-text">*_drossleMS_*: Wie lange warten Sie auf Semaphore.</block>
  <block id="aa363bec6acd7600aedeee5377470bde" category="list-text">*_maxRequestms_*: Wie lange soll diese Anfrage laufen lassen?</block>
  <block id="351b695725caafd647abb749c50a7e6f" category="list-text">*_ipWhitelist_*: Eine kommagetrennte Liste von IP-Adressen, die nicht ratenbegrenzt ist. (Dies können vCenter-, ESXi- und SRA-IPs sein)</block>
  <block id="058e1fe7e6afcd3bcff86482a583976b" category="list-text">*_maxRequestsPerSec_*: Die maximale Anzahl von Anfragen einer Verbindung pro Sekunde.</block>
  <block id="1c1496a21f967d57b50fe5ebb0e62125" category="paragraph">*Standardwerte in der _dosfilterParams-Datei_:*</block>
  <block id="ca49b2b6b0c4bb4aa0596879eef667f5" category="inline-image-macro">Multipathing-Konnektivität</block>
  <block id="de3698c6b3c2722a1b82c99522519f4f" category="paragraph"><block ref="de3698c6b3c2722a1b82c99522519f4f" category="inline-image-macro-rx" type="image"></block></block>
  <block id="9afb15056a8bc8b4402d55eba2274dad" category="summary">Security Hardening Guide for ONTAP Tools for VMware vSphere, NTP Configuration.</block>
  <block id="48edbd8a951efb0ad617f7868b27a6c0" category="paragraph">Manchmal können Sicherheitsprobleme aufgrund von Diskrepanzen bei der Konfiguration der Netzwerkzeit auftreten. Es ist wichtig, dass alle Geräte innerhalb eines Netzwerks über genaue Zeiteinstellungen verfügen, um solche Probleme zu vermeiden.</block>
  <block id="a55d703ea2930e8e72e94ce11c3297ab" category="section-title">*Virtuelles Gerät*</block>
  <block id="5a65f4c9ca6d99f573528348d36914fa" category="paragraph">Sie können den/die NTP-Server über die Wartungskonsole in der virtuellen Appliance konfigurieren.  Benutzer können die NTP-Server-Details unter der Option _System Configuration_ =&gt; _Add New NTP Server_ hinzufügen</block>
  <block id="90ab4fc7a1c530bceac36bda71efbc5f" category="paragraph">Standardmäßig lautet der Service für NTP ntpd. Es handelt sich hierbei um einen Legacy-Service, der in bestimmten Fällen für virtuelle Maschinen nicht gut funktioniert.</block>
  <block id="6801ecdb823d018c7370042d27b5ee2c" category="section-title">*Debian*</block>
  <block id="dc3bb5166d38d2aec53765aca7c0d24c" category="paragraph">Auf Debian kann der Benutzer auf die Datei /etc/ntp.conf zugreifen, um ntp-Serverdetails zu erhalten.</block>
  <block id="fec65f0c33739368ce5845fadb6cb880" category="paragraph">Diese Anleitungen gelten sowohl für die Anwendungen als auch für das Gastbetriebssystem des Geräts selbst.</block>
  <block id="5aaf8fc9424d7872cf500fb7adcb3b97" category="summary">Security Hardening Guide for ONTAP Tools for VMware vSphere, Konfiguration von Passwortrichtlinien.</block>
  <block id="53477738359cebc12d4e01727ee8b5e7" category="paragraph">Benutzer, die ONTAP-Tools zum ersten Mal bereitstellen oder ein Upgrade auf Version 9.12 oder höher durchführen, müssen die Richtlinie für starkes Kennwort sowohl für den Administrator als auch für Datenbankbenutzer befolgen. Während des Bereitstellungsprozesses werden neue Benutzer aufgefordert, ihre Passwörter einzugeben. Für Brownfield-Benutzer, die auf Version 9.12 oder höher aktualisieren, wird die Option zur Einhaltung der Richtlinie für starke Kennwörter in der Wartungskonsole verfügbar sein.</block>
  <block id="7afd9be20a239f83f98bdbc7f90512ef" category="list-text">Sobald sich der Benutzer in der maint-Konsole anmeldet, werden die Passwörter anhand der komplexen Regelsammlung überprüft. Wenn er nicht befolgt wird, wird der Benutzer aufgefordert, das gleiche zurückzusetzen.</block>
  <block id="1e048d74f3d9375186833526a00f4309" category="list-text">Die Standardgültigkeit des Passworts beträgt 90 Tage, und nach 75 Tagen erhält der Benutzer die Benachrichtigung, das Kennwort zu ändern.</block>
  <block id="dbb73ef3f8e9b61d387a809281a654cc" category="list-text">Es ist erforderlich, in jedem Zyklus ein neues Passwort festzulegen, das System nimmt nicht das letzte Passwort als neues Passwort.</block>
  <block id="210b05d5e7375bcb02bcf09a873bbda6" category="list-text">Wenn sich ein Benutzer an der maint-Konsole anmeldet, überprüft er vor dem Laden des Hauptmenüs nach den Passwortrichtlinien wie den folgenden Screenshots:</block>
  <block id="bc66b4fce20ef49b05e4a54f053e65a6" category="inline-image-macro">Hauptmenü,Breite=468,Höhe=116</block>
  <block id="4661acb75237098be364b3c05d7d9b7c" category="paragraph"><block ref="4661acb75237098be364b3c05d7d9b7c" category="inline-image-macro-rx" type="image"></block></block>
  <block id="8f09c69f731d4d6f2e8d6135082aa8e4" category="list-text">Wenn die Kennwortrichtlinie oder das Upgrade-Setup von ONTAP Tools 9.11 oder früher nicht verwendet wurde. Der Benutzer wird dann den folgenden Bildschirm sehen, um das Passwort zurückzusetzen:</block>
  <block id="4499d4bfb9610056781e2e61956dd0ca" category="inline-image-macro">Bildschirm zum Zurücksetzen des Passworts, Breite=468, Höhe=116</block>
  <block id="a84278f7bf3ee281e39e467fca1fe454" category="paragraph"><block ref="a84278f7bf3ee281e39e467fca1fe454" category="inline-image-macro-rx" type="image"></block></block>
  <block id="47e6664ac0a72e1d719cfc288383b4f8" category="list-text">Wenn der Benutzer versucht, ein schwaches Passwort festzulegen oder das letzte Passwort erneut gibt, wird der Benutzer folgende Fehlermeldung sehen:</block>
  <block id="fe04952c38b6a9a7db1a75bbb4905232" category="inline-image-macro">Schwacher Passwortfehler, Breite=468, Höhe=101</block>
  <block id="2c59de9ea5183e73d5fa6382fa7f3fc1" category="paragraph"><block ref="2c59de9ea5183e73d5fa6382fa7f3fc1" category="inline-image-macro-rx" type="image"></block></block>
  <block id="8463a61dd94ff5934036d4c8a8936e32" category="doc">Zeitüberschreitung Bei Inaktivität</block>
  <block id="704c7fc6c4d40b9af6aafb7cf9b44022" category="paragraph">Um unbefugten Zugriff zu verhindern, wird ein Inaktivitäts-Timeout eingerichtet, das automatisch Benutzer abmeldet, die für einen bestimmten Zeitraum inaktiv sind, während autorisierte Ressourcen verwendet werden. So wird sichergestellt, dass nur autorisierte Benutzer auf die Ressourcen zugreifen können und die Sicherheit gewahrt bleibt.</block>
  <block id="80f9ebebc8f94450338d28bcb4a2164d" category="inline-link">Konfigurieren Sie den Wert für die Zeitüberschreitung des vSphere-Clients</block>
  <block id="3748702da26b5718f1f7c769ce58b009" category="list-text">Standardmäßig werden die vSphere Client-Sitzungen nach 120 Minuten Leerlaufzeit geschlossen, sodass sich der Benutzer erneut anmelden muss, um die Verwendung des Clients fortzusetzen. Sie können den Timeout-Wert ändern, indem Sie die Datei webclient.properties bearbeiten. Sie können das Timeout des vSphere-Clients konfigurieren<block ref="ec8715bde1a929a6f64160d5372f4574" category="inline-link-rx"></block></block>
  <block id="e4306c4cf124046d8f90518752cd5021" category="list-text">Die Abmeldezeit für ONTAP-Tools beträgt 30 Minuten für die Web-cli-Sitzung.</block>
  <block id="7fb0cadcdae6feac05c7b8274b97b3c7" category="sidebar">Übersicht über die ONTAP-Tools zur Härtung</block>
  <block id="10b486896b1a91f5d38214dfb30c4263" category="sidebar">Überprüfen der Integrität der Installationspakete für ONTAP-Tools</block>
  <block id="7c7c4cc4771d6114aa8b4d38f5817a85" category="sidebar">Ports und Protokolle</block>
  <block id="919e26ea009ce7fa2dcc499a27ccc6aa" category="sidebar">Access Points (Benutzer)</block>
  <block id="333dd505db0523536c6885f47456518c" category="sidebar">Gegenseitiges TLS</block>
  <block id="6a3f3799bc264d32568bb7169ddd515a" category="sidebar">HTTPS-Zertifikat</block>
  <block id="4b7ad38343c4b2010cf81e6b2933e186" category="sidebar">Anmeldebanner</block>
  <block id="bfdb89134acf30ff5da790924cefa0ac" category="sidebar">Zeitüberschreitung bei Inaktivität</block>
  <block id="cb197b06aa92a742d01e256d58d3e34b" category="sidebar">DOS-Angriff</block>
  <block id="d870a4bd29f74339732a958574d2c4eb" category="sidebar">NTP-Konfiguration</block>
  <block id="339ce1d4220ba7045e3a35ef35279ad3" category="sidebar">Passwortrichtlinien</block>
  <block id="3698338a95ad1801e25ed154adaf1a70" category="summary">Leitfaden zur Erhöhung der Sicherheit für ONTAP Tools für VMware vSphere, Überblick und Einführung.</block>
  <block id="47d29daaa26fbfa9c9a069d4cd023ac1" category="doc">Leitfaden zur Erhöhung der Sicherheit für ONTAP Tools für VMware vSphere</block>
  <block id="fe0e23a929284a636c245a362ada6472" category="paragraph">Der Security Hardening Guide für ONTAP-Tools für VMware vSphere enthält umfassende Anweisungen zur Konfiguration der sichersten Einstellungen.</block>
  <block id="bc86ce4caa57368f9094a93dd72ffd28" category="paragraph">Wo der Domino-Modus erforderlich ist, bietet NetApp SnapMirror Synchronous (SM-S) an. Darüber hinaus gibt es Optionen auf Applikationsebene wie Oracle DataGuard oder SQL Server Always On Availability Groups. Für die Festplattenspiegelung auf Betriebssystemebene kann eine Option sein. Wenden Sie sich an Ihren NetApp oder Ihr Partner Account Team, um weitere Informationen und Optionen zu erhalten.</block>
  <block id="8059304404c995bd8a349056e7d40e32" category="inline-link">_NetApp-Lösungen für die Virtualisierung mit VMware von Broadcom_</block>
  <block id="cc5363f91541e771ebd890b2330cb8ba" category="paragraph"><block ref="cc5363f91541e771ebd890b2330cb8ba" category="inline-link-rx"></block></block>
  <block id="c90946faf2ed37e17d936e0b3acde7a7" category="summary">Die Nachfrage nach der Verschlüsselung von Daten im Ruhezustand steigt immer weiter an und umfasst damit mehr als herkömmliche Daten für alle gespeicherten Datentypen.</block>
  <block id="b1e1676a19b222da72a55b95aa86d3e9" category="paragraph">In den folgenden Ressourcen erhalten Sie ausführliche Informationen zu weiteren Sicherheitsthemen.</block>
  <block id="e6bd708199eed7d8bf33ef8308be0b3e" category="inline-link-macro">Technische Sicherheitsberichte</block>
  <block id="fb7f6edb08c080e81bd91a922a9d08f8" category="list-text"><block ref="fb7f6edb08c080e81bd91a922a9d08f8" category="inline-link-macro-rx"></block></block>
  <block id="ff8c9f0959eb84d21416835c3f9c89d6" category="inline-link-macro">Leitfäden zur Erhöhung der Sicherheit</block>
  <block id="589b1ebfaed268e1548717208be6131f" category="list-text"><block ref="589b1ebfaed268e1548717208be6131f" category="inline-link-macro-rx"></block></block>
  <block id="e44de13cc4b452eccde9961bbe9da42b" category="inline-link-macro">Produktdokumentation zu ONTAP Sicherheit und Datenverschlüsselung</block>
  <block id="c76e848365d3a2199dfced47ddbf8690" category="list-text"><block ref="c76e848365d3a2199dfced47ddbf8690" category="inline-link-macro-rx"></block></block>
  <block id="0e8f1228bcea37dc870b3c00ddfebb6b" category="list-text">*First-Party-Angebote.* Amazon FSX for NetApp ONTAP, Google Cloud NetApp Volumes und Azure NetApp Files for ANF bieten hochperformante, Multiprotokoll-gemanagte Storage-Services in den führenden Public-Cloud-Umgebungen. Sie können direkt von VMware Cloud on AWS (VMC on AWS), Azure VMware Solution (AVS) und Google Cloud VMware Engine (GCVE) als Datastores oder Storage für Gastbetriebssysteme (GOS) und Rechnungsinstanzen verwendet werden.</block>
  <block id="2c95b4b56070479502ffc85344c7d8b2" category="list-text">*Cloud-Services.* mit BlueXP Backup und Recovery oder SnapMirror Cloud sichern Sie Daten von lokalen Systemen, die Public-Cloud-Storage nutzen. Cloud Sync hilft bei der Migration und bei der Synchronisierung Ihrer Daten in NAS-, Objektspeicher- und Cloud Volumes Service-Storage. BlueXP Disaster Recovery bietet eine kostengünstige und effiziente Lösung zur Nutzung von NetApp Technologien als Grundlage für eine robuste und fähige Disaster-Recovery-Lösung für DR in die Cloud, DR in On-Premises-Umgebungen und lokale Umgebungen.</block>
  <block id="1a523ba4ece9125275b92ebb41e3cf23" category="inline-link-macro">Cloud Volumes ONTAP-Dokumentation</block>
  <block id="e4501a9ef639002a658af0113906add9" category="list-text"><block ref="e4501a9ef639002a658af0113906add9" category="inline-link-macro-rx"></block></block>
  <block id="7436b418bed093768e360d32105591d2" category="inline-link-macro">ONTAP Select-Dokumentation</block>
  <block id="82fe32c89de73116e2188701ca735c39" category="list-text"><block ref="82fe32c89de73116e2188701ca735c39" category="inline-link-macro-rx"></block></block>
  <block id="c756b6b1ba6b752239a8d49eda5ad2a4" category="inline-link-macro">BlueXP Backup- und Recovery-Dokumentation</block>
  <block id="f99b2e5828a20fe99efc9b8c25b42d83" category="list-text"><block ref="f99b2e5828a20fe99efc9b8c25b42d83" category="inline-link-macro-rx"></block></block>
  <block id="b044111e167403664c9e3432eca1f25a" category="inline-link-macro">BlueXP Disaster Recovery-Dokumentation</block>
  <block id="ec39a9c8e559c87efc4a1ac3c8dfd136" category="list-text"><block ref="ec39a9c8e559c87efc4a1ac3c8dfd136" category="inline-link-macro-rx"></block></block>
  <block id="f5c286d9dc4cdaffcb8e5c86388e6ed8" category="list-text"><block ref="f5c286d9dc4cdaffcb8e5c86388e6ed8" category="inline-link-macro-rx"></block></block>
  <block id="4dab2f9796b0af6304d00e21e2b9dfb4" category="inline-link-macro">VMware Cloud auf AWS</block>
  <block id="560d5b2cd40977bd7b77b31d7088f657" category="list-text"><block ref="560d5b2cd40977bd7b77b31d7088f657" category="inline-link-macro-rx"></block></block>
  <block id="e6aa20b5d3923ee0466f5d7e7bfed821" category="inline-link-macro">Was ist Azure NetApp Files?
</block>
  <block id="810dc5c2e801b56b11325dd2aca0775c" category="list-text"><block ref="810dc5c2e801b56b11325dd2aca0775c" category="inline-link-macro-rx"></block></block>
  <block id="9789f0612fda153726aa8ad87265e4b9" category="inline-link-macro">Azure VMware Lösung</block>
  <block id="f068ad668d317d445bc0da61125c318f" category="list-text"><block ref="f068ad668d317d445bc0da61125c318f" category="inline-link-macro-rx"></block></block>
  <block id="8cb69aa5b1609d0a39f0fcd576c07df6" category="inline-link-macro">Google Cloud VMware Engine</block>
  <block id="dadb2f85467c159b9d8387744cdd0ce6" category="list-text"><block ref="dadb2f85467c159b9d8387744cdd0ce6" category="inline-link-macro-rx"></block></block>
  <block id="e6288bb2b93ca74537fbff66317f01d9" category="inline-link-macro">Was ist Google Cloud NetApp Volumes?</block>
  <block id="e19aa8cca2f701dba5fc123376bebdb5" category="list-text"><block ref="e19aa8cca2f701dba5fc123376bebdb5" category="inline-link-macro-rx"></block></block>
  <block id="fe0d13586251492122aca2ce92a53779" category="cell">Behalten Sie die Standardeinstellung (0) bei, können sie jedoch bei Bedarf geändert werden.
Weitere Informationen finden Sie unter <block ref="b982e0cf7a2ddd8de49cd5a4b5cefbd7" category="inline-link-macro-rx"></block></block>
  <block id="b44c4aa4ed40a2c01a89549bc8746cbd" category="cell">Sind die meisten vSphere 6.X Versionen auf 512 MB eingestellt.
Für 6.5U3, 6.7U3 und 7.0 oder höher auf Standard (1024 MB) gesetzt.</block>
  <block id="f3c4f211a4831f8f7c223705b0fccd5d" category="paragraph">Durchsatzbegrenzungen sind bei der Steuerung von Service-Levels, dem Management unbekannter Workloads oder beim Testen von Applikationen vor der Implementierung nützlich, um sicherzustellen, dass sie sich nicht auf andere Workloads in der Produktion auswirken. Sie können auch zur Beschränkung eines als problematisch identifizierten Workloads eingesetzt werden.</block>
  <block id="5ab12aefceb83851b73081a686bd30ab" category="section-title">Unterstützung von ONTAP QoS-Richtlinien</block>
  <block id="8aa45e69fd8330d5e4d2f6e16309ff41" category="paragraph">Minimale Service-Level auf Basis der IOPS werden ebenfalls unterstützt, um SAN-Objekten in ONTAP 9.2 und NAS-Objekten in ONTAP 9.3 eine konsistente Performance bereitzustellen.</block>
  <block id="98d229565d14d4d286f57a588a784c1e" category="inline-link-macro">Performance Monitoring und Management – Überblick</block>
  <block id="b7629a90716592b8dcae7480ad66d224" category="paragraph">Siehe <block ref="de4b907e949a377b72cd0f0b0595d155" category="inline-link-macro-rx"></block> Finden Sie weitere Informationen.</block>
  <block id="81eba6b819a671b6c8962061afd13a78" category="section-title">Nicht-VVols NFS-Datastores</block>
  <block id="5bafceb9f14cf4b07086992cd31326a5" category="paragraph">Eine ONTAP QoS-Richtlinie kann auf den gesamten Datenspeicher oder auf einzelne VMDK-Dateien darin angewendet werden. Es ist jedoch wichtig zu beachten, dass alle VMs eines herkömmlichen NFS-Datenspeichers (ohne VVols) eine gemeinsame I/O-Warteschlange von einem bestimmten Host verwenden. Wenn eine VM durch eine ONTAP QoS-Richtlinie gedrosselt ist, werden in der Praxis alle I/O-Vorgänge für diesen Datastore scheinbar für diesen Host gedrosselt.</block>
  <block id="1520a4910ffa6efc27379931cc307d0b" category="paragraph">*Beispiel:*
* Sie konfigurieren eine QoS-Begrenzung auf vm1.vmdk für ein Volume, das als herkömmlicher NFS-Datenspeicher durch Host esxi-01 gemountet wird.
* Der gleiche Host (esxi-01) verwendet vm2.vmdk und es ist auf dem gleichen Volume.
* Wenn vm1.vmdk gedrosselt wird, dann wird vm2.vmdk auch scheinen gedrosselt zu sein, da es sich die gleiche IO-Warteschlange mit vm1.vmdk teilt.</block>
  <block id="caf18b31d986fc095c3c0cd6a1f19f9e" category="paragraph">Ab vSphere 6.5 können Sie bei Datastores, die nicht über VVols verfügen, granulare Dateilimits managen. Sie nutzen dazu Storage Policy-basiertes Management (SPBM) mit Storage I/O Control (SIOC) v2.</block>
  <block id="c28029445f9803e209550517678387a9" category="paragraph">Weitere Informationen zum Leistungsmanagement mit SIOC- und SPBM-Richtlinien finden Sie unter den folgenden Links.</block>
  <block id="8bef032522908e987555b77fb50dbb85" category="inline-link-macro">SPBM Host-basierte Regeln: SIOC v2</block>
  <block id="98a7507b3e8be801707c07e336bc5506" category="inline-link-macro">Managen Sie Storage-I/O-Ressourcen mit vSphere</block>
  <block id="37b0c263c77e85e59d9f00438e870fa4" category="paragraph"><block ref="ea373db41b8c05446b96b46f4bbe1141" category="inline-link-macro-rx"></block>
<block ref="c5badf4fd27434d786395fe52801b533" category="inline-link-macro-rx"></block></block>
  <block id="cd6af799ac31b0fbf81bcbba1d3d6bf1" category="list-text">Die Politik muss auf das angewendet werden<block ref="3b9aef0680339707b430261c2f800255" prefix=" " category="inline-code"></block> Die das tatsächliche Image des virtuellen Laufwerks enthält, nicht das<block ref="aa26c73336ecfd98ed727b3e249c7f90" prefix=" " category="inline-code"></block> (Deskriptordatei für virtuelle Festplatten) oder<block ref="092eb4dd2ca488962f4c22b3e8cc4ca0" prefix=" " category="inline-code"></block> (VM-Deskriptordatei).</block>
  <block id="9aa28dc739b66a86d9f73ba24b9dd4c2" category="paragraph">FlexGroup Datastores bieten erweiterte QoS-Funktionen, wenn ONTAP Tools für VMware vSphere 9.8 und höher verwendet werden. Sie können ganz einfach QoS für alle VMs in einem Datastore oder für bestimmte VMs festlegen. Weitere Informationen finden Sie im Abschnitt „FlexGroup“ dieses Berichts. Beachten Sie, dass die zuvor erwähnten Einschränkungen von QoS bei herkömmlichen NFS-Datastores weiterhin gelten.</block>
  <block id="b5380e34617914719b059f6d55b97b66" category="section-title">VMFS-Datastores</block>
  <block id="b74c42e3de8c5b901769e263bf236711" category="paragraph">Die QoS-Richtlinien können mithilfe von ONTAP LUNs auf das FlexVol Volume, das die LUNs enthält, oder auf einzelne LUNs angewendet werden, jedoch nicht auf einzelne VMDK-Dateien, weil ONTAP das VMFS Filesystem nicht erkennt.</block>
  <block id="e122a7a507ac1f580cc5b9f2632a851c" category="section-title">VVols Datastores</block>
  <block id="be1ca5bcad3b34c08e2b2d7bfc8a95e3" category="paragraph">Die minimale und/oder maximale QoS kann problemlos auf einzelnen VMs oder VMDKs festgelegt werden, ohne dass andere VMs oder VMDK durch das richtlinienbasierte Storage-Management und VVols beeinträchtigt werden.</block>
  <block id="f5ac983ee93a7017d283971566bc828e" category="paragraph">Wenn Sie das Storage-Funktionsprofil für den vVol Container erstellen, geben Sie unter der Performance-Funktion einen IOPS-Wert für max und/oder min an und verweisen dann mit der Storage-Richtlinie der VM auf dieses Storage-Funktionsprofil. Verwenden Sie diese Richtlinie beim Erstellen der VM oder beim Anwenden der Richtlinie auf eine vorhandene VM.</block>
  <block id="1f23e960871a0a77a49731a6b47646c5" category="inline-link-macro">VMware vSphere Virtual Volumes (VVols) mit ONTAP</block>
  <block id="8743b08c4d707cd145a4e7acce189480" category="cell">Ja (der Datastore kann gedrosselt werden)</block>
  <block id="b2891bc898f2a418aa349b623568299e" category="summary">Active IQ Unified Manager unterstützt Monitoring und Fehlerbehebung bei NetApp Storage- und Performance-Problemen in Ihrer VMware vSphere Umgebung.</block>
  <block id="b7ce758ec5c25cb23edfa4ead0d0097e" category="admonition">*NetApp empfiehlt* mindestens 15% freien Speicherplatz, wenn rotierende Laufwerke verwendet werden. Dazu gehört der gesamte ungenutzte Speicherplatz, einschließlich freiem Speicherplatz innerhalb des Aggregats oder eines Volumes und sämtlicher freier Speicherplatz, der aufgrund der vollständigen Bereitstellung zugewiesen wird, aber nicht von tatsächlichen Daten genutzt wird. Die Performance wird beeinträchtigt, da der freie Speicherplatz sich etwa bei 10 % befindet.</block>
  <block id="5a7f49be424e282d22b4c3889f5cd341" category="paragraph">MetroCluster ist eine ONTAP-Funktion, die Ihre Oracle Datenbanken mit einer standortübergreifenden synchronen RPO=0-Spiegelung sichern kann. Sie lässt sich auf bis zu Hunderte von Datenbanken auf einem einzigen MetroCluster System skalieren.</block>
  <block id="30160293c6cf2d26d13cbd294af7fb4c" category="paragraph">Darüber hinaus ist die Bedienung einfach. Die Verwendung von MetroCluster trägt nicht notwendigerweise zur Ergänzung oder Änderung der besten Racks für den Betrieb von Enterprise-Applikationen und -Datenbanken bei.</block>
  <block id="c77924ff430fa98df17795d4f5c5885f" category="doc">HTTPS-Zertifikat der ONTAP-Tools</block>
  <block id="641cc5f60c71fe9f9a2e15c5cc680ec3" category="summary">Security Hardening Guide for ONTAP Tools for VMware vSphere, TCP Ports and Protocols</block>
  <block id="872cba2a662a784946eb48675276dee2" category="doc">Mutual TLS (zertifikatbasierte Authentifizierung)</block>
  <block id="5a4857ad48868097d482a4a448f48bd9" category="doc">Maximale Anzahl gleichzeitiger Anfragen pro Benutzer (Netzwerksicherheitsschutz/DOS-Angriff)</block>
  <block id="0af211d5cda5cc4bb17dd7751e741595" category="doc">NTP-Konfiguration (Network Time Protocol</block>
  <block id="273eb98a312e607888044e87e0210015" category="section-title">NetApp VVols Unterstützung</block>
  <block id="551e3b0a48f99995daac658b17a1de62" category="list-text">*Besseres Kapazitätsmanagement.* VASA- und ONTAP-Tools ermöglichen bei Bedarf eine Anzeige der Storage-Kapazität bis auf die einzelne Aggregatebene und ermöglichen bei geringer Kapazität mehrere Alarmebenen.</block>
  <block id="38653f1dd04b13c5276f4f4a82e8506f" category="admonition">Der Begriff „Konnektivität“ bezieht sich auf die Clusterverbindung, die für die standortübergreifende Replizierung verwendet wird. Er bezieht sich nicht auf die Host-Protokolle. Unabhängig von der Art der Verbindung, die für die Kommunikation zwischen den Clustern verwendet wird, werden alle Host-seitigen Protokolle wie gewohnt in einer MetroCluster-Konfiguration unterstützt.</block>
  <block id="6f343e6c5daed91ac953577dd5e1e06e" category="doc">Verlust der Replikationskonnektivität</block>
  <block id="6f04ca443b44ef3da2adedd7c0cf36fa" category="paragraph">In den folgenden Beispielen wird davon ausgegangen, dass Standort A als bevorzugter Standort konfiguriert ist.</block>
  <block id="f553ed0b13f8b38932589ba34432ab5a" category="paragraph">Wenn die SM-AS-Replikation unterbrochen wird, kann die Schreib-I/O nicht abgeschlossen werden, da ein Cluster Änderungen nicht auf den anderen Standort replizieren kann.</block>
  <block id="2ef1646841c5728f668289819e837437" category="section-title">Standort A (bevorzugte Website)</block>
  <block id="47d7b1b43310c2ce615be2e71aed414d" category="paragraph">Das Ergebnis eines Ausfalls der Replikationsverbindung auf dem bevorzugten Standort ist eine ca. 15-Sekunden-Pause bei der Schreib-I/O-Verarbeitung, da ONTAP erneut replizierte Schreibvorgänge versucht, bevor festgestellt wird, dass die Replikationsverbindung wirklich nicht erreichbar ist. Nach 15 Sekunden wird die I/O-Verarbeitung von Lese- und Schreibzugriffen von Standort A fortgesetzt. Die SAN-Pfade ändern sich nicht, und die LUNs bleiben online.</block>
  <block id="bb713afeab82bcb45a45d3cc1411e3eb" category="section-title">Standort B</block>
  <block id="dc68c1abbc392a93905ce712c45999eb" category="paragraph">Da Standort B nicht der bevorzugte Standort für SnapMirror Active Sync ist, sind die LUN-Pfade nach ca. 15 Sekunden nicht mehr verfügbar.</block>
  <block id="3784559d5087e35dc7372b1c51cf920e" category="section-title">Ausfall des Storage-Systems</block>
  <block id="901afcf3108cffc70b3209e8613babf0" category="paragraph">Das Ergebnis eines Storage-Systemausfalls ist nahezu identisch mit dem Ergebnis des Verlusts der Replizierungsverbindung. Am überlebenden Standort sollte eine I/O-Pause von etwa 15 Sekunden stattfinden. Nach Ablauf dieses Zeitraums von 15 Sekunden wird die E/A-Vorgänge wie gewohnt an diesem Standort fortgesetzt.</block>
  <block id="0ce6f0c843cdd28d3ff7284039730959" category="section-title">Verlust des Mediators</block>
  <block id="6b9600f6a35d60a0773b02ff771e5be1" category="paragraph">Der Mediator hat keine direkte Kontrolle über Storage-Vorgänge. Er fungiert als alternativer Kontrollpfad zwischen Clustern. Die Lösung bietet insbesondere automatisierte Failover-Prozesse ohne Split-Brain-Szenario. Im normalen Betrieb repliziert jedes Cluster Änderungen an seinem Partner. Daher kann jedes Cluster überprüfen, ob das Partner-Cluster online ist und Daten bereitstellt. Wenn die Replikationsverbindung fehlschlägt, wird die Replikation beendet.</block>
  <block id="878273b0ad33bf4f330480ad6262d08c" category="paragraph">Der Grund für einen sicheren automatisierten Failover ist der Mediator, der darauf zurückzuführen ist, dass ein Storage-Cluster andernfalls nicht feststellen kann, ob der Ausfall einer bidirektionalen Kommunikation auf einen Netzwerkausfall oder einen tatsächlichen Storage-Ausfall zurückzuführen ist.</block>
  <block id="7bccc28ee4a5fac4fa93b691ca9733bc" category="paragraph">Der Mediator bietet jedem Cluster einen alternativen Pfad zur Überprüfung des Integrität seines Partners. Die Szenarien sind wie folgt:</block>
  <block id="7ba0268be94dd09db527214e2f2862b4" category="list-text">Wenn ein Cluster seinen Partner direkt kontaktieren kann, sind die Replizierungsservices betriebsbereit. Keine Aktion erforderlich.</block>
  <block id="5c3163ce1d24869bf6e6594bcd37901d" category="list-text">Wenn ein bevorzugter Standort nicht direkt mit dem Partner oder über den Mediator in Kontakt treten kann, wird davon ausgegangen, dass der Partner entweder tatsächlich nicht verfügbar ist oder isoliert wurde und seine LUN-Pfade offline geschaltet hat. Der bevorzugte Standort setzt dann den Status RPO=0 frei und setzt die Verarbeitung von Lese- und Schreib-I/O fort.</block>
  <block id="e2c06d8468f627e9f8fdc9380204f00f" category="list-text">Wenn ein nicht bevorzugter Standort seinen Partner nicht direkt kontaktieren kann, ihn aber über den Mediator kontaktieren kann, nimmt er seine Pfade offline und wartet auf die Rückkehr der Replikationsverbindung.</block>
  <block id="f7fd4567c8be042fc3f9be5b45124d76" category="list-text">Wenn ein nicht bevorzugter Standort keine direkte Kontaktaufnahme mit dem Partner oder über einen betrieblichen Mediator bietet, nimmt er an, dass der Partner entweder tatsächlich nicht verfügbar ist oder isoliert war und seine LUN-Pfade offline geschaltet hat. Der nicht bevorzugte Standort setzt dann den Status RPO=0 frei und verarbeitet sowohl Lese- als auch Schreib-I/O weiter. Er übernimmt die Rolle der Replikationsquelle und wird der neue bevorzugte Standort.</block>
  <block id="1286d22222876588b31270c148d8c480" category="paragraph">Wenn der Mediator vollständig nicht verfügbar ist:</block>
  <block id="3cc95aae492aaec45753d6e6b7d91c12" category="list-text">Wenn keine Replizierungsservices aus irgendeinem Grund verfügbar sind, beispielsweise der Ausfall des nicht bevorzugten Standorts oder des Storage-Systems, wird der bevorzugte Standort den Zustand RPO=0 freigeben und die I/O-Verarbeitung für Lese- und Schreibvorgänge wird wieder aufgenommen. Der nicht bevorzugte Standort nimmt seine Pfade offline.</block>
  <block id="92395625e4bc805c933a9b4f52c5c594" category="list-text">Ein Ausfall des bevorzugten Standorts führt zu einem Ausfall, da der nicht bevorzugte Standort nicht verifizieren kann, dass der gegenteilige Standort wirklich offline ist. Daher ist es für den nicht bevorzugten Standort nicht sicher, die Services wieder aufzunehmen.</block>
  <block id="3450c1f3c1166bb5f38dd42c45eb1b70" category="section-title">Dienste werden wiederhergestellt</block>
  <block id="5a0a7b8380bda809ef1b210c3bb41b93" category="paragraph">Wenn ein Fehler behoben wurde, wie z. B. die Wiederherstellung der Site-to-Site-Verbindung oder das Einschalten eines ausgefallenen Systems, erkennen die SnapMirror Active Sync-Endpunkte automatisch, dass eine fehlerhafte Replikationsbeziehung vorhanden ist, und versetzen sie wieder in den Zustand RPO=0. Sobald die synchrone Replizierung wiederhergestellt ist, werden die fehlerhaften Pfade wieder online geschaltet.</block>
  <block id="8f3219fa36fad5d789373fdf883d28b1" category="paragraph">In vielen Fällen erkennen Cluster-Applikationen automatisch die Rückgabe ausgefallener Pfade, und diese Applikationen sind ebenfalls wieder online. In anderen Fällen ist möglicherweise ein SAN-Scan auf Host-Ebene erforderlich oder Applikationen müssen manuell wieder online geschaltet werden. Es hängt von der Anwendung und ihrer Konfiguration ab, und im Allgemeinen lassen sich solche Aufgaben leicht automatisieren. ONTAP selbst behebt selbstständig und sollte keinen Benutzereingriff erfordern, um den RPO=0-Storage-Betrieb wiederaufzunehmen.</block>
  <block id="52ec8762530eb85c0734309b6747a861" category="section-title">Manueller Failover</block>
  <block id="afe62727c9717a186ce014753f902c8a" category="paragraph">Das Ändern des bevorzugten Standorts erfordert eine einfache Bedienung. I/O-Vorgänge werden für eine oder zwei Sekunden angehalten, da zwischen den Clustern die Berechtigung für das Replikationsverhalten wechselt, die E/A-Vorgänge sind jedoch ansonsten nicht betroffen.</block>
  <block id="2d430e41b39275608ed6816ee435f2c4" category="paragraph">Der Mediator ist nicht wirklich ein Tiebreak, obwohl das ist effektiv die Funktion, die es bietet. Er führt keine Aktionen durch. Stattdessen stellt er einen alternativen Kommunikationskanal für die Kommunikation zwischen Cluster und Cluster bereit.</block>
  <block id="caeb8c810d4965bfc8ad369332977e54" category="inline-image-macro">Diagramm der aktiven Synchronisierung von SnapMirror mit dem Mediator</block>
  <block id="4c67fbe8dcb303fbb7a06384a1e166ff" category="paragraph"><block ref="4c67fbe8dcb303fbb7a06384a1e166ff" category="inline-image-macro-rx" type="image"></block></block>
  <block id="2767cf2bbeaf5540c1e615745cfead81" category="paragraph">Die #1 Herausforderung mit automatisiertem Failover ist das Split-Brain-Problem, und dieses Problem tritt auf, wenn Ihre zwei Standorte die Verbindung miteinander verlieren. Was soll geschehen? Sie möchten nicht, dass sich zwei verschiedene Standorte als verbleibende Kopien der Daten bezeichnen, aber wie kann ein einzelner Standort den Unterschied zwischen dem tatsächlichen Verlust des anderen Standorts und der Unfähigkeit, mit dem gegenüberliegenden Standort zu kommunizieren, erkennen?</block>
  <block id="661d0a9679d7d1af0f76f646c941e5cc" category="paragraph">Hier betritt der Mediator das Bild. Wenn jeder Standort an einem dritten Standort platziert wird und über eine separate Netzwerkverbindung zu diesem Standort verfügt, haben Sie für jeden Standort einen zusätzlichen Pfad, um den Zustand des anderen zu überprüfen. Sehen Sie sich das Bild oben noch einmal an und betrachten Sie die folgenden Szenarien.</block>
  <block id="d0c23d9c3fc0a960d3362e7ba10243f5" category="list-text">Was passiert, wenn der Mediator ausfällt oder von einem oder beiden Standorten nicht erreichbar ist?</block>
  <block id="123c024944e0a1dd3f71661e6d0e5005" category="list-text">Die beiden Cluster können weiterhin über dieselbe Verbindung miteinander kommunizieren, die für Replikationsdienste verwendet wird.</block>
  <block id="00a2d2948ebb25dba32d9366b8655dfc" category="list-text">Für die Daten wird noch eine RPO=0-Sicherung verwendet</block>
  <block id="7ac2e4e468f2c732023f530f71ac9dbb" category="list-text">Was passiert, wenn Standort A ausfällt?</block>
  <block id="ce89434a9a13b39dccbbded97d674de0" category="list-text">An Standort B sehen Sie, dass beide Kommunikationskanäle ausgefallen sind.</block>
  <block id="0a22d4bfa3bb021654f02d82cdbb9422" category="list-text">Standort B übernimmt die Datenservices, jedoch ohne RPO=0-Spiegelung</block>
  <block id="2a7681a43b5e9647d24566f342ab0b27" category="list-text">Was passiert, wenn Standort B ausfällt?</block>
  <block id="1c8ff8325f78bcc084c3c197606b5482" category="list-text">An Standort A sehen Sie, dass beide Kommunikationskanäle ausgefallen sind.</block>
  <block id="8a8e5bd62b16753898cefd37bdf7ccb4" category="list-text">Standort A übernimmt die Datenservices, aber ohne RPO=0-Spiegelung</block>
  <block id="11b1ce76911fece428e0edd5c9e7682f" category="paragraph">Es gibt ein anderes Szenario zu berücksichtigen: Verlust der Datenreplikationsverbindung. Wenn die Replikationsverbindung zwischen Standorten verloren geht, wird eine RPO=0-Spiegelung offensichtlich unmöglich sein. Was soll dann geschehen?</block>
  <block id="3e2a78d8ed8be1e5b40ff81cc9e2a2d6" category="paragraph">Dies wird durch den bevorzugten Standortstatus gesteuert. In einer SM-AS-Beziehung ist einer der Standorte zweitrangig zum anderen. Dies hat keine Auswirkungen auf den normalen Betrieb, und der gesamte Datenzugriff ist symmetrisch. Wenn die Replikation jedoch unterbrochen wird, muss die Verbindung unterbrochen werden, um den Betrieb wieder aufzunehmen. Das Ergebnis: Der bevorzugte Standort setzt den Betrieb ohne Spiegelung fort und der sekundäre Standort hält die I/O-Verarbeitung an, bis die Replizierungskommunikation wiederhergestellt ist.</block>
  <block id="225caaa9ecf3ac7dfb141c808a1c1651" category="paragraph">SnapMirror Active Sync (früher als SnapMirror Business Continuity oder SM-bc bekannt) ermöglicht es einzelnen SQL Server Datenbanken und Applikationen, den Service im Falle von Störungen aufrechtzuerhalten, wobei ein transparenter Failover auf den Storage ohne manuelle Eingriffe erfolgt.</block>
  <block id="700cb79deb6ef819c0aacbb228c5a16b" category="paragraph">Die aktive SnapMirror-Synchronisierung (SM-AS) basiert auf SnapMirror synchron. Beide bieten synchrone RPO=0-Replikation von Daten, aber SM-AS führt die Lösung weiter, indem es für SAN-Daten eine Recovery Time Objective von fast null bietet. Dies wird durch eine Automatisierung erreicht, die SAN-Pfade managt und die Datenverfügbarkeit bereithält. Fehler bei Standort-, Controller- und Kommunikationsfehlern werden automatisch von ONTAP behoben.</block>
  <block id="c159224eea5fb4a30b3cc9be6e90cd6c" category="paragraph">SnapMirror Active Sync liefert LUNs, die an zwei verschiedenen Standorten vorhanden sind. Im normalen Betrieb gibt es keine „Quelle“ oder „Ziel“. Die Richtung ist bidirektional. Alle Lese-I/O-Vorgänge, die auf einen bestimmten LUN-Pfad gerichtet sind, werden vom lokalen Controller mithilfe seiner lokalen Kopie der Daten verarbeitet. Alle Schreibvorgänge werden auf den Remote-Partner repliziert sowie lokal geschrieben, bevor sie bestätigt werden.</block>
  <block id="4a85397e66688e4e619ffe832e627548" category="inline-image-macro">Übersicht über SnapMirror Active Sync</block>
  <block id="773253d9f54d4d8d3d596bef999303fa" category="paragraph"><block ref="773253d9f54d4d8d3d596bef999303fa" category="inline-image-macro-rx" type="image"></block></block>
  <block id="ddc1e5dc440806435445e7a532ad1198" category="paragraph">Logisch gesehen gleicht das Verhalten einer einzelnen Gruppe von LUNs. I/O kann auf diese logischen LUNs über SAN-Pfade gelenkt werden, die auf zwei verschiedenen Clustern existieren, aber die Daten sind immer die gleichen. Das I/O-Verhalten ist symmetrisch. Dies ist für viele aktiv/aktiv-Applikationskonfigurationen von großer Bedeutung.</block>
  <block id="ddc5ca7c74db522011a3b2ddccda5b40" category="inline-image-macro">Logisches Design mit SnapMirror Active Sync</block>
  <block id="a91ae760520ff89a08c2f2db6a6c2ebd" category="paragraph"><block ref="a91ae760520ff89a08c2f2db6a6c2ebd" category="inline-image-macro-rx" type="image"></block></block>
  <block id="daadd24f5163a10f9a2f016019883886" category="section-title">Pfadmanagement</block>
  <block id="9bb9bee58b329761e36c7d9bf8900d6f" category="paragraph">Es gibt zwei einheitliche und uneinheitliche Ansätze für die synchrone Netzwerktopologie von SnapMirror. Bei der Auswahl zwischen einheitlichem und uneinheitlichem Zugriff muss vor allem berücksichtigt werden, ob das SAN standortübergreifend erweitert werden kann oder muss. SnapMirror Active Sync kann in beiden Situationen verwendet werden.</block>
  <block id="1f46c4a6c97a8add22ae7c23181c23d8" category="paragraph">Der ONTAP Mediator ist eine Softwareanwendung, die von der NetApp Support-Website heruntergeladen wird und normalerweise auf einer virtuellen Maschine bereitgestellt wird.</block>
  <block id="91e17573d84d7d626cc27e989a3957bc" category="inline-link-macro">ONTAP-Dokumentation über SnapMirror Active Sync</block>
  <block id="ae81dd17263728532627374843ee0ff8" category="paragraph">Informationen zu Planungs- und Konfigurationsschritten finden Sie unter <block ref="44f5d8051247437425911d96f84db248" category="inline-link-macro-rx"></block> .</block>
  <block id="5abea2d56204d1c15aa063dc90ccfb4f" category="paragraph">SnapMirror Active Sync betrachtet einen Standort als „Quelle“ und den anderen als „Ziel“. Dies impliziert eine One-Way-Replikationsbeziehung, aber dies gilt nicht für das IO-Verhalten. Die Replizierung ist bidirektional und symmetrisch, und die I/O-Reaktionszeiten sind auf beiden Seiten der Spiegelung identisch.</block>
  <block id="e5086daa5843466ff5f04f20d4c19d74" category="paragraph">Die<block ref="36cd38f49b9afa08222c0dc9ebfe35eb" prefix=" " category="inline-code"></block> Bezeichnung steuert den bevorzugten Standort. Wenn die Replizierungsverbindung verloren geht, stellen die LUN-Pfade auf der Quellkopie weiterhin Daten bereit, während die LUN-Pfade auf der Zielkopie erst dann wieder verfügbar sind, wenn die Replikation wiederhergestellt ist und SnapMirror wieder in den synchronen Zustand wechselt. Die Pfade setzen dann das Bereitstellen von Daten fort.</block>
  <block id="a0bbe7bd1e93252f1267d98703905552" category="paragraph">Die Sourcing/Ziel-Konfiguration kann über Systemmanager angezeigt werden:</block>
  <block id="13d0a54956b2dd09d5e909acadfb1694" category="inline-image-macro">SM-Screenshot der SM-as-Quelle</block>
  <block id="70547bc22a604012e7d30f238d3ab5c8" category="paragraph"><block ref="70547bc22a604012e7d30f238d3ab5c8" category="inline-image-macro-rx" type="image"></block></block>
  <block id="5b53b7d18b58af25bcf47dc21678038f" category="paragraph">Oder über die CLI:</block>
  <block id="498b1bbe0e017f1ad405fe4c77fb8ea1" category="paragraph">Der Schlüssel ist, dass die Quelle die SVM für Cluster1 ist. Wie oben erwähnt, beschreiben die Begriffe „Quelle“ und „Ziel“ nicht den Fluss replizierter Daten. Beide Standorte können einen Schreibvorgang verarbeiten und am anderen Standort replizieren. Beide Cluster sind Quellen und Ziele. Der Effekt der Festlegung eines Clusters als Quelle steuert einfach, welches Cluster als Lese-/Schreib-Speichersystem überlebt, wenn die Replikationsverbindung verloren geht.</block>
  <block id="18a07170e14f5cbc26533f0a136dfdd0" category="inline-image-macro">SnapMirror Active Sync uneinheitliches Netzwerk</block>
  <block id="9347dda18b33bf2c717916c013eb47bc" category="paragraph"><block ref="9347dda18b33bf2c717916c013eb47bc" category="inline-image-macro-rx" type="image"></block></block>
  <block id="9532c9e7afcd43c7d9bb5740695fc259" category="paragraph">Der Hauptvorteil dieses Ansatzes ist die SAN-Einfachheit – Sie müssen kein SAN mehr über das Netzwerk erweitern. Einige Kunden verfügen nicht über eine Konnektivität mit niedriger Latenz zwischen den Standorten und haben nicht die Infrastruktur, um den FC SAN-Datenverkehr über ein standortverbundes Netzwerk zu Tunneln.</block>
  <block id="439caacb19fa16f2c21563703ed645d7" category="paragraph">Der Nachteil eines uneinheitlichen Zugriffs besteht darin, dass bestimmte Ausfallszenarien, einschließlich des Verlusts der Replikationsverbindung, dazu führen, dass einige Hosts den Zugriff auf den Speicher verlieren. Applikationen, die als einzelne Instanzen ausgeführt werden, wie z. B. eine Datenbank ohne Cluster, die grundsätzlich nur auf einem einzelnen Host bei einem beliebigen Mount ausgeführt wird, würden ausfallen, wenn die lokale Storage-Konnektivität verloren geht. Die Daten bleiben zwar weiterhin geschützt, aber der Datenbankserver würde nicht mehr darauf zugreifen können. Es müsste an einem Remote-Standort neu gestartet werden, vorzugsweise durch einen automatisierten Prozess. VMware HA kann beispielsweise eine heruntergefahrenen Pfade auf einem Server erkennen und eine VM auf einem anderen Server neu starten, auf dem Pfade verfügbar sind.</block>
  <block id="5a2e2e33df9734d8ef57bcb93643d89a" category="paragraph">Im Gegensatz dazu kann eine Cluster-Anwendung wie Oracle RAC einen Service bereitstellen, der gleichzeitig an zwei verschiedenen Standorten verfügbar ist. Der Verlust einer Website bedeutet nicht, dass der Anwendungsdienst als Ganzes verloren geht. Instanzen sind nach wie vor verfügbar und werden am verbleibenden Standort ausgeführt.</block>
  <block id="9fad44ede04fb95c16ea64be5e51836c" category="paragraph">In vielen Fällen wäre die zusätzliche Latenz, wenn eine Applikation, die auf den Storage über eine Site-to-Site-Verbindung zugreift, nicht akzeptabel. Dies bedeutet, dass die verbesserte Verfügbarkeit von einheitlichem Netzwerk minimal ist, da der Verlust von Speicher an einem Standort dazu führen würde, dass die Dienste auf diesem ausgefallenen Standort sowieso heruntergefahren werden müssen.</block>
  <block id="15da96ce40136a3259e04ae0dc25fc95" category="paragraph">Es gibt redundante Pfade durch den lokalen Cluster, die aus Gründen der Einfachheit nicht auf diesen Diagrammen angezeigt werden. ONTAP Storage-Systeme sind HA selbst, daher sollte ein Controller-Ausfall nicht zu einem Standortausfall führen. Es sollte lediglich zu einer Änderung führen, in der lokale Pfade auf dem betroffenen Standort verwendet werden.</block>
  <block id="bb12626aefb7c36f9b13694d35dadd5d" category="paragraph">Eine wichtige Funktion von SM-AS ist die Möglichkeit, die Speichersysteme so zu konfigurieren, dass sie wissen, wo sich die Hosts befinden. Wenn Sie die LUNs einem bestimmten Host zuordnen, können Sie angeben, ob sie einem bestimmten Storage-System proximal sind oder nicht.</block>
  <block id="4590bab77464ba1a9c8ee6b8095eef60" category="inline-image-macro">SnapMirror Active Sync AFF einheitliche Vernetzung</block>
  <block id="988f5836c7a60d1374a4804da23d51e4" category="paragraph"><block ref="988f5836c7a60d1374a4804da23d51e4" category="inline-image-macro-rx" type="image"></block></block>
  <block id="2b2fdfa824f1319cc47992b1277faa78" category="paragraph">Im normalen Betrieb sind alle E/A-Vorgänge lokal. Lese- und Schreibvorgänge werden vom lokalen Speicher-Array gewartet. Schreib-I/O muss natürlich auch vom lokalen Controller auf das Remote-System repliziert werden, bevor sie bestätigt wird. Alle Lese-I/O-Vorgänge werden jedoch lokal gewartet und es kommt keine zusätzliche Latenz durch das Durchlaufen der SAN-Verbindung zwischen den Standorten zu.</block>
  <block id="97b2bb0cfbae3fe49433caf537e819ec" category="paragraph">Die nicht optimierten Pfade werden nur dann verwendet, wenn alle aktiven/optimierten Pfade verloren gehen. Wenn beispielsweise das gesamte Array an Standort A Strom verloren hätte, könnten die Hosts an Standort A weiterhin auf Pfade zum Array an Standort B zugreifen und bleiben daher betriebsbereit, obwohl die Latenz höher wäre.</block>
  <block id="5ab085bf88338486e590861594d3947a" category="paragraph">NetApp ASA Systeme bieten aktiv/aktiv-Multipathing über alle Pfade eines Clusters hinweg. Dies gilt auch für SM-AS Konfigurationen.</block>
  <block id="6511f0e2589c08e82bc8dd3e9bc93185" category="inline-image-macro">SnapMirror Active Sync ASA einheitliche Vernetzung</block>
  <block id="1d818a68fead665559afc9e582394a7d" category="paragraph"><block ref="1d818a68fead665559afc9e582394a7d" category="inline-image-macro-rx" type="image"></block></block>
  <block id="c3fe6771b27ffcb15a75ae88f6d57c4b" category="paragraph">Eine ASA-Konfiguration mit nicht-einheitlichem Zugriff würde im Wesentlichen auf die gleiche Weise funktionieren wie mit AFF. Bei einheitlichem Zugriff würde IO das WAN überqueren. Dies kann wünschenswert sein oder auch nicht.</block>
  <block id="498a9faf271a73dcf026b1041b66ef2a" category="paragraph">Wenn die beiden Standorte mit Glasfaserverbindung 100 Meter voneinander entfernt wären, sollte keine erkennbare zusätzliche Latenz über das WAN entstehen. Wenn jedoch die Standorte weit voneinander entfernt wären, würde die Performance beim Lesen an beiden Standorten darunter leiden. Im Gegensatz dazu würden bei AFF diese WAN-überschneidenden Pfade nur verwendet, wenn keine lokalen Pfade verfügbar wären und die tägliche Performance besser wäre, da alle I/O-Vorgänge lokal wären. ASA mit einem nicht einheitlichen Zugriffsnetzwerk wäre eine Option, um die Kosten- und Funktionsvorteile von ASA zu nutzen, ohne dass sich eine Beeinträchtigung des Zugriffs auf die standortübergreifende Latenz ergeben würde.</block>
  <block id="15ab92b8042738f6acc912e2d98ae17a" category="paragraph">ASA mit SM-AS in einer Konfiguration mit niedriger Latenz bietet zwei interessante Vorteile. Zunächst verdoppelt es die Performance bei jedem einzelnen Host *, da IO von doppelt so vielen Controllern mit doppelt so vielen Pfaden gewartet werden kann. Zweitens bietet er in einer Umgebung mit einem einzigen Standort eine extreme Verfügbarkeit, da ein komplettes Storage-System ohne Unterbrechung des Host-Zugriffs verloren gehen könnte.</block>
  <block id="41c719032afe187deaf4b4b2af41aa36" category="paragraph">Temperaturempfindliche Speichereffizienz (TSSE) ist ab ONTAP 9.8 verfügbar. Es basiert auf Block-Zugriffs-Heatmaps, um selten genutzte Blöcke zu identifizieren und sie effizienter zu komprimieren.</block>
  <block id="8284eb5a9d9bef8f0f1ece257656d0fe" category="summary">Weitere Informationen zu Epic auf ONTAP</block>
  <block id="345245c83d2b2c4c2a5eb9f2887da627" category="paragraph">Weitere Informationen zu den in diesem Dokument beschriebenen Daten finden Sie in den folgenden Dokumenten bzw. auf den folgenden Websites:</block>
  <block id="bc4fe2352063642d68529f2aa0ca7ca3" category="inline-link-macro">NetApp Produktdokumentation</block>
  <block id="0897605072547ea93ef25a4b355ac5f6" category="list-text"><block ref="0897605072547ea93ef25a4b355ac5f6" category="inline-link-macro-rx"></block></block>
  <block id="5358757de9216cc29d688612a3d72fe1" category="inline-link-macro">ONTAP 9-Dokumentation</block>
  <block id="3973235e869d719328cf4b37e59562e5" category="list-text"><block ref="3973235e869d719328cf4b37e59562e5" category="inline-link-macro-rx"></block></block>
  <block id="c9cefb6ed873a418f8c8232dca6879d2" category="inline-link-macro">Konsistenzgruppen</block>
  <block id="10d6aff3e86ebe3712a2616b45008fb3" category="list-text"><block ref="10d6aff3e86ebe3712a2616b45008fb3" category="inline-link-macro-rx"></block></block>
  <block id="0d3cc094235677831b5697c5bab76394" category="inline-link-macro">Dokumentationsressourcen für ONTAP und ONTAP System Manager</block>
  <block id="3b2c8ae1d985c88d539bf7851e067922" category="list-text"><block ref="3b2c8ae1d985c88d539bf7851e067922" category="inline-link-macro-rx"></block></block>
  <block id="e9bf68ed54b6d1ff63e8bda8c0b6a70f" category="inline-link-macro">TR-3930i: NetApp Sizing Guidelines for Epic</block>
  <block id="920aa6d447930899ad56517c79fd12fc" category="list-text"><block ref="b6ff45309a811037a92ca1e6262c74a8" category="inline-link-macro-rx"></block> (NetApp-Anmeldung erforderlich)</block>
  <block id="840f4016fe968750d45ea4545f2f1893" category="section-title">Epic Customer Guidance Dokumente</block>
  <block id="a839c9787ec80291f4b34dc5451786a5" category="paragraph">Epic bietet Kunden die folgenden Dokumente zur Anleitung zu Servern, Storage und Netzwerk. In diesem technischen Bericht wird auf diese Dokumente Bezug genommen.</block>
  <block id="67bab37630accce3ea259fb1601fca2e" category="list-text">Überlegungen Zum Storage Area Network</block>
  <block id="0fe2c5f32f45114c27e3b7a7ca0ba0ad" category="list-text">Business Continuity – Lösungsleitfaden Für Technische Lösungen</block>
  <block id="2163e1881d92ff554d8969998a7d618f" category="list-text">Leitfaden Für Die All-Flash-Referenzarchitektur</block>
  <block id="b332ca8d59cd86adb52c28aef982cc69" category="list-text">Speicherprodukte und Technologiestatus</block>
  <block id="a0107bedc201b2df3694df573f65ae32" category="list-text">Überlegungen Zu Epic Cloud</block>
  <block id="0cf7d14c360e7f8755d749057b348718" category="list-text">Hardware Configuration Guide (kundenspezifisch)</block>
  <block id="50923dab9020d1e069418763d0bd15a3" category="list-text">Empfehlungen zum Layout von Datenbank-Storage (kundenspezifisch)</block>
  <block id="4e1249f2da2104e56555dec5d4d51e6a" category="summary">Epic mit vier Nodes</block>
  <block id="7baf69c8e83ff08e0583426d011bc650" category="paragraph">Die Abbildungen unten zeigen das Storage-Layout für eine Architektur mit vier Nodes: Ein HA-Paar in Produktionsumgebungen und ein HA-Paar in Disaster Recovery. Die Größe der Controller und die Anzahl der Festplatten basieren auf dem letztgenannten Größenabbild.</block>
  <block id="70ccc987c3596ca363e7b8d980858220" category="paragraph">NetApp garantiert eine minimale Performance auf Bodenebene durch die Annahme der von SLM empfohlenen AQoS-Richtlinien. Epic unterstützt die Konsolidierung von Storage Pools in ONTAP auf deutlich weniger Hardware. Weitere Informationen finden Sie im Dokument mit den vierteljährlichen Epic-CHATS. Im Grunde können Pool1, Pool2 und NAS1 (aufgelistet im Epic Hardware Configuration Guide) alle auf einem einzigen HA-Paar ausgeführt werden, wobei die Workloads gleichmäßig über die beiden Controller verteilt werden. Bei der Disaster Recovery sind Epic Pool 3 und NAS 3 auch auf die beiden Controller des HA-Paars aufgeteilt.</block>
  <block id="34557edb2bf88cd4f837f2384ead1830" category="paragraph">Umgebungen mit vollständigen Testkopien (z. B. SUP, REL und PJX) werden entweder aus der Epic Produktion, dem Epic Report oder der Epic Disaster Recovery geklont. Informationen zu Epic Backup und -Aktualisierung finden Sie im Abschnitt „Datenmanagement“.</block>
  <block id="5c8423bf649a8a50f33626549581cce9" category="section-title">Architektur mit vier Nodes</block>
  <block id="f42ac2802f367c372897f3fb06218d2d" category="inline-image-macro">Epic 4-Node-Architektur</block>
  <block id="848f13dbd455e0190684bb8052ef94d6" category="paragraph"><block ref="848f13dbd455e0190684bb8052ef94d6" category="inline-image-macro-rx" type="image"></block></block>
  <block id="14b8013c3961c3eb9f2d01aff2c9d430" category="section-title">Workload-Platzierung mit vier Nodes</block>
  <block id="8e576f920186d1c7001b6f1da66f2359" category="inline-image-macro">Epic 4 Node-Platzierung</block>
  <block id="34594b0fc43be2425a0a68f14928bc8d" category="paragraph"><block ref="34594b0fc43be2425a0a68f14928bc8d" category="inline-image-macro-rx" type="image"></block></block>
  <block id="145a177f694025424b761a3b22ca2f6b" category="summary">Epic 6-Node-Architektur</block>
  <block id="7682e26e53b5af35c13c22f5034f788a" category="paragraph">Kunden möchten mit einem Design mit sechs Nodes beginnen oder nahtlos horizontal auf vier bis sechs Nodes skalieren, mit wachsender Nachfrage. Dank horizontaler Skalierung können Workloads unterbrechungsfrei zwischen Nodes verschoben und im Cluster gleichmäßig verteilt werden.</block>
  <block id="121ec45abd57b78d2c63e13a01970fbf" category="paragraph">Diese Architektur bietet den besten Performance- und Kapazitätsausgleich im Cluster. Epic Production, Epic Report und Epic Test werden alle auf dem ersten HA-Paar ausgeführt. Das zweite HA-Paar wird für Clarity, Hyperspace, VMware, NAS1 und die verbleibenden Epic-Workloads verwendet. Die Disaster Recovery ist mit der Architektur mit vier Nodes im vorherigen Abschnitt identisch.</block>
  <block id="628e6fdb5831f5144730fdfb620a1a2a" category="section-title">Architektur mit sechs Nodes</block>
  <block id="5d28884b83f218379ec90c789e2e382b" category="inline-image-macro">Epic 6-Node-Architektur</block>
  <block id="a5cf77b4851a534d4e7f8cbfc10e7734" category="paragraph"><block ref="a5cf77b4851a534d4e7f8cbfc10e7734" category="inline-image-macro-rx" type="image"></block></block>
  <block id="214d949b6d12c893a63f49d862292640" category="section-title">Workload-Platzierung mit sechs Nodes</block>
  <block id="65ea1ebb7d9c2d858f8701fcbecd438d" category="inline-image-macro">Epic 6 Node-Platzierung</block>
  <block id="77e8933cdf526d7670fcd797f57f81a7" category="paragraph"><block ref="77e8933cdf526d7670fcd797f57f81a7" category="inline-image-macro-rx" type="image"></block></block>
  <block id="a3ab98b5217808d71238c069623fa0ff" category="summary">Epic 8-Node-Architektur</block>
  <block id="ab4e8eb50674d5d1bee79a6682b49325" category="paragraph">Die Abbildungen unten zeigen die Scale-out-Architektur mit acht Nodes. Ebenfalls möglich ist, mit vier Nodes zu beginnen und auf sechs Nodes zu skalieren, wobei die Skalierung auf acht Nodes und mehr fortgesetzt werden kann. Diese Architektur sorgt für das beste Verhältnis zwischen Performance und Kapazität über die sechs produktiven Nodes hinweg.</block>
  <block id="0ef10ffde0c0ebd21feabe96ec890f72" category="paragraph">In diesem Design werden die Testumgebungen in diesem Bericht statt in der Produktion geklont. Dadurch werden Testumgebungen und Integritätsprüfungen aus der Produktion ausgelagert.</block>
  <block id="211891edb616b1a6e25c469a73cd07c9" category="section-title">Architektur mit acht Nodes</block>
  <block id="3b31cb391c5fb328f6bcf768909f68a2" category="paragraph"><block ref="3b31cb391c5fb328f6bcf768909f68a2" category="inline-image-macro-rx" type="image"></block></block>
  <block id="c344a01a287e43ab2ef2793cf550435a" category="section-title">Workload-Platzierung mit acht Nodes</block>
  <block id="80a544c0b0187e121e581e019d0902a9" category="inline-image-macro">Epic 8 Node-Platzierung</block>
  <block id="e374fdf5079d6709ca8206b4c77be000" category="paragraph"><block ref="e374fdf5079d6709ca8206b4c77be000" category="inline-image-macro-rx" type="image"></block></block>
  <block id="17e9ce8e967d60724d7f53d467470a10" category="summary">Epic Architektur</block>
  <block id="b1830ae1e99adb346ce1da353cd6c089" category="paragraph">Dieser Abschnitt beschreibt die Epic Softwareumgebung und die wichtigsten Komponenten, für die Storage erforderlich ist. Es enthält wichtige Überlegungen, die als Leitfaden für das Storage-Design dienen sollen.</block>
  <block id="4db273f137d059bd4ffd23818c367214" category="paragraph">Epic hat seinen Hauptsitz in Verona, Wisconsin, und stellt Software für mittlere bis große medizinische Gruppen, Krankenhäuser und integrierte Organisationen im Gesundheitswesen her. Zu den Kunden zählen auch kommunale Krankenhäuser, akademische Einrichtungen, Kinderorganisationen, Sicherheitsnetzbetreiber und Systeme mit mehreren Krankenhäusern. Epic-integrierte Software umfasst klinische Funktionen, Zugriffs- und Umsatzfunktionen und gilt auch für zuhause.</block>
  <block id="27cd823999704d6e14cdee1e83bef7f0" category="paragraph">Es geht nicht um den Rahmen dieses Dokuments, um die breite Palette von Funktionen zu decken, die von Epic-Software unterstützt werden. Aus Sicht des Storage-Systems nutzt alle Epic Software jedoch für jede Implementierung eine einzige patientenorientierte Datenbank. Epic geht von der InterSystems Caché-Datenbank auf die neue InterSystems Iris-Datenbank über. Da die Speicheranforderungen für Caché und Iris gleich sind, werden wir im Rest dieses Dokuments die Datenbank als Iris bezeichnen. Iris ist für die Betriebssysteme AIX und Linux verfügbar.</block>
  <block id="4e25ab2bd75f003927aabf72608c3610" category="section-title">InterSystems Iris</block>
  <block id="0af82ac784148ef3818d2fe5b04763fa" category="paragraph">InterSystems Iris ist die Datenbank, die von der Epic-Anwendung verwendet wird. In dieser Datenbank ist der Datenserver der Zugriffspunkt für dauerhaft gespeicherte Daten. Der Anwendungsserver verwaltet Datenbankabfragen und stellt Datenanfragen an den Datenserver. In den meisten Epic-Softwareumgebungen reicht die Verwendung der SMP-Architektur (symmetrischer Multiprozessor) in einem einzelnen Datenbankserver aus, um die Datenbankanforderungen von Epic-Applikationen zu erfüllen. In großen Implementierungen kann ein verteiltes Modell mit dem Enterprise Caché Protocol (ECP) von InterSystems unterstützt werden.</block>
  <block id="8dfea4e602a85bafff792b879b2d056b" category="paragraph">Durch die Verwendung von Failover-fähiger Cluster-Hardware kann ein Standby-Datenserver auf denselben Speicher zugreifen wie der primäre Datenserver. Außerdem kann der Standby-Datenserver während eines Hardwareausfalls Verarbeitungsaufgaben übernehmen.</block>
  <block id="c77ab217583518f1b67702b591baf929" category="paragraph">InterSystems stellt zudem Technologien bereit, um Anforderungen an Datenreplizierung, Disaster Recovery und Hochverfügbarkeit zu erfüllen. Die Replikationstechnologie von InterSystems wird verwendet, um eine Iris-Datenbank synchron oder asynchron von einem primären Datenserver auf einen oder mehrere sekundäre Datenserver zu replizieren. NetApp SnapMirror wird für die Replizierung von WebBLOB Storage oder für Backup und Disaster Recovery verwendet.</block>
  <block id="df2e58f9c6f746010ccdd5cf07673500" category="paragraph">Die aktualisierte Iris-Datenbank hat viele Vorteile:</block>
  <block id="f8ffd660138274e3b7b31f900153e4b6" category="list-text">Erhöht die Skalierbarkeit und ermöglicht größeren Unternehmen mit mehreren Epic-Instanzen die Konsolidierung in einer größeren Instanz.</block>
  <block id="242ef58734fef1421d6cc1e1154bd041" category="list-text">Ein Lizenzurlaub, bei dem Kunden jetzt zwischen AIX und Red hat Enterprise Linux (RHEL) wechseln können, ohne dafür eine neue Plattformlizenz zu bezahlen.</block>
  <block id="08a78b85487809e2116574e4ff97dec5" category="section-title">Caché Datenbankserver und Storage-Verwendung</block>
  <block id="4cc33a0396ddd5a29816a3a40d01a6b5" category="list-text">*Produktion* in Epic-Softwareumgebungen wird eine einzelne patientenorientierte Datenbank bereitgestellt. In den Hardwareanforderungen von Epic wird der physische Server, der den primären Lese-/Schreib-Iris-Datenserver hostet, als Produktionsserver bezeichnet. Dieser Server erfordert hochperformanten All-Flash-Storage für Dateien, die zur primären Datenbankinstanz gehören. Für Hochverfügbarkeit unterstützt Epic die Verwendung eines Failover-Datenbankserver, der Zugriff auf dieselben Dateien hat. Iris nutzt Epic Mirror zur Replizierung zu schreibgeschützten Berichten, zur Disaster Recovery und zur Unterstützung schreibgeschützter Kopien. Aus Gründen der Business Continuity kann jeder Datenbanktyp in den Lese-/Schreibmodus umgeschaltet werden.</block>
  <block id="cdbcd4cba3ac1d1b15d302df69df1198" category="list-text">*Report* Ein berichtender Spiegel-Datenbank-Server bietet schreibgeschützten Zugriff auf Produktionsdaten. Es hostet einen Iris-Datenserver, der als Backup-Spiegel des Produktions-Iris-Datenservers konfiguriert ist. Der berichtende Datenbankserver hat die gleichen Speicherkapazitäten wie der Produktions-Datenbankserver. Die Schreib-Performance für Berichte ist dieselbe wie für die Produktion, aber die Lese-Workload-Merkmale unterscheiden sich und haben eine andere Größe.</block>
  <block id="10938a446d1d37c3afcad8aac850f514" category="list-text">*Unterstützt nur-Lesen* dieser Datenbankserver ist optional und nicht in der Abbildung unten dargestellt. Ein Spiegeldatenbankserver kann auch zur Unterstützung von Epic implementiert werden, um schreibgeschützte Funktionen zu unterstützen, in denen eine Kopie der Produktion im schreibgeschützten Modus zugänglich ist. Aus Gründen der Business Continuity kann dieser Datenbanktyp in den Lese-/Schreibmodus umgeschaltet werden.</block>
  <block id="32042e6794444e49e092cad346b66b28" category="list-text">*Disaster Recovery* um Business Continuity- und Disaster Recovery-Ziele zu erreichen, wird ein Disaster Recovery-Spiegeldatenbankserver üblicherweise an einem Standort bereitgestellt, der geografisch getrennt von den Produktions- und/oder Reporting-Spiegeldatenbankservern ist. Ein Datenbank-Server mit Disaster Recovery-Spiegelung hostet auch einen Iris-Datenserver, der als Backup-Spiegelung des Iris-Datenservers der Produktionsumgebung konfiguriert ist. Wenn der Produktionsstandort längere Zeit nicht mehr verfügbar ist, kann dieser Datenbankserver für die Backup-Spiegelung so konfiguriert werden, dass er als gespiegelte Lese-/Schreibinstanz (SRW) fungiert. Der Backup-Mirror-Datenbankserver hat die gleichen Dateispeicheranforderungen wie der Produktions-Datenbankserver. Im Gegensatz dazu wird die Größe des Datenbank-Storage für die Backup-Spiegelung aus Sicht der Performance für Business Continuity mit dem Produktions-Storage identisch sein.</block>
  <block id="d05c7e253f48b314f8488aba0e85b3b4" category="inline-image-macro">Epic IRIS ODB</block>
  <block id="90cb7e5b029ac3a1b754d9f0d75bfbb9" category="paragraph"><block ref="90cb7e5b029ac3a1b754d9f0d75bfbb9" category="inline-image-macro-rx" type="image"></block></block>
  <block id="b101ffd23847579c0cf3cb028fc314ee" category="list-text">*Test* Gesundheitseinrichtungen stellen häufig Entwicklungs-, Test- und Staging-Umgebungen bereit. Zusätzliche Iris-Datenserver für diese Umgebungen benötigen ebenfalls Storage, der durch dasselbe Storage-System untergebracht werden kann. Epic bietet besondere Anforderungen und Einschränkungen, um zusätzlichen Storage aus einem Shared Storage-System bereitzustellen. Diese speziellen Anforderungen werden durch die in diesem Dokument enthaltenen Best Practices allgemein adressiert.</block>
  <block id="5c52f2993ead7ce33b4600c9955d9657" category="paragraph">Zusätzlich zu den Iris ODB-Datenservern umfassen Epic-Softwareumgebungen in der Regel weitere Komponenten wie die folgenden und wie in der folgenden Abbildung dargestellt:</block>
  <block id="c352fde153a95627d87a161b60e9e211" category="list-text">Ein Oracle oder Microsoft SQL Server Datenbankserver als Back-End zu den Clarity Tools für Business-Reporting von Epic</block>
  <block id="72e364f0316ce75144d3bcbc76627977" category="admonition">Clarity wird verwendet, um Berichte über Daten zu erstellen, die täglich aus der Iris-Berichtsdatenbank extrahiert wurden.</block>
  <block id="9c7d7e54119a875f7f730fac34d86939" category="list-text">WebBLOB-Server (SMB)</block>
  <block id="7f140c68d528349ef23a9adde829618a" category="list-text">Mehrzweck-Datenbankserver</block>
  <block id="db670b036ce103ccc68fe4a2f746bcbb" category="list-text">Virtuelle Mehrzweck-Maschinen (VMs)</block>
  <block id="46f49acf82ce78d08668061bb78e679a" category="list-text">Hyperspace für Client-Zugriff</block>
  <block id="847ce339c80d3a544fdd1e3ea23f89ed" category="inline-image-macro">Epic Datenbank</block>
  <block id="ddc0ff60fb0cba83974547cb2fd7a40e" category="paragraph"><block ref="ddc0ff60fb0cba83974547cb2fd7a40e" category="inline-image-macro-rx" type="image"></block></block>
  <block id="300c335aaf3a97bb6020a19be0b8ca96" category="paragraph">Die Storage-Anforderungen all dieser Workloads, Pools, NAS- und SAN-Protokolle können konsolidiert und von einem einzigen ONTAP Cluster gehostet werden. Durch diese Konsolidierung können Organisationen im Gesundheitswesen eine einzelne Datenmanagementstrategie für alle Epic- und nicht Epic-Workloads entwickeln.</block>
  <block id="4a7de8a55a290fb3a0a4efd490ec781b" category="section-title">Operative Datenbank-Workloads</block>
  <block id="a627911d38337b81dddab9711a61842c" category="paragraph">Jeder Epic-Datenbankserver führt I/O für die folgenden Dateitypen aus:</block>
  <block id="458648f675593beefe031e7b7ba9fcdd" category="list-text">Datenbankdateien</block>
  <block id="07599d3aee2c26c36cae1cb305a0d120" category="list-text">Journaldateien</block>
  <block id="b9c807a9cbbe80f3b710d64a47334ecf" category="list-text">Anwendungsdateien</block>
  <block id="4b11d0da3422be079767b9c81ea967cb" category="paragraph">Der Workload eines einzelnen Datenbankservers hängt von seiner Rolle in der Epic Softwareumgebung ab. So kommt beispielsweise bei Produktionsdatenbankdateien in der Regel der anspruchsvollste Workload vor, der aus 100 % zufälligen I/O-Anfragen besteht. Der Workload einer gespiegelten Datenbank ist in der Regel weniger anspruchsvoll und weist weniger Leseanforderungen auf. Journal-Datei-Workloads sind überwiegend sequenziell.</block>
  <block id="6908d08a516bc86d102ccafa6c2ece9d" category="paragraph">Epic unterhält ein Workload-Modell für Storage-Performance-Benchmarks und Kunden-Workloads. Weitere Informationen zum Epic-Workload-Modell, Benchmark-Ergebnisse und Hinweise NetApp zur richtigen Storage-Dimensionierung für Epic-Umgebungen finden Sie unter (Anmeldung zum<block ref="81c36a9e6d224e9d42c9630dffaa9434" category="inline-link-rx"></block> NetApp erforderlich).</block>
  <block id="f71d6b15e61746171efc106f495b60cd" category="paragraph">Darüber hinaus stellt Epic jedem Kunden einen individuellen Hardware-Konfigurationsleitfaden mit I/O-Projektionen und Storage-Kapazitätsanforderungen zur Verfügung. Die endgültigen Storage-Anforderungen können Entwicklungs-, Test- und/oder Staging-Umgebungen sowie weitere ergänzende Workloads umfassen, die konsolidiert werden können. Kunden können über den Hardware-Konfigurationsleitfaden die gesamten Storage-Anforderungen an NetApp kommunizieren. Dieser Leitfaden enthält alle Daten, die für die Größe einer Epic Implementierung erforderlich sind.</block>
  <block id="c7a0b1a5f7a146b9b72947b2bba045ed" category="paragraph">Während der Implementierungsphase bietet Epic einen Leitfaden für das Layout von Datenbank-Storage, der granularere Details auf LUN-Ebene bietet, die für ein erweitertes Storage-Design verwendet werden können. Beachten Sie, dass es sich beim Leitfaden zum Layout von Datenbank-Storage um allgemeine Storage-Empfehlungen handelt, die für NetApp nicht spezifisch sind. Dieser Leitfaden erläutert Ihnen das beste Storage-Layout für NetApp.</block>
  <block id="ab3b24e1e2a3f9784a9c6cad8db20a76" category="summary">Epic Sizing</block>
  <block id="706a0ec5dbae849750639964dc6b852f" category="paragraph">Eine der wichtigsten Überlegungen zur Architektur bei der Dimensionierung einer Epic Storage-Umgebung ist die Größe der ODB-Datenbank.</block>
  <block id="abe227b9b306ed7c0527800b44a2f1d0" category="paragraph">Mithilfe des unten dargestellten Diagramms können Sie eine kleine und mittlere Epic Storage-Architektur auswählen. Diese Designs umfassen die Ausführung aller im Hardware-Konfigurationsleitfaden aufgeführten Workloads. Der Dimensionierungsbaum basiert auf Daten aus über 100 Hardware-Konfigurationsleitfäden und sollte weitgehend akkurat geschätzt werden.</block>
  <block id="be0322041cc582d91f5f470df1ef2ba4" category="paragraph">Es ist wichtig zu beachten, dass dies nur ein Ausgangspunkt ist. Bestätigen Sie alle Epic Designs gemeinsam mit unserem Epic Alliance Team. Das Team ist unter der Epic@NetApp.com erreichbar. Jede Implementierung muss Kundenanfragen erfüllen und dabei die von Epic und NetApp empfohlenen Best Practices einhalten.</block>
  <block id="20e8e833e31ffb5776ac701658144bd3" category="list-text">Kleine Epic Architektur mit einer Epic Datenbank unter 10 TB</block>
  <block id="5d8ee21ec144a04a8287c0171b9a8527" category="list-text">Mittelgroße Epic-Architektur mit einer Epic-Datenbank von 10 TB bis 50 TB</block>
  <block id="79ac7005a81d3ad4f64a1a5f5db00c7e" category="list-text">Große Epic-Architektur mit einer Epic-Datenbank von über 50 TB</block>
  <block id="650c96055e8747d8b8a4c503b32aea8d" category="inline-image-macro">Anleitung zum Epic Sizing</block>
  <block id="1c2004236bc67ea5f8fcc53a967ce698" category="paragraph"><block ref="1c2004236bc67ea5f8fcc53a967ce698" category="inline-image-macro-rx" type="image"></block></block>
  <block id="431d999c23fb9c149586aa971425928e" category="summary">Epic-Storage-Anforderungen</block>
  <block id="8afb3f2b206e0239313a484fe51999d7" category="paragraph">Dedizierte Storage-Ressourcen werden in der Regel für die Produktionsdatenbank bereitgestellt, während gespiegelte Datenbankinstanzen sekundäre Storage-Ressourcen gemeinsam mit anderen Softwarekomponenten von Epic nutzen, beispielsweise den Tools zur Klarstellung-Berichterstellung.</block>
  <block id="d31081e8762bd0903adf5d3f0b769e73" category="paragraph">Andere Software-Storage-Umgebungen, die beispielsweise für Applikations- und Systemdateien verwendet werden, werden von den sekundären Storage-Ressourcen ebenfalls bereitgestellt.</block>
  <block id="ed879cdc9a77683dfbe5be59441dec43" category="paragraph">Neben den Größenüberlegungen sind bei Epic die folgenden zusätzlichen Regeln für das Storage-Layout sowie wichtige Überlegungen anzustellen:</block>
  <block id="ba76cf08b10101b5fee8e2e6d0da268e" category="list-text">Seit 2020 müssen sich alle Workloads von betrieblichen Datenbanken (ODB) auf All-Flash-Arrays befinden.</block>
  <block id="542f0f7c69fde2bd55b5a36854a84fa3" category="list-text">EPIC empfiehlt, dass sich jeder Speicherpool auf separater physischer Hardware befindet, einschließlich Pool1, Pool2, Pool3, NAS1 und NAS2.</block>
  <block id="919d67cad73121e6af0b2b20169d947b" category="admonition">Ein Node in einem Cluster kann als Speicherpool angesehen werden. Mit ONTAP 9.4 oder höher und AQoS können Sie geschützte Pools mithilfe von Richtlinien erstellen.</block>
  <block id="4449a7b5d2b5dfb4b1bad2ed2f86b5fd" category="list-text">Neue Empfehlung für Epic 3-2-1 Backup</block>
  <block id="b560499317ab0447790f01d7acd5401b" category="list-text">Kopie am Remote-Standort (Disaster Recovery)</block>
  <block id="09d6c5f51239ea513e80e433549993f4" category="list-text">Eine der Kopien muss sich auf einer anderen Storage-Plattform als der primären Kopie befinden</block>
  <block id="df5be706f042b26557014aa77c608eb6" category="list-text">Datenkopien zu erstellen</block>
  <block id="47311b53e9ae798223e097c4249a0668" category="admonition">Kunden, die NetApp SnapMirror zur Sicherung von NetApp verwenden, entsprechen nicht den 3-2-1-1-Empfehlungen. Der Grund dafür ist, dass ONTAP für ONTAP die zweite oben aufgeführte Anforderung nicht erfüllt. Mit SnapMirror direkt aus ONTAP können Sie Objekt-Storage vor Ort (z. B. über StorageGRID) oder in der Cloud nutzen, um Epic-Anforderungen zu erfüllen.</block>
  <block id="fef74abf89ce7520dbdabb6ae983ee4b" category="paragraph">Weitere Informationen zu Storage-Richtlinien finden Sie in den folgenden Epic-Leitfäden, die in Galaxy zur Verfügung stehen:</block>
  <block id="bed1a6db332388739ca169835c6c3425" category="list-text">Überlegungen zu SAN</block>
  <block id="9080906c2106e0567c91717cc003d027" category="list-text">Speicherprodukte und Technologiestatus (SPATs)</block>
  <block id="81b560a1124b6edc7fe2858e332661b6" category="list-text">Hardware-Konfigurationshandbuch</block>
  <block id="db11e37a9250a9db4d48618851771913" category="summary">Konfiguration von Epic-Storage-Snapshots</block>
  <block id="4cbb03cae0c07fb5c8e5d5e6df97064e" category="doc">Epic Konfiguration zur Storage-Effizienz</block>
  <block id="bc9c71d0bf41f8b2c8bde032eb1d97d0" category="paragraph">Bei Applikationen, deren Storage auf mehrere Volumes verteilt ist und für die der Workload eine oder mehrere LUNs mit entsprechender Menge vorhanden ist, müssen die Inhalte zusammen gesichert werden, damit für die konsistente Datensicherung CGS erforderlich sind.</block>
  <block id="d43039551f875a24f15f154306be2586" category="paragraph">Konsistenzgruppen (kurz CGS) bieten diese und vieles mehr. Sie können jede Nacht verwendet werden, um mithilfe einer Richtlinie konsistente On-Demand- oder geplante Snapshots zu erstellen. Sie können damit Daten wiederherstellen, klonen und sogar replizieren.</block>
  <block id="cbe00f2bdd9a3c8aaaf5f8159a8750c1" category="paragraph">Weitere Informationen zu CGS finden Sie im <block ref="acc09d7e0c59ae3310cafae5d547383a" category="inline-link-macro-rx"></block></block>
  <block id="e62f16b222adc7aecc70ff7b56e652dc" category="paragraph">Sobald die Volumes und LUNs wie in den vorherigen Abschnitten dieses Dokuments beschrieben bereitgestellt wurden, können sie dann in einen Satz von CGS konfiguriert werden. Es wird empfohlen, diese wie in der folgenden Abbildung dargestellt einzurichten:</block>
  <block id="7dd784e39903a42471f96a6d0d6ae420" category="inline-image-macro">Epic Consistency Group-Layout</block>
  <block id="909e3795441b963b86fcad387196919f" category="paragraph"><block ref="909e3795441b963b86fcad387196919f" category="inline-image-macro-rx" type="image"></block></block>
  <block id="45dbe5c870e6de731da235aab8c51046" category="paragraph">Ein nächtlicher CG-Snapshot-Zeitplan sollte auf jedem der Child-CGS festgelegt werden, die den Volumes zugeordnet sind, die Speicher für die Produktionsdatenbank bereitstellen. Dies führt zu einer neuen Reihe konsistenter Backups dieser CGS jede Nacht. Diese können dann für das Klonen der Produktionsdatenbank für nichtproduktive Umgebungen wie beispielsweise Entwicklungs- und Testumgebungen verwendet werden. NetApp hat für Epic proprietäre, CG-basierte automatisierte Ansible-Workflows entwickelt, um das Backup von Produktionsdatenbanken sowie die Aktualisierungs- und Testumgebungen zu automatisieren.</block>
  <block id="a93362f9de0aff40adccba50a939a6fa" category="paragraph">CG-Snapshots können zur Unterstützung der Wiederherstellungsvorgänge der Produktionsdatenbank von Epic verwendet werden.</block>
  <block id="4bf6f09e67952b918985533c270b0668" category="paragraph">Deaktivieren Sie bei SAN-Volumes die Standard-Snapshot-Richtlinie für jedes Volume, das für CGS verwendet wird. Diese Snapshots werden in der Regel von der verwendeten Backup-Applikation oder dem NetApp Automatisierungsservice „Epic Ansible“ gemanagt.</block>
  <block id="7082753514151e011ff039e6fe23eba2" category="paragraph">Deaktivieren Sie bei SAN-Volumes die Standard-Snapshot-Richtlinie für jedes Volume. Diese Snapshots werden in der Regel von einer Backup-Applikation oder von Epic Ansible Automation gemanagt.[NS2]</block>
  <block id="669d2da75a76d767cc8259b5f37cacc3" category="paragraph">WebBLOB und VMware Datensätze sollten als reine Volumes konfiguriert werden, die nicht CGS zugewiesen sind. Mit SnapMirror können Snapshots unabhängig von der Produktion auf Storage-Systemen erstellt werden.</block>
  <block id="5a6fb6753e1c4f748d223c8e329133fb" category="paragraph">Nach Abschluss der Konfiguration sieht die Konfiguration wie folgt aus:</block>
  <block id="926ce6c3b85cb26b415b6fc7e43f2879" category="inline-image-macro">Epic mit CG-Snapshots</block>
  <block id="a578d209aa95904cf62330de918a003d" category="paragraph"><block ref="a578d209aa95904cf62330de918a003d" category="inline-image-macro-rx" type="image"></block></block>
  <block id="193e03aa51a18f1e5d72b6a9dd240ed7" category="paragraph">ONTAP Inline-Effizienzfunktionen sind standardmäßig aktiviert und funktionieren unabhängig von Storage-Protokoll, Applikation oder Storage-Tier.</block>
  <block id="f900dc6292f9bf9dcf98291ad5846d4b" category="paragraph">Effizienzgewinne reduzieren die Menge der Daten, die auf teuren Flash-Storage geschrieben werden, und verringern die Anzahl der erforderlichen Laufwerke. ONTAP bewahrt die Effizienz durch Replizierung. Jede dieser Effizienzen hat selbst für eine latenzempfindliche Applikation wie Epic kaum bis gar keine Auswirkungen auf die Performance.</block>
  <block id="936d07f32ce5afcd55e98c4a8cbce642" category="paragraph">*NetApp empfiehlt*, alle Effizienzeinstellungen zu aktivieren, um die Festplattenauslastung zu maximieren. Diese Einstellungen sind auf AFF- und ASA-basierten Systemen standardmäßig aktiviert.</block>
  <block id="d4a551eeebb2992a4ffff1ad1083479a" category="paragraph">Diese Storage-Effizienz wird durch die folgenden Funktionen ermöglicht:</block>
  <block id="cc0df1dcdfa0541f9c5dde8e0b570a36" category="list-text">Deduplizierung spart auf Primär-Storage Platz, indem es redundante Kopien von Blöcken eines Volumes, das LUNs hostet, entfernt. Diese empfohlene Option ist standardmäßig aktiviert.</block>
  <block id="db37fd6533f852997b6ae076ae5e86e0" category="list-text">Bei der Inline-Komprimierung wird die Datenmenge, die auf Festplatte geschrieben werden muss, reduziert. Bei Epic-Workloads wird zudem eine erhebliche Speicherersparnis erzielt. Diese empfohlene Option ist standardmäßig aktiviert.</block>
  <block id="c8afff3d362092825c7eda5b0b6b39af" category="list-text">Die Inline-Data-Compaction benötigt 4-kb-Blöcke, die weniger als die Hälfte voll sind, und kombiniert sie in einem einzelnen Block. Diese empfohlene Option ist standardmäßig aktiviert.</block>
  <block id="b158d6ac5a43e19d1be67d7ba849494d" category="list-text">Thin Replication ist eine zentrale Plattform im Portfolio der NetApp Software für Datensicherung, das auch die NetApp SnapMirror Software umfasst. SnapMirror Thin Replication sichert geschäftskritische Daten und minimiert gleichzeitig die Anforderungen an die Storage-Kapazität. *NetApp empfiehlt*, diese Option zu aktivieren.</block>
  <block id="13fb0a52e16b7b3839f151b0847cd0d1" category="list-text">Deduplizierung von Aggregaten: Die Deduplizierung befindet sich immer auf Volume-Ebene. Mit ONTAP 9.2 wurde die Aggregatdeduplizierung verfügbar und ermöglicht dadurch zusätzliche Einsparungen bei der Festplattenreduzierung. Die nachgelagerte Deduplizierung für Aggregate wurde mit ONTAP 9.3 hinzugefügt. *NetApp empfiehlt*, diese Option zu aktivieren.</block>
  <block id="46269eefafcbb08ab4667022c6491767" category="summary">Epic- und Dateiprotokolle</block>
  <block id="73a17ad0a24141989f242b5b099eec4f" category="paragraph">Die Kombination von NAS und SAN auf demselben All-Flash-Array wird unterstützt.</block>
  <block id="688ffca27fb85bfb71c8a7632a871ec1" category="paragraph">*NetApp empfiehlt* die Verwendung von FlexGroup Volumes für NAS Shares, wie WebBLOB (wenn verfügbar).</block>
  <block id="55449fd175613962d882ab450cfc5f3c" category="inline-link-macro">FabricPool</block>
  <block id="1cd6516d7f54730b868069e2db63b528" category="paragraph">WebBLOB ist bis zu 95% kalte Daten. Optional können Sie Speicherplatz auf Ihrem All-Flash-Array freigeben und mithilfe der Funktion von ONTAP Backups und kalte Daten in Objektspeicher vor Ort oder in der Cloud verschieben<block ref="e722be9b80fb0aa9c356c4300710e630" category="inline-link-macro-rx"></block>. All dies lässt sich ohne spürbare Leistungseinbußen realisieren. FabricPool ist eine integrierte Funktion von ONTAP. Die Kunden können einen kalten (oder inaktiven) Datenbericht erstellen, um zu prüfen, welche Vorteile durch FabricPool erzielt werden könnten. Sie können über eine Richtlinie das Alter der Daten für das Tiering festlegen. Epic-Kunden konnten mit dieser Funktion deutliche Einsparungen erzielen.</block>
  <block id="fc3145f7fe85a0cdfffb31f4034ada3b" category="summary">Epic auf ONTAP - Host Utilities</block>
  <block id="f2f8d65f0baeae6d15c1016784ddadbf" category="paragraph">Die NetApp Host Utilities sind Softwarepakete für verschiedene Betriebssysteme, die Management Utilities wie CLI-Binärdateien, Multipath-Treiber und andere wichtige Dateien enthalten<block ref="3a8e4afa851609127d534b04e7d29c08" prefix=" " category="inline-code"></block>, die für einen ordnungsgemäßen SAN-Betrieb erforderlich sind.</block>
  <block id="23cfb87b0ff3960aab8099cbbb63b50b" category="inline-link-macro">San-Hosts</block>
  <block id="b0a8af1d81527bdd65ec0e8fb2a4b4e0" category="paragraph">*NetApp empfiehlt* die Installation der NetApp-Hostdienstprogramme auf Hosts, die mit NetApp-Speichersystemen verbunden sind und auf diese zugreifen. Weitere Informationen finden Sie unter <block ref="b2445b429f9784426e3dc1158e67c70d" category="inline-link-macro-rx"></block> und <block ref="7d7a0f71ed9bf2fe94a10fa9e1aeb381" category="inline-link-macro-rx"></block> in der Dokumentation.</block>
  <block id="54ed29e411552372c5b6b9e2e1617e6d" category="admonition">Bei AIX ist es besonders wichtig, dass die Host Utilities vor dem Erkennen von LUNs installiert werden. Dadurch wird sichergestellt, dass das LUN-Multipathing-Verhalten korrekt konfiguriert ist. Wenn die Erkennung ohne die Host Utilities ausgeführt wurde, müssen die LUNs mit dem Befehl vom System dekonstruiert und dann über einen Neustart wieder erkannt<block ref="fe54d1c30028417038a4ffe0fc5eb503" prefix=" " category="inline-code"></block> werden<block ref="9442434081221d63eaaa2c4799062866" prefix=" " category="inline-code"></block>.</block>
  <block id="e7a8be60cff2fec7b8ed793c4d2dddfe" category="summary">Epic LUN- und Volume-Konfiguration</block>
  <block id="acdbb325faec5e3e2d86b3dbbaf90c54" category="paragraph">Das Dokument mit Empfehlungen für das Epic Datenbank-Storage-Layout enthält Anweisungen zu Größe und Anzahl der LUNs für jede Datenbank.</block>
  <block id="a620ecf0a73eb209a145eb913a29bc5a" category="paragraph">Es ist wichtig, dieses Dokument mit der Epic DBA- und Epic-Unterstützung zu prüfen und die Anzahl der LUNs und LUN-Größen festzulegen, da ggf. Anpassungen erforderlich sind. Diese Storage-Empfehlungen sind wichtig für die HBA-Warteschlangentiefe, die Storage-Performance, den einfachen Betrieb und die einfache Erweiterung.</block>
  <block id="4442aab15395384af6aee974a5a581bc" category="paragraph">Verwenden Sie für die Berücksichtigung der Warteschlangentiefe des Server-Betriebssystems mindestens acht LUNs (eine LUN pro Volume) für eine Datenbank. Erhöhen Sie die Anzahl der LUNs um ein Vielfaches von acht. Für größere Umgebungen können mehr LUNs erforderlich sein. Verwenden Sie die gleiche Anzahl von Volumes (insgesamt acht, vier pro Node) und fügen Sie LUNs in einem Vielfachen von acht hinzu. Epic empfiehlt, die LUNs &lt; 8 TB und die Anzahl der LUNs &lt; 32 zu behalten. Mit diesem Ansatz können Sie Ihre Epic-Umgebung einfach skalieren.</block>
  <block id="21ebeea1f14a138dfebb1b9599a4c3dd" category="paragraph">Zur Maximierung der Performance für einen Workload, z. B. Epic ODB oder Clarity, funktioniert jedes Layout auch am besten für NetApp Storage. Die Schreib-I/O-Vorgänge werden unter Verwendung von acht Volumes gleichmäßig über Controller verteilt, wodurch die CPU-Auslastung optimiert wird. Für Replizierung und Backup sollte die Anzahl der Volumes zur Vereinfachung des Betriebs auf acht begrenzt werden.</block>
  <block id="90adf331b7868c7af46ad6e035e93844" category="section-title">Skalierungsoptionen</block>
  <block id="68b3c97d7d89bcf19218f4db828d1f62" category="paragraph">Wenn der Server mehr Storage benötigt, empfiehlt es sich, die LUNs mit Volumes am einfachsten zu vergrößern. Die zweite Option besteht darin, den Volume-Gruppen jeweils ein Vielfaches von acht LUNs hinzuzufügen (eine pro vorhandenem Volume). Durch das Hinzufügen von nur einer oder zwei LUNs kann ein Hotspot erstellt und die Performance beeinträchtigt werden.</block>
  <block id="b323bb297a66afc2eea7b89dd4019798" category="section-title">Volume- und 8-LUN-Layout</block>
  <block id="66d125e77a6185ef5cf43b2a6bffea80" category="inline-image-macro">Epic 8-LUN-Layout</block>
  <block id="99a54ac1eaa5bd57b4c4c29d6a63f1ac" category="paragraph"><block ref="99a54ac1eaa5bd57b4c4c29d6a63f1ac" category="inline-image-macro-rx" type="image"></block></block>
  <block id="671393f63bb9ef1dfab4931884d7598d" category="section-title">Volume- und 16-LUN-Layout</block>
  <block id="0c02643253fbb18be382d4c1ad110e4f" category="inline-image-macro">Epic 16-LUN-Layout</block>
  <block id="3a69aefd67fab9412b92fa902b37d20e" category="paragraph"><block ref="3a69aefd67fab9412b92fa902b37d20e" category="inline-image-macro-rx" type="image"></block></block>
  <block id="f7b18cfcf87c0f98b439b4c7b3c92c97" category="section-title">Volume- und 24-LUN-Layout</block>
  <block id="6fc8903e912275e630ffd2c72a17adcb" category="inline-image-macro">Epic 24-LUN-Layout</block>
  <block id="c07e1713e37928ebf0d4e02dbd302844" category="paragraph"><block ref="c07e1713e37928ebf0d4e02dbd302844" category="inline-image-macro-rx" type="image"></block></block>
  <block id="b7e595df4f29c46e9a8f1aa1c32fd1bf" category="section-title">Volume- und 32-LUN-Layout</block>
  <block id="7ae005614524eafc3188227a0e2095bb" category="inline-image-macro">Epic 32-LUN-Layout</block>
  <block id="45c0c385be9c9d903ed267e0455dc156" category="paragraph"><block ref="45c0c385be9c9d903ed267e0455dc156" category="inline-image-macro-rx" type="image"></block></block>
  <block id="3d120048605480bd8cf2a834fe477bb8" category="list-text">Verwenden Sie entweder 8, 16, 24 oder 32 LUNs in 8 Volumes.</block>
  <block id="bad4d79343f04d8b64999875ad328086" category="list-text">Gleichen Sie die Workloads über das HA-Paar aus, um Performance und Effizienz zu maximieren.</block>
  <block id="cd4b8ea65e0732fcfa8614a31fb02e8f" category="list-text">Erstellen Sie LUNs mit der erwarteten Größe für 3 Jahre Wachstum, um zu vermeiden, dass Sie die 10-fachen LUN-Erweiterungseinschränkungen erreichen.</block>
  <block id="26b591a32a8a235db7f742995589a455" category="list-text">Verwendung von Thin Provisioning Volumes und LUNs</block>
  <block id="9fb0aaeb125238e83485971ae6b7e2fc" category="list-text">Verwenden Sie mindestens acht DB-LUNs, zwei Journal-LUNs und zwei App-LUNs. Diese Konfiguration maximiert die Storage Performance und die Warteschlangentiefe des Betriebssystems. Mehr können bei Bedarf aus Kapazitäts- oder anderen Gründen verwendet werden.</block>
  <block id="ba5e00554d1ba1b8ca0fc2eaeda44672" category="list-text">Wenn Sie LUNs zu Volume-Gruppen hinzufügen müssen, fügen Sie jeweils acht LUNs hinzu.</block>
  <block id="08c8ec6a9fefcf9c5c7dff1258aacf67" category="list-text">Konsistenzgruppen (CGS) sind für die gemeinsame Sicherung der Volume-Gruppe und LUNs erforderlich.</block>
  <block id="f513b81222a1b44f552ae172aeb697c9" category="list-text">Verwenden Sie QoS nicht während des Genio Storage oder während einer I/O-Performance.</block>
  <block id="b77a34499937c829a2c5c5a1cdc17f05" category="list-text">Nach dem Genio- oder Clarity-Test empfiehlt NetApp, den Storage zu löschen und neu bereitzustellen, bevor die Produktionsdaten geladen werden.</block>
  <block id="186bc47646aec5e2abf40b2605a16461" category="list-text">Es ist wichtig, dass<block ref="54645d7349dcc4fe9d7d19985a40c2c9" prefix=" " category="inline-code"></block> „aktiviert“ für die LUNs festgelegt wird. Ist dies nicht der Fall, werden alle gelöschten Daten auf den LUNs von ONTAP nicht erkannt, sodass möglicherweise Kapazitätsprobleme auftreten können. Weitere Informationen finden Sie im Quick Reference Guide zur Epic Storage-Konfiguration.</block>
  <block id="153c47e1653d87804319da1fe347f29b" category="summary">Epic-Performance-Management</block>
  <block id="912975539030067a6f1874223cda9b75" category="paragraph">Die meisten All-Flash-Arrays können die für Epic-Workloads erforderliche Performance liefern. Das Alleinstellungsmerkmal von NetApp besteht in der Möglichkeit, Performance-Richtlinien auf Bodenebene festzulegen und für jede Applikation eine konsistente Performance zu garantieren.</block>
  <block id="edfc4c86589e31546a8fb2d8b085b8c9" category="section-title">Servicequalität (QoS)</block>
  <block id="8080e6f80ff76b571829265ee2917e87" category="paragraph">NetApp empfiehlt die Verwendung von QoS. QoS hat den Vorteil, alle Epic-Workloads zu konsolidieren. Sämtliche Storage-Protokolle und -Pools können auf weniger Hardware ausgeführt werden. Es müssen keine separaten Storage-Pools mehr verwendet werden.</block>
  <block id="44c2f7cd7c41efaae67916780bf96079" category="list-text">NetApp empfiehlt, alle Workloads im Cluster einer QoS-Richtlinie zuzuweisen, um den Reserven im Cluster besser zu managen.</block>
  <block id="d90d7fd828646173964451dadbe758c1" category="list-text">NetApp empfiehlt, alle Workloads gleichmäßig über das HA-Paar verteilt zu haben.</block>
  <block id="a77412a540f86bfc443abfcef22db414" category="list-text">Verwenden Sie bei I/O-Tests keine QoS-Richtlinien, da andernfalls der Genio-Test fehlschlägt. Verschiedene Produktions-Workloads sollten 2-4 Wochen lang analysiert werden, bevor QoS-Richtlinien zugewiesen werden.</block>
  <block id="c36c006dced61f1e460ff43516fa7294" category="summary">Epic auf ONTAP - Protokolle</block>
  <block id="0f079f7da3a6e5c8a52a3aca113fd3e6" category="paragraph">FCP ist das bevorzugte Protokoll für die Bereitstellung von LUNs.</block>
  <block id="97ec588f063ba0f25df2b93eda2639c8" category="paragraph">*NetApp empfiehlt* Einzel-Initiator-Zoning: Ein Initiator pro Zone mit allen erforderlichen Ziel-Ports auf der Speicherung unter Verwendung von weltweiten Port-Namen (WWPNs). Das Vorhandensein von mehr als einem Initiator in einer einzelnen Zone führt wahrscheinlich zu intermittierendem HBA-Crosstalk, was zu erheblichen Störungen führt.</block>
  <block id="880bc24c60e1d6eb559f57440c27f955" category="paragraph">Nach der Erstellung der LUN ordnen Sie die LUN der Initiatorgruppe zu, die die WWPNs des Hosts enthält, um den Zugriff zu ermöglichen.</block>
  <block id="d8609b6e37acaad082fff919da3c6691" category="paragraph">NetApp unterstützt auch den Einsatz von NVMe/FC (sofern Sie Versionen von AIX und RHEL Betriebssystemen haben) und verbessert die Performance. FCP und NVMe/FC können gleichzeitig im selben Fabric vorhanden sein.</block>
  <block id="2586e87005565fe66fa19a3af50f632c" category="summary">Storage-Dimensionierung für Epic</block>
  <block id="5e3e8694114d62a133f86c691b0d1d68" category="paragraph">Bestätigen Sie alle Epic Designs gemeinsam mit unserem Epic Alliance Team. Das Team ist unter der Epic@NetApp.com erreichbar. Jede Implementierung muss Kundenanfragen erfüllen und dabei die von Epic und NetApp empfohlenen Best Practices einhalten.</block>
  <block id="c77c62342c6d144df81571fee8e8a4e8" category="paragraph">Informationen zum Verwenden von NetApp Sizing Tools zum Bestimmen der richtigen RAID-Gruppengröße und der Anzahl der RAID-Gruppen für die Storage-Anforderungen in der Epic Software-Umgebung finden Sie unter <block ref="b6ff45309a811037a92ca1e6262c74a8" category="inline-link-macro-rx"></block> (NetApp-Anmeldung erforderlich).</block>
  <block id="6fff8a4d9dc5db1aefc3e3a86c26dbd7" category="admonition">Zugriff auf das NetApp Field Portal ist erforderlich.</block>
  <block id="1484157a851d4053990c9bf55fb59e58" category="summary">Epic auf ONTAP, Beispielimplementierung – Aggregate</block>
  <block id="e8f3d574ecc3fdf160285a0c9a5a7046" category="paragraph">Die aktuelle Dokumentation zum Bereitstellen von Aggregaten finden Sie unter <block ref="a20b867e80db6cfd4fb8336d84c1fcc5" category="inline-link-macro-rx"></block>.</block>
  <block id="f2871c08962452bb98a23ee83880a7dd" category="admonition">Die Standardeinstellungen optimieren Performance und Kapazität. Pro Node wird ein großes Aggregat erstellt.</block>
  <block id="46b67d0e3c5baea52f318c392862983a" category="summary">Epic auf ONTAP – Beispielbereitstellung – Dateisysteme</block>
  <block id="ff902a0a262960f8bf9189923c1ff7d9" category="paragraph">Informationen über das Mounten von LUNs, das Erstellen von Volume-Gruppen und logischen Volumes und das Konfigurieren der Dateisysteme finden Sie im Quick Reference Guide zur Epic-Speicherkonfiguration. Verwenden Sie die folgenden Beispielbefehle, um Epic-Produktionsserver für RHEL einzurichten.</block>
  <block id="f54e4ad7066826fa3c18dcb8a62ddd03" category="section-title">Dateisystem- und Mount-Optionen</block>
  <block id="ca487f2e3d336dbbead8ef7ce6f1dc6c" category="paragraph">Nachdem die LUNs erstellt und zugeordnet wurden und das Zoning abgeschlossen ist, gehen Sie wie folgt vor, um dem Server Speicher hinzuzufügen.</block>
  <block id="232aac42537fa2491071068a6dacfe10" category="admonition">In diesem Beispiel haben wir 8 LUNs mit 1024 GB für die DB, 2 LUNs mit 1024 GB für Journale und 2 LUNs mit 1024 GB für Applikationsinstallationen verwendet.</block>
  <block id="cdf658da371089bb06d8dc79b4ae2e2e" category="section-title">Asynchroner I/O</block>
  <block id="86c5811d8dc490e1467aed33484d89f2" category="paragraph">Eine Kopie des Whitepapers „Überlegungen zum Epic SAN“ und des Dokuments „Kurzübersicht zur Storage-Konfiguration“ enthalten Details zur Konfiguration der Hosts und zur Verbindung mit dem Storage. In diesem Abschnitt wird die Einrichtung eines Red hat Enterprise Linux-Hosts erläutert. AIX-Details finden Sie in den referenzierten Dokumenten.</block>
  <block id="81cba0a3715b5297330ade2beaa9dc7d" category="summary">Epic auf ONTAP, Beispielbereitstellung – LUNs</block>
  <block id="30baf86157b500051493cda4ff1be0c3" category="paragraph">&gt;&gt;&gt; Platzhalter für beschreibenden Satz oder Absatz</block>
  <block id="6aa18085c59b7b34360d5dff15bf6e84" category="paragraph">LUN erstellen</block>
  <block id="6a761ce5c1cf1122b1aaea2aec3cfc85" category="paragraph">So erstellen Sie eine LUN:<block ref="18244093c882fe4e65acdec6d61f9f95" category="inline-link-rx"></block></block>
  <block id="8ede5bcd129cb7b52add8d233c5e5867" category="paragraph">Fügen Sie Volumes zur CG hinzu</block>
  <block id="4bd3faeb169131a1ecbf00026f146a6f" category="paragraph">So erstellen oder ändern Sie Konsistenzgruppen:<block ref="52ad8c1ba289849e0c6ebe85b916c91a" category="inline-link-rx"></block></block>
  <block id="1093fcc785c9f0e59ff32142b73ff3a5" category="paragraph">LUN zuordnen</block>
  <block id="9e5dabcfb97ff08290166a6dd001c95e" category="paragraph">So ordnen Sie die LUN zu:<block ref="8b5a6c46b302a4299b28e891d262657f" category="inline-link-rx"></block></block>
  <block id="73a228bbb9b14b5f966f329a201b115d" category="paragraph">Je nach Version von ONTAP kann die Standardeinstellung für fraktionelle Reserve auf dem Volume 100 % betragen. Diese Konfiguration sollte auf 0 gesetzt werden.</block>
  <block id="b13aace07f0445cf1ff16be054bcc01e" category="summary">Epic auf ONTAP, Beispielimplementierung</block>
  <block id="cca1d6e8ff542fcd4d1315f7f9033467" category="paragraph">Dieser Abschnitt führt Sie durch eine vollständige, erweiterte Konfiguration eines ONTAP-Clusters sowie die Bereitstellung und Bereitstellung von Storage für einen Epic-Server.</block>
  <block id="df8f0aad2b90e887efba35ecbc7e6546" category="paragraph">Zur Erfassung von Details und zur einfacheren Dokumentation wird die Befehlszeile verwendet. Ist die GUI bevorzugt, können Sie alle Einstellungen in System Manager bereitstellen.</block>
  <block id="d706ffe8745cb55ce7b7d1c7dfa458dd" category="paragraph">In der Vergangenheit ist die erste Masseneinrichtung für größere Projekte in der Regel mit den Befehlen in Tabelle 1 schneller, insbesondere wenn Sie die Befehle in einem Arbeitsblatt verketten. Diese Liste von Befehlen dient auch als ausgezeichnete Build-Dokumentation.</block>
  <block id="12c8172fde8f9b3f81c3ed0d2805f3d4" category="paragraph">Eine weitere Bereitstellungsoption ist Ansible mit Automatisierungs-Skripten ab Tag 0 und Tag 1. NetApp bietet Hunderte von bestehenden Ansible-Playbooks zum Download an, einschließlich der Ansible Galaxy-Sammlung über den Befehl „Installation der NetApp.ONTAP“ für die Sammlung.</block>
  <block id="80455b40cdd7b477184d76f23f747af1" category="paragraph">Die GUI eignet sich auch hervorragend mit einer einfachen LUN auf einer Seite und gemeinsam genutztem Provisioning. GUI eignet sich am besten für Vorgänge beim Hinzufügen, Ändern oder Löschen von Speicher. Beide Optionen sind in Ordnung, wenn Sie die Best Practices-Storage-Einstellungen in Tabelle 1 anwenden.</block>
  <block id="d7441f24bee2c1922760ce27f5e978ec" category="paragraph">Das vollständige Cluster-Setup und die Storage-/Host-Bereitstellung sollten bei der Bereitstellung nicht länger als eine Stunde in Anspruch nehmen.</block>
  <block id="3924c8a2656c72cc0c7d6a27f9ad08ea" category="paragraph">*Best Practice Speichereinstellungen*</block>
  <block id="51ac4bf63a0c6a9cefa7ba69b4154ef1" category="cell">Einstellung</block>
  <block id="689202409e48743b914713f96d93947c" category="cell">Wert</block>
  <block id="2ee34178bb8415b7d7234cd27b83aed6" category="cell">Aggregat</block>
  <block id="ae6bd8a277455ba774ffc217a2f8a539" category="cell">Standard-Autoprovisionierung, ein ADP-Aggregat pro Node mit RAID DEP</block>
  <block id="63555da25f96be027cd722d9f281cffa" category="cell">Zwei SVMs bei Verwendung von multiprocol (Block SVM und SMB/NFS SVM) Epic- und Protokoll-Namenskonvention verwenden Verwenden Sie den richtigen Sicherheitsstil</block>
  <block id="aec0819ab194aee8aab8a5e40aad8343" category="cell">Volume-Speicherplatzgarantie</block>
  <block id="334c4a4c42fdb79d7ebc3e73b517e6f8" category="cell">Keine</block>
  <block id="d374cbba7f603e6be8e6450a6b3feb45" category="cell">Richtlinie für Volume-Snapshots</block>
  <block id="8285885018fdba6d87edc774237f0e24" category="cell">Automatische Volumengröße</block>
  <block id="4d200fce73a8e1cc965cfc2c43343824" category="cell">Wachsen</block>
  <block id="948111f101cd4e13c4a1e21775e9742d" category="cell">Maximale automatische Größenanpassung des Volumes</block>
  <block id="fde7826aacd46b7610beed2b0ef8d81e" category="cell">2T- oder 2 X-LUN-Größe</block>
  <block id="ca714a7089246aff7f85bbda97ba660b" category="cell">Automatisches Löschen von Volume-Snapshots</block>
  <block id="a10311459433adf322f2590a4987c423" category="cell">Aktiviert</block>
  <block id="6bb41960470a19d97556b9240ac0b3ff" category="cell">Volume-Größe</block>
  <block id="9bba382221ce257d09f3f50492803815" category="cell">1.5 x LUN-Größe</block>
  <block id="dc85eb20ed6dd71700df76a7f9acd32b" category="cell">Volume-Layout</block>
  <block id="370e47099244ee43c575d91449bb7e36" category="cell">Sogar auf Controller verteilt</block>
  <block id="ca301c828c76b7b6b1a9b621ba37f603" category="cell">Initiatorgruppen</block>
  <block id="d6c56de514a4a8fe23f54d6b42dd23af" category="cell">Betriebssystem bei Verwendung mit physischen Servern, VMware-Typ bei Verwendung mit ESX.</block>
  <block id="bbd68bf55ab562da30e73449f1b87424" category="summary">Epic auf ONTAP, Beispielimplementierung – Protokolle</block>
  <block id="589b83bd3c7696acc0086c7f233e64e3" category="doc">Epic auf ONTAP, Beispielimplementierung – Plattformen</block>
  <block id="05f72896244a448167caa4a410eb4ecf" category="paragraph">Für Epic ODB-Datenbanken, Journal und Applikations-Workloads empfiehlt Epic die Bereitstellung von Storage für Server als FCP-LUNs.</block>
  <block id="d2ec366dff0b7a3eda35e6acf91f6898" category="paragraph">NetApp-Kunden haben alle Epic-Workloads erfolgreich in der Azure-Cloud ausgeführt. AWS kann auch Epic-Workloads ausführen. Mit NetApp Cloud Volumes ONTAP und NetApp Cloud Volumes Services bietet NetApp die Funktionen der Enterprise-Klasse und die Performance, die für eine effektive Ausführung von Epic in der Cloud erforderlich sind. Cloud-Optionen von NetApp bieten Block-over-iSCSI und Datei über NFS oder SMB.</block>
  <block id="a3a5d01a736b2f5bc994d2ff7b0da8c5" category="summary">Epic auf ONTAP, Beispielbereitstellung – FCP-Protokoll</block>
  <block id="dc0e595680f18844673c0930ecf1ca6b" category="paragraph">Sobald die SVM erstellt wurde, müssen Sie Protokolle hinzufügen.</block>
  <block id="263a628397d3f78e3b61f40863b0868f" category="paragraph">Klicken Sie zum Erstellen von FCP-Daten-LIFs auf <block ref="8f46d074b2d46f416cb36dce6f8bd76d" category="inline-link-macro-rx"></block>.</block>
  <block id="663c2decd75c93fbad53179b6016c0d1" category="paragraph">Initiatorgruppen erlauben den Serverzugriff auf LUNs. Um eine iGroup zu erstellen, klicken Sie auf <block ref="c6345eebd77187b1723cf47095ffbd0f" category="inline-link-macro-rx"></block>.</block>
  <block id="93f7209bec244568b95ac2f8f50896d7" category="summary">Epic auf ONTAP, Beispielimplementierung – SVMs</block>
  <block id="39ce7fcbb7e39843ad7e81e460901359" category="paragraph">NetApp virtualisiert den Storage und der Benutzerzugriff erfolgt über die SVM.</block>
  <block id="8c22ab38f47a2a116e19714defb5f005" category="paragraph">Für Epic gibt es eine FCP-SVM und eine SMB-SVM. Je nachdem, wie Sie Ihren Storage managen und möglicherweise Mandantenfähigkeit verwenden möchten, können Sie mehr SVMs verwenden.</block>
  <block id="99889a411324e95bb7313521d10d972b" category="paragraph">Klicken Sie zum Einrichten einer SVM auf <block ref="cb8e547e3f29ee63864aeed75c724bf1" category="inline-link-macro-rx"></block></block>
  <block id="abf025601df70d9ba931fe67b1cc4a79" category="summary">Epic auf ONTAP, Beispielbereitstellung – Volumes</block>
  <block id="02594c71ca915876e2512e474d8b462d" category="paragraph">Informationen zum Erstellen von Volumes finden Sie unter<block ref="b6546da834bc505016791ad9e7ec9d1e" category="inline-link-rx"></block></block>
  <block id="36e2bc00e48fbcfe5ae35a8a6056788c" category="admonition">Ab ONTAP 9.7 ist die Aggregat- und Volume-Verschlüsselung bei einer NVE-Lizenz sowie integriertem oder externen Verschlüsselungsmanagement standardmäßig aktiviert. Um die Deduplizierung auf Volume-Ebene zu aktivieren, legen Sie den Fehler add -encrypt am Befehl Volume create/modify fest (falls bereits eine NVE Lizenz vorhanden ist).</block>
  <block id="7a112f92e38a30129ab13d18331e21ce" category="section-title">Automatisches Löschen von Snapshots</block>
  <block id="f118801694edca90b5e446d3fe39cda0" category="paragraph">So löschen Sie Snapshots automatisch:<block ref="ecea59d033fb2dad1d38e75fbdc1c820" category="inline-link-rx"></block></block>
  <block id="e324e68086bb9c9e930b9d1168f1de5d" category="summary">Epic auf ONTAP Verfügbarkeit</block>
  <block id="8339b88fddad3cfe433f684a83da04eb" category="paragraph">Die Kernbestandteil von ONTAP ist der unterbrechungsfreie Betrieb, der Ihnen ermöglicht, kostspielige Unterbrechungen Ihres Geschäftsbetriebs zu vermeiden.</block>
  <block id="ca7972f1caf7a478b4f931e986112b62" category="paragraph">NetApp bietet basierend auf Produktionsdaten, die über NetApp Active IQ „Home“ genannt werden, eine Verfügbarkeit von über 99.999999 %. Jedes HA-Paar im Cluster weist keinen Single Point of Failure auf. ONTAP wurde im Jahr 1992 entwickelt und ist die weltweit am häufigsten eingesetzte Datenmanagement-Software für zuverlässigen Storage. Dank des proaktiven Monitorings und der automatischen Lösung von 97 % der Probleme durch Active IQ ist die Verfügbarkeit jetzt noch höher und es werden deutlich weniger Support-Cases unterstützt.</block>
  <block id="43a21754b069ee170b1858e00c4072df" category="paragraph">Epic empfiehlt den Einsatz von HA-Storage-Systemen, um den Ausfall von Hardwarekomponenten zu verringern. Diese Empfehlung erstreckt sich von der Basishardware (z. B. redundante Netzteile) bis hin zu Netzwerken (z. B. Multipath-Netzwerke).</block>
  <block id="58992d12f27d940ad0ced78080987a44" category="paragraph">Wenn Sie Storage-Upgrades oder vertikale/horizontale Skalierungen oder den Ausgleich von Workloads im Cluster benötigen, haben die Patientenversorgung keine Auswirkungen. Unter Umständen werden Daten verschoben, doch dank Datenmigrationen oder aufwendiger Upgrades muss die Patientenversorgung nie mehr gestört werden. Entscheiden Sie sich für zukunftsweisende Technologie und vermeiden Sie Hardware-Bindung. NetApp bietet sogar eine Verfügbarkeitsgarantie von 100 %.</block>
  <block id="071a41c2bfc6123073746c5459501089" category="inline-link-macro">Zuverlässigkeit, Verfügbarkeit, Wartungsfreundlichkeit und Sicherheit der NetApp ONTAP</block>
  <block id="c38a90bcf1a9aa5905c0c4e6fbac331b" category="paragraph">Weitere Informationen zur Zuverlässigkeit, Verfügbarkeit, Wartungsfreundlichkeit und Sicherheitsfunktionen von NetApp finden Sie im <block ref="c4f5dec030b0bd57318efbed2e25e7a7" category="inline-link-macro-rx"></block>Whitepaper.</block>
  <block id="12a3215a6e17b2d4e547b8c9ddb4db59" category="summary">Epic Datensicherung</block>
  <block id="e2f4ffd17d85ea8c667a8fa10eaa81bd" category="doc">Epic-Cloning</block>
  <block id="d777ff146845d0c5a59b451dcdd63eff" category="paragraph">Epic erkennt an, dass die auf dem Storage Node basierende NetApp Snapshot Technologie im Vergleich zu herkömmlichen dateibasierten Backups keine Performance auf Produktions-Workloads auswirkt. Wenn Snapshot Backups als Recovery-Quelle für die Produktionsdatenbank verwendet werden, muss die Backup-Methode unter Berücksichtigung der Datenbankkonsistenz implementiert werden.</block>
  <block id="7952c7481879c42a618f85cfab77dc0b" category="paragraph">Ein Snapshot ist eine zeitpunktgenaue, schreibgeschützte Backup-Kopie eines Volumes. NetApp FlexClone® erstellt einen Snapshot und macht ihn sofort lesbar und beschreibbar. FlexClone Volumes bieten einen hervorragenden Mehrwert: Sie erstellen schreibgeschützte, applikationskonsistente Snapshots und erstellen beschreibbare FlexClone-Volumes aus Produktionsdaten. Diese native Funktion hat erhebliche Auswirkungen auf Storage-Einsparungen, Betriebszeiten und Automatisierungsfunktionen.</block>
  <block id="dd12a5170cc960b0ed4afe4f23fd727b" category="paragraph">Für die Aktualisierung kommen FlexClone Volumes zum Einsatz.</block>
  <block id="198010d8bf8a3ff0f084ffd059a9a1da" category="section-title">Datenmanagement</block>
  <block id="31949f92a027681e10fa0ae809d62bcd" category="paragraph">Als Teil der Lösung bietet NetApp unter Verwendung nativer ONTAP Tools eine vollständig automatisierte Lösung für Backup- und Testaktualisierungen. Diese Lösung wurde zur Vereinfachung des Epic Datenmanagements entwickelt, insbesondere für die großen Community von Epic-Datenbankadministratoren (DBAs):</block>
  <block id="bde282ad1e62a3d8d64bcbebfd99918c" category="list-text">Epic Mirror wird zur Replizierung von Daten zu Disaster Recovery and Report (grün gekennzeichnet) verwendet.</block>
  <block id="89c4ed523f61f9d3446182d80a53c8b6" category="list-text">Tägliche Daten-Dumps von Bericht zu Klarheit.</block>
  <block id="c3d37da5ec71148ab4d2292c5fc62181" category="list-text">Automatisierte Backups von NetApp (gelb gekennzeichnet).</block>
  <block id="f417d841f910f45d16ac64fccd850be8" category="list-text">Automatische NetApp-Testaktualisierung von SUP, REP und anderen (blau gekennzeichnet).</block>
  <block id="2a7a6372892435f5d284e730df618c35" category="list-text">Die Testumgebungen eignen sich nicht für kleinere Squash-Kopien, sondern für vollständige Kopierumgebungen.</block>
  <block id="88b2dcefc205727508d4017aacafaafd" category="paragraph">Weitere Informationen erhalten Sie über NetApp-Services für Epic-Applikationen.</block>
  <block id="4e3e281c19bf41f436de9fe363daa25c" category="paragraph">Consistency Groups (kurz CGS) bieten diese und weitere Funktionen. Sie können jede Nacht verwendet werden, um mithilfe einer Richtlinie konsistente On-Demand- oder geplante Snapshots zu erstellen. Sie können damit Daten wiederherstellen, klonen und sogar replizieren.</block>
  <block id="f5de57234e68f27cbd82acfc80f2d5cb" category="inline-link-macro">NetApp-Dokumentation zu Konsistenzgruppen</block>
  <block id="f7e43fe614bf53ff9721f7494c0caf16" category="paragraph">Weitere Informationen zu CGS finden Sie unter <block ref="0efc681e89d66f1c589ad00a8ae67afa" category="inline-link-macro-rx"></block></block>
  <block id="9f42082fda47a6a26e6c6e6bcc14526e" category="section-title">Konfigurieren von Konsistenzgruppen für Epic</block>
  <block id="7d381e222258ec55bcf215684ae70097" category="inline-image-macro">Epic und Consistency Groups</block>
  <block id="4eb8f91ee432a8e69615314961db4f88" category="paragraph"><block ref="4eb8f91ee432a8e69615314961db4f88" category="inline-image-macro-rx" type="image"></block></block>
  <block id="4294e0bf4b67190ddcb91a01b3c4356c" category="section-title">Snapshots von Konsistenzgruppen</block>
  <block id="86cfb45aaa741215d8d8d58223948dcc" category="paragraph">WebBLOB und VMare Datensätze sollten als nur Volumes konfiguriert werden, die nicht mit CGS verbunden sind. SnapMirror oder SnapVault Technologie können verwendet werden, um die Snapshot Kopien auf Storage-Systemen zu speichern, die nicht mit der Produktionsumgebung verbunden sind.</block>
  <block id="f5445123331a1b74a34fd1f392e9e946" category="inline-image-macro">Epic CG-Snapshots</block>
  <block id="92aabd14a16803d9fa65ca4f94488322" category="paragraph"><block ref="92aabd14a16803d9fa65ca4f94488322" category="inline-image-macro-rx" type="image"></block></block>
  <block id="9c71026f6349a81cd2a3cfcfa8ea9ceb" category="summary">Epic auf ONTAP-Konsolidierung</block>
  <block id="569a30a99d3744f0871a7c36162f469c" category="paragraph">Eine der größten Herausforderungen im Gesundheitswesen stellt die Ineffizienz von siloartigen Umgebungen dar.</block>
  <block id="87b522a84cee7651eb3ea5c59d4c9a35" category="paragraph">Mehrere Punktlösungen werden von verschiedenen Gruppen erstellt, die den Fortschritt behindern. Eine einheitliche Datenmanagement-Strategie steigert die Effizienz und beschleunigt die Transformation. Mit bahnbrechender Technologie wie der Digitalisierung von Patientenakten, Ransomware und generativer KI ist eine Konsolidierung erforderlich.</block>
  <block id="882f36b3d76e4e5b9ac08850c1e7a15a" category="paragraph">Mit ONTAP lassen sich File-/Block-/Objektspeicher und alle Ihre Tier-0/1/2/3-Workloads lokal und in der Cloud konsolidieren, alle laufen auf ONTAP.</block>
  <block id="0b153e0d191221abe8c325950ab9a03b" category="summary">Epic auf ONTAP Effizienz</block>
  <block id="12ca553efec3e73a9120d8c050d405a9" category="paragraph">Epic läuft auf All-Flash-Arrays, bei denen die meisten Kosten auf Festplatten anfallen. Daher ist Storage-Effizienz ein entscheidender Faktor für Kosteneinsparungen.</block>
  <block id="aac466a460fe672b45d2e43b476f772e" category="paragraph">Die Inline-Storage-Effizienz von NetApp sorgt ohne Auswirkungen auf die Performance für erstklassige Storage-Einsparungen und wir bieten sogar eine Garantie für schriftliche Effizienz bei den All-Flash-Arrays.</block>
  <block id="436525ee58a117eefbde96bb2f0cae02" category="paragraph">Bei der Berechnung der Storage-Effizienz ist es wichtig, die nutzbare bis effektive Kapazität zu messen.</block>
  <block id="bbf547b93564824fe00f04646269a56e" category="list-text">*Raw Capacity* bevor ein RAID angewendet wird, Größe der Festplatte nach Anzahl der Festplatten.</block>
  <block id="29cabda6c17d251ab3bba7feb41646fc" category="list-text">*Nutzbare Kapazität* nach der RAID-Anwendung, wie viel nutzbarer Speicherplatz zur Verfügung steht.</block>
  <block id="9edb4e889ca40d3881217c9433ecb376" category="list-text">*Effektive Kapazität* wie viel Speicher wird bereitgestellt und dem Host oder Client zur Verfügung gestellt.</block>
  <block id="21e10a177fbffc69f63f5dcdb44175f5" category="paragraph">Die Abbildung unten zeigt eine beispielhafte Effizienzberechnung einer typischen Epic Implementierung mit allen Workloads, die 852 TB effektiver Storage benötigen, und einer Effizienz von 5.2:1, die insgesamt 1,32 PB effektive Daten liefert.</block>
  <block id="c8c2fd097c28062ec912ad0d2f9bbe11" category="admonition">Je nach Anzahl der Festplatten variiert die nutzbare Kapazität geringfügig.</block>
  <block id="f0a0c9b18011b122c315d8970dbcce49" category="inline-image-macro">Epic-Storage-Effizienz</block>
  <block id="d145883b21ccbccb471abc3a5dac40f3" category="paragraph"><block ref="d145883b21ccbccb471abc3a5dac40f3" category="inline-image-macro-rx" type="image"></block></block>
  <block id="bf8acecf163b9e71e2e9331bc87c5e90" category="admonition">NetApp verwendet weder NetApp Snapshot Technologie noch Thin Provisioning, um die Effizienz im Garantieprogramm zu berechnen. Dies würde eine unrealistische Effizienz von 30-100:1 zeigen, die nicht bedeuten, wenn man die tatsächliche Storage-Kapazität eindimensioniert.</block>
  <block id="f4f81a0ab85224d234bc465988b5b871" category="summary">Epic auf ONTAP – Übersicht</block>
  <block id="35b4c598dd74c59e91d978c12210cc03" category="doc">EPIC auf ONTAP</block>
  <block id="52aaae321a4b3f69eaf3448c22d2d55c" category="paragraph">Epic ist einfacher mit ONTAP.</block>
  <block id="76d68daeac9b3e7a8cfb0c3d904ac7de" category="paragraph">ONTAP ist eine Datenmanagementplattform, mit der Sie Epic-Workloads konsolidieren und gleichzeitig alle Anforderungen hinsichtlich Performance, Datensicherung und Datenmanagement erfüllen können.</block>
  <block id="85e6f9f2e3539043a3a60b16af5a654f" category="paragraph">Nur auf NetApp können Sie alle Ihre Workloads im Gesundheitswesen für SAN, NAS und Objekt-Storage auf einer einzigen hochverfügbaren Datenmanagementplattform standardisieren. ONTAP ist die weltweit am häufigsten eingesetzte Storage-Softwareplattform mit fast 30 Jahren kontinuierlicher Innovation. Mit nativen ONTAP-Datenmanagement-Tools und Applikationsintegration lassen sich alle Epic-Herausforderungen bewältigen. Es muss keine Vielzahl von Tools von Drittanbietern erworben werden, um Lücken in der Lösung zu schließen.</block>
  <block id="54fed3d17b1233bcc7ab7b0d3f6eddca" category="paragraph">Viele Storage-Anbieter bieten herkömmlichen, zuverlässigen und schnellen Block-Storage. Sie funktionieren gut, werden aber in der Regel in Silos für die Ausführung eines einzelnen Workloads implementiert, z. B. für Produktion, Bericht, Klarheit, VDI, VMware und NAS. Jedes dieser Silos verfügt über unterschiedliche Hardware und unterschiedliche Management Tools, die typischerweise von unterschiedlichen IT-Gruppen gemanagt werden. Dieser herkömmliche Ansatz trägt zu dem größten Problem im Gesundheitswesen bei, das heute besteht: Der Komplexität.</block>
  <block id="58ad32957aed88a3c14ef81e5f254bd3" category="paragraph">Mit NetApp wird das Datenmanagement einfacher und effizienter. Anstatt das Problem mit übergroßen Silos zu lösen, verwendet ONTAP Innovation und Technologie, um für jeden Workload ein konsistentes und garantiertes SLA über ein beliebiges Protokoll mit integrierter Datensicherung zu ermöglichen. Diese Funktionen und Tools eignen sich auch für die Cloud Ihrer Wahl, wie unten dargestellt.</block>
  <block id="0d9c98d3832defc892b59655abcfacfc" category="inline-image-macro">Skalierung und Einfachheit für das Gesundheitswesen mit ONTAP</block>
  <block id="4277ed476100dc05b0a4e0e837c3d9b9" category="paragraph"><block ref="4277ed476100dc05b0a4e0e837c3d9b9" category="inline-image-macro-rx" type="image"></block></block>
  <block id="63cc4e5664cd9022b0fc1580682f9995" category="summary">Epic auf die Performance von ONTAP</block>
  <block id="6762c0b102338c245945f7d09da83fa4" category="paragraph">ONTAP führte 2009 Flash-Technologien ein und unterstützt SSDs seit 2010. Aufgrund dieser langen Erfahrung mit Flash-Storage kann NetApp die ONTAP Funktionen anpassen, um die SSD-Performance zu optimieren und die Beständigkeit von Flash-Medien zu verbessern, während die umfassenden Funktionen von ONTAP weiterhin genutzt werden.</block>
  <block id="8e0c14b7c96a8748c07c3916510ebd8b" category="paragraph">Seit dem Jahr 2020 müssen alle Epic ODB Workloads auf All-Flash-Storage laufen. Epic Workloads arbeiten in der Regel mit ca. 1,000 bis 2,000 IOPS pro Terabyte Storage (8-KB-Block, Lese- und Schreibverhältnis von 75 %/25 % und 100 % zufällige Zugriffe). Epic ist sehr latenzempfindlich, und eine hohe Latenz wirkt sich sichtbar auf die Benutzerfreundlichkeit und betriebliche Aufgaben aus, wie das Ausführen von Berichten, Backups, Integritätsprüfungen und die Dauer von Umgebungsaktualisierungen.</block>
  <block id="4a9928f04affb2a4fe321fb9e2b003fd" category="list-text">Der einschränkende Faktor für All-Flash-Arrays sind nicht die Laufwerke, sondern die Auslastung der Controller.</block>
  <block id="11a9dfbe9a015ca567ecd5f77959dc1f" category="list-text">ONTAP nutzt eine aktiv/aktiv-Architektur. Aus Performance-Gründen schreiben beide Nodes im HA-Paar auf die Laufwerke.</block>
  <block id="3a5c5db6ac361794af4c29a626c584f5" category="list-text">Dies führt zu einer maximierten CPU-Auslastung. Dies ist der wichtigste Faktor, der es NetApp ermöglicht, die beste Epic-Performance der Branche zu veröffentlichen.</block>
  <block id="2f6b137b593d6a732992bc619827ee36" category="list-text">Die Technologien NetApp RAID DP, Advanced Disk Partitioning (ADP) und WAFL erfüllen alle Epic-Anforderungen. Alle Workloads verteilen die I/O-Vorgänge über alle Festplatten. Ohne Engpässe.</block>
  <block id="0ce3d3de991180e05a5a0818efc312ef" category="list-text">ONTAP ist schreiboptimiert. Schreibvorgänge werden nach dem Schreiben auf gespiegelte NVRAM bestätigt, bevor sie mit Inline-Speichergeschwindigkeit auf Festplatte geschrieben werden.</block>
  <block id="ca65639a4e206f319202179e1d9a4ff4" category="list-text">Mit WAFL, NVRAM und der modularen Architektur kann NetApp Software einsetzen, um Innovationen durch Inline-Effizienz, Verschlüsselung und Performance umzusetzen. Außerdem können NetApp neue Funktionen und Funktionen ohne Auswirkungen auf die Performance einführen.</block>
  <block id="f13349f1404495d87711104e964b1cef" category="list-text">In der Vergangenheit verzeichnet jede neue Version von ONTAP eine Performance- und Effizienzsteigerung im Bereich von 30 bis 50 %. Die Performance ist optimal, wenn Sie bei ONTAP auf dem neuesten Stand bleiben.</block>
  <block id="70d9e3c68d42d6ec42ab792e06c44357" category="section-title">NVMe</block>
  <block id="42fa4afed0f941a60ea00e82671c421d" category="paragraph">Wenn die Performance oberste Priorität hat, unterstützt NetApp auch NVMe/FC, das FC-SAN-Protokoll der nächsten Generation.</block>
  <block id="1e4d4890d3f90b04841095b53183f782" category="paragraph">Wie in der Abbildung unten zu sehen ist, erreichten unsere Genio Tests unter Verwendung des NVMe/FC-Protokolls im Vergleich zum FC-Protokoll eine wesentlich höhere Anzahl an IOPS. Die NVMe/FC-vernetzte Lösung erreichte über 700.000 IOPS, bevor sie den Schreibzyklus-Schwellenwert von 45 Sekunden erreicht. Durch den Austausch von SCSI-Befehlen durch NVMe wird die Auslastung des Hosts deutlich verringert.</block>
  <block id="0b336e7e203efd55d17ecfaa03c134f5" category="inline-image-macro">Epic Genio Diagramm</block>
  <block id="293e47073bb00e514b986ef28fbbb2e1" category="paragraph"><block ref="293e47073bb00e514b986ef28fbbb2e1" category="inline-image-macro-rx" type="image"></block></block>
  <block id="9ff8735576a58daba5ba20974f92a584" category="summary">Epic auf ONTAP Skalierbarkeit</block>
  <block id="1027782bab261904e6adf43f2b137951" category="paragraph">Der Epic Hardware Configuration Guide verzeichnet ~in 3 Jahren ein Wachstum von 20 % pro Jahr. Umgebungen können jedoch auch unerwartet wachsen.</block>
  <block id="39e52e339ff39b5620d48036812e09ec" category="paragraph">NetApp ermöglicht die nahtlose Skalierung von Performance und Kapazität auf bis zu 12 Nodes für NAS-, SAN- und Objekt-Cluster. Infolgedessen können Sie unterbrechungsfrei vertikal und horizontal skalieren und Ihr Unternehmen wächst.</block>
  <block id="49c5cdfe08202e574deefaa3d26c332a" category="inline-link-macro">NetApp Verifizierte Architektur – Erfolgsgeschichte auf modernem SAN</block>
  <block id="12cb4beeba752f647712c749e3076fbf" category="paragraph">Epic Iris bietet zusätzliche Skalierungsmöglichkeiten. Mit dieser Lösung können größere Kunden mit mehreren Epic-Instanzen in einer einzigen Instanz konsolidiert werden. Das <block ref="5d13d58e489e8fe43bb727515c1d9ff2" category="inline-link-macro-rx"></block> Dokument zeigt, dass Epic konsolidierte Workloads nahtlos auf 720.000 IOPS in einer einzelnen HA skalieren und horizontal auf über 4 Mio. IOPS in einem Cluster skalieren kann. Ein unterbrechungsfreies Scale-up ist möglich, indem Controller-Upgrades durchgeführt oder Festplatten zu vorhandenen Clustern hinzugefügt werden.</block>
  <block id="196097993185ddccf60443ac3f86e89f" category="paragraph">NAS-, SAN- und Objektdaten können zudem unterbrechungsfrei zwischen Nodes im Cluster verschoben werden. Jedes HA-Paar im Cluster kann eine beliebige Kombination aus Systemtypen und Größen von ONTAP FAS und AFF sein. Sie können Ihre Workloads über ein einzelnes Cluster verteilen, um Ihre Storage-Investition zu maximieren.</block>
  <block id="6abd4957b4615c500bd2a32425316211" category="paragraph">ONTAP bietet zudem die Option, Objektspeicher auf StorageGRID oder der Cloud als Backup-Ziel und/oder automatisches Tiering-Ziel für Cold Storage zu verwenden. Diese Funktion ermöglicht es Ihnen, teure All-Flash-Festplatten, Tiering von Snapshots und kalte Daten automatisch auf Objekt zu verlagern.</block>
  <block id="8d3f76f475a5e81f188f700429c71a9e" category="paragraph">So läuft Epic mit dem NetApp Produktportfolio einfach besser und nutzt ONTAP, diverse Protokolle, StorageGRID und die Cloud Ihrer Wahl. Diese Produkte bieten Optionen für Disaster Recovery, Archivierung, Analyse, Tiering und vieles mehr.</block>
  <block id="1e3ccee88df9552e4751ba4ad173de96" category="summary">Epic auf ONTAP Sicherheit</block>
  <block id="32e1bf49ff775c9283c882fe7f3ec873" category="paragraph">Sicherheit ist heute die wichtigste Herausforderung für Organisationen und Führungskräfte im Gesundheitswesen. Das Management der IT war noch nie so schwierig wie heute und Unternehmen stehen vor der Herausforderung, Compliance, Daten-Governance, Virenschutz und Ransomware zu managen.</block>
  <block id="0d6ce08690a300cb8ce00d77e1c31a5e" category="inline-link-macro">Security Hardening Guide for ONTAP</block>
  <block id="40e97866feb53c48881b34e5f97ca61f" category="paragraph">Ein vollständiger Leitfaden zu Epic und Storage Security geht über den Umfang dieses Dokuments hinaus. Er enthält jedoch alle umfangreichen und erweiterten Sicherheitsfunktionen, die ONTAP bietet.<block ref="471b0f2cfbad0f2ec4e54df06500d580" category="inline-link-macro-rx"></block></block>
  <block id="1408f921f799efe0e585aeba5fa9ed58" category="inline-link-macro">TR-4569</block>
  <block id="f87d3f3c7c3e6e9ab1153c53d78b40b1" category="paragraph">NetApp Active IQ Unified Manager überwacht anhand der in enthaltenen Informationen auf Sicherheitsverletzungen <block ref="1b2ca6a48e0bf52cc6351680c5ee6170" category="inline-link-macro-rx"></block>und meldet diese im Dashboard, um das Sicherheitsmanagement zu vereinfachen. Diese Tools können Ihr Unternehmen dabei unterstützen, Ihre Sicherheitsziele zum Schutz, zur Erkennung und zur Abwehr von Angriffen zu erfüllen.</block>
  <block id="a1fbc1f9fe3c77416c96f491ba933c4a" category="inline-link-macro">NetApp FPolicy</block>
  <block id="96f76ddccc0adcaf0fc9a2fc6c834374" category="inline-link-macro">Multi-Faktor-Authentifizierung (MFA)</block>
  <block id="a754af2f708573321f76c2d36bea9f02" category="paragraph">NetApp arbeitet mit Anbietern von Sicherheitslösungen zusammen, um Ihr Sicherheitsangebot durch Integration von <block ref="48fae993b7aeb9944f39dadc4618e2b6" category="inline-link-macro-rx"></block> Software zu erweitern. Darüber hinaus <block ref="b5ae90d3a2df9b77f2db834fe0dc7b7d" category="inline-link-macro-rx"></block>kann hinzugefügt werden, um Ihre Epic-Umgebung vor unberechtigtem Zugriff mit durchgesickerten Anmeldeinformationen zu schützen.</block>
  <block id="91d998bab304adb0fd27858a4295dcac" category="inline-link-macro">Cyber-Vault: ONTAP</block>
  <block id="e9c1c7d4ca860adb553d87c2c1d9ef35" category="inline-link-macro">NetApp Lösung gegen Ransomware</block>
  <block id="63ab8aaa1f35361bdf5c84b3d1dd4765" category="inline-link-macro">NetApp und Zero Trust</block>
  <block id="a67fe844bc4d3e5cf5ebac11dea00b01" category="paragraph">Schließlich <block ref="79305a8b61b53acc892506e6bcd6f49f" category="inline-link-macro-rx"></block>bieten native ONTAP Snapshot Kopien und unveränderliche SnapLock Technologien mit , eine einzigartige Air Gap-Funktion zum Schutz Ihrer Patientendaten vor Ransomware. Siehe NetApp-Dokumentation auf <block ref="910d0617bba4ff1a9d41b4b330096d85" category="inline-link-macro-rx"></block>. Für einen strategischeren Sicherheitsansatz siehe <block ref="aaf21a490837c9b392e59c6c9110165d" category="inline-link-macro-rx"></block>.</block>
  <block id="28caba838e7a7ff7ff218bae8235208a" category="summary">Epic Snapshots und Klonen</block>
  <block id="8ad0c394ef79d2b18983fabe84f658fc" category="paragraph">Ein Snapshot ist eine Point-in-Time-Kopie eines Volumes, die schreibgeschützt ist.</block>
  <block id="568043eed0b44992a4b4a1cc21f8b633" category="paragraph">Ein Snapshot setzt eine logische Sperre auf alle Blöcke des aktiven File-Systems. NetApp ONTAP Snapshot Kopien sind nahezu sofort erstellt und verwenden keinen zusätzlichen Storage.</block>
  <block id="04a00b93a2db41946dc3d263cf6c294c" category="paragraph">Write Anwhere File Layout, kurz WAFL, ist ein schreibgeschütztes Dateisystem. Es führt keine zusätzlichen I/O-Vorgänge durch, wie das Kopieren der Daten in einen Snapshot geschützten Block vor dem Überschreiben. Es werden keine Daten verschoben. Snapshots wirken sich daher nicht auf die Storage-Kapazität oder Performance aus. Snapshots sorgen für enorme Storage-Einsparungen und erweitern die Backup-Lösung.</block>
  <block id="a701f5d044348085ee4f97fef6636845" category="section-title">FlexClone</block>
  <block id="16f57611d3d44b74ce1c1b134cb8e798" category="paragraph">Ein NetApp ONTAP FlexClone Volume ist ein Klon eines vorhandenen Volumes oder ein Snapshot eines vorhandenen Volumes. Sie dient ansonsten wie jedes andere ONTAP Volume und kann selbst geklont, durch Snapshots geschützt und mit QoS-Richtlinien konfiguriert werden.</block>
  <block id="e0c3236990160654182a79c89baab29f" category="paragraph">Wie bei Snapshots benötigt ein FlexClone Volume zum Zeitpunkt der Erstellung keinen zusätzlichen Speicherplatz. Nur Änderungen am Klon erfordern zusätzliche Kapazität.</block>
  <block id="9a9ff90c24c36543bf54b1f2136fd9e0" category="paragraph">Epic benötigt 10 bis 30 Kopien der Produktionsdatenbanken für verschiedene betriebliche Anforderungen wie Streaming-Backups, Integritätsprüfungen und Staging-Upgrade-Umgebungen. Mit der Umstellung auf häufigere Upgrades ist die Notwendigkeit einer Lösung auf Basis von FlexClone Volumes gestiegen.</block>
  <block id="dad816f366caa0b8cae67daaeb85deb0" category="admonition">NetApp bietet als Teil der Lösung mit Ansible und nativen NetApp-Tools eine vollständig automatisierte Epic-Backup-Lösung und eine Epic-Aktualisierungslösung.</block>
  <block id="53f102b306ab1aba43e9e20701dc32f2" category="doc">Epic auf ONTAP</block>
  <block id="3bfc8ddd725eed5b674d1a480bc650d2" category="paragraph">Der Schlüssel zur digitalen Transformation liegt darin, mehr aus Ihren Daten zu machen.</block>
  <block id="2b82426bc0bfa4ea3efb779d165629a4" category="admonition">Diese Dokumentation ersetzt die zuvor veröffentlichten technischen Berichte _TR-3923: NetApp Best Practices for Epic_.</block>
  <block id="fdf0f3f3a2bae0d66ca5ecb45e1f3d85" category="paragraph">Krankenhäuser benötigen große Datenmengen, um die digitale Transformation zu starten. Ein Teil des Prozesses der Behandlung von Patienten, der Verwaltung von Personalplänen und medizinischen Ressourcen ist, dass Informationen gesammelt und verarbeitet werden. Viele Aktionen werden jedoch immer noch manuell oder über veraltete Systeme ausgeführt. Eine Konstante besteht darin, dass die Datenmenge exponentiell wächst und daher immer schwieriger zu managen ist.</block>
  <block id="74090662bfdb94e49dfc286a6946cc5c" category="paragraph">Die größte Ursache dieses Problems besteht darin, dass Krankenhausdaten häufig in Datensilos gespeichert werden. Zu viel Zeit wird für manuelle Eingaben und Updates aufgewendet, die zu Burnout und Fehlern führen. Dieses Dokument betrifft einen Teil der Gesundheitsdaten, Epic Electronic Health Records (EHR). Die hier abgedeckte Datenmanagementstrategie kann und sollte jedoch auf alle Gesundheitsdaten angewendet werden. NetApp hat sich in der Modernisierung und Vereinfachung digitaler Infrastrukturen bewährt. Unsere intelligente Dateninfrastruktur bildet die Grundlage für die digitale Transformation.</block>
  <block id="b68a1112138fe0c0c1beafc2aea18949" category="paragraph">NetApp bietet eine zentrale Datenmanagement-Lösung für alle Anforderungen im Gesundheitswesen und begleitet Krankenhäuser durch ihren Weg in die digitale Transformation. Das Gesundheitswesen baut eine Grundlage mit Struktur und intelligenten Lösungen auf und kann den vollen Wert dieser wertvollen Informationen ausschöpfen. Dieses Rahmenwerk kann Medizinern helfen, Krankheiten schneller zu diagnostizieren und individuelle Behandlungspläne zu entwickeln, um Entscheidungsprozesse in Notfallsituationen besser zu unterstützen. Sie können außerdem Ihre eigene intelligente Dateninfrastruktur aufbauen und Ihr Krankenhaus in die Lage versetzen, Datensilos zu entsperren, die Interoperabilität der Daten zu erleichtern und vertrauliche Patientendaten zu schützen.</block>
  <block id="6b5a360f7d15046cfd935db963e3da66" category="paragraph">Verwenden Sie dieses Dokument als Leitfaden für die erfolgreiche Erstellung und Implementierung von Epic EHR. Anstatt mehrere Epic-Silos zu erstellen, bauen Sie eine einzelne Epic-Dateninfrastruktur auf und verändern Sie Ihr Krankenhaus.</block>
  <block id="261addf78c7b2c961032b3dd08ba0b1f" category="section-title">Zweck</block>
  <block id="b8954f46b76dae4370058c7bd9ae0c97" category="paragraph">Dieses Dokument beschreibt Best Practices für die Integration von NetApp Storage in eine Epic-Softwareumgebung. Es enthält die folgenden Abschnitte:</block>
  <block id="6c5022fa1197168f37c42e56f80d36aa" category="list-text">Technisches Verständnis der Epic-Softwareumgebung und ihrer Storage-Anforderungen in verschiedenen Konfigurationen.</block>
  <block id="a9da176a5abe5a618e8ed6a88bdbf21b" category="list-text">In epischen Storage-Überlegungen werden die wichtigsten Entscheidungsfaktoren für Epic-Lösungen beschrieben.</block>
  <block id="8b3f0902644adbb7e0435ae9c9a284e2" category="list-text">NetApp Storage-Empfehlungen zur Beschreibung von Best Practices für die NetApp Storage-Konfiguration zur Erfüllung der Storage-Anforderungen von Epic</block>
  <block id="5d113f2038d289f391614c39043629e8" category="section-title">Umfang</block>
  <block id="217eb7cf1ad7503a42f61ad3cffb47fb" category="paragraph">Dieses Dokument behandelt nicht die folgenden Themen:</block>
  <block id="f900b8ede5ddf3a0471176dab052001d" category="list-text">Quantitative Performance-Anforderungen und Hinweise zur Dimensionierung, die in behandelt werden<block ref="81c36a9e6d224e9d42c9630dffaa9434" category="inline-link-rx"></block> (NetApp Anmeldung erforderlich)</block>
  <block id="ef1e6887c579ba81dcedc3f4e3793cd2" category="section-title">Zielgruppe</block>
  <block id="2f215f5ba67ae7978ce4846a4a1cdc08" category="paragraph">NetApp geht davon aus, dass der Leser über folgende Hintergrundwissen verfügt:</block>
  <block id="6cfde17644914efa6d0a4d0c21a8f9db" category="list-text">Ein solides Verständnis der SAN- und NAS-Konzepte</block>
  <block id="c86456e4299d4b0ad43e030d0fdd0af5" category="list-text">Technische Vertrautheit mit ONTAP Storage-Systemen</block>
  <block id="59eba43c04b655e21f087af920398a2a" category="list-text">Technische Vertrautheit mit der Konfiguration und Administration von ONTAP</block>
  <block id="b7c45da218d2e29ec939a31cdd81f2be" category="paragraph">Ein NetApp Storage Controller mit ONTAP kann folgende Workloads in einer Windows Server-Umgebung unterstützen:</block>
  <block id="ba0a81fc95587bfae3e0ac0bf44e562a" category="paragraph">Das NetApp PowerShell Toolkit (PSTK) ist ein PowerShell Modul, das eine End-to-End-Automatisierung bietet und die Storage-Administration von ONTAP ermöglicht. Das ONTAP Modul enthält über 2,000 Cmdlets und unterstützt Sie bei der Administration von FAS, NetApp All Flash FAS (AFF), Standard-Hardware und Cloud-Ressourcen.</block>
  <block id="f43e8d35e63b48bc73d5dc074df829ae" category="paragraph">Microsoft ODX, auch als Copy Offload bezeichnet, ermöglicht direkte Datentransfers innerhalb eines Speichergeräts oder zwischen kompatiblen Speichergeräten, ohne die Daten über den Hostcomputer zu übertragen. ONTAP unterstützt die ODX Funktion für CIFS- und SAN-Protokolle. ODX kann potenziell die Performance verbessern, wenn Kopien sich innerhalb desselben Volumes befinden, senkt die Auslastung von CPU und Arbeitsspeicher im Client und reduziert die Auslastung der Netzwerk-I/O-Bandbreite.</block>
  <block id="e033e4d03ff089011c11d7d2fa80d475" category="paragraph">Hyper-V Cluster-Server (sogenannte Nodes) werden über das physische Netzwerk und über Cluster-Software verbunden. Diese Nodes verwenden Shared Storage zur Speicherung der VM-Dateien, darunter Konfiguration, VHD-Dateien (virtuelle Festplatte) und Snapshot-Kopien. Beim gemeinsam genutzten Storage kann es sich um eine NetApp SMB/CIFS-Freigabe oder einen CSV auf einer NetApp LUN handeln, wie unten gezeigt. Dieser Shared-Storage bietet einen konsistenten und verteilten Namespace, auf den alle Nodes im Cluster gleichzeitig zugreifen können. Wenn daher ein Node im Cluster ausfällt, stellt der andere Node Services für den Prozess Failover bereit. Failover-Cluster können mithilfe des Failover Cluster Manager Snap-ins und der Windows PowerShell Cmdlets für Failover-Clustering gemanagt werden.</block>
  <block id="72960f8df48c0debc035454921c1b5d3" category="list-text">Bei der gemeinsam genutzten Live-Migration wird die VM auf einer SMB-Freigabe gespeichert. Wenn Sie eine VM live migrieren, bleibt der VM-Storage auf der zentralen SMB Share, sodass der andere Node sofort darauf zugreifen kann, wie unten dargestellt.</block>
  <block id="5a002cf26db634ca1b1fff75461d2427" category="list-title">ONTAP läuft auf NetApp Storage Controllern. Es ist in mehreren Formaten erhältlich.</block>
  <block id="6011d6a859985e0b6d7c13b0907896ca" category="paragraph">ONTAP bietet hochverfügbaren Storage, bei dem mehrere Pfade vom Storage Controller zum Windows Server existieren können. Multipathing ist die Fähigkeit, mehrere Datenpfade von einem Server zu einem Storage-Array zu haben. Multipathing schützt vor Hardware-Ausfällen (Kabelschnitte, Switch- und Host Bus Adapter- [HBA]-Ausfall usw.) und kann durch die Verwendung der aggregierten Performance mehrerer Verbindungen höhere Performance-Grenzwerte bieten. Wenn ein Pfad oder eine Verbindung nicht mehr verfügbar ist, verschiebt die Multipathing-Software die Last automatisch auf einen der anderen verfügbaren Pfade. Die MPIO-Funktion kombiniert mehrere physische Pfade zum Storage als einen einzigen logischen Pfad, der für den Datenzugriff verwendet wird, um Storage Resiliency und Load Balancing zu ermöglichen. Um diese Funktion verwenden zu können, muss die MPIO-Funktion auf Windows Server aktiviert sein.</block>
  <block id="8bbfb9edfb90853f3f9439d77ce59441" category="list-text">CLI-Befehle auf ONTAP</block>
  <block id="e72f4a4f5fcfab964176e743af1f3196" category="paragraph">Das NAS-Protokoll CIFS ist nativ in ONTAP integriert. Aus diesem Grund benötigt Windows Server keine zusätzliche Client-Software für den Zugriff auf die Daten auf ONTAP. Ein NetApp Storage Controller wird im Netzwerk als nativer File Server angezeigt und unterstützt die Microsoft Active Directory-Authentifizierung.</block>
  <block id="ab2d1dd1b024fd7be648c22c4a43997b" category="list-text">Bestimmte CIFS-Managementaufgaben können mit Microsoft Management Console (MMC) ausgeführt werden. Bevor Sie diese Aufgaben ausführen, müssen Sie die MMC mithilfe der MMC-Menübefehle mit dem ONTAP-Speicher verbinden.</block>
  <block id="1d5ba6202d85466a95188b58c37dccc8" category="list-text">NetApp empfiehlt, Hyper-V Hosts und ONTAP Storage mit einem 10-GB-Netzwerk zu verbinden, sofern vorhanden. Bei einer 1-GB-Netzwerkverbindung empfiehlt NetApp die Erstellung einer Schnittstellengruppe, die aus mehreren 1-GB-Ports besteht.</block>
  <block id="79d467fa842b9015f2f12ada4fa1c7fc" category="paragraph">Die SQL Server-Performance weist mehrere Abhängigkeiten von der CPU- und Kernkonfiguration auf.</block>
  <block id="dcfe06c0ddbb3b395286872a95182eb3" category="paragraph">Hyperthreading bezieht sich entweder auf die Implementierung von simultanem Multithreading (SMT), was die Parallelisierung von Berechnungen auf x86-Prozessoren verbessert. SMT ist sowohl für Intel als auch für AMD Prozessoren verfügbar.</block>
  <block id="2e84c18998d7c7a831a62718e10a98c5" category="paragraph">Hyperthreading führt zu logischen CPUs, die für das Betriebssystem als physische CPUs angezeigt werden. SQL Server erkennt dann die zusätzlichen CPUs und nutzt sie so, als gäbe es mehr Kerne als physisch vorhanden. Durch eine stärkere Parallelisierung kann die Performance erheblich gesteigert werden.</block>
  <block id="4403546c5938500562a9d8677deb1c38" category="section-title">Kerne und Lizenzierung</block>
  <block id="c325fde2d51a1ea2ddd8c9dd51cfdf9c" category="paragraph">Das Bild unten zeigt die SQL Server-Protokollmeldung nach dem Start, die die Durchsetzung des Core-Limits anzeigt.</block>
  <block id="e59dc8056170cddb696ba9bff812c325" category="paragraph">SQL Server verwendet alle CPUs, die über das Betriebssystem verfügbar sind (wenn die Prozessorkern-Lizenz gewählt wird). Es erstellt auch Scheduler f0r jede CPU, um die Ressourcen für jeden beliebigen Workload optimal zu nutzen. Beim Multitasking kann das Betriebssystem oder andere Anwendungen auf dem Server die Prozess-Threads von einem Prozessor zum anderen wechseln. SQL Server ist eine ressourcenintensive Applikation und die Performance kann in diesem Fall beeinträchtigt werden. Um die Auswirkungen zu minimieren, können Sie die Prozessoren so konfigurieren, dass die gesamte SQL Server-Last an eine vorgewählte Prozessorgruppe weitergeleitet wird. Dies wird durch die CPU Affinitätsmaske erreicht.</block>
  <block id="e40a45aed8142e74026f5aa7a6ee1122" category="paragraph">Die Affinity I/O-Maskenoption bindet SQL Server-Festplatten-I/O an eine Teilmenge von CPUs. In SQL Server-OLTP-Umgebungen kann diese Erweiterung die Performance von SQL Server-Threads, die I/O-Vorgänge ausgeben, erheblich verbessern.</block>
  <block id="a58930419c1b1a276791665c876f8f80" category="paragraph">Dies ist zwar hilfreich bei umfangreichen Abfragen, kann jedoch zu Leistungsproblemen und zur Begrenzung der Parallelität führen. Ein besserer Ansatz besteht darin, die Parallelität auf die Anzahl der physischen Kerne in einem einzelnen CPU-Socket zu beschränken. Auf einem Server mit zwei physischen CPU-Sockeln mit 12 Kernen pro Socket sollte, unabhängig von Hyperthreading,<block ref="1abedc5f6ab369ab9fcd16d6b7990638" prefix=" " category="inline-code"></block> auf 12 gesetzt werden.<block ref="1abedc5f6ab369ab9fcd16d6b7990638" prefix=" " category="inline-code"></block> Die zu verwendende CPU kann nicht eingeschränkt oder diktiert werden. Stattdessen beschränkt es die Anzahl der CPUs, die von einer einzelnen Batch-Abfrage verwendet werden können.</block>
  <block id="16dce493fdad96fe48148f47dc552bfb" category="admonition">*NetApp empfiehlt* für DSS wie Data Warehouses, beginnen Sie mit<block ref="1abedc5f6ab369ab9fcd16d6b7990638" prefix=" " category="inline-code"></block> 50 und erkunden Sie Tuning auf oder ab, falls erforderlich. Stellen Sie sicher, dass Sie die kritischen Abfragen in Ihrer Anwendung messen, wenn Sie Änderungen vornehmen.</block>
  <block id="0add8d6e7be42a854cd7e0161c7f9bcc" category="paragraph">Normalerweise wird für jede Abfrage ein separater Betriebssystem-Thread erstellt. Wenn Hunderte von gleichzeitigen Verbindungen zu SQL Server hergestellt werden, kann eine Konfiguration mit einem Thread pro Abfrage übermäßige Systemressourcen beanspruchen. Die<block ref="a9233738c364090bbc418d8a0e08a79e" prefix=" " category="inline-code"></block> Option verbessert die Leistung, indem SQL Server in die Lage versetzt wird, einen Pool von Worker-Threads zu erstellen, die gemeinsam eine größere Anzahl von Abfrage-Anforderungen verarbeiten können.</block>
  <block id="29daf9697a41a463dbe76855a2053a87" category="paragraph">Das folgende Beispiel zeigt, wie Sie die Option Max. Work Threads mit T-SQL konfigurieren.</block>
  <block id="7039e99a0fdcc0d3ef80074ed42bf4e4" category="paragraph">Strategien für Datenbank-Backups sollten auf den ermittelte geschäftliche Anforderungen basieren, nicht auf theoretischen Möglichkeiten. Durch die Kombination der Snapshot Technologie von ONTAP und der Nutzung der Microsoft SQL Server APIs können Sie schnell applikationskonsistente Backups unabhängig von der Größe der Benutzerdatenbanken erstellen. Für erweiterte oder horizontal skalierbare Datenmanagement-Anforderungen bietet NetApp SnapCenter.</block>
  <block id="4cd6720df070a1784bae9c6f525f483d" category="admonition">*NetApp empfiehlt* SnapCenter zum Erstellen von Snapshot Kopien zu verwenden. Die im Folgenden beschriebene T-SQL-Methode funktioniert ebenfalls, SnapCenter bietet jedoch eine vollständige Automatisierung für Backup-, Restore- und Klonprozesse. Außerdem wird eine Erkennung durchgeführt, um sicherzustellen, dass die richtigen Snapshots erstellt werden. Es ist keine Vorkonfiguration erforderlich.</block>
  <block id="97e73fdb42ddecb63ed519a484ff3c9b" category="paragraph">SQL Server erfordert außerdem eine Koordination zwischen OS und Storage, um sicherzustellen, dass bei der Erstellung die korrekten Daten in Snapshots vorhanden sind. In den meisten Fällen ist die einzige sichere Methode, dies mit SnapCenter oder T-SQL zu tun. Ohne diese zusätzliche Koordination erstellte Snapshots sind unter Umständen nicht zuverlässig wiederherstellbar.</block>
  <block id="9d831cc4c73b8a2915ce6d8928d55915" category="paragraph">Die SQL Server Funktion zur Replizierung von Always-on-Verfügbarkeitsgruppen kann sich als hervorragende Option anbieten. NetApp bietet Optionen zur Integration von Datensicherung mit Always-on. In einigen Fällen empfiehlt es sich jedoch, die ONTAP Replizierungstechnologie in Betracht zu ziehen. Es gibt drei grundlegende Optionen.</block>
  <block id="dd432f8769e4d3db8acf8bf9df94680a" category="paragraph">Die SnapMirror-Technologie bietet eine schnelle und flexible Unternehmenslösung zur Replizierung von Daten über LANs und WANs. Die SnapMirror Technologie überträgt nach Erstellung der ersten Spiegelung nur geänderte Datenblöcke an das Zielsystem, wodurch die Anforderungen an die Netzwerkbandbreite erheblich gesenkt werden. Sie kann im synchronen oder asynchronen Modus konfiguriert werden.</block>
  <block id="fce5f84d02dbd5c9e3cfedf18e809644" category="summary">Microsoft SQL Server und NetApp MetroCluster</block>
  <block id="41900c1c2eb669e401e4abee9091b40b" category="paragraph">Für die Microsoft SQL Server-Bereitstellung mit einer MetroCluster-Umgebung ist eine Erklärung des physischen Designs eines MetroCluster-Systems erforderlich.</block>
  <block id="60359402d81f6673550c3df489efb034" category="paragraph">MetroCluster spiegelt Daten und Konfiguration synchron zwischen zwei ONTAP Clustern in separaten Speicherorten oder Ausfall-Domains. MetroCluster bietet kontinuierlich verfügbaren Storage für Applikationen, indem zwei Ziele automatisch gemanagt werden:</block>
  <block id="abf0aedf685f58c3cf3abe3e41ca549c" category="list-text">Recovery Point Objective (RPO) von null durch synchrones Spiegeln von Daten, die auf das Cluster geschrieben werden.</block>
  <block id="1194ac032fc54dbcf779ac23fbc73dd5" category="list-text">Recovery Time Objective (RTO) von nahezu null durch Spiegelung der Konfiguration und Automatisierung des Datenzugriffs am zweiten Standort</block>
  <block id="9aa45379205d52916f2958359e92c1c8" category="paragraph">MetroCluster sorgt mit automatischer Datenspiegelung und Konfiguration zwischen den beiden unabhängigen Clustern an den beiden Standorten für Einfachheit. Wenn Storage innerhalb eines Clusters bereitgestellt wird, wird dieser automatisch auf das zweite Cluster am zweiten Standort gespiegelt. NetApp SyncMirror® bietet eine vollständige Kopie aller Daten mit einem RPO von null. Das bedeutet, dass Workloads von einem Standort jederzeit auf den gegenüberliegenden Standort umgeschaltet werden können und weiterhin Daten ohne Datenverlust bereitstellen können. MetroCluster managt die Umschaltung von Zugriff auf NAS- und SAN-bereitgestellte Daten am zweiten Standort. Das Design von MetroCluster als validierte Lösung umfasst die Größenanpassung und Konfiguration, die eine Umschaltung innerhalb der Protokollzeitlimits oder früher (in der Regel weniger als 120 Sekunden) ermöglicht. Die RPO ergibt sich so gut wie kein Wert, und Applikationen können ohne Ausfälle auf ihre Daten zugreifen.MetroCluster ist in verschiedenen Variationen über das Back-End Storage Fabric verfügbar.</block>
  <block id="47985b72c36de8197de21d06517f1a03" category="paragraph">Die Sicherung für SQL Server mit MetroCluster basiert auf SyncMirror, die eine synchrone Spiegelungstechnologie zur maximalen Performance-Skalierung bietet.</block>
  <block id="96a36329b3e96b1619b1d635a806bb3f" category="summary">Microsoft SQL Server mit NetApp MetroCluster</block>
  <block id="9fcf1f7a4c16b536048dc5875b61d4b1" category="paragraph">Eine Option für den Schutz von SQL Server Datenbanken mit einem RPO von null ist MetroCluster. MetroCluster ist eine einfache, hochperformante RPO=0-Replizierungstechnologie, mit der Sie eine gesamte Infrastruktur mühelos über mehrere Standorte hinweg replizieren können.</block>
  <block id="8a7f9ca86b522c62318d9b1eb5b51625" category="paragraph">SQL Server kann auf einem einzigen MetroCluster System bis zu Tausende von Datenbanken skalieren. Es könnten eigenständige SQL Server-Instanzen oder Failover-Cluster-Instanzen geben. Das MetroCluster System führt nicht unbedingt zu oder ändert keine Best Practices für das Management einer Datenbank.</block>
  <block id="43135677cc98d0d6872d9ec24bef82e4" category="paragraph">Eine vollständige Erklärung von MetroCluster geht über den Umfang dieses Dokuments hinaus, aber die Prinzipien sind einfach. MetroCluster kann eine Replizierungslösung RPO=0 mit schnellem Failover bereitstellen. Was Sie auf dieser Grundlage aufbauen, hängt von Ihren Anforderungen ab.</block>
  <block id="046797023ebbfe8b6dd24d4fc7cab0be" category="paragraph">Beispielsweise könnte ein grundlegendes schnelles DR-Verfahren nach einem plötzlichen Standortausfall folgende grundlegende Schritte durchführen:</block>
  <block id="f8b79e7c0c5914411ccfda1cbaa25414" category="list-text">Erzwingen einer MetroCluster-Umschaltung</block>
  <block id="740d5da1ae0060ef63f290d3aed64929" category="list-text">Durchführen der Erkennung von FC/iSCSI-LUNs (nur SAN)</block>
  <block id="89c617c4ed9cac275e572a67b1c52b29" category="list-text">Mounten Sie File-Systeme</block>
  <block id="f08329445e69fa49a8223bf365f09e47" category="list-text">Starten Sie SQL Services</block>
  <block id="b7aabeea5e06b7371d4372fd7f698f88" category="paragraph">Die primäre Anforderung dieses Ansatzes ist ein Betriebssystem, das am Remote Standort ausgeführt wird. Es muss mit SQL Server-Setup vorkonfiguriert sein und sollte mit einer gleichwertigen Build-Version aktualisiert werden. SQL Server-Systemdatenbanken können auch am Remote-Standort gespiegelt und gemountet werden, wenn ein Notfall deklariert wird.</block>
  <block id="a61003199d91214045b9d3344f527cbd" category="paragraph">Wenn die Volumes, Dateisysteme und der Datastore, die virtualisierte Datenbanken hosten, vor dem Switchover nicht am Disaster-Recovery-Standort verwendet werden, müssen sie nicht auf zugehörigen Volumes festgelegt<block ref="3b93661df04b698b1340ff2da3238d16" prefix=" " category="inline-code"></block> werden.</block>
  <block id="2b963bb5f627aa0ffda026b4d97b6474" category="summary">Microsoft SQL Server- und SM-AS-Failover-Szenarien</block>
  <block id="bdd8b094b65ec8544e0ad9724a1984ad" category="doc">Microsoft SQL Server und SnapMirror Active Sync Failover</block>
  <block id="b0bb805a1ea68ad43ceacf65954b4b4b" category="paragraph">Die Planung einer vollständigen Applikationsarchitektur für die aktive Synchronisierung von SnapMirror erfordert ein Verständnis dafür, wie SM-AS in verschiedenen geplanten und ungeplanten Failover-Szenarien reagiert.</block>
  <block id="3fc36a3b35bb877723ed17d69c3d57fb" category="summary">Microsoft SQL Server und der ONTAP Mediator</block>
  <block id="511d40fa1b50f901027391480dff9c67" category="paragraph">Der Mediator ist für die sichere Automatisierung des Failover erforderlich. Idealerweise würde er an einem unabhängigen dritten Standort platziert werden, kann aber dennoch für die meisten Anforderungen funktionieren, wenn er mit einem der an der Replikation beteiligten Cluster kolokiert wird.</block>
  <block id="cef962a099d3ff2b834420d0c97f5bc1" category="summary">Microsoft SQL Server und SM-als uneinheitlicher Zugriff</block>
  <block id="3ef2650b3e4d89df6ea0248f432e6c2c" category="paragraph">Uneinheitliches Netzwerk durch Zugriff bedeutet, dass jeder Host nur Zugriff auf Ports im lokalen Storage-System hat. Das SAN wird nicht über Standorte (oder Ausfall-Domains am selben Standort) erweitert.</block>
  <block id="7c58b14cf1e27ff24a5b6a0656848380" category="summary">Microsoft SQL Server und SnapMirror Active Sync</block>
  <block id="a87e788e43169f3df56a7eee4e9bd79c" category="doc">SQL Server und SnapMirror Active Sync</block>
  <block id="64affc06244780af1f97a68438460989" category="paragraph">Mit SnapMirror Active Sync können einzelne SQL Server-Datenbanken und -Applikationen bei Storage- und Netzwerkstörungen den Betrieb fortsetzen. Der Storage Failover ist transparent, ohne dass manuelle Eingriffe erforderlich sind.</block>
  <block id="0a19f1ca74616d0d4972b2c02c3f7fee" category="paragraph">Ab ONTAP 9.15.1 unterstützt SnapMirror Active Sync neben der bestehenden asymmetrischen Konfiguration auch die symmetrische aktiv/aktiv-Architektur. Symmetrische aktiv/aktiv-Funktion bietet synchrone bidirektionale Replikation für Business Continuity und Disaster Recovery. Es hilft Ihnen, Ihren Datenzugriff für kritische SAN-Workloads durch gleichzeitigen Lese- und Schreibzugriff auf Daten über mehrere Ausfall-Domains hinweg zu schützen. So wird ein unterbrechungsfreier Betrieb sichergestellt und Ausfallzeiten bei Notfällen oder Systemausfällen werden minimiert.</block>
  <block id="d17dae030927ab027095c1e44df8fbf1" category="paragraph">SQL-Server-Hosts greifen über Fibre Channel(FC)- oder iSCSI-LUNs auf Speicher zu. Replizierung zwischen jedem Cluster, das eine Kopie der replizierten Daten hostet. Da es sich bei dieser Funktion um die Replizierung auf Storage-Ebene handelt, können SQL Server-Instanzen auf eigenständigen Host- oder Failover-Cluster-Instanzen Lese-/Schreibvorgänge durchführen. Informationen zu Planungs- und Konfigurationsschritten finden Sie unter <block ref="44f5d8051247437425911d96f84db248" category="inline-link-macro-rx"></block> .</block>
  <block id="3fa280b2a8840046be265109ff9f0696" category="section-title">Architektur von SnapMirror aktiv mit symmetrischer aktiv/aktiv-Lösung</block>
  <block id="a93a13dd139e6bb0629cd0ace446c595" category="paragraph"><block ref="a93a13dd139e6bb0629cd0ace446c595" category="inline-image-macro-rx" type="image"></block></block>
  <block id="b2f689709f901a51858096aaf9637eb4" category="paragraph">**Synchrone Replikation**</block>
  <block id="71ed2e619a97a27f6fe533714a412725" category="paragraph">Im normalen Betrieb ist jede Kopie jederzeit ein synchrones RPO=0-Replikat, mit einer Ausnahme. Wenn Daten nicht repliziert werden können, gibt ONTAP die Notwendigkeit zur Replizierung von Daten frei und stellt die E/A-Bereitstellung an einem Standort wieder her, während die LUNs am anderen Standort offline geschaltet werden.</block>
  <block id="331b5c7e8c01ec1e37b60de5a5efba6c" category="paragraph">**Storage Hardware**</block>
  <block id="c2048364072b2c53688aeb273047e068" category="paragraph">**ONTAP Mediator**</block>
  <block id="588068838f06fe39dec548e2210ddf2f" category="paragraph">Der ONTAP Mediator ist eine Softwareanwendung, die von der NetApp-Unterstützung heruntergeladen wird und normalerweise auf einer kleinen virtuellen Maschine bereitgestellt wird. Der ONTAP Mediator ist kein Tiebreak. Es handelt sich um einen alternativen Kommunikationskanal für die beiden Cluster, die an der aktiven synchronen SnapMirror-Replikation beteiligt sind. Der automatisierte Betrieb wird durch ONTAP basierend auf den Antworten gesteuert, die der Partner über direkte Verbindungen und den Mediator erhält.</block>
  <block id="6fda40460e9cbbefd704b9c01ea2c18c" category="summary">Microsoft SQL Server und SM-als bevorzugte Website</block>
  <block id="ac420a2d1b21182a80148191683bc9f7" category="paragraph">Das aktive Synchronisierungsverhalten von SnapMirror ist symmetrisch, mit einer wichtigen Ausnahme: Konfiguration des bevorzugten Standorts.</block>
  <block id="309d61d5c343d8fd53c2c585c64587b6" category="doc">Microsoft SQL Server mit aktiver SnapMirror-Synchronisierung</block>
  <block id="3ccf31c781c6df95109107d66199a8f6" category="section-title">Eigenständige Instanz von SQL Server</block>
  <block id="bd53ef250bf6f7a012cc7ed01c62f8cd" category="inline-link-macro">SQL Server auf ONTAP</block>
  <block id="d0d5aa4f66a3624a91ad4b11bbc2250f" category="paragraph">Die Best Practices für das Datei-Layout und die Serverkonfiguration sind dieselben, wie in der  Dokumentation empfohlen<block ref="c89905de54b32eb552c83786b7590df9" category="inline-link-macro-rx"></block>.</block>
  <block id="a489ffed938ef1b9e86889bc413501ee" category="inline-link-macro">Einheitlich</block>
  <block id="ee289d07b4c3d14dcc9cc162df065f99" category="paragraph">Mit einer eigenständigen Einrichtung konnte SQL Server nur an einem Standort ausgeführt werden. Vermutlich <block ref="b88135502d93a28469d0c520f73e7e7d" category="inline-link-macro-rx"></block>würde der Zugriff genutzt werden.</block>
  <block id="50c137c2af8db8930fd9881b4ece5d99" category="inline-image-macro">Ein Host mit einheitlichem Zugriff</block>
  <block id="60ee5124219a8203e5bacd7d5a1f38fa" category="paragraph"><block ref="60ee5124219a8203e5bacd7d5a1f38fa" category="inline-image-macro-rx" type="image"></block></block>
  <block id="327efeeabe7ad30552ef39a61284b93b" category="paragraph">Bei einem einheitlichen Zugriff würde ein Storage-Ausfall an einem der Standorte den Datenbankbetrieb nicht unterbrechen. Ein kompletter Standortausfall am Standort, der den Datenbankserver einschloss, würde natürlich zu einem Ausfall führen.</block>
  <block id="5bc465d840f8a329d264340f938ab253" category="paragraph">Einige Kunden konnten ein Betriebssystem konfigurieren, das am Remote-Standort mit einem vorkonfigurierten SQL Server-Setup ausgeführt wird, das mit einer gleichwertigen Build-Version wie die der Produktionsinstanz aktualisiert wurde. Bei einem Failover müsste die eigenständige Instanz von SQL Server am alternativen Standort aktiviert, die LUNS ermittelt und die Datenbank gestartet werden. Der vollständige Prozess kann mit dem Cmdlet "Windows PowerShell" automatisiert werden, da Storage-seitig kein Betrieb erforderlich ist.</block>
  <block id="ea7bbefe18f9ad7af62904dd6da19c33" category="inline-link-macro">Uneinheitlich</block>
  <block id="b53a3ef75c6e33a4f8945589cbe8beee" category="paragraph"><block ref="56d4a3cb11d7ad2b0e560d2cc8afde2c" category="inline-link-macro-rx"></block> Zugriff könnte auch verwendet werden, aber das Ergebnis wäre ein Datenbankausfall, wenn das Storage-System, in dem der Datenbankserver lokalisiert war, ausgefallen wäre, da die Datenbank keine Pfade zum Storage hätte. Dies kann in einigen Fällen noch akzeptabel sein. SnapMirror Active Sync bietet weiterhin RPO=0-Datensicherheit. Im Falle eines Standortausfalls wäre die noch verbleibende Kopie aktiv und bereit, den Betrieb mit demselben Verfahren wie oben beschrieben fortzusetzen.</block>
  <block id="a2e30e760061ab42fd5030c552306712" category="paragraph">Ein einfacher, automatisierter Failover-Prozess lässt sich mit der Verwendung eines virtuellen Hosts leichter konfigurieren. Wenn beispielsweise SQL Server-Datendateien zusammen mit einer Boot-VMDK synchron auf den sekundären Storage repliziert werden, könnte im Notfall die gesamte Umgebung am alternativen Standort aktiviert werden. Ein Administrator kann den Host am verbleibenden Standort entweder manuell aktivieren oder den Prozess über einen Service wie VMware HA automatisieren.</block>
  <block id="f04055c72ba32a95b9830308e4508cb1" category="section-title">SQL Server Failover-Cluster-Instanz</block>
  <block id="21fd4631916f5c0e63ef67f98d413d6a" category="paragraph">SQL Server Failover-Instanzen können auch auf einem Windows Failover Cluster gehostet werden, der auf einem physischen Server oder einem virtuellen Server als Gastbetriebssystem läuft. Diese Architektur mit mehreren Hosts bietet SQL Server Instanzen und Storage Resiliency. Diese Implementierung ist besonders in anspruchsvollen Umgebungen hilfreich, die robuste Failover-Prozesse suchen und gleichzeitig eine verbesserte Performance beibehalten. Wenn bei einem Failover-Cluster-Setup ein Host oder primärer Speicher betroffen ist, erfolgt ein Failover von SQL Services auf den sekundären Host, und gleichzeitig steht der sekundäre Speicher zur Verfügung, um IO bereitzustellen. Es sind kein Automatisierungsskript und keine Eingriffe durch den Administrator erforderlich.</block>
  <block id="58daef667c5e83ded89d712a683c532b" category="summary">Microsoft SQL Server und SM-als einheitlicher Zugriff</block>
  <block id="b8c5c455d22a62b43bd58615af0a569e" category="paragraph">Ein einheitliches Netzwerk für den Zugriff bedeutet, dass Hosts auf Pfade auf beiden Seiten (oder auf Ausfall-Domains innerhalb desselben Standorts) zugreifen können.</block>
  <block id="b7986b8213ecdd63004689cc2dedb163" category="doc">Disaster Recovery für Microsoft SQL Server mit SnapMirror</block>
  <block id="fdbd6bb11fa9bf15b9e53c8f359e821c" category="list-text">Bei Verwendung von SMB muss die Ziel-SVM Mitglied derselben Active Directory-Domäne sein, der die Quell-SVM angehört, damit die in NAS-Dateien gespeicherten Zugriffssteuerungslisten (Access Control Lists, ACLs) während der Wiederherstellung nach einem Notfall nicht beschädigt werden.</block>
  <block id="d5a7d81e3f495111d65621aede2e26a4" category="list-text">Die Verwendung von Ziel-Volume-Namen, die mit den Namen des Quell-Volume übereinstimmen, ist nicht erforderlich, kann jedoch das Mounten von Ziel-Volumes in das Ziel einfacher gestalten. Wenn SMB verwendet wird, müssen Sie den Ziel-NAS-Namespace in Pfaden und Verzeichnisstruktur mit dem Quell-Namespace identisch machen.</block>
  <block id="4dd0779e2b7f46c37eb5ec38cdde3bb6" category="list-text">Synchrone Replizierung, bei der eine schnelle Datenwiederherstellung erforderlich ist, und asynchrone Lösungen bieten Flexibilität bei RPO.</block>
  <block id="829f5bea960b53b5f1e50e8288a740d4" category="paragraph">Die Trennung zwischen der Platzierung logischer Objekte in den Dateigruppen und physischen Datenbankdateien ermöglicht es Ihnen, das Layout von Datenbankdateien zu optimieren und so das Storage-Subsystem optimal zu nutzen. Die Anzahl der Datendateien, die einen mitgebenden Workload unterstützen, kann nach Bedarf variiert werden, um I/O-Anforderungen und erwartete Kapazität ohne Auswirkungen auf die Applikation zu erfüllen. Diese Variationen im Datenbank-Layout sind für Anwendungsentwickler transparent, die die Datenbankobjekte in Dateigruppen statt in Datenbankdateien platzieren.</block>
  <block id="df42f2a843e18c9b5f6adccba2ffcbf9" category="paragraph">Die Aufgabe „Volume Maintenance durchführen“ wird in SQL Server 2016 vereinfacht und später während des Installationsprozesses als Option bereitgestellt. In dieser Abbildung wird die Option angezeigt, dem SQL Server-Datenbank-Engine-Service die Berechtigung zum Ausführen der Volume-Wartungsaufgabe zu gewähren.</block>
  <block id="8c0ea2438ef8ec6eede8f5ae6996c5a0" category="paragraph">Eine weitere wichtige Datenbankoption, die die Größe der Datenbankdateien steuert, ist Autoshrink. Wenn diese Option aktiviert ist, verkleinert SQL Server die Datenbankdateien regelmäßig, reduziert deren Größe und gibt Speicherplatz für das Betriebssystem frei. Dieser Vorgang ist ressourcenintensiv und nur selten sinnvoll, da die Datenbankdateien nach einiger Zeit wieder wachsen, wenn neue Daten in das System gelangen. Autoshrink sollte in der Datenbank nicht aktiviert sein.</block>
  <block id="58083df92a650a52d9ea6e4d544ffbd8" category="list-text">Erstellen Sie das Host-Protokollverzeichnis auf einem dedizierten Volume, auf das SnapCenter Transaktionsprotokolle kopiert.</block>
  <block id="42ad46ab967ef7af8287ca8a5c12f2d1" category="paragraph">Im folgenden Abschnitt werden die SQL Server-Speichereinstellungen erläutert, die zur Optimierung der Datenbankleistung erforderlich sind.</block>
  <block id="4e6b5a65dde214f8e25b7adbac9c9583" category="paragraph">Mit der Option „Max. Serverspeicher“ wird die maximale Speichergröße festgelegt, die die SQL Server-Instanz verwenden kann. Sie wird in der Regel verwendet, wenn mehrere Anwendungen auf demselben Server ausgeführt werden, auf dem SQL Server ausgeführt wird, und Sie sicherstellen möchten, dass diese Anwendungen über genügend Arbeitsspeicher verfügen, um ordnungsgemäß zu funktionieren.</block>
  <block id="fb182faf8ee092092118e8cbb568ae0d" category="paragraph">Einige Anwendungen verwenden nur den verfügbaren Speicher, wenn sie starten, und fordern keinen zusätzlichen Speicher an, selbst wenn sie unter Speicherdruck stehen. Hier kommt die maximale Serverspeichereinstellung ins Spiel.</block>
  <block id="c7af956e7765f16c090e0eab836fb42a" category="paragraph">Die Verwendung von SQL Server Management Studio zur Anpassung des minimalen oder maximalen Serverspeichers erfordert einen Neustart des SQL Server-Dienstes. Sie können den Serverspeicher auch mithilfe von Transact SQL (T-SQL) anpassen, indem Sie diesen Code verwenden:</block>
  <block id="0abaf38d5c4f997a2507fe6e57682b0b" category="paragraph">NUMA (Ununiform Memory Access) ist eine Technologie zur Optimierung des Speicherzugriffs, die dazu beiträgt, eine zusätzliche Belastung des Prozessorbusses zu vermeiden.</block>
  <block id="87dfe44a851d25873ee6bd0691ac0c6e" category="paragraph">Wenn NUMA auf einem Server konfiguriert ist, auf dem SQL Server installiert ist, ist keine zusätzliche Konfiguration erforderlich, da SQL Server NUMA-fähig ist und auf NUMA-Hardware eine gute Performance erzielt.</block>
  <block id="7cf384150f5c5317f48125af9bf68c31" category="paragraph">Die Option Index create Memory ist eine weitere erweiterte Option, die normalerweise nicht von den Standardwerten geändert werden muss.</block>
  <block id="64e199b7fb089ffb63f4c4396bac2cd1" category="paragraph">Standardmäßig weist die Einstellung Min. Speicher pro Abfrage &gt;= bis 1024 KB für jede auszufüllende Abfrage zu. Es empfiehlt sich, diese Einstellung auf den Standardwert zu belassen, damit SQL Server die für Indexerstellung zugewiesene Speichermenge dynamisch verwalten kann. Wenn SQL Server jedoch über mehr RAM verfügt, als für eine effiziente Ausführung erforderlich ist, kann die Leistung einiger Abfragen erhöht werden, wenn Sie diese Einstellung erhöhen. Solange also auf dem Server, der nicht von SQL Server verwendet wird, Speicher verfügbar ist, können andere Anwendungen oder das Betriebssystem diese Einstellung erhöhen, was die gesamte SQL Server-Leistung verbessern kann. Wenn kein freier Speicher verfügbar ist, kann eine Erhöhung dieser Einstellung die Gesamtleistung beeinträchtigen.</block>
  <block id="d4b44030b0eb9280a0e984cd6565e3b3" category="list-text">SnapCenter Backup Software, einschließlich der folgenden Komponenten:</block>
  <block id="95a3a11e77f5071b2a97b611548b866c" category="paragraph">Dieser Abschnitt zu Best Practices beschränkt sich auf technisches Design basierend auf den von NetApp für die Storage-Infrastruktur empfohlenen Prinzipien und bevorzugten Standards. Die End-to-End-Implementierung ist nicht im Umfang enthalten.</block>
  <block id="133dcf168b09b98ab4e6c6f77a743a14" category="paragraph">Informationen zur Kompatibilität der NetApp-Produkte finden Sie unter <block ref="18e7fcfe606d6d0912191de7fd9eb56a" category="inline-link-macro-rx"></block>.</block>
  <block id="961963c19132b001c6c7289519292be5" category="paragraph">Vor der Bereitstellung von SQL Server müssen Sie die Workload-Anforderungen der von Ihren SQL Server-Datenbankinstanzen unterstützten Applikationen kennen. Jede Applikation hat unterschiedliche Anforderungen an Kapazität, Performance und Verfügbarkeit. Daher sollte jede Datenbank für eine optimale Unterstützung dieser Anforderungen entworfen werden. Viele Unternehmen klassifizieren Datenbanken in mehrere Management-Tiers unter Verwendung von Anwendungsanforderungen zur Definition von SLAs. SQL Server-Workloads werden häufig wie unten beschrieben kategorisiert:</block>
  <block id="d748a2050ea9de15020c9f22e1c8a802" category="list-text">OLTP sind häufig die kritischsten Datenbanken in einem Unternehmen. Diese Datenbanken stehen in der Regel kundenorientierten Applikationen gegenüber und werden als unverzichtbar für die Kernprozesse des Unternehmens angesehen. Für geschäftskritische OLTP-Datenbanken und die von ihnen unterstützten Applikationen gibt es oft SLAs, die hohe Performance erfordern, nur mit Einschränkungen bei der Performance verbunden sind und maximale Verfügbarkeit erfordern. Sie können auch Kandidaten für „Always On“-Failover-Cluster oder „Always On“-Verfügbarkeitsgruppen sein. Der I/O-Mix bei diesen Datenbanktypen ist in der Regel durch 75 bis 90 % zufällige Lesevorgänge und 25 bis 10 % Schreibzugriffe gekennzeichnet.</block>
  <block id="542d0aad1735dc536565c8a1ec4b579f" category="list-text">Decision Support System (DSS)-Datenbanken, auch als Data Warehouses bezeichnet Diese Datenbanken sind für viele Unternehmen, die auf Analysen für ihr Geschäft vertrauen, geschäftskritisch. Diese Datenbanken sind sensibel für die CPU-Auslastung und Lesevorgänge von der Festplatte, wenn Abfragen ausgeführt werden. In vielen Unternehmen sind DSS-Datenbanken zum Monats-, Quartals- und Jahresende am wichtigsten Dieser Workload verfügt in der Regel über eine I/O-Kombination mit Lesevorgängen von fast 100 % und der I/O-Durchsatz ist oft wichtiger als IOPS.</block>
  <block id="af2a7a153f6f4904cc7384604809cf3c" category="paragraph">SQL Server kann als einzelne Instanz pro Server oder als mehrere Instanzen konfiguriert werden. Die richtige Entscheidung hängt in der Regel von Faktoren ab, z. B. ob der Server für die Produktion oder Entwicklung verwendet werden soll, ob die Instanz für den Geschäftsbetrieb und die Leistungsziele entscheidend ist.</block>
  <block id="8f66f557400d1222f6352dbc8811ec03" category="paragraph">Konfigurationen für gemeinsam genutzte Instanzen sind zwar zunächst einfacher zu konfigurieren, können jedoch zu Problemen führen, bei denen Ressourcen aufgeteilt oder gesperrt werden. Dies führt wiederum zu Leistungsproblemen für andere Anwendungen, bei denen Datenbanken auf der gemeinsam genutzten SQL Server-Instanz gehostet werden.</block>
  <block id="40d6bdd2be551c4046e5ad632c4dfe71" category="paragraph">Die Kombination aus ONTAP Storage-Lösungen und Microsoft SQL Server ermöglicht Datenbank-Storage-Designs der Enterprise-Klasse, die auch die anspruchsvollsten Applikationsanforderungen von heute erfüllen.</block>
  <block id="9e020f08745f560c049f43a1b5ead190" category="paragraph">Um eine SQL Server auf ONTAP-Lösung zu optimieren, müssen Sie das I/O-Muster und die Merkmale des SQL Servers kennen. Ein gut konzipiertes Storage Layout für eine SQL Server Datenbank muss die Performance-Anforderungen von SQL Server unterstützen und gleichzeitig maximale Management-Fähigkeit der Infrastruktur als Ganzes bieten. Ein gutes Storage-Layout ermöglicht außerdem eine erfolgreiche Erstimplementierung und ein reibungsloses Wachstum der Umgebung im Laufe der Zeit, während das Unternehmen wächst.</block>
  <block id="9eb2f26da39e165d9e3a1c48b5798690" category="paragraph">Volumes werden erstellt und befinden sich in den Aggregaten. Dieser Begriff verursacht manchmal Verwirrung, weil ein ONTAP Volume keine LUN ist. Ein ONTAP Volume ist ein Management-Container für Daten. Ein Volume kann Dateien, LUNs oder sogar S3 Objekte enthalten. Ein Volume benötigt keinen Speicherplatz, sondern wird nur für das Management der enthaltenen Daten verwendet.</block>
  <block id="64999620676d7bc615f88fabe256272f" category="list-text">Wenn Sie SQL Server auf einer SMB-Freigabe installieren, stellen Sie sicher, dass Unicode auf den SMB-Volumes zum Erstellen von Ordnern aktiviert ist.</block>
  <block id="b0874893f6e3e7030833ec488dc9da90" category="list-text">Legen Sie Benutzerdatendateien <block ref="937f38432beb92fdba3d47780720bdf7" prefix="(" category="inline-code"></block>) auf separaten Volumes, da es sich um zufällige Lese-/Schreib-Workloads handelt. Es ist üblich, Transaktions-Log-Backups häufiger zu erstellen als Datenbank-Backups. Legen Sie daher Transaktions-Log-Dateien <block ref="cb15b9539ae0d984ff8740c68b52561c" prefix="(" category="inline-code"></block>) auf ein separates Volume oder VMDK aus den Datendateien, so dass für jedes Volume unabhängige Backup-Zeitpläne erstellt werden können. Durch diese Trennung werden auch die I/O-Vorgänge bei sequenziellen Schreibvorgängen aus den I/O-Vorgängen für zufällige Lese-/Schreibzugriffe von Datendateien isoliert und die SQL Server Performance deutlich verbessert.</block>
  <block id="f7f1cb17ab4960f1a8e4c28a7fbccb0d" category="list-text">Mischen Sie keine Datenbank- und nicht-Datenbankdateien, wie z. B. Dateien mit Volltextsuche, auf derselben LUN.</block>
  <block id="42f55cb17aa10ac3e6774a14ecdda568" category="list-text">Wenn sekundäre Datenbankdateien (als Teil einer Dateigruppe) auf separate Volumes platziert werden, wird die Performance der SQL Server Datenbank verbessert. Diese Trennung ist nur gültig, wenn die Datei der Datenbank<block ref="772b0ad30c2d9d895a7319f2110cc750" prefix=" " category="inline-code"></block> ihre LUN nicht mit anderen Dateien teilt<block ref="937f38432beb92fdba3d47780720bdf7" prefix=" " category="inline-code"></block>.</block>
  <block id="58ccb3e2006ffa82c78944061df94b30" category="paragraph">Die ONTAP Storage-Effizienz wurde für das Speichern und Managen von SQL Server-Daten optimiert, sodass Sie den Speicherplatz auf ein Minimum beschränken und keine Auswirkungen auf die Performance haben.</block>
  <block id="f250d23ef016c3e48436a679c5dab1df" category="list-text">Aktivieren Sie die Deduplizierung auf Volumes mit SQL Server-Datendateien nicht, es sei denn, das Volume ist bekannt, dass es mehrere Kopien derselben Daten enthält, wie beispielsweise die Wiederherstellung von Datenbanken aus Backups auf einem einzelnen Volume.</block>
  <block id="1bf0b8f469689ae42fe419468737011f" category="paragraph">Die Tempdb-Datenbank kann stark genutzt werden. Neben der optimalen Platzierung von Benutzerdatenbankdateien auf ONTAP ist die Platzierung von tempdb-Datendateien auch wichtig, um die Zuweisungskonflikte zu verringern</block>
  <block id="6f80e9146e2184b9ec3c46b6d7dd903f" category="paragraph">Seitenkonflikte können auf den Seiten Global Allocation Map (GAM), Shared Global Allocation Map (SGAM) oder Page Free Space (PFS) auftreten, wenn SQL Server auf spezielle Systemseiten schreiben muss, um neue Objekte zuzuweisen. Verriegelungen sperren diese Seiten im Speicher. In einer stark ausgelasteten SQL Server-Instanz kann es lange dauern, bis ein Latch auf einer Systemseite in tempdb abgerufen wird. Dies führt zu längeren Abfragezeiten und wird als Latch Contention bezeichnet. Lesen Sie die folgenden Best Practices für das Erstellen von tempdb-Datendateien:</block>
  <block id="255bd5c00b8de4b2f998369f766b2b5e" category="paragraph">In diesem Abschnitt geht es um die Remote-Datensicherung, bei der Daten zum sicheren externen Storage und zur Disaster Recovery an einen Remote-Standort repliziert werden. Beachten Sie, dass diese Tabellen keine Datensicherung für die synchrone Spiegelung berücksichtigen. Informationen zu dieser Anforderung finden Sie in der NetApp MetroCluster-Dokumentation einschließlich <block ref="06d0604733fc7f7e3f9befaa1239278d" category="inline-link-macro-rx"></block> und <block ref="87f167af2516b222ddeec7405581e8b8" category="inline-link-macro-rx"></block></block>
  <block id="ad8d77efdce72d372a83929b50a127d1" category="paragraph">Ein absturzkonsistentes Backup eines Datensatzes bezieht sich auf die Erfassung der gesamten Datensatzstruktur zu einem bestimmten Zeitpunkt. Wenn der Datensatz in einem einzigen Volume gespeichert wird, ist der Vorgang einfach. Ein Snapshot kann jederzeit erstellt werden. Wenn ein Datensatz in mehreren Volumes gespeichert ist, muss ein Snapshot einer Konsistenzgruppe (CG) erstellt werden. Für das Erstellen von Snapshots von Konsistenzgruppen stehen verschiedene Optionen zur Verfügung, darunter NetApp SnapCenter-Software, native Funktionen von ONTAP-Konsistenzgruppen und vom Benutzer verwaltete Skripts.</block>
  <block id="3c27a8d13352bda1e272615270d5faba" category="paragraph">Ein absturzkonsistentes Backup einer Datenbank bezieht sich auf die Erfassung der gesamten Datenbankstruktur, einschließlich Datendateien, Wiederherstellungsprotokolle und Kontrolldateien zu einem bestimmten Zeitpunkt. Wenn die Datenbank in einem einzigen Volume gespeichert wird, ist der Vorgang einfach. Ein Snapshot kann jederzeit erstellt werden. Wenn eine Datenbank in mehreren Volumes gespeichert ist, muss ein Snapshot einer Konsistenzgruppe (CG) erstellt werden. Für das Erstellen von Snapshots von Konsistenzgruppen stehen verschiedene Optionen zur Verfügung, darunter NetApp SnapCenter-Software, native Funktionen von ONTAP-Konsistenzgruppen und vom Benutzer verwaltete Skripts.</block>
  <block id="7589d3d0b42e170fc0a459aba0b87fd6" category="paragraph">VBSR wird bevorzugt, wenn eine Datenbank sehr groß ist oder wenn sie so schnell wie möglich wiederhergestellt werden muss, und die Verwendung von VBSR erfordert die Isolierung der Datendateien. In einer NFS-Umgebung müssen die Datendateien einer bestimmten Datenbank in dedizierten Volumes gespeichert werden, die nicht durch andere Dateitypen kontaminiert sind. In einer SAN-Umgebung müssen Datendateien in dedizierten LUNs auf dedizierten Volumes gespeichert werden. Wenn ein Volume-Manager verwendet wird (einschließlich Oracle Automatic Storage Management [ASM]), muss die Festplattengruppe auch für Datendateien reserviert sein.</block>
  <block id="d8c4de538bee7abe14d3527c343afee7" category="paragraph">Wenn ein erweiterter aktiv/aktiv-RAC erforderlich ist, sollte die aktive SnapMirror-Synchronisierung anstelle von MetroCluster in Betracht gezogen werden. Die SM-AS-Replikation ermöglicht die bevorzugte Replikation der Daten. Daher kann ein erweiterter RAC-Cluster erstellt werden, in dem alle Lesevorgänge lokal stattfinden. Die Lese-I/O-Vorgänge gehen nie über Standorte hinweg, wodurch die geringstmögliche Latenz erzielt wird. Alle Schreibvorgänge müssen weiterhin die Verbindung zwischen den Standorten übertragen, dieser Traffic ist bei jeder Lösung mit synchroner Spiegelung jedoch unvermeidlich.</block>
  <block id="be6001fc2ab5bfaacf123c0267e58a0a" category="admonition">Wenn Boot-LUNs, einschließlich virtualisierter Boot-Festplatten, mit Oracle RAC verwendet werden, muss der<block ref="7c13d022c9fda6e792c4349a043a6c74" prefix=" " category="inline-code"></block> Parameter möglicherweise geändert werden. Weitere Informationen zu RAC-Timeout-Parametern finden Sie unter <block ref="1d616ba7dc3776fc2ecabd1cd20f420f" category="inline-link-macro-rx"></block>.</block>
  <block id="2bd32004298994cdcbc5a1672f2b8c95" category="summary">Oracle Single Instance mit SnapMirror Active Sync</block>
  <block id="b704207e8ec8f6bbae1764da78cb5232" category="doc">Oracle mit aktiver SnapMirror-Synchronisierung</block>
  <block id="93c480a0b6f335159bf44fcf2e2bec93" category="paragraph">Die Verwendung von SnapMirror Active Sync trägt nicht notwendigerweise zur Ergänzung oder Änderung von Best Practices für den Betrieb einer Datenbank bei.</block>
  <block id="30bca7463d8c7b2108be553aabb70549" category="paragraph">Die beste Architektur hängt von den geschäftlichen Anforderungen ab. Wenn das Ziel zum Beispiel ist, RPO=0 Schutz gegen Datenverlust zu haben, aber das RTO entspannt ist, dann kann die Verwendung von Oracle Single Instance Datenbanken und die Replikation der LUNs mit SM-AS ausreichen und auch preisgünstiger von einem Oracle Lizenzierungs-Standard sein. Ein Ausfall des Remote-Standorts würde den Betrieb nicht unterbrechen, und der Verlust des primären Standorts würde zu LUNs am noch intakten Standort führen, die online und einsatzbereit sind.</block>
  <block id="34ff499afe445b877410a8ae3527a84f" category="paragraph">Bei einer strikeren RTO würde die grundlegende aktiv/Passiv-Automatisierung über Skripte oder Clusterware wie Pacemaker oder Ansible die Failover-Zeit verbessern. Beispielsweise könnte VMware HA konfiguriert werden, um den VM-Ausfall am primären Standort zu erkennen und die VM am Remote-Standort zu aktivieren.</block>
  <block id="63cededd737a118049828e08a2a7e8c7" category="paragraph">Für ein extrem schnelles Failover konnte Oracle RAC über alle Standorte hinweg implementiert werden. Die RTO wäre im Grunde null, da die Datenbank jederzeit online und auf beiden Standorten verfügbar wäre.</block>
  <block id="5d0f4a1c768b6f8fc0a38580e17ef62b" category="summary">Oracle Extended RAC mit aktiver SnapMirror-Synchronisierung</block>
  <block id="596d088e9c01a0ceec54de24c763f7c4" category="paragraph">Viele Kunden optimieren ihre RTO, indem sie einen Oracle RAC Cluster über mehrere Standorte verteilen und damit eine vollständig aktiv/aktiv-Konfiguration erzielen. Das gesamte Design wird komplizierter, da es die Quorumverwaltung von Oracle RAC beinhalten muss.</block>
  <block id="c128a5dcb6cee18ff198d2c48937cea1" category="paragraph">Herkömmliche erweiterte RAC-Cluster stützten sich auf ASM-Spiegelung für Datensicherheit. Dieser Ansatz funktioniert, erfordert aber auch zahlreiche manuelle Konfigurationsschritte und führt zu einem Overhead in der Netzwerkinfrastruktur. Da hingegen die SnapMirror Active Sync die Verantwortung für die Datenreplizierung übernehmen kann, wird die Lösung erheblich vereinfacht. Vorgänge wie die Synchronisierung, die Neusynchronisierung nach Unterbrechungen, Failover und das Quorum-Management sind einfacher. Zudem muss das SAN nicht auf mehrere Standorte verteilt werden, was SAN-Design und -Management vereinfacht.</block>
  <block id="74f82d1e2ecf05353775ddaeb38bcddd" category="paragraph">Sie sind für das Verständnis der RAC-Funktionalität auf SnapMirror Active Sync von zentraler Bedeutung, wenn Storage als einheitlicher Satz von LUNs angezeigt wird, die auf gespiegelter Storage gehostet werden. Beispiel:</block>
  <block id="68dcade92edf97d052fd54be2dc6b60c" category="inline-image-macro">Logischer Oracle-Zugriff</block>
  <block id="9c4d005ebe3fb148615bd62837da8194" category="paragraph"><block ref="9c4d005ebe3fb148615bd62837da8194" category="inline-image-macro-rx" type="image"></block></block>
  <block id="b88e2c412773438221951df1b45aa00c" category="paragraph">Es gibt keine primäre Kopie oder gespiegelte Kopie. Logisch gesehen, es gibt nur eine einzige Kopie jeder LUN, und diese LUN ist auf SAN-Pfaden verfügbar, die sich auf zwei verschiedenen Storage-Systemen befinden. Aus Host-Sicht gibt es keine Storage-Failover, sondern Pfadänderungen. Verschiedene Fehlerereignisse können zum Verlust bestimmter Pfade zum LUN führen, während andere Pfade online bleiben. SnapMirror Active Sync stellt sicher, dass über alle Betriebspfade hinweg dieselben Daten verfügbar sind.</block>
  <block id="31c1d5c6e07b5b3d11e0585af763dc13" category="paragraph">In dieser Beispielkonfiguration sind die ASM-Festplatten so konfiguriert, wie sie in jeder RAC-Konfiguration mit einem einzigen Standort auf Enterprise Storage vorhanden wären. Da das Speichersystem Datenschutz bietet, würde ASM externe Redundanz verwendet werden.</block>
  <block id="f016e2bef75fc762898fb65f21fb9836" category="section-title">Einheitlicher oder uninformierten Zugriff</block>
  <block id="c70bc2caf8d25b4a91254d72909c7e75" category="paragraph">Die wichtigste Überlegung bei Oracle RAC on SnapMirror Active Sync ist, ob ein einheitlicher oder nicht einheitlicher Zugriff verwendet werden soll.</block>
  <block id="7cb7fcd446ee1d35aa27d2af5532e2ea" category="paragraph">Einheitlicher Zugriff bedeutet, dass jeder Host Pfade auf beiden Clustern sehen kann. Uneinheitlicher Zugriff bedeutet, dass Hosts nur Pfade zum lokalen Cluster sehen können.</block>
  <block id="53934f78a11bd68da05d52b4f83ec9fc" category="paragraph">Keine Option wird ausdrücklich empfohlen oder abgeraten. Einige Kunden verfügen über Dark Fibre, um Standorte miteinander zu verbinden, andere verfügen entweder über keine solche Konnektivität oder ihre SAN-Infrastruktur unterstützt keine Long-Distance-ISL.</block>
  <block id="53772793c4bb5f4dac1c6c0eade7403c" category="section-title">Uneinheitlicher Zugriff</block>
  <block id="68f708ec0a3f5091f6e432be351bf3cc" category="paragraph">Ein uneinheitlicher Zugriff ist aus SAN-Sicht einfacher zu konfigurieren.</block>
  <block id="b923541ce3d0d3feae5be9fb6199f770" category="inline-image-macro">Uneinheitlicher Oracle RAC Zugriff</block>
  <block id="b7d64dea0436c64fbd1652575479aa68" category="paragraph"><block ref="b7d64dea0436c64fbd1652575479aa68" category="inline-image-macro-rx" type="image"></block></block>
  <block id="58dcba3041e7c7c3352e2c7f9f497eb5" category="inline-link-macro">Uneinheitlicher Zugriff</block>
  <block id="52aa3a00cda7a8077552683f88c13e20" category="paragraph">Der größte Nachteil des <block ref="56ad9440d84c5145567c05e340b22f93" category="inline-link-macro-rx"></block>Ansatzes besteht darin, dass der Verlust der ONTAP-Konnektivität am Standort oder der Verlust eines Storage-Systems zum Verlust der Datenbankinstanzen an einem Standort führen kann. Dies ist natürlich nicht wünschenswert, aber es kann ein akzeptables Risiko im Austausch für eine einfachere SAN-Konfiguration sein.</block>
  <block id="4f7fff29e362cc56706e5fe5d34884b6" category="section-title">Einheitlicher Zugriff</block>
  <block id="df51b4c411cdf825abbf60c345b2886c" category="paragraph">Für einen einheitlichen Zugriff muss das SAN standortübergreifend erweitert werden. Der Hauptvorteil besteht darin, dass der Verlust eines Storage-Systems nicht zum Verlust einer Datenbankinstanz führt. Stattdessen würde dies zu einer Änderung des Multipathing führen, in der Pfade derzeit verwendet werden.</block>
  <block id="bb79831252d74e631b0f64224932d6e6" category="paragraph">Es gibt mehrere Möglichkeiten, uneinheitlichen Zugriff zu konfigurieren.</block>
  <block id="8ce82023c8f26832db1e79b32f2f7171" category="admonition">In den Diagrammen unten sind auch aktive, aber nicht optimierte Pfade vorhanden, die bei einfachen Controller-Ausfällen verwendet werden würden. Diese Pfade werden jedoch nicht im Interesse der Vereinfachung der Diagramme angezeigt.</block>
  <block id="15670c5b41a372b781730d1a2677ed57" category="section-title">AFF mit Annäherungseinstellungen</block>
  <block id="e2b2f6d768064f58804cd8cb2f295266" category="paragraph">Bei erheblichen Latenzzeiten zwischen den Standorten können AFF Systeme mit Host-Näherungseinstellungen konfiguriert werden. So kann jedes Speichersystem erkennen, welche Hosts lokal und welche Remote sind, und Pfadprioritäten entsprechend zuweisen.</block>
  <block id="5ffb901a29d38d4d4f7c462ffbd0ad7f" category="inline-image-macro">RAC mit einheitlichem Zugriff</block>
  <block id="efdefb56bfdac6f3b9194f8ce94b9ae9" category="paragraph"><block ref="efdefb56bfdac6f3b9194f8ce94b9ae9" category="inline-image-macro-rx" type="image"></block></block>
  <block id="e106da7d1088531e38324a9d76b9a988" category="paragraph">Im normalen Betrieb würde jede Oracle-Instanz bevorzugt die lokalen aktiven/optimierten Pfade verwenden. Folglich werden alle Lesezugriffe von der lokalen Kopie der Blöcke bedient. So wird eine möglichst geringe Latenz erzielt. Schreib-I/O wird ähnlich über Pfade zum lokalen Controller gesendet. Die I/O muss noch repliziert werden, bevor sie bestätigt werden kann, und somit würde die zusätzliche Latenz beim Überqueren des Site-to-Site-Netzwerks nach wie vor entstehen. Dies kann in einer Lösung zur synchronen Replizierung jedoch nicht vermieden werden.</block>
  <block id="889ea87800d5947e36cda77f7d1b16b4" category="section-title">ASA / AFF ohne Näherungseinstellungen</block>
  <block id="a92e6cf0c610f6072564d047ffa516be" category="paragraph">Falls keine nennenswerte Latenz zwischen den Standorten erforderlich ist, können AFF Systeme ohne Host-Näherungseinstellungen konfiguriert oder ASA verwendet werden.</block>
  <block id="7760495be1250342ca752bb9cdc684ee" category="paragraph"><block ref="7760495be1250342ca752bb9cdc684ee" category="inline-image-macro-rx" type="image"></block></block>
  <block id="6bad71277bac94b02b8ad4267095b9e1" category="paragraph">Jeder Host kann alle Betriebspfade auf beiden Storage-Systemen verwenden. Dies verbessert potenziell die Performance erheblich, da jeder Host das Performance-Potenzial von zwei, nicht nur einem Cluster, nutzen kann.</block>
  <block id="56473d7f95c121dd938752e3ee76807e" category="paragraph">Mit ASA gelten nicht nur alle Pfade zu beiden Clustern als aktiv und optimiert, sondern auch die Pfade auf den Partner-Controllern wären aktiv. Das Ergebnis wären ständig All-aktiv-SAN-Pfade auf dem gesamten Cluster.</block>
  <block id="22ede275964a6c559bc3f8f6d60e47ff" category="admonition">ASA-Systeme können auch in einer uneinheitlichen Zugriffskonfiguration verwendet werden. Da keine standortübergreifenden Pfade vorhanden sind, würde die Performance nicht durch I/O über den ISL beeinträchtigt.</block>
  <block id="263e6bb36868a481a8831ae728712452" category="summary">Aktive Synchronisierung mit Oracle SnapMirror</block>
  <block id="d1864442faf9b87db487ba86c72892a9" category="paragraph">Die unten erläuterten Beispiele zeigen einige der zahlreichen Optionen für die Bereitstellung von Oracle Single-Instance-Datenbanken mit SnapMirror Active Sync-Replikation.</block>
  <block id="1bd45bc28deaa95af95dd782d94963fa" category="inline-image-macro">Oracle SI mit uneinheitlichem Zugriff</block>
  <block id="c94bc93663b0f709f6a6885553fe9153" category="paragraph"><block ref="c94bc93663b0f709f6a6885553fe9153" category="inline-image-macro-rx" type="image"></block></block>
  <block id="b29b2b92ebd168fbb013ea878c5c1075" category="paragraph">SnapMirror Active Sync liefert eine synchrone Kopie der Daten am Disaster Recovery-Standort. Für die Verfügbarkeit dieser Daten sind jedoch ein Betriebssystem und die zugehörigen Applikationen erforderlich. Eine grundlegende Automatisierung kann die Failover-Zeit der gesamten Umgebung deutlich verbessern. Clusterware Produkte wie Pacemaker werden häufig eingesetzt, um über die Standorte hinweg einen Cluster zu erstellen, in vielen Fällen ist der Failover-Prozess mit einfachen Skripten angesteuert.</block>
  <block id="1e953e50c45ead229eae67ed609a3a1e" category="paragraph">Wenn die primären Knoten verloren gehen, stellt die Clusterware (oder Skripte) die Datenbanken am alternativen Standort online. Eine Option besteht darin, Standby-Server zu erstellen, die für die SAN-Ressourcen, aus denen die Datenbank besteht, vorkonfiguriert sind. Wenn der primäre Standort ausfällt, führt die Clusterware- oder skriptbasierte Alternative eine Abfolge von Aktionen durch, die der folgenden ähneln:</block>
  <block id="861373097ad1daff7221c81f1ffb8bd2" category="list-text">Fehler am primären Standort erkennen</block>
  <block id="471eb645214523a67e69f308fd203df5" category="list-text">Führen Sie eine Erkennung von FC- oder iSCSI-LUNs durch</block>
  <block id="aaae9cdb1dc3caf850c21cbae6cb7dd9" category="paragraph">Die eigentliche Aktivierung ist einfach. Befehle wie die LUN-Erkennung erfordern nur einige wenige Befehle pro FC-Port. Das Mounten von Dateisystemen ist nichts anderes als ein<block ref="19822b1b15d9eefc54c07ab49f87b100" prefix=" " category="inline-code"></block> Befehl, und sowohl Datenbanken als auch ASM können über die CLI mit einem einzigen Befehl gestartet und gestoppt werden.</block>
  <block id="81760e5cf8ff0457c9927339fe0d7987" category="list-text">Manuelles Starten von Datenbanken oder Konfigurieren der virtuellen Maschinen, um die Datenbanken automatisch zu starten.</block>
  <block id="17ad404af9b36204bcadadd073042e1c" category="paragraph">Beispielsweise kann ein ESX Cluster mehrere Standorte umfassen. Bei einem Notfall können die Virtual Machines nach dem Switchover am Disaster Recovery-Standort online geschaltet werden.</block>
  <block id="06c019e625f0595de8528ac88cab1815" category="section-title">Schutz vor Storage-Ausfällen</block>
  <block id="0559887c95614fe388b0dc7f8914a2db" category="paragraph">Das Diagramm oben zeigt die Verwendung von <block ref="56ad9440d84c5145567c05e340b22f93" category="inline-link-macro-rx"></block>, wo das SAN nicht über Standorte verteilt ist. Dies ist unter Umständen einfacher zu konfigurieren und ist angesichts der aktuellen SAN-Funktionen in einigen Fällen die einzige Option, was aber auch bedeutet, dass ein Ausfall des primären Storage-Systems einen Datenbankausfall bis zum Failover der Applikation zur Folge hätte.</block>
  <block id="5cbfe7798787ccd0e92fcaceca503314" category="inline-link-macro">Einheitlicher Zugriff</block>
  <block id="38a1b98129ec5892c90cdda2967b957c" category="paragraph">Für zusätzliche Ausfallsicherheit könnte die Lösung mit implementiert werden<block ref="8c3bdda17267bf34dafd546406201d13" category="inline-link-macro-rx"></block>. Dies würde es den Anwendungen ermöglichen, den Betrieb mit den Pfaden fortzusetzen, die vom gegenüberliegenden Standort angezeigt werden.</block>
  <block id="d77ca8b041bbb54326491bb1aeb6fadf" category="doc">Oracle und SnapMirror Active Sync – RAC Tiebreaker</block>
  <block id="084a4c8d8d6223c9b3c0749a4097bf8f" category="paragraph">Während Extended RAC mit SnapMirror Active Sync eine symmetrische Architektur in Bezug auf IO ist, gibt es eine Ausnahme, die mit Split-Brain-Management verbunden ist.</block>
  <block id="f0dd83fae83b5ba13b4e8c760578f47f" category="paragraph">Was passiert, wenn die Replikationsverbindung verloren geht und keiner der Standorte über ein Quorum verfügt? Was soll geschehen? Diese Frage bezieht sich sowohl auf das Oracle RAC- als auch auf das ONTAP-Verhalten. Wenn Änderungen nicht standortübergreifend repliziert werden können und Sie den Betrieb wieder aufnehmen möchten, muss einer der Standorte überleben und der andere Standort muss nicht mehr verfügbar sein.</block>
  <block id="58dc7db50a2e11a9de9e6cc092c59d4d" category="inline-link-macro">ONTAP Mediator</block>
  <block id="406abe933a832eefeb56c0390b824200" category="paragraph">Das <block ref="6663df057ff17bcadb83516e8fd742b7" category="inline-link-macro-rx"></block> löst diese Anforderung auf ONTAP-Ebene. Es gibt mehrere Optionen für RAC Tiebreaking.</block>
  <block id="1a7cedbe5066c42f7f901bffc7905f59" category="section-title">Oracle Tiebreakers</block>
  <block id="236ca78f8e5b54033dbbd133abe79e60" category="paragraph">Die beste Methode zur Verwaltung von Split-Brain Oracle RAC-Risiken ist die Verwendung einer ungeraden Anzahl von RAC-Knoten, vorzugsweise unter Verwendung eines Tiebreaker am dritten Standort. Wenn ein dritter Standort nicht verfügbar ist, könnte die Tiebreaker Instanz auf einem Standort der beiden Standorte platziert werden und somit einen bevorzugten Survivor-Standort darstellen.</block>
  <block id="af70ccf0217a2356d2bcebb6fab5a25b" category="section-title">Oracle und css_Critical</block>
  <block id="e7c7cde013ca0177a945b339ca49fe1d" category="paragraph">Bei einer geraden Anzahl von Knoten ist das standardmäßige Oracle RAC-Verhalten, dass einer der Knoten im Cluster als wichtiger angesehen wird als die anderen Knoten. Der Standort mit diesem Knoten mit höherer Priorität übersteht die Standortisolierung, während die Knoten am anderen Standort entfernt werden. Die Priorisierung basiert auf mehreren Faktoren, aber Sie können dieses Verhalten auch über die Einstellung steuern<block ref="6124b1cbe711d49d95a1c41d4e2701b4" prefix=" " category="inline-code"></block>.</block>
  <block id="1a79a4d60de6718e8e5b326e338ae533" category="inline-link-macro">Beispiel</block>
  <block id="6ae35dd8ffe888ab910de0d5d5e3a45a" category="paragraph">In der <block ref="0a286933b98d5160dd3059338d9fd4c7" category="inline-link-macro-rx"></block> Architektur sind die Hostnamen für die RAC-Knoten jfs12 und jfs13. Die aktuellen Einstellungen für<block ref="6124b1cbe711d49d95a1c41d4e2701b4" prefix=" " category="inline-code"></block> sind wie folgt:</block>
  <block id="d05ceaa3e91a5aa94cbe7a8ac83ad048" category="paragraph">Wenn der Standort mit jfs12 der bevorzugte Standort sein soll, ändern Sie diesen Wert für einen Knoten an Standort A in Ja, und starten Sie die Dienste neu.</block>
  <block id="d7e25fc2f6769b17d752bc3cde6d0266" category="summary">Aktives synchronen Failover von Oracle und SnapMirror</block>
  <block id="f0972b46fa5671c4841c63750ccdebb9" category="doc">SnapMirror Active Sync – Verlust der Oracle RAC-Replikation</block>
  <block id="8a6c7e7b423c27776aad808ff121ced3" category="paragraph">Der Verlust der Oracle RAC-Replikationsverbindung führt zu einem ähnlichen Ergebnis wie der Verlust der SnapMirror-Konnektivität, mit Ausnahme der standardmäßig kürzeren Timeouts. In den Standardeinstellungen wartet ein Oracle RAC-Knoten 200 Sekunden nach Verlust der Speicherverbindung, bevor er entfernt wird, aber er wartet nur 30 Sekunden nach Verlust des RAC-Netzwerk-Heartbeat.</block>
  <block id="488bd272072f6a962dc0b8f69fc362b8" category="paragraph">Die CRS-Meldungen ähneln denen unten. Sie können die Zeitlimitüberschreitung von 30 Sekunden sehen. Da css_Critical auf jfs12 gesetzt wurde, befindet sich an Ort A, das wird die Website zu überleben und jfs13 auf Standort B wird entfernt werden.</block>
  <block id="80f8fcffb7c608c6d25e68111eb38b2d" category="summary">Oracle und SnapMirror Active Sync – manuelles Failover</block>
  <block id="0d97f4e7841f881ee429033c005d63ef" category="paragraph">Der Begriff „Failover“ bezieht sich nicht auf die Richtung der Replizierung mit SnapMirror Active Sync, da es sich um eine bidirektionale Replizierungstechnologie handelt. Stattdessen bezieht sich „Failover“ darauf, welches Speichersystem bei einem Ausfall der bevorzugte Standort ist.</block>
  <block id="669420d9776c5672f6a1edf3bbf41604" category="paragraph">Möglicherweise möchten Sie beispielsweise ein Failover ausführen, um den bevorzugten Standort zu ändern, bevor Sie einen Standort zu Wartungszwecken herunterfahren oder bevor Sie einen DR-Test durchführen.</block>
  <block id="1a5cda38e5f45bb47ad55cae28a62d58" category="paragraph">GUI-Beispiel:</block>
  <block id="dd8c4d4a8c7db09b331d019e9ba3a643" category="inline-image-macro">Systemmanager-Clip des SM-als bevorzugte Site</block>
  <block id="50925d08482582f6a3178736557a9d6c" category="paragraph"><block ref="50925d08482582f6a3178736557a9d6c" category="inline-image-macro-rx" type="image"></block></block>
  <block id="beecd91de8185d21ecef9d430e18d4dc" category="paragraph">Beispiel für eine Rückänderung über die CLI:</block>
  <block id="3877a64f9dbc63cfbdca7e30e66c42a2" category="summary">Fehler beim Mediator bei Oracle und SnapMirror Active Sync</block>
  <block id="9daff60ceba391d4160d5e54f14037d8" category="paragraph">Der Mediator hat keine direkte Kontrolle über Storage-Vorgänge. Er fungiert als alternativer Kontrollpfad zwischen Clustern. Die Lösung bietet insbesondere automatisierte Failover-Prozesse ohne Split-Brain-Szenario.</block>
  <block id="13ce4a149213a3a389210201d2850f7c" category="paragraph">Im normalen Betrieb repliziert jedes Cluster Änderungen an seinem Partner. Daher kann jedes Cluster überprüfen, ob das Partner-Cluster online ist und Daten bereitstellt. Wenn die Replikationsverbindung fehlschlägt, wird die Replikation beendet.</block>
  <block id="381c63e25d0de40f5be6677aa1346b0e" category="paragraph">Der Grund für den sicheren automatisierten Betrieb ist ein Mediator, der andernfalls nicht feststellen kann, ob der Ausfall einer bidirektionalen Kommunikation auf einen Netzwerkausfall oder einen tatsächlichen Storage-Ausfall zurückzuführen ist.</block>
  <block id="e51e3b7c6dff02b4ab76ceab7b000eb8" category="list-text">Ein Ausfall der Replikationsdienste führt aus irgendeinem Grund dazu, dass der bevorzugte Standort den Zustand RPO=0 freigibt und die Lese- und Schreib-I/O-Verarbeitung wieder aufgenommen wird. Der nicht bevorzugte Standort nimmt seine Pfade offline.</block>
  <block id="957e90f10d8268ba74dc866bec3ed2fa" category="doc">SnapMirror Active Sync – Totalausfall der Konnektivität</block>
  <block id="4470e5dd55a8a6253ccfd5dca462e193" category="paragraph">Wenn die Replikationsverbindung zwischen den Standorten vollständig unterbrochen wird, werden sowohl die aktive SnapMirror-Synchronisierung als auch die Oracle RAC-Verbindung unterbrochen.</block>
  <block id="cd48b65842aa3ca6718bf9d05c82a272" category="paragraph">Die Split-Brain-Erkennung von Oracle RAC ist vom Heartbeat des Oracle RAC Storage abhängig. Wenn der Verlust der Site-to-Site-Konnektivität zu einem gleichzeitigen Verlust sowohl des RAC-Netzwerk-Heartbeat als auch der Speicherreplikationsdienste führt, können die RAC-Standorte weder über das RAC-Interconnect noch über die RAC-Abstimmungs-Laufwerke standortübergreifend kommunizieren. Das Ergebnis einer geraden Anzahl von Knoten kann die Entfernung beider Standorte unter den Standardeinstellungen sein. Das genaue Verhalten hängt von der Reihenfolge der Ereignisse und dem Timing des RAC-Netzwerks und der Disk-Heartbeat-Abfragen ab.</block>
  <block id="d76ecbc1eb83c7a69064a5bb857c85c9" category="inline-link-macro">Tiebreaker</block>
  <block id="acef32b29f0767a38d288d2517fd82e4" category="paragraph">Das Risiko eines Ausfalls von 2 Standorten kann auf zwei Arten behoben werden. Zunächst kann eine <block ref="5eea39840e87052aa3c632ea60116716" category="inline-link-macro-rx"></block> Konfiguration verwendet werden.</block>
  <block id="f52f16ab0735aaefebdd18f6be0a1d9a" category="paragraph">Wenn kein dritter Standort verfügbar ist, kann dieses Risiko durch Anpassung des Parameters für die Fehlzählung im RAC-Cluster behoben werden. Unter den Standardeinstellungen beträgt das Heartbeat-Timeout des RAC-Netzwerks 30 Sekunden. Dies wird normalerweise von RAC verwendet, um fehlerhafte RAC-Knoten zu identifizieren und aus dem Cluster zu entfernen. Es hat auch eine Verbindung zum Abstimmmedium Heartbeat.</block>
  <block id="c2b5462b64e9fc5a5b57a31b233c8857" category="paragraph">Wenn beispielsweise das Verbindungsrohr, das den Datenverkehr zwischen den Standorten für Oracle RAC und Speicherreplikationsdienste transportiert, durch einen Bagger gekürzt wird, beginnt der 30-Sekunden-Countdown für die Fehlzählung. Wenn der bevorzugte RAC-Standortknoten den Kontakt zum anderen Standort nicht innerhalb von 30 Sekunden wiederherstellen kann und er auch nicht die Abstimmdisks verwenden kann, um zu bestätigen, dass sich der entgegengesetzte Standort innerhalb desselben 30-Sekunden-Fensters befindet, werden die bevorzugten Standortknoten ebenfalls entfernt. Das Ergebnis ist ein vollständiger Ausfall der Datenbank.</block>
  <block id="db651e89ed200f74c11c1ae1c5d2b32d" category="paragraph">Je nachdem, wann die Abfrage der Fehlzählung erfolgt, sind 30 Sekunden möglicherweise nicht genügend Zeit für die SnapMirror Active Sync, um die Zeit zu verkürzen und die Speicherung auf dem bevorzugten Standort zu ermöglichen, um die Dienste wieder aufzunehmen, bevor das 30-Sekunden-Fenster abläuft. Dieses 30-Sekunden-Fenster kann vergrößert werden.</block>
  <block id="dbdaf8435bc0aa414eca4876e6c308d2" category="paragraph">Mit diesem Wert kann das Speichersystem am bevorzugten Standort den Betrieb wieder aufnehmen, bevor das Timeout für die Fehlzählung abläuft. Das Ergebnis ist eine Entfernung nur der Knoten am Standort, an dem die LUN-Pfade entfernt wurden. Beispiel unten:</block>
  <block id="e231fb490ed068ec9b23beb7dd95220e" category="paragraph">Der Oracle Support rät dringend davon ab, die Parameter „Fehlstellen“ oder „Disktimeout“ zu ändern, um Konfigurationsprobleme zu lösen. Eine Änderung dieser Parameter kann jedoch in vielen Fällen gerechtfertigt und unvermeidbar sein, einschließlich Konfigurationen für SAN-Booting, virtualisierte Konfigurationen und Speicherreplikation. Wenn Sie beispielsweise Stabilitätsprobleme mit einem SAN- oder IP-Netzwerk hatten, das zu RAC-Räumungen führte, sollten Sie das zugrunde liegende Problem beheben und die Werte des Misscount- oder Disktimeout nicht aufladen. Durch das Ändern von Timeouts zur Behebung von Konfigurationsfehlern wird ein Problem maskiert und kein Problem gelöst. Die Änderung dieser Parameter zur ordnungsgemäßen Konfiguration einer RAC-Umgebung basierend auf Designaspekten der zugrunde liegenden Infrastruktur unterscheidet sich und entspricht den Oracle-Support-Anweisungen. Bei SAN-Bootvorgang ist es üblich, Fehlstellen bis zu 200 anzupassen, um Disktimeout zu entsprechen. Weitere Informationen finden Sie unter<block ref="ba774c2123df7855b7691ef15a557a8c" category="inline-link-macro-rx"></block>.</block>
  <block id="cb5f127f654db8aa1847f5e9e6c5118e" category="summary">Oracle und SnapMirror Active Sync - Service-Wiederherstellung</block>
  <block id="894265e54aece56ce813211a980cdfe2" category="paragraph">SnapMirror bietet Selbstreparatur. SnapMirror Active Sync erkennt automatisch eine fehlerhafte Replikationsbeziehung und versetzt sie zurück in den Zustand RPO=0. Sobald die synchrone Replikation wiederhergestellt ist, werden die Pfade wieder online geschaltet.</block>
  <block id="c512ae64a9e9927a6314fdd926760a72" category="paragraph">In vielen Fällen erkennen Cluster-Applikationen automatisch die Rückgabe ausgefallener Pfade, und diese Applikationen sind ebenfalls wieder online. In anderen Fällen ist möglicherweise ein SAN-Scan auf Host-Ebene erforderlich oder Applikationen müssen manuell wieder online geschaltet werden.</block>
  <block id="c83e8f0dcd2a1773693ac06c752f2ddd" category="paragraph">Es hängt von der Anwendung und ihrer Konfiguration ab, und im Allgemeinen können solche Aufgaben leicht automatisiert werden. Die SnapMirror Active Sync Software selbst wird automatisch behoben und sollte nach der Wiederherstellung der Stromversorgung und Konnektivität keinen Benutzereingriff erfordern, um die RPO=0-Speichervorgänge wiederaufzunehmen.</block>
  <block id="b55bb91b49684d644a2314af260f5584" category="doc">Beispielarchitektur für Oracle mit SnapMirror Active Sync</block>
  <block id="8ba2bb4ebd97f28e7d98a180d8b06c1a" category="paragraph">Die in diesen Abschnitten gezeigten detaillierten Fehlerbeispiele basieren auf der unten dargestellten Architektur.</block>
  <block id="5d10516d162cd9f09cf5dc249d499836" category="admonition">Dies ist nur eine von vielen Optionen für Oracle Datenbanken auf SnapMirror Active Sync. Dieses Design wurde gewählt, weil es einige der komplizierteren Szenarien illustriert.</block>
  <block id="ce2a1d07c1660eb99608e2aeb9ffda20" category="inline-link-macro">Bevorzugter Standort</block>
  <block id="93d235b3b3adb1fcc631cc9720911156" category="paragraph">Bei diesem Design wird davon ausgegangen, dass Standort A auf der eingestellt ist<block ref="0a07e9e6141ed78f0b7013768c76c4a9" category="inline-link-macro-rx"></block>.</block>
  <block id="3f35d87b783da3d0c6ab04c2222e88cc" category="inline-image-macro">Beispiel für Oracle auf SM-AS-Design</block>
  <block id="65d5312cbb5d7497db9571af7d19e76d" category="paragraph"><block ref="65d5312cbb5d7497db9571af7d19e76d" category="inline-image-macro-rx" type="image"></block></block>
  <block id="af82c537bd0f6778b9916dc78da8c55a" category="paragraph">Das Ergebnis eines Storage-System- oder Standortausfalls ist nahezu identisch mit dem Ergebnis des Verlusts der Replizierungsverbindung. Am verbleibenden Standort sollte eine I/O-Pause von etwa 15 Sekunden bei Schreibvorgängen stattfinden. Nach Ablauf dieses Zeitraums von 15 Sekunden wird die E/A-Vorgänge wie gewohnt an diesem Standort fortgesetzt.</block>
  <block id="24f2945a7c742e126cdf5dcdfb08ec89" category="paragraph">Wenn nur das Speichersystem betroffen war, gehen die Speicherdienste des Oracle RAC-Knotens am ausgefallenen Standort verloren und führen vor der Entfernung und dem anschließenden Neustart denselben Countdown für die 200-Sekunden-Zeitüberschreitung für die Festplatte ein.</block>
  <block id="0cff2ae5f5c305f11d8d8a333bd1da2c" category="paragraph">Der SAN-Pfadstatus auf dem RAC-Knoten, der die Speicherdienste verloren hat, sieht wie folgt aus:</block>
  <block id="e33921fb5e166f433573176882c09294" category="paragraph">Der linux-Host hat den Verlust der Pfade viel schneller als 200 Sekunden erkannt, aber aus Sicht der Datenbank werden die Clientverbindungen zum Host auf dem ausgefallenen Standort unter den standardmäßigen Oracle RAC-Einstellungen weiterhin 200 Sekunden lang eingefroren. Die vollständigen Datenbankvorgänge werden erst nach Abschluss der Entfernung fortgesetzt.</block>
  <block id="db34db4ac9bd7910a9eded83e1ff30f9" category="paragraph">In der Zwischenzeit zeichnet der Oracle RAC-Knoten am gegenüberliegenden Standort den Verlust des anderen RAC-Knotens auf. Ansonsten funktioniert es wie gewohnt.</block>
  <block id="ad08bdae586fc05e13eff6bd6d9f0ee2" category="paragraph">Wenn der SnapMirror Active Sync Replication Link verwendet wird, kann die Schreib-I/O nicht abgeschlossen werden, da ein Cluster Änderungen nicht am anderen Standort replizieren könnte.</block>
  <block id="564a95e2696af68aa1c3a4953ecd1ce9" category="section-title">Standort A</block>
  <block id="18fe52a46fad1fa4c9873ea81b6e885f" category="paragraph">Das Ergebnis eines Ausfalls einer Replikationsverbindung an Standort A ist eine ca. 15-Sekunden-Pause bei der Schreib-I/O-Verarbeitung, da ONTAP versucht, Schreibvorgänge zu replizieren, bevor es feststellt, dass die Replikationsverbindung wirklich nicht funktionsfähig ist. Nach 15 Sekunden wird das ONTAP Cluster vor Ort A mit der Lese- und Schreib-I/O-Verarbeitung fortgesetzt. Die SAN-Pfade ändern sich nicht, und die LUNs bleiben online.</block>
  <block id="657e60cc43634129c8f9314afdd5c48b" category="paragraph">Die Replikationsverbindung wurde mit dem Zeitstempel 15:19:44 geschnitten. Die erste Warnung von Oracle RAC kommt 100 Sekunden später, da sich das 200-Sekunden-Timeout (gesteuert durch den Oracle RAC Parameter disktimeout) nähert.</block>
  <block id="f29d4b902de1f9de93b8673047367cb3" category="paragraph">Sobald das 200-Sekunden-Zeitlimit für Abstimmdatenträger erreicht wurde, wird dieser Oracle RAC-Knoten selbst aus dem Cluster entfernt und neu gestartet.</block>
  <block id="b864ca7c3ca6173547e4a66ba059c688" category="summary">Oracle, SM-AS, Active Sync, Mediator</block>
  <block id="01682ebdcacbc827e3ef8083e7b4c060" category="summary">Oracle und SM-als uneinheitlicher Zugriff</block>
  <block id="f6dbb06140585fe45ddeaf49cae9418e" category="summary">Oracle und SnapMirror Active Sync</block>
  <block id="d6e7dde9799f39cece01cbc4b2728cdf" category="paragraph">Mit SnapMirror Active Sync können Sie Oracle-Datenbankumgebungen mit extrem hoher Verfügbarkeit aufbauen, in denen LUNs von zwei verschiedenen Storage-Clustern verfügbar sind.</block>
  <block id="13645bda5e7e84c7e547caee4c6325b2" category="paragraph">Bei SnapMirror Active Sync gibt es keine „primäre“ und „sekundäre“ Kopie der Daten. Jedes Cluster kann Lese-I/O aus seiner lokalen Kopie der Daten bereitstellen, und jedes Cluster repliziert einen Schreibvorgang auf seinen Partner. Das Ergebnis ist ein symmetrisches I/O-Verhalten.</block>
  <block id="36b94662a6696bbaee0152aefe40ed13" category="paragraph">So können Sie unter anderem Oracle RAC als erweiterten Cluster mit operativen Instanzen an beiden Standorten ausführen. Alternativ können Sie RPO=0 aktiv/Passiv-Datenbank-Cluster erstellen, bei denen Datenbanken mit einer Instanz bei einem Standortausfall zwischen Standorten verschoben werden können. Dieser Prozess kann über Produkte wie Pacemaker oder VMware HA automatisiert werden. Die Grundlage für all diese Optionen ist die synchrone Replizierung, die über SnapMirror Active Sync gemanagt wird.</block>
  <block id="d1a363496283d60774ff9c9c51ffe653" category="paragraph">Im normalen Betrieb bietet SnapMirror Active Sync jederzeit ein synchrones RPO=0-Replikat, mit einer Ausnahme. Wenn Daten nicht repliziert werden können, gibt ONTAP die Notwendigkeit zur Replizierung von Daten frei und stellt die E/A-Bereitstellung an einem Standort wieder her, während die LUNs am anderen Standort offline geschaltet werden.</block>
  <block id="72f31f711b37dc7ce7b34e00dd56b5ae" category="paragraph">Der ONTAP Mediator ist eine Softwareanwendung, die von der NetApp-Unterstützung heruntergeladen wird und normalerweise auf einer kleinen virtuellen Maschine bereitgestellt wird. Bei Verwendung mit SnapMirror Active Sync ist der ONTAP Mediator kein Tiebreaker. Es handelt sich um einen alternativen Kommunikationskanal für die beiden Cluster, die an der aktiven synchronen SnapMirror-Replikation beteiligt sind. Der automatisierte Betrieb wird durch ONTAP basierend auf den Antworten gesteuert, die der Partner über direkte Verbindungen und den Mediator erhält.</block>
  <block id="b06e5b68abe4f23a716133f1fa1a4472" category="summary">Oracle und SM-als bevorzugter Standort</block>
  <block id="6d14548094c3b2a5b0a69049f8497fa8" category="doc">Oracle und SM-als einheitlicher Zugriff</block>
  <block id="36ba939f32969ddc6751427944e3569d" category="paragraph">Die wichtigste Anforderung ist die Isolierung der Datendateien in einem oder mehreren dedizierten Volumes. Sie müssen durch einen anderen Dateityp nicht kontaminiert sein. Der Grund dafür ist, sicherzustellen, dass die Datendateireplikation völlig unabhängig von der Replikation anderer Datentypen wie Archivprotokollen ist. Weitere Informationen zu Dateilayouts und wichtige Details zur Sicherstellung, dass das Speicherlayout Snapshot-freundlich ist, finden Sie unter <block ref="c87a7212960b8cc2aca49adb1795e291" category="inline-link-macro-rx"></block>.</block>
  <block id="64c93f87be857cf43f66e341a377693a" category="paragraph">Das Betriebssystem kann erkennen, dass<block ref="9ad16912cc0ba54a259b1a15d233c264" prefix=" " category="inline-code"></block> und<block ref="1da707b60c3b55bd01cb7df7c171a368" prefix=" " category="inline-code"></block> befinden sich auf demselben Volume, das gleiche Quelldateisystem ist. Das Betriebssystem verwendet dann dasselbe Geräte-Handle für den Zugriff auf die Daten. Dadurch wird die Verwendung von OS Caching und bestimmten anderen Vorgängen verbessert, es beeinträchtigt jedoch DNFS. Wenn DNFS auf eine Datei zugreifen muss, wie z.B.<block ref="1737629d60b511cf45957529a8f046e2" prefix=" " category="inline-code"></block> , ON<block ref="1da707b60c3b55bd01cb7df7c171a368" prefix=" " category="inline-code"></block>, könnte es fälschlicherweise versuchen, den falschen Pfad zu den Daten zu verwenden. Das Ergebnis ist ein I/O-Vorgang, der fehlgeschlagen ist. Fügen Sie in diesen Konfigurationen die Mount-Option zu jedem NFS-Dateisystem hinzu<block ref="4740c034d97ca90a96b8050082816605" prefix=" " category="inline-code"></block>, das ein Quell-Volume mit einem anderen NFS-Dateisystem auf diesem Host gemeinsam nutzt. Dies zwingt das Linux-Betriebssystem, einem unabhängigen Device Handle für dieses Dateisystem zuzuweisen.</block>
  <block id="30d0582d4c5a0b280454304dcff9c28f" category="paragraph">Solaris 11 beinhaltete eine Änderung bei der Verarbeitung großer I/O-Vorgänge, die zu schwerwiegenden Leistungsproblemen auf SAN-Speicher-Arrays führen können. Das Problem ist dokumentiert NetApp Tracking Fehlerbericht 630173, "Solaris 11 ZFS Leistungsregression."</block>
  <block id="c041cd46ed4caba0947ab494ac5bb1d9" category="paragraph">Dies ist kein ONTAP-Bug. Es handelt sich um einen Solaris-Fehler, der unter Solaris Defects 7199305 und 7082975 nachverfolgt wird.</block>
  <block id="a83650b3549679a1591275f0f9cd38e0" category="paragraph">Sie können den Oracle Support konsultieren, um herauszufinden, ob Ihre Version von Solaris 11 betroffen ist, oder Sie können die Problemumgehung testen, indem Sie auf einen kleineren Wert wechseln<block ref="954f82599f7db0212a28fd448e03bdf3" prefix=" " category="inline-code"></block>.</block>
  <block id="65711e429e82120028dff2df28b4b92d" category="paragraph">Dazu führen Sie den folgenden Befehl als root aus:</block>
  <block id="f0eff8c4d89b44c0cf8b8c323abeaa20" category="paragraph">Wenn Sie einen Offline-Prozess benötigen, verzögern Sie die Neuermittlung oder den Neustart von Diensten, bis der<block ref="1ab15ef2c245a4434fcea3cc3149f4f4" prefix=" " category="inline-code"></block> Befehl anzeigt, dass alle Migration erfolgreich und abgeschlossen ist. Anschließend können Sie den Migrationsprozess wie unter beschrieben abschließen<block ref="bfe85b0236d22bff0d0d13e4e3fecb98" category="inline-link-macro-rx"></block>.</block>
  <block id="dbd3f1a817d137c052ec01ef60940131" category="inline-link">ONTAP Dokumentation zum Importieren fremder LUNs</block>
  <block id="1e63fd2b99b2fef1f5d2ae61ebd5b4a3" category="paragraph">Die Verfahren zur Migration von SAN-Ressourcen mit FLI sind in NetApp dokumentiert<block ref="3a5bd22edab525577a3f095f01c2551b" category="inline-link-rx"></block>.</block>
  <block id="4fc57ef291a6bc6be0f3f8c81e4f9b44" category="paragraph">Das folgende Flussdiagramm zeigt die Arten von Überlegungen, welche Migrationspfade am besten geeignet sind. Sie können mit der rechten Maustaste auf das Bild klicken und es in einer neuen Registerkarte öffnen, um die Lesbarkeit zu verbessern.</block>
  <block id="17ed98f652bb960b10b3357e44785f38" category="inline-image-macro">Flussdiagramm der Migration</block>
  <block id="f420007a3c90420b30780388a0e05dbc" category="paragraph"><block ref="6ee66f70cd2b82e9dd03128a50a38de9" category="inline-image-macro-rx" type="image"></block>.</block>
  <block id="fb3e908ec44ab0657a364e90a649adee" category="admonition">In der Anmerkung zu<block ref="4740c034d97ca90a96b8050082816605" prefix=" " category="inline-code"></block> in <block ref="6ada863f3548cecdf5d7a06e9c4201cd" category="inline-link-macro-rx"></block> finden Sie ein Linux-spezifisches dNFS-Problem, das zu ungewöhnlichen Ergebnissen führen kann.</block>
  <block id="7dde30333f2f14b8935a6f394318dd19" category="paragraph">Nicht aufgelöste Direktive in &lt;stdin&gt; - include::lun-Alignment.adoc[]</block>
  <block id="baeaebf1ec4083a8b5a2278617a67479" category="paragraph">Siehe auch die Diskussion über die Ausrichtung der Kompressionsblöcke im Abschnitt <block ref="936e7f76dd05eeca0e082776361dee38" category="inline-link-macro-rx"></block>. Jedes Layout, das an 8-KB-Komprimierungsblockgrenzen ausgerichtet ist, ist auch an 4-KB-Grenzen ausgerichtet.</block>
  <block id="62cc2062c0bb14fd9ee41267e11b9824" category="paragraph">Ungelöste Direktive in &lt;stdin&gt; - include::Database-falignment-warnungen.adoc[]</block>
  <block id="779e77f14b9599d3021cc1c978703d8e" category="paragraph">Die NetApp Datenmanagementplattformen mit ONTAP gehören zu den am weitesten verbreiteten Storage-Lösungen für SRM. Die Gründe hierfür sind vielfältig: Eine sichere, hochperformante, einheitliche Protokoll-Datenmanagementplattform (NAS und SAN zusammen), die branchenweit definierte Storage-Effizienz, Mandantenfähigkeit, Quality-of-Service-Kontrollen, Datensicherung mit platzsparenden Snapshots und Replizierung mit SnapMirror bietet. Dabei werden native Hybrid-Multi-Cloud-Integrationen für die Sicherung von VMware Workloads sowie eine Fülle von Automatisierungs- und Orchestrierungs-Tools blitzschnell verfügbar.</block>
  <block id="f05beeac7e71de747ac87dd710b5100f" category="list-text">Das vCenter Plug-in, ehemals Virtual Storage Console (VSC), vereinfacht Storage-Management- und Effizienzfunktionen, verbessert die Verfügbarkeit und senkt die Storage-Kosten und den Betriebsaufwand – sei es bei SAN oder NAS. Dieses Plug-in nutzt Best Practices für die Bereitstellung von Datastores und optimiert ESXi Hosteinstellungen für NFS- und Block-Storage-Umgebungen. Wegen all dieser Vorteile empfiehlt NetApp bei der Nutzung von vSphere bei Systemen mit ONTAP dieses Plug-in.</block>
  <block id="0452707f6213531bbf4f41e5f16cdead" category="paragraph">Das Klonen kann – in der Regel auf VM-, vVol- oder Datastore-Ebene – durch mehrere Verfahren auf Systeme mit ONTAP verlagert werden. Hierzu zählen:</block>
  <block id="a4ec3dfc79c8493b1e9fcaee6b06b5a8" category="summary">ONTAP bietet umfassende Unterstützung für die Hybrid Cloud.</block>
  <block id="14f3c0fa1399e57b5998b528cc5f3515" category="paragraph">Bei ONTAP handelt es sich unter anderem um ein horizontal skalierbares NAS-Array der Enterprise-Klasse. ONTAP ermöglicht VMware vSphere den gleichzeitigen Zugriff auf NFS-verbundene Datastores von vielen ESXi Hosts und übertrifft dabei die für VMFS Dateisysteme auferlegten Grenzen bei Weitem. Die Verwendung von NFS mit vSphere bietet einige Vorteile in Bezug auf Benutzerfreundlichkeit, Storage-Effizienz und Sichtbarkeit, wie im  Abschnitt erwähnt<block ref="72b655a9973dbed02099f5e63762d591" category="inline-link-macro-rx"></block>.</block>
  <block id="98d3ef3a1c0f94d4ecc34aef4673ce8b" category="paragraph">ONTAP bietet Block-Storage der Enterprise-Klasse für VMware vSphere unter Verwendung von iSCSI, Fibre Channel Protocol (FCP, kurz FC) und NVMe over Fabrics (NVMe-of). Nachfolgend finden Sie Best Practices zur Implementierung von Blockprotokollen für VM-Storage mit vSphere und ONTAP.</block>
  <block id="8fabf36e54be557b0b3c16cc181ccef7" category="paragraph">SLM beschränkt die Nodes, die Pfade an eine bestimmte LUN weitergeben. Eine Best Practice von NetApp sieht mindestens eine logische Schnittstelle (Logical Interface, LIF) pro Node pro SVM und die Verwendung von SLM vor, um die Pfade zu begrenzen, die an den Node weitergegeben werden, der die LUN und deren HA-Partner hostet. Es sind zwar noch andere Pfade vorhanden, doch werden diese standardmäßig nicht weitergegeben. Die weitergegebenen Pfade können mit den Node-Argumenten zum Hinzufügen oder Entfernen der Berichterstellung in SLM geändert werden. Beachten Sie, dass in Versionen vor 8.3 erstellte LUNs alle Pfade weitergeben. Sie müssen geändert werden, damit nur die Pfade zum Hosting-HA-Paar weitergegeben werden. Weitere Informationen zu SLM finden Sie in Abschnitt 5.9 von<block ref="a2bae1e6a9d1ce7703c1f95bb6ec1d6e" category="inline-link-rx"></block>. Um die für eine LUN verfügbaren Pfade weiter zu reduzieren, kann auch die frühere Portsatzmethode verwendet werden. Portsätze tragen dazu bei, die Anzahl der sichtbaren Pfade zu verringern, durch die Initiatoren in einer Initiatorgruppe LUNs ausfindig machen können.</block>
  <block id="9f429a2a2cb5d9d82853efd63cd4f4ce" category="paragraph">Sieben Protokolle werden für die Anbindung von VMware vSphere an Datastores auf einem System mit ONTAP verwendet:</block>
  <block id="0783a52eeafbf6085600efe3ac209a9c" category="paragraph">Systeme mit ONTAP unterstützen alle wichtigen Storage-Protokolle, sodass die Kunden abhängig von der vorhandenen und geplanten Netzwerkinfrastruktur und den Fähigkeiten der Mitarbeiter das für ihre Umgebung am besten geeignete Protokoll auswählen können. Bei von NetApp durchgeführten Tests zeigten sich generell nur geringfügige Unterschiede zwischen Protokollen, die mit ähnlichen Übertragungsgeschwindigkeiten ausgeführt wurden. Daher empfiehlt es sich, den Schwerpunkt in erster Linie auf die Netzwerkinfrastruktur und die Fähigkeiten der Mitarbeiter und erst in zweiter Linie auf die ursprüngliche Protokoll-Performance zu legen.</block>
  <block id="ef7f5a0b4e0866202a92874cc85a2fd2" category="list-text">Virtual Machines, bei denen eine präzisere Migration erforderlich ist, sind unter anderem Datenbanken und Applikationen mit Nutzung von Attached Storage. Bei diesen sollten Sie die Migration im Allgemeinen mit den Applikationstools managen. Für Oracle empfiehlt sich zur Migration der Datenbankdateien die Nutzung von Oracle-Tools wie RMAN oder ASM. Weitere Informationen finden Sie unter<block ref="74eec9db95a0d18126c677c2dcc1d6f1" category="inline-link-rx"></block> . Ganz ähnlich kommen für SQL Server entweder SQL Server Management Studio oder NetApp Tools wie SnapManager für SQL Server oder SnapCenter in Betracht.</block>
  <block id="4adbfc6cb6ee114e186e01168d24a89b" category="paragraph">Wenn Sie vSphere mit Systemen mit ONTAP verwenden, ist es eine Best Practice, das ONTAP Tools für VMware vSphere Plug-in (früher Virtual Storage Console) zu installieren und zu verwenden. Dieses vCenter Plug-in vereinfacht das Storage-Management, erhöht die Verfügbarkeit und senkt die Storage-Kosten und den Betriebsaufwand – sei es bei SAN oder bei NAS. Dieses Plug-in nutzt Best Practices für die Bereitstellung von Datastores und optimiert die ESXi Hosteinstellungen für Multipath- und HBA-Timeouts (diese sind in Anhang B beschrieben). Da es sich um ein vCenter Plug-in handelt, ist es für alle vSphere Webclients verfügbar, die eine Verbindung mit dem vCenter Server herstellen.</block>
  <block id="8c796e9ce33123891311630e9843b227" category="paragraph">Wenn Sie vSphere mit Systemen mit ONTAP verwenden, ist die Konfiguration von Netzwerkeinstellungen einfach und erfolgt ähnlich wie andere Netzwerkkonfigurationen. Folgende Punkte sind dabei zu berücksichtigen:</block>
  <block id="14089d94b95e85f3c910fdfee996412f" category="list-text">NetApp empfiehlt eine Deaktivierung der Netzwerk- Flusssteuerung nur an den Cluster-Netzwerkports innerhalb eines ONTAP Clusters. Für die übrigen Netzwerkports, die für Daten-Traffic verwendet werden, gibt NetApp im Hinblick auf Best Practices keine weiteren Empfehlungen. Diese Ports sollten Sie nach Bedarf aktivieren oder deaktivieren. Weitere Informationen zur Flusssteuerung finden Sie unter<block ref="3455dc2b3e7cd12a38e508bec690aa67" category="inline-link-rx"></block>.</block>
  <block id="0ced8c876dd50237bb9d7be301c0041e" category="paragraph">Um ein Höchstmaß an Datensicherung zu gewährleisten, ziehen Sie eine VMware vSphere Metro Storage Cluster (vMSC) Konfiguration mit NetApp MetroCluster in Betracht. VMSC ist eine von VMware zertifizierte Lösung, die synchrone Replizierung mit Array-basiertem Clustering kombiniert. So profitieren Sie von denselben Vorteilen wie ein Hochverfügbarkeits-Cluster, sind aber zum Schutz vor Standortausfällen auf separate Standorte verteilt. NetApp MetroCluster bietet kostengünstige Konfigurationen für die synchrone Replizierung mit transparentem Recovery nach dem Ausfall einer einzelnen Storage-Komponente sowie Recovery mit nur einem Befehl im Falle eines Standortausfalls. VMSC wird ausführlicher in beschrieben.<block ref="252912db8b98e51588a897ab91b985e0" category="inline-link-rx"></block></block>
  <block id="49dd4c85a332616209949697018e3112" category="paragraph">Systeme mit ONTAP vereinfachen die Sicherung sämtlicher Daten durch Verschlüsselung im Ruhezustand. NetApp Storage Encryption (NSE) verwendet Self-Encrypting Drives mit ONTAP, um SAN- und NAS-Daten zu sichern. NetApp bietet darüber hinaus NetApp Volume Encryption und NetApp Aggregate Encryption als einen einfachen, softwarebasierten Ansatz zur Verschlüsselung von Volumes auf Festplattenlaufwerken. Für diese Softwareverschlüsselung sind keine speziellen Festplatten oder externen Schlüsselmanager erforderlich. Es ist für ONTAP Kunden kostenlos verfügbar. Sie können ein Upgrade durchführen und mit der Nutzung von IT beginnen, ohne dass es zu Unterbrechungen für Ihre Clients oder Applikationen kommt. Außerdem sind sie gemäß FIPS 140-2 Level 1 Standard validiert, einschließlich Onboard Key Manager.</block>
  <block id="66149c951df015fb5f7aa252fa92bf64" category="paragraph">Wenn Sie vSphere mit Systemen mit ONTAP verwenden, ist die Konfiguration von Netzwerkeinstellungen einfach und erfolgt ähnlich wie andere Netzwerkkonfigurationen.</block>
  <block id="cd03a9a043676fafd38958ae11e85ca8" category="list-text">NetApp empfiehlt eine Deaktivierung der Netzwerk- Flusssteuerung nur an den Cluster-Netzwerkports innerhalb eines ONTAP Clusters. Für die übrigen Netzwerkports, die für Daten-Traffic verwendet werden, gibt NetApp im Hinblick auf Best Practices keine weiteren Empfehlungen. Sie sollten diese Funktion nach Bedarf aktivieren oder deaktivieren. Weitere Informationen zur Flusssteuerung finden Sie unter<block ref="3455dc2b3e7cd12a38e508bec690aa67" category="inline-link-rx"></block>.</block>
  <block id="e2a6c1d09f3da3b6ad069ff3692e7526" category="paragraph">SLM beschränkt die Nodes, die Pfade an eine bestimmte LUN weitergeben. Eine Best Practice von NetApp sieht mindestens eine logische Schnittstelle (Logical Interface, LIF) pro Node pro SVM und die Verwendung von SLM vor, um die Pfade zu begrenzen, die an den Node weitergegeben werden, der die LUN und deren HA-Partner hostet. Es sind zwar noch andere Pfade vorhanden, doch werden diese standardmäßig nicht weitergegeben. Die weitergegebenen Pfade können mit den Node-Argumenten zum Hinzufügen oder Entfernen der Berichterstellung in SLM geändert werden. Beachten Sie, dass in Versionen vor 8.3 erstellte LUNs alle Pfade weitergeben. Sie müssen geändert werden, damit nur die Pfade zum Hosting-HA-Paar weitergegeben werden. Weitere Informationen zu SLM finden Sie in Abschnitt 5.9 von<block ref="a2bae1e6a9d1ce7703c1f95bb6ec1d6e" category="inline-link-rx"></block>. Um die für eine LUN verfügbaren Pfade weiter zu reduzieren, kann auch die frühere Portsatzmethode verwendet werden. Portsätze tragen dazu bei, die Anzahl der sichtbaren Pfade zu verringern, durch die Initiatoren in einer Initiatorgruppe LUNs ausfindig machen können.</block>
  <block id="2fd6bc533dedf28bc2b73262be20ec88" category="list-text">*Unified Storage.* Systeme mit ONTAP werden auf unterschiedliche Weise vereinheitlicht. Dieser Ansatz bezog sich ursprünglich auf NAS- und SAN-Protokolle. ONTAP ist dabei weiterhin eine der führenden Plattformen für SAN und bietet in Bezug auf NAS die ursprünglichen Stärken. Bei vSphere könnte dieser Ansatz auch für ein einheitliches System für Virtual Desktop Infrastructure (VDI) in Kombination mit einer virtuellen Serverinfrastruktur (VSI) stehen. Systeme mit ONTAP sind bei VSI in der Regel kostengünstiger als herkömmliche Enterprise-Arrays, bieten gleichzeitig aber fortschrittliche Storage-Effizienzfunktionen, mit denen Sie im selben System auch VDI gerecht werden können. ONTAP vereint außerdem eine Reihe von Storage-Medien – von SSDs bis SATA – und kann diese problemlos in die Cloud erweitern. Auf diese Weise müssen Sie nicht ein Flash-Array für Performance-Zwecke, ein SATA-Array für Archive und separate Systeme für die Cloud erwerben. Sie alle sind in ONTAP integriert.</block>
  <block id="bfcd75cb17a665212ae0889e787a7330" category="paragraph">Systeme mit ONTAP können die Storage QoS-Funktion nutzen, um den Durchsatz in Megabit pro Sekunde und/oder die Anzahl der I/O-Vorgänge pro Sekunde (IOPS) für unterschiedliche Storage-Objekte wie Dateien, LUNs, Volumes oder ganze SVMs zu beschränken.</block>
  <block id="5c60617d5730a7a614bfa0383a2affbd" category="admonition">Dies gilt nicht für VVols.</block>
  <block id="b78488659ce8aad3288a0ad7f27c84fb" category="admonition">VVols erfordert die Verwendung von ONTAP Tools für VMware vSphere, die als VASA Provider für ONTAP fungiert. Weitere Informationen zu VVols finden Sie unter <block ref="2486a36e844a24e9d1cf22351b1f1514" category="inline-link-macro-rx"></block>Best Practices.</block>
  <block id="58da89a03db394d8f73b6516f5a75882" category="paragraph">ONTAP QoS und VMware vSphere Storage I/O Control (SIOC) sind Technologien, die sich gegenseitig ergänzen und die vSphere und Storage-Administratoren gemeinsam nutzen können, um die Performance von vSphere VMs zu managen, die auf Systemen mit ONTAP ausgeführt werden. Wie in der folgenden Tabelle zu sehen ist, hat jedes Tool seine eigenen Stärken. Aufgrund des unterschiedlichen Umfangs von VMware vCenter und ONTAP kann es sein, dass einige Objekte von einem System erkannt und gemanagt werden können, vom anderen jedoch nicht.</block>
  <block id="bf0dee50832ba16a28008161612fd476" category="paragraph">ONTAP vereinheitlicht den Storage durch einen vereinfachten, softwaredefinierten Ansatz für sicheres und effizientes Management, verbesserte Performance und nahtlose Skalierbarkeit. Dieser Ansatz verbessert die Datensicherung und ermöglicht eine effektive Nutzung der Cloud-Ressourcen.</block>
  <block id="4021b205a984157ad20475f3b9a2555c" category="paragraph">Eine Storage Virtual Machine (SVM) ist die Einheit der sicheren Mandantenfähigkeit in ONTAP. Es handelt sich um ein logisches Konstrukt, das den Client-Zugriff auf Systeme mit ONTAP ermöglicht. SVMs können Daten gleichzeitig über mehrere Datenzugriffsprotokolle über logische Schnittstellen (Logical Interfaces, LIFs) bereitstellen. SVMs ermöglichen den Datenzugriff auf Dateiebene über NAS-Protokolle wie CIFS und NFS sowie den Datenzugriff auf Blockebene über SAN-Protokolle wie iSCSI, FC/FCoE und NVMe. SVMs können SAN- und NAS-Clients unabhängig gleichzeitig sowie mit S3 Daten bereitstellen.</block>
  <block id="a6848cf51eef6140971685faf5c7a111" category="paragraph">Bei vSphere könnte dieser Ansatz auch für ein einheitliches System für Virtual Desktop Infrastructure (VDI) in Kombination mit einer virtuellen Serverinfrastruktur (VSI) stehen. Systeme mit ONTAP sind bei VSI in der Regel kostengünstiger als herkömmliche Enterprise-Arrays, bieten gleichzeitig aber fortschrittliche Storage-Effizienzfunktionen, mit denen Sie im selben System auch VDI gerecht werden können. ONTAP vereint außerdem eine Reihe von Storage-Medien – von SSDs bis SATA – und kann diese problemlos in die Cloud erweitern. Auf diese Weise müssen Sie nicht ein Flash-Array für Performance-Zwecke, ein SATA-Array für Archive und separate Systeme für die Cloud erwerben. Sie alle sind in ONTAP integriert.</block>
  <block id="57ee1b10de81b96324fc044045bf301b" category="paragraph">Die ONTAP Tools für VMware vSphere sind eine Reihe von Tools, die ONTAP Storage zusammen mit vSphere verwenden. Das vCenter Plug-in, ehemals Virtual Storage Console (VSC), vereinfacht Storage-Management- und Effizienzfunktionen, verbessert die Verfügbarkeit und senkt die Storage-Kosten und den Betriebsaufwand – sei es bei SAN oder NAS. Dieses Plug-in nutzt Best Practices für die Bereitstellung von Datastores und optimiert ESXi Hosteinstellungen für NFS- und Block-Storage-Umgebungen. Wegen all dieser Vorteile empfiehlt NetApp, bei der Nutzung von vSphere mit Systemen mit ONTAP diese ONTAP Tools als Best Practice zu verwenden. Sie umfasst eine Server-Appliance, Erweiterungen der Benutzeroberfläche für vCenter, VASA Provider und Storage Replication Adapter. Nahezu alles in ONTAP Tools lässt sich mithilfe einfacher REST-APIs automatisieren – auch mit den meisten modernen Automatisierungs-Tools nutzbar.</block>
  <block id="cc3733ae6ca3afdf9111b89656a361ec" category="admonition">Nichts in diesem Dokument ersetzt<block ref="3a8301f60ad04eb873ae8dea15ee0495" category="inline-link-rx"></block></block>
  <block id="3a5159fa23e450460f03e5a885a6c51c" category="paragraph">*[HINWEIS]</block>
  <block id="551c27ef819331dd5874755df2de0717" category="list-text">Sie können einen ereignisbasierten Alarm erstellen, der ausgelöst wird, wenn eine virtuelle Maschine gegen eine VM-Host-Affinitätsregel verstößt. Fügen Sie im vSphere Client einen neuen Alarm für die virtuelle Maschine hinzu und wählen Sie als Ereignisauslöser „VM verletzt VM-Host Affinity Rule“ aus. Weitere Informationen zum Erstellen und Bearbeiten von Alarmen finden Sie in<block ref="d28eed239632aed40803fe0ed2311ba6" category="inline-link-rx"></block> der Dokumentation.</block>
  <block id="cf9d4e4ac552bd9a3e678245cbad762e" category="paragraph">VMSC-Lösungen werden sowohl mit NetApp® MetroCluster als auch mit SnapMirror Active Sync (ehemals SnapMirror Business Continuity oder SMBC) unterstützt und bieten erweiterte Business Continuity, wenn eine oder mehrere Ausfalldomänen insgesamt ausfallen. Die Widerstandsfähigkeit gegenüber verschiedenen Fehlermodi hängt davon ab, welche Konfigurationsoptionen Sie wählen.</block>
  <block id="c318fda3c53cc76f920bd8b8613c2f2a" category="paragraph">Die ONTAP-Architektur ist eine flexible und skalierbare Storage-Plattform, die SAN- (FCP, iSCSI und NVMe-of) und NAS-Services (NFS v3 und v4.1) für Datastores bietet. Die Storage-Systeme NetApp AFF, ASA und FAS nutzen das ONTAP Betriebssystem, um zusätzliche Protokolle für Gast-Storage-Zugriff wie S3 und SMB anzubieten.</block>
  <block id="38a3ee6a82a0cd6547497066c8ff59be" category="paragraph">NetApp SnapMirror Active Sync bietet granularen Schutz von Datenspeichern mit FCP- und iSCSI SAN-Protokollen, sodass Sie nur Workloads mit hoher Priorität selektiv schützen können. Er bietet symmetrischen aktiv/aktiv-Zugriff an beiden gespiegelten Standorten, während es sich bei NetApp MetroCluster um eine aktiv/Standby-Lösung handelt.</block>
  <block id="d235766de3bd77aa4983181862c135d7" category="admonition">Es gibt keine Änderung im MetroCluster Verhalten in diesem Szenario, und alle Datenspeicher sind weiterhin von ihren jeweiligen Seiten intakt.</block>
  <block id="14d627875de76f84985b8a3b27c96633" category="list-text">Während dieses Zeitraums gibt es keine Auswirkungen auf die I/O-Vorgänge der virtuellen Maschine, aber die Performance ist beeinträchtigt, da über ISL-Links auf die Daten vom Remote-Festplatten-Shelf aus zugegriffen wird.</block>
  <block id="290cbdcb6abb203d45786213f3c43662" category="admonition">Während dieses Zeitraums bleiben die virtuellen Maschinen aktiv, und es gibt keine Änderung im MetroCluster-Verhalten in diesem Szenario. Alle Datenspeicher bleiben von ihren jeweiligen Seiten intakt.</block>
  <block id="9a18dc3c2c9a217a632ca45b5b93a004" category="admonition">Der HA-Master startet die Neustartversuche nicht, bis der Platzierungsmanager den geeigneten Storage findet. Im Falle eines vollständigen Standortausfalls steht dies also nach der Umschaltung zur Verfügung.</block>
  <block id="e3f530e977d74053c6d70eb84886e756" category="sidebar">Epic</block>
  <block id="faeaec9eda6bc4c8cb6e1a9156a858be" category="sidebar">Gesteigerte</block>
  <block id="4289ad5c614f8037a8810e869b56facd" category="sidebar">Konsolidierung</block>
  <block id="bc967dc2d57e6eff184a821bf7577a80" category="sidebar">Skalierbarkeit</block>
  <block id="86a69bd2f501f95ed83bb7014fac7e30" category="sidebar">Snapshots und Klonen</block>
  <block id="58bc025e75c4b3fd1adf3ea672dd4424" category="sidebar">Epische Architektur und Design</block>
  <block id="92672a7a2b909945fbfa9f44f057c7a1" category="sidebar">Dimensionierung</block>
  <block id="41b36522e1e1020bad35b072e2f4caeb" category="sidebar">Storage-Anforderungen erfüllt</block>
  <block id="ddb04c3c1835c48b34dced1c453450e9" category="sidebar">Konfiguration und Best Practices</block>
  <block id="3e23b54348b2b8424e24a426570dc9f2" category="sidebar">Host Utilities</block>
  <block id="6e19e43ad6dccde81e8d288a0c9ce380" category="sidebar">LUN- und Volume-Konfiguration</block>
  <block id="a2ddb4431dcf2a898d386c1f3b82c926" category="sidebar">Fileservices</block>
  <block id="9985b4390c40137573e6da05caf85874" category="sidebar">Protokolle</block>
  <block id="894445e8ea6df545bfbc4247797a2162" category="sidebar">Storage-Dimensionierung</block>
  <block id="051c025eb3cc7303bf3116bffdb7e95d" category="sidebar">Architektur von MetroCluster</block>
  <block id="a1930436e8ea74adaf54ff76d868829b" category="sidebar">SQL Server mit MetroCluster</block>
  <block id="33673f825b847c6610f7c7115da0bbde" category="sidebar">Bevorzugter Standort</block>
  <block id="57848b20be1bd330229089d2e136d982" category="sidebar">Netzwerktopologie</block>
  <block id="3a1c15502275494d5a57fe7f225fa951" category="sidebar">SQL Server mit SM-AS</block>
  <block id="e30da740afd1eb5c7fc5ade391ee694d" category="sidebar">Oracle Konfigurationen</block>
  <block id="3d9396f3fc3010d09439c0c007a897aa" category="sidebar">Einzelne Oracle Instanz</block>
  <block id="87e1ed5a71c4eb8ff653400f9dd917a0" category="sidebar">Oracle RAC und Tiebreakers</block>
  <block id="4a1483b5d5deb83e67e3184b05202d31" category="sidebar">Beispielarchitektur</block>
  <block id="8b7786259860e64596ef2df3349d25e1" category="sidebar">RAC-Verbindungsfehler</block>
  <block id="d65b736af70982866f4f95287faab7ac" category="sidebar">SnapMirror-Kommunikationsfehler</block>
  <block id="9b7da52d3b31b3d0a034473609a069e5" category="sidebar">Totalausfall der Netzwerk-Verbindung</block>
  <block id="1163a74136e40a05bc320e61f567a4df" category="sidebar">Standortausfall</block>
  <block id="29a8ec19326654cdf89bb8516725720a" category="sidebar">Fehler beim Mediator</block>
  <block id="38ea1deaf691187fc38ab915566310c5" category="sidebar">Servicewiederherstellung nach Fehler</block>
  <block id="9132b772b1437e64c8acac2be79a7a0f" category="sidebar">Manuelle Failover</block>
  <block id="05b66cacce7105ad4a548ef5054997f0" category="doc">Annäherungseinstellungen</block>
  <block id="eccafd14420b568fff639755b251d1bd" category="paragraph">Proximity bezieht sich auf eine Clusterkonfiguration, die angibt, dass eine bestimmte Host-WWN- oder iSCSI-Initiator-ID zu einem lokalen Host gehört. Dies ist ein zweiter optionaler Schritt für die Konfiguration des LUN-Zugriffs.</block>
  <block id="d99ea4fad7d0a9010283969d9ddf89aa" category="paragraph">Der erste Schritt ist die übliche igroup-Konfiguration. Jede LUN muss einer Initiatorgruppe zugeordnet werden, die die WWN/iSCSI-IDs der Hosts enthält, die Zugriff auf diese LUN benötigen. Dadurch wird gesteuert, welcher Host _Access_ zu einer LUN hat.</block>
  <block id="bba32ce1a7b85dd9ba50c49b1a53fe4c" category="paragraph">Der zweite, optionale Schritt ist die Konfiguration der Host-Nähe. Dies kontrolliert nicht den Zugriff, es steuert _Priority_.</block>
  <block id="98271aec6138a28e374c8f3724a535b6" category="paragraph">Beispielsweise kann ein Host an Standort A für den Zugriff auf eine LUN konfiguriert werden, die durch SnapMirror Active Sync geschützt ist. Da das SAN über Standorte erweitert wird, stehen diesem LUN Pfade über Storage an Standort A oder Storage an Standort B zur Verfügung</block>
  <block id="81959299f52ddb567fd902f3e1ae5311" category="paragraph">Ohne Annäherungseinstellungen verwendet der Host beide Speichersysteme gleichmäßig, da beide Speichersysteme aktive/optimierte Pfade anbieten. Wenn die SAN-Latenz und/oder Bandbreite zwischen Standorten begrenzt ist, ist dies möglicherweise nicht erwünscht, und Sie sollten sicherstellen, dass während des normalen Betriebs jeder Host bevorzugt Pfade zum lokalen Speichersystem verwendet. Diese Konfiguration erfolgt durch Hinzufügen der Host-WWN/iSCSI-ID zum lokalen Cluster als proximaler Host. Dies kann unter der CLI oder Systemmanager ausgeführt werden.</block>
  <block id="f08670cf73640d447fb34d01b0f6418f" category="paragraph">Bei einem AFF System werden die Pfade nach dem Konfigurieren von Host-Nähe wie unten dargestellt angezeigt.</block>
  <block id="85a2564342597a065a3926228b7e9fcd" category="paragraph">SQL Server kann so konfiguriert werden, dass er auf verschiedene Weise mit SnapMirror Active Sync arbeitet. Die richtige Antwort hängt von der verfügbaren Netzwerkkonnektivität, den RPO-Anforderungen und den Verfügbarkeitsanforderungen ab.</block>
  <block id="d60999416cfb11e4fa51f3dbe7386d64" category="paragraph">SM-AS und MetroCluster sind in der Gesamtfunktionalität ähnlich, es gibt jedoch wichtige Unterschiede in der Art und Weise, wie die RPO=0-Replikation implementiert und gemanagt wird. Die asynchronen und synchronen SnapMirror können auch im Rahmen eines DR-Plans eingesetzt werden, sind aber nicht als Technologien für die HA-Replizierung konzipiert.</block>
  <block id="e122442a5a5728cd45ffc4ddd170a36f" category="list-text">Eine MetroCluster-Konfiguration ähnelt eher einem integrierten Cluster mit über mehrere Standorte verteilten Nodes. SM-AS verhält sich wie zwei ansonsten unabhängige Cluster, die zusammenarbeiten, um ausgewählte RPO=0 synchron replizierte LUNs bereitzustellen.</block>
  <block id="dbbbd9519f58f000ae618fd3a3f2f38c" category="list-text">Die Daten in einer MetroCluster-Konfiguration sind zu einem bestimmten Zeitpunkt nur von einem bestimmten Standort aus zugänglich. Eine zweite Kopie der Daten befindet sich am gegenüberliegenden Standort, die Daten sind jedoch passiv. Ohne Failover des Speichersystems ist der Zugriff nicht möglich.</block>
  <block id="72d2ba33caf4a678d0b02c7a4a562662" category="list-text">MetroCluster und SM-As führen die Spiegelung auf verschiedenen Ebenen durch. Die MetroCluster Spiegelung wird auf der RAID-Schicht durchgeführt. Die Low-Level-Daten werden mithilfe von SyncMirror in einem gespiegelten Format gespeichert. Die Verwendung der Spiegelung ist in den LUN-, Volume- und Protokollebenen praktisch unsichtbar.</block>
  <block id="33613d29368b0b312d33bd204505f12b" category="list-text">Im Gegensatz dazu erfolgt die SM-AS-Spiegelung auf der Protokollebene. Die beiden Cluster sind insgesamt unabhängige Cluster. Sobald die beiden Datenkopien synchron sind, müssen die beiden Cluster nur noch Schreibvorgänge spiegeln. Wenn ein Schreibvorgang auf einem Cluster stattfindet, wird er in das andere Cluster repliziert. Der Schreibvorgang wird dem Host nur dann bestätigt, wenn der Schreibvorgang auf beiden Seiten abgeschlossen ist. Anders als dieses Verhalten bei der Protokollaufteilung sind die beiden Cluster ansonsten normale ONTAP-Cluster.</block>
  <block id="c6ce2d48c3feb0ac105233b1d47d6785" category="list-text">Die Hauptrolle bei MetroCluster ist die umfangreiche Replizierung. Sie können ein gesamtes Array mit RPO=0 und RTO von nahezu null replizieren. Dies vereinfacht den Failover-Prozess, da es nur eine „Sache“ für Failover gibt und lässt sich hinsichtlich Kapazität und IOPS extrem gut skalieren.</block>
  <block id="22c8f7d4250a7117dcec5e795cf01979" category="list-text">Ein wichtiger Anwendungsfall für SM-AS ist die granulare Replizierung. Manchmal möchten Sie nicht alle Daten als eine Einheit replizieren oder bestimmte Workloads selektiv ausfallsicher durchführen.</block>
  <block id="466f5e6d5af96a45aba533d742d1f30e" category="list-text">Ein weiterer wichtiger Anwendungsfall für SM-As ist der aktiv/aktiv-Betrieb. Dort sollen vollständig nutzbare Datenkopien auf zwei verschiedenen Clustern verfügbar sein, die sich an zwei verschiedenen Standorten mit identischen Performance-Merkmalen befinden und auf Wunsch nicht über Standorte verteilt werden müssen. Sie können Ihre Applikationen bereits auf beiden Standorten ausführen, wodurch sich die RTO während eines Failover verringert.</block>
  <block id="3ab2196c18fe1cd72bf252059597387b" category="section-title">NetApp MetroCluster und SnapMirror Active Sync</block>
  <block id="95da439be4f858e05c58cde4b5946324" category="paragraph">Für viele Kunden benötigt DR mehr als nur einen Remote-Besitz von Daten, sondern muss in der Lage sein, diese Daten schnell zu nutzen. NetApp bietet zwei Technologien zur Erfüllung dieser Anforderungen: MetroCluster und SnapMirror Active Sync</block>
  <block id="3e6b18011054436086bb6cf813b0cb23" category="section-title">Vergleich von SM-AS und MCC</block>
  <block id="4ab72a69aa303ab33d610a6f6e594e68" category="section-title">Prüfsummen</block>
  <block id="0a4a3e3ef29b5fc4ea4e557f166532b4" category="paragraph">Die SnapMirror Active Sync (SM-AS) basiert auf SnapMirror Synchronous. Mit MetroCluster ist jeder ONTAP Controller für die Replizierung seiner Laufwerksdaten an einen Remote-Standort verantwortlich. Bei SnapMirror Active Sync haben Sie im Grunde zwei verschiedene ONTAP-Systeme, die unabhängige Kopien Ihrer LUN-Daten führen, aber zusammenarbeiten, um eine einzige Instanz dieser LUN zu präsentieren. Auf Host-Ebene handelt es sich um eine einzelne LUN-Einheit.</block>
  <block id="2f589372957b08ce09879b8a2ab6e58c" category="section-title">Aktivieren von dNFS</block>
  <block id="1a89d30b5bb0e2de334e029001a92f8a" category="paragraph">Oracle dNFS kann mit NFSv3 ohne Konfiguration arbeiten, die über die Aktivierung der dNFS Library hinaus erforderlich ist (siehe Oracle Dokumentation für den spezifischen Befehl erforderlich), aber wenn dNFS keine Verbindung herstellen kann, kann es im Hintergrund zurück zum Kernel NFS Client zurückkehren. In einem solchen Fall kann die Performance erheblich beeinträchtigt werden.</block>
  <block id="9cb0e5a02b9facf3c0e654f08ceceda4" category="paragraph">Wenn Sie dNFS-Multiplexing über mehrere Schnittstellen, mit NFSv4.X, oder Verschlüsselung verwenden möchten, müssen Sie eine oranfstab-Datei konfigurieren. Die Syntax ist extrem streng. Kleine Fehler in der Datei können dazu führen, dass der Start hängend oder umgangen die oranfstab-Datei.</block>
  <block id="3c04631b254b4b0e971a2c82517f50cb" category="paragraph">Der einzige Weg, um sicher zu sein, dNFS funktioniert wie erwartet, ist die Abfrage der V-dnfs-Tabellen.</block>
  <block id="21986e459426718105f7dc1a16e88bd0" category="paragraph">Unten finden Sie ein Beispiel für eine Oranfstab-Datei unter /etc. Dies ist einer von mehreren Speicherorten, an denen eine oranfstab-Datei platziert werden kann.</block>
  <block id="1d4338d2de6e933a6941c8605def3a39" category="paragraph">Im ersten Schritt wird überprüft, ob dNFS für die angegebenen Dateisysteme betriebsbereit ist:</block>
  <block id="75bc8bd82bedf61e91351f8667b97e13" category="paragraph">Diese Ausgabe zeigt an, dass dNFS mit diesen beiden Dateisystemen verwendet wird, aber es bedeutet *Not*, dass oranfstab betriebsbereit ist. Wenn ein Fehler aufgetreten ist, hätte dNFS die NFS-Dateisysteme des Hosts automatisch erkannt und Sie können immer noch die gleiche Ausgabe von diesem Befehl sehen.</block>
  <block id="f4327e73856b2069e10b045605d3fbe9" category="paragraph">Multipathing kann wie folgt überprüft werden:</block>
  <block id="99dc2d8bc11eb27fb0b920ca969e43e1" category="paragraph">Das sind die Verbindungen, die dNFS verwendet. Für jeden SVRNAME-Eintrag sind zwei Pfade und Kanäle sichtbar. Das bedeutet, dass Multipathing funktioniert, was bedeutet, dass die oranfstab-Datei erkannt und verarbeitet wurde.</block>
  <block id="d99de907aa06fa04c7b19ffceff68307" category="admonition">Die folgenden Abschnitte sind aktuell ab ONTAP 9.15.1, aber das Lease- und Sperrverhalten sowie Tuning-Optionen können von Version zu Version wechseln. Wenn Sie NFSv4-Leasing/Lock-Timeouts einstellen müssen, konsultieren Sie bitte den NetApp-Support für die neuesten Informationen.</block>
  <block id="4bf95f1ea9ecf4f42a7b323970405bb4" category="paragraph">Diese Kulanzzeit steuert die Rückgewinnung von Leasing-Verträgen während Änderungen an der Netzwerkschnittstelle, aber es gibt eine zweite Kulanzzeit, die die Rückgewinnung während des Speicher-Failovers steuert<block ref="d75aeb1a8759ea7475fe0ddcd2058ca1" prefix=" " category="inline-code"></block>. Hierbei handelt es sich um eine Option auf Node-Ebene.</block>
  <block id="0ba7c8a7759adb519c715ac93fdee2a6" category="paragraph">Wenn Sie beispielsweise häufig LIF-Failovers durchführen mussten und die Gnadenfrist reduzieren mussten, würden Sie ändern<block ref="b1280b7c6d986647dff33b87ce276327" prefix=" " category="inline-code"></block>. Wenn Sie die IO Wiederaufnahme Zeit während des Controller-Failovers verbessern wollten, müssten Sie ändern<block ref="d75aeb1a8759ea7475fe0ddcd2058ca1" prefix=" " category="inline-code"></block>.</block>
  <block id="3f3edac1c3ed7bfad53f9d42b9a0c5a7" category="paragraph">Ändern Sie diese Werte nur mit Vorsicht und nach vollständiger Kenntnis der Risiken und Konsequenzen. Die I/O-Pausen, die mit Failover- und Migrationsvorgängen mit NFSv4.X verbunden sind, können nicht vollständig vermieden werden. Sperrfristen, Lease- und Kulanzfristen sind Teil der NFS RFC. Für viele Kunden ist NFSv3 vorzuziehen, da Failover-Zeiten schneller sind.</block>
  <block id="167503c08e9285f5c4a4a61a0123897b" category="paragraph">Die Kulanzzeit und die Leasingdauer sind miteinander verknüpft. Wie bereits erwähnt, beträgt das standardmäßige Leasingzeitlimit 30 Sekunden, was bedeutet, dass NFSv4-Clients mindestens alle 30 Sekunden beim Server einchecken müssen, oder sie verlieren ihre Leasingverhältnisse und damit ihre Sperren. Die Kulanzzeit ist vorhanden, um einem NFS-Server zu ermöglichen, Lease/Lock-Daten neu zu erstellen, und es ist standardmäßig 45 Sekunden. Die Kulanzzeit muss länger als die Leasingfrist sein. Dadurch wird sichergestellt, dass eine NFS-Client-Umgebung, die zur Verlängerung von Leasingverträgen mindestens alle 30 Sekunden entwickelt wurde, nach einem Neustart beim Server einchecken kann. Eine Nachfrist von 45 Sekunden sorgt dafür, dass alle Kunden, die erwarten, ihre Mietverträge mindestens alle 30 Sekunden auf jeden Fall die Möglichkeit haben, dies zu tun.</block>
  <block id="6be050126f2df5ae7cd944ce446241c2" category="paragraph">Wenn ein Timeout von 30 Sekunden nicht akzeptabel ist, können Sie die Leasingdauer verlängern.</block>
  <block id="38d530634a8d32e8775a37109fc08542" category="paragraph">Wenn Sie das Lease-Timeout auf 60 Sekunden erhöhen möchten, um einem Netzwerkausfall von 60 Sekunden standzuhalten, müssen Sie auch die Kulanzzeit verlängern. Das bedeutet, dass Sie längere I/O-Pausen während Controller-Failover erleben.</block>
  <block id="ac76821692fdb2dae1d98babe804fb8f" category="paragraph">Das sollte normalerweise kein Problem sein. In der Regel aktualisieren ONTAP Controller nur ein oder zwei Mal pro Jahr, und ein ungeplanter Failover aufgrund von Hardwareausfällen ist äußerst selten. Darüber hinaus würden Sie bei einem Netzwerk, wo ein Netzwerkausfall von 60 Sekunden zu besorgen war und Sie eine Leasingzeit von 60 Sekunden benötigen, wahrscheinlich auch keinem seltenen Storage-System-Failover widersprechen, was zu einer Pause von 61 Sekunden führt. Sie haben bereits bestätigt, dass Sie ein Netzwerk haben, das ziemlich häufig über 60 Sekunden anhält.</block>
  <block id="66bb028b9d7030985c1ce3bb4f5a27e9" category="searchtitle">= Microsoft SQL Server CPU-Konfiguration</block>
  <block id="73f7848a74f8a1b6c78ac0dfbca44874" category="searchtitle">Microsoft SQL Server Datensicherung mit NetApp Managementsoftware</block>
  <block id="fa67fb4719e92db117cccfc693cf2a40" category="searchtitle">Icrosoft SQL Server tempdb-Dateien</block>
  <block id="3a80e1e29c96d0837b12aef65785a41a" category="doc">Konfigurationsparameter</block>
  <block id="4bb9f72180425717a416a5037a100cca" category="searchtitle">MySQL und InnoDB Dateistruktur</block>
  <block id="9a41c04f55c43c1bfac6d1e3fb0992de" category="doc">NFSv3-Steckplatztabellen</block>
  <block id="dce12836eeccf070f53590dc5d86633d" category="doc">Blockgrößen</block>
  <block id="b4aa7c0127688b4c623685cfeed4ab97" category="searchtitle">Timeouts für Oracle Real Application Clusters</block>
  <block id="6fae8d228f324e0e0a9c817b746b9bd0" category="doc">RAC-Timeouts</block>
  <block id="b64e3a0dc81f710f886a442e1a397bb9" category="doc">Tools für Datenbankmanagement und Automatisierung</block>
  <block id="87921e5ee2cf87589874871f0d361694" category="doc">Datenbankverfügbarkeit</block>
  <block id="accd3900cbd3e4d55a38c8518052015c" category="doc">Prüfsummen und Datenintegrität</block>
  <block id="5611150a7a2a706d09d267b466f083d6" category="paragraph">Mit einem echten Storage-Array wird die Datenintegrität durch die Verwendung von Prüfsummen auf mehreren Ebenen gesichert. Wenn Daten in einem IP-basierten Netzwerk beschädigt sind, weist die TCP-Schicht (Transmission Control Protocol) die Paketdaten zurück und fordert eine erneute Übertragung an. Das FC-Protokoll umfasst Prüfsummen sowie eingekapselte SCSI-Daten. Nachdem es sich auf dem Array befindet, verfügt ONTAP über RAID- und Prüfsummenschutz. Es kann zu einer Beschädigung kommen, aber wie in den meisten Enterprise-Arrays wird sie erkannt und korrigiert. In der Regel fällt ein ganzes Laufwerk aus, was zu einer RAID-Neuerstellung führt, und die Datenbankintegrität bleibt davon unberührt. Es ist immer noch möglich, dass einzelne Bytes auf einem Laufwerk durch kosmische Strahlung oder fehlerhafte Blitzzellen beschädigt werden. In diesem Fall würde die Paritätsprüfung fehlschlagen, das Laufwerk würde ausfallen und eine RAID-Wiederherstellung würde beginnen. Auch hier bleibt die Datenintegrität erhalten. Die letzte Verteidigungslinie ist die Verwendung von Prüfsummen. Wenn zum Beispiel ein katastrophaler Firmware-Fehler auf einem Laufwerk Daten in einer Weise beschädigt, die irgendwie nicht durch eine RAID-Paritätsprüfung erkannt wurde, würde die Prüfsumme nicht übereinstimmen und ONTAP würde die Übertragung eines beschädigten Blocks verhindern, bevor die Oracle Datenbank den Block empfangen konnte.</block>
  <block id="cd985f59320d95026288fa5ff6dcf58a" category="doc">Online-Backups</block>
  <block id="9c030078e0f444e80ad1c24e215220a4" category="doc">Datensicherung mit ONTAP</block>
  <block id="6bb9a5aec98309d1414b32f7e0751415" category="doc">Planung der Datensicherheit</block>
  <block id="06db8225d3ec2b43755ddc9358302450" category="doc">RTO, RPO und SLA-Planung</block>
  <block id="5d827e5d4c92779310224878dc408813" category="summary">Oracle Database DR mit MetroCluster</block>
  <block id="f0b223afb696335287bc2ed9501b5b33" category="doc">Disaster Recovery mit MetroCluster</block>
  <block id="67be2475945283126e261087dcc833b7" category="doc">MetroCluster und NVFAIL</block>
  <block id="13262509535baae73d0cc7ab06de1fc6" category="searchtitle">Oracle Extended RAC auf MetroCluster</block>
  <block id="7993038617c805a663d50702c4494b68" category="doc">Oracle Extended RAC</block>
  <block id="75b233b87417347976ef489e09961c77" category="doc">Oracle Single Instance</block>
  <block id="aa51324dbf5137dc80d29f127e7265f1" category="doc">Oracle Single Instance</block>
  <block id="254742fbeffe659f98f802792b541c45" category="doc">RAC Tiebreaker</block>
  <block id="aeee9c92b22d6f18b8670c9f330ea7c9" category="doc">Totaler Fehler bei der Netzwerkverbindung</block>
  <block id="0ef819e6ae8a74f9c8ac0158a24b1627" category="doc">Servicewiederherstellung</block>
  <block id="8367f9ef79397d7d8f1367cb4392c602" category="summary">Aktive Synchronisierung von Oracle und SnapMirror – Standortausfall</block>
  <block id="5c8784e46ac32f6b1c9b78f954c1a473" category="summary">SnapMirror Active Sync – SnapMirror-Kommunikationsfehler</block>
  <block id="7ed82538ab967cd6e2949aa971158dc9" category="searchtitle">Oracle, SnapMirror Active Sync und ONTAP Mediator</block>
  <block id="db2170f38b52c13c31e0572c709b6f8a" category="searchtitle">Überblick über die aktive Synchronisierung von Oracle und SnapMirror</block>
  <block id="e9a673980d8df30ed4173e4b7c4ecd2c" category="searchtitle">Bevorzugter Standort für Oracle und SnapMirror Active Sync</block>
  <block id="45d97b0c2ca342ad882153a1245f9897" category="doc">Bevorzugter Standort für SnapMirror Active Sync</block>
  <block id="0bb4536e921f4d96dd701c1afe746457" category="doc">ASMLib/AFD (ASM-Filtertreiber)</block>
  <block id="3ee9e6a595247d1965be0d0c6cd0f22e" category="doc">Migration von Datendateien</block>
  <block id="a9761df551af398a0fc2974288502d8a" category="doc">Host-Daten werden kopiert</block>
  <block id="d437e7d8b6513f91bbd58e0b1e08c90b" category="doc">Migrationsplanung</block>
  <block id="4c7e0d612a5f0e09f272083633991979" category="doc">TCP/IP- und ethernet-Konfiguration</block>
  <block id="9f2dc5f476d250d879dc58a583f93fd1" category="searchtitle">FC SAN-Konfiguration für Oracle Datenbanken</block>
  <block id="08f606415b4ad7fb864651eac04fcd8e" category="searchtitle">Optimierung und Benchmarking der Oracle Datenbank-Performance</block>
  <block id="b30e3ef968d39d109aea4a92750c9792" category="doc">Veraltete NFSv3-Sperren</block>
  <block id="1d19e21de0f93b11f5ceb8b535a3afc2" category="doc">Kapazitätsmanagement</block>
  <block id="8d409dbefa8e76403d7adf4cbd315d3d" category="doc">ONTAP Failover/Switchover</block>
  <block id="0f03a4e25b68a1a67d3be8f0d989ef8b" category="doc">Performance-Management mit ONTAP QoS</block>
  <block id="555472a0fed2f80e0848845e538a08af" category="doc">Oracle Direct NFS (dNFS)</block>
  <block id="92d0288d9e0c6ed5ffd8dcd2e0900101" category="paragraph">Zum Zeitpunkt der Erstellung dieses Berichts funktioniert dNFS-Multipathing nicht mit NFSv4.1 und aktuellen Versionen von Oracle Database. Eine oranfstab-Datei, die NFSv4.1 als Protokoll angibt, kann nur eine Single Path-Anweisung für einen bestimmten Export verwenden. Der Grund dafür ist, dass ONTAP Client ID Trunking nicht unterstützt. Oracle Database Patches zur Behebung dieser Einschränkung sind möglicherweise in Zukunft verfügbar.</block>
  <block id="3fdf18c0210b51691df97c8be74021a3" category="doc">ASM Reclamation Utility (ASMRU)</block>
  <block id="d63f66214ae30c5b9b10653f1e503492" category="doc">LUN-Ausrichtung</block>
  <block id="38baeef7dc11ad9438aa5b211745d9f6" category="searchtitle">LUN-Größe der Oracle-Datenbank und LVM-Anpassung</block>
  <block id="f447f8c9180e11bc4219ace93e7431fb" category="doc">LUN-Größe und LVM-Größe</block>
  <block id="c8d869d733bd5299001f9d90b9f00d90" category="doc">LUN-Dimensionierung und LUN-Anzahl</block>
  <block id="4ee97146242ed12473141d7ecad00ee1" category="doc">Backup-Tiering</block>
  <block id="e7cba09a05ca3e6ccdd30a1aa0a7d3af" category="doc">Tiering von Archivprotokollen</block>
  <block id="c1affdf2bd2e606557f2bf12c69af8f8" category="doc">Tiering von partiellen Dateien</block>
  <block id="3bf6f5a455b1889775d79713ad4e19ed" category="doc">Abrufrichtlinien</block>
  <block id="636d94f9e0fb52c5d7560661b059b203" category="doc">Snapshot Tiering</block>
  <block id="bf5f70c9080724ef27b09203705b1abc" category="doc">Vollständiges Datei-Tiering</block>
  <block id="acf5662e9defb6f8df644e0d8b4caa28" category="doc">Initialisierungsparameter</block>
  <block id="03095e209912bbf8e40f883c88befe49" category="searchtitle">Native PostgreSQL Datensicherung</block>
  <block id="300e5ea4a0c7d02b8a4efeba5a1f6faf" category="doc">Nativer Ddta-Schutz</block>
  <block id="1c3ee6977dcad0709502549dd7b111f3" category="sidebar">Oracle DR mit MetroCluster</block>
</blocks>